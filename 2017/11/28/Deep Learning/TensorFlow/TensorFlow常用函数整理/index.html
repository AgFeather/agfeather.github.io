<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,TensorFlow," />










<meta name="description" content="TensorFlow常用方法整理">
<meta name="keywords" content="Deep Learning,TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow常用函数整理">
<meta property="og:url" content="http://yoursite.com/2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/index.html">
<meta property="og:site_name" content="Black Feather">
<meta property="og:description" content="TensorFlow常用方法整理">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-04-10T08:20:29.813Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow常用函数整理">
<meta name="twitter:description" content="TensorFlow常用方法整理">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/"/>





  <title>TensorFlow常用函数整理 | Black Feather</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Black Feather</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dongfang Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Black Feather">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow常用函数整理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-28T00:00:00+09:00">
                2017-11-28
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>TensorFlow常用方法整理</p>
<a id="more"></a>
<h2 id="变量相关"><a href="#变量相关" class="headerlink" title="变量相关"></a>变量相关</h2><p>涉及到新建变量有两个函数，一个是tf.Variable(),一个是tf.get_variable()， 两个函数的签名如下：</p>
<ul>
<li>tf.Variable(initial_value=None, …)</li>
<li>tf.get_variable(name, shape=None, initializer=None, …)<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3></li>
</ul>
<ol>
<li>使用tf.Variable()时，如果有命名冲突，系统会自动处理，get_variable()则会报错<br>Variable()会自动修改冲突的名称：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(3, name=&apos;w1&apos;)</span><br><span class="line">w2 = tf.Variable(1, name=&apos;w1&apos;)</span><br><span class="line">print(w1.name)</span><br><span class="line">print(w2.name)</span><br><span class="line"></span><br><span class="line">w1:0</span><br><span class="line">w1_1:0</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>get_variable()报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.get_variable(name=&apos;w1&apos;, initializer=1)</span><br><span class="line">w2 = tf.get_variable(name=&apos;w1&apos;, initializer=2)</span><br><span class="line"></span><br><span class="line">#ValueError: Variable w_1 already exists, disallowed. Did</span><br><span class="line">#you mean to set reuse=True in VarScope?</span><br></pre></td></tr></table></figure></p>
<p>基于这两个函数的特性，<strong>当我们需要共享变量的时候，需要使用tf.get_variable()</strong>。在其他情况下，这两个的用法是一样的.</p>
<h3 id="变量共享"><a href="#变量共享" class="headerlink" title="变量共享"></a>变量共享</h3><p>通过设置reuse=True可以实现变量共享：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&apos;scope1&apos;):</span><br><span class="line">    w1 = tf.get_variable(&apos;w1&apos;, shape=[])</span><br><span class="line">    w2 = tf.Variable(0.0, name=&apos;w2&apos;)</span><br><span class="line">with tf.variable_scope(&apos;scope1&apos;, reuse=True):</span><br><span class="line">    w1_p = tf.get_variable(&apos;w1&apos;, shape=[])</span><br><span class="line">    w2_p = tf.Variable(0.0, name=&apos;w2&apos;)</span><br><span class="line"></span><br><span class="line">print(w1 is w1_p)  # True</span><br><span class="line">print(w2 is w2_p)  # False</span><br></pre></td></tr></table></figure></p>
<h2 id="tensor形状相关操作"><a href="#tensor形状相关操作" class="headerlink" title="tensor形状相关操作"></a>tensor形状相关操作</h2><h3 id="tf-shape-input"><a href="#tf-shape-input" class="headerlink" title="tf.shape(input)"></a>tf.shape(input)</h3><p>返回输入tensor的形状</p>
<h3 id="tf-size-input"><a href="#tf-size-input" class="headerlink" title="tf.size(input)"></a>tf.size(input)</h3><p>返回输入tensor中所有元素的个数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]</span><br><span class="line">tf.size(t) ==&gt; 12</span><br></pre></td></tr></table></figure></p>
<h3 id="tf-rank-input"><a href="#tf-rank-input" class="headerlink" title="tf.rank(input)"></a>tf.rank(input)</h3><p>返回输入tensor的维度数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]</span><br><span class="line"># shape of tensor &apos;t&apos; is [2, 2, 3]</span><br><span class="line">tf.rank(t) ==&gt; 3</span><br></pre></td></tr></table></figure></p>
<h3 id="tf-reshape-input-shape"><a href="#tf-reshape-input-shape" class="headerlink" title="tf.reshape(input, shape)"></a>tf.reshape(input, shape)</h3><p>改变输入tensor的形状为shape，shape是一个list，表示每个维度的大小。可以让某个维度的值为-1，tf会自动计算该维度大小。</p>
<h3 id="tf-expand-dims-input-axis-None"><a href="#tf-expand-dims-input-axis-None" class="headerlink" title="tf.expand_dims(input, axis=None)"></a>tf.expand_dims(input, axis=None)</h3><p>在第axis位置上增加一个维度。<br>如果要将批量维度添加到单个元素，则此操作非常有用。 例如，如果有一个单一形状的图片：image=[height，width，channels]，可以使用expand_dims(image, 0)使其成为一批图像（目前仅有一个），这将使形状 images=[1，height，width，channels]。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line"># &apos;t&apos; is a tensor of shape [2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 0)) ===&gt; [1,2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 1)) ===&gt; [2,1,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 2)) ===&gt; [2,3,1,5]</span><br></pre></td></tr></table></figure></p>
<h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze()"></a>tf.squeeze()</h3><p>从tensor中删除所有大小是1的维度<br>给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸为1的维度，可以通过指定squeeze_dims来删除特定尺寸为1的维度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is a tensor of shape [1, 3, 1, 4]</span><br><span class="line">tf.shape(tf.squeeze(t)) ===&gt; [3, 4]</span><br></pre></td></tr></table></figure></p>
<h3 id="tf-split-value-name-or-size-split-axis-0"><a href="#tf-split-value-name-or-size-split-axis-0" class="headerlink" title="tf.split(value, name_or_size_split, axis=0)"></a>tf.split(value, name_or_size_split, axis=0)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度不变</p>
<ul>
<li>value：输入的tensor</li>
<li>num_or_size_split：如果是整数你， 就将tensor分割成n个子tensor。如果是个tensor T，就将输入的tensor分割为len(T)个子tensor</li>
<li>axis：按照指定维度分割，指向的维度的大小一定能被num_or_size_split整除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.split(A, num_or_size_split=3, axis=1)</span><br><span class="line">a1 = [ [[1],[4]],   [[2],[5]],   [[3],[6]] ]</span><br><span class="line">a2 = tf.split(A, num_or_size_split=2, axis=0)</span><br><span class="line">a2 = [ [[1,2,3]],  [[4,5,6]] ]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="tf-unstack-value-num-axis"><a href="#tf-unstack-value-num-axis" class="headerlink" title="tf.unstack(value, num, axis)"></a>tf.unstack(value, num, axis)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度-1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.unstack(A, num=3, axis=1)</span><br><span class="line">a1 = [ [1, 4],   [2, 5],   [3, 6] ]</span><br><span class="line">a2 = tf.unstack(A, num=2, axis=0)</span><br><span class="line">a2 = [ [1,2,3],  [4,5,6] ]</span><br></pre></td></tr></table></figure></p>
<h2 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h2><p>TensorFlow提供三种卷积操作，分别对应着1维，2维，3维数据。</p>
<h3 id="tf-nn-conv1d"><a href="#tf-nn-conv1d" class="headerlink" title="tf.nn.conv1d()"></a>tf.nn.conv1d()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv1d(</span><br><span class="line">    value,</span><br><span class="line">    filters,</span><br><span class="line">    stride,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=None,</span><br><span class="line">    data_format=None,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>常见的卷积输入都为2维，但特殊情况下也会输入1维tensor。和2维处理原理相同，只是形状需要改变。<br>其中：</p>
<ol>
<li>input为输入tensor，形状为[batch, in_width, in_channels]</li>
<li>filter为卷积核，形状为[filter_width, in_channels, output_channels]</li>
<li>strides为步长，形状为一个整数</li>
<li>padding为对input的padding方法， 有两个取值：’VALID’和’SAME’，前者表示不做0填充，后者表示对input进行0填充使得卷积的输出大小和输入相同。</li>
</ol>
<p>实际上，一维卷积方法在运行时，会把数据增加一维，然后使用conv2d计算。</p>
<h3 id="tf-nn-conv2d"><a href="#tf-nn-conv2d" class="headerlink" title="tf.nn.conv2d()"></a>tf.nn.conv2d()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=True,</span><br><span class="line">    data_format=&apos;NHWC&apos;,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ol>
<li>input为输入tensor，形状为[batch, n_height, in_width, in_channels]</li>
<li>filter为卷积核，形状为[filter_height, filter_width, in_channels, output_channels]</li>
<li>strides为步长，形状为[1, step_dim1, step_dim2, 1]</li>
<li>padding为对input的padding方法， 有两个取值：’VALID’和’SAME’，前者表示不做0填充，后者表示对input进行0填充使得卷积的输出大小和输入相同。<h3 id="tf-nn-conv3d"><a href="#tf-nn-conv3d" class="headerlink" title="tf.nn.conv3d()"></a>tf.nn.conv3d()</h3>和商量个相同，只是把维度提高到了三维</li>
<li>input形状为：[batch, in_depth, in_height, in_width, in_channels]</li>
<li>filter形状为：[filter_depth, filter_height, filter_width, in_channels, output_channels]<h3 id="tf-pad"><a href="#tf-pad" class="headerlink" title="tf.pad()"></a>tf.pad()</h3>tf.pad(tensor, paddings, mode)<br>对一个tensor根据指定的padding进行填充，padding是一个shape=[n,2]的int型张量。n表示的是输入tensor的rank。For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension.<br>model有三个模式：</li>
<li>‘CONSTANT’ 表示用指定值填充，通常为0</li>
<li>‘REFLECT’ 表示映射填充。</li>
<li>‘SYMMETRIC’ 表示是对称填充<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tf = tf.constant([[1,2,3],[4,5,6]])</span><br><span class="line">paddings = tf.constant([[1,1],[2,2]])</span><br><span class="line"># &apos;constant_values&apos; is 0.</span><br><span class="line"># rank of &apos;t&apos; is 2.</span><br><span class="line">tf.pad(t, paddings, &quot;CONSTANT&quot;)  # [[0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">                                 #  [0, 0, 1, 2, 3, 0, 0],</span><br><span class="line">                                 #  [0, 0, 4, 5, 6, 0, 0],</span><br><span class="line">                                 #  [0, 0, 0, 0, 0, 0, 0]]</span><br><span class="line"></span><br><span class="line">tf.pad(t, paddings, &quot;REFLECT&quot;)  # [[6, 5, 4, 5, 6, 5, 4],</span><br><span class="line">                                #  [3, 2, 1, 2, 3, 2, 1],</span><br><span class="line">                                #  [6, 5, 4, 5, 6, 5, 4],</span><br><span class="line">                                #  [3, 2, 1, 2, 3, 2, 1]]</span><br><span class="line"></span><br><span class="line">tf.pad(t, paddings, &quot;SYMMETRIC&quot;)  # [[2, 1, 1, 2, 3, 3, 2],</span><br><span class="line">                                  #  [2, 1, 1, 2, 3, 3, 2],</span><br><span class="line">                                  #  [5, 4, 4, 5, 6, 6, 5],</span><br><span class="line">                                  #  [5, 4, 4, 5, 6, 6, 5]]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>从第一个例子可以看出，在维度D=1，上下两端都padding了一个行，正好等于paddings[0,0]和paddings[0,1]。同理，在维度D=2，左右两端padding大小等于paddings[1,0]和paddings[1,1]</p>
<h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><h3 id="tf-control-dependencies-和tf-identity"><a href="#tf-control-dependencies-和tf-identity" class="headerlink" title="tf.control_dependencies()和tf.identity()"></a>tf.control_dependencies()和tf.identity()</h3><p>该函数是用来设计控制计算流图的，给图中的某些计算指定顺序，总结起来就是：在执行某些op，tensor之前，某些op，tensor需要首先被执行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([a, b, c]):</span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br><span class="line">    # d and e will only run after a, b, c have executed.</span><br></pre></td></tr></table></figure></p>
<p>对于tf.identity()的功能，首先看如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(1.0)</span><br><span class="line">y = tf.Variable(0.0)</span><br><span class="line"></span><br><span class="line">#返回一个op，表示给变量x加1的操作</span><br><span class="line">x_plus_1 = tf.assign_add(x, 1)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i in xrange(5):</span><br><span class="line">        print(y.eval())#按照我们的预期，由于control_dependencies的作用，所以应该执行print前都会先执行x_plus_1，但是这种情况会出问题</span><br></pre></td></tr></table></figure></p>
<p>这个打印的是1，1，1，1，1 。可以看到，没有达到我们预期的效果，y只被赋值了一次。<br>而如果想完成这个目的，需要将代码做如下改动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(1.0)</span><br><span class="line">y = tf.Variable(0.0)</span><br><span class="line">x_plus_1 = tf.assign_add(x, 1)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)#修改部分</span><br><span class="line"></span><br><span class="line">with tf.Session() as session:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i in xrange(5):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure></p>
<p>这时候打印的是2，3，4，5，6</p>
<p>解释：对于control_dependencies这个管理器，只有当里面的操作是一个op时，才会生效，也就是先执行传入的参数op，再执行里面的op。而y=x仅仅是tensor的一个简单赋值，不是定义的op，所以在图中不会形成一个节点，这样该管理器就失效了。tf.identity是返回一个一模一样新的tensor的op，这会增加一个新节点到gragh中，这时control_dependencies就会生效，所以第二种情况的输出符合预期。</p>
<h3 id="tf-assign-x-new-number"><a href="#tf-assign-x-new-number" class="headerlink" title="tf.assign(x, new_number)"></a>tf.assign(x, new_number)</h3><p>这个函数的功能主要是把A的值变为new_number</p>
<h2 id="TensorFlow优化器方法和梯度裁剪"><a href="#TensorFlow优化器方法和梯度裁剪" class="headerlink" title="TensorFlow优化器方法和梯度裁剪"></a>TensorFlow优化器方法和梯度裁剪</h2><h3 id="tf-train-AdamOptimizer"><a href="#tf-train-AdamOptimizer" class="headerlink" title="tf.train.AdamOptimizer()"></a>tf.train.AdamOptimizer()</h3><h4 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h4><ul>
<li>compute_gradients(loss)<br>  根据传入的loss计算梯度，返回值是一个list，list中的元素是(gradient, variable)对，表示对每个变量的更新梯度</li>
<li>apply_gradients(grads_and_vars,global_step=None)<br>  参数grads_and_vars是一个(gradient,variable)的list。（compute_gradients）的返回值<br>  对每个variable根据其gradient进行更新。<br>  返回一个operation，表示运行该apply_gradients</li>
<li>minimize(loss) 同时运行compute_gradients() 和 apply_gradients()</li>
</ul>
<p>通常情况下，直接使用minimize()函数即可，但当需要对梯度进行操作时（梯度裁剪），就需要分别运行两个步骤。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">gradients = optimizer.compute_gradients(loss)</span><br><span class="line">clipped_gradients = []</span><br><span class="line">for grad, var in gradients:</span><br><span class="line">    if grad is not None:</span><br><span class="line">        grad = tf.clip_by_value(grad, -1., 1.)</span><br><span class="line">        clipped_gradients.append((grad, var))</span><br><span class="line">train_op = optimizer.apply_gradients(clipped_gradients)</span><br></pre></td></tr></table></figure></p>
<h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><p>在TensorFlow中，提供如下几种梯度裁剪方法：tf.clip_by_norm, tf.clip_by_global_norm, tf.clip_by_average_norm, tf.clip_by_value.</p>
<ul>
<li>tf.clip_by_norm(tensor, clip_norm)<br>如果输入tensor的l2范式值大于clip_norm，则tensor = tensor * clip_norm / l2(tensor)</li>
<li>tf.clip_by_global_norm(t_list, clip_norm)<br>对t_list中的各个梯度tensor进行裁剪，t_list[i] <em> clip_norm / max(global_norm, clip_norm)<br>并且：global_norm = sqrt(sum([l2norm(t)\</em>*2 for t in t_list]))</li>
<li>tf.clip_by_value(t, clip_value_min, clip_value_max)<br>最直接的裁剪方法，直接裁剪输入tensor的最大值和最小值。</li>
</ul>
<h2 id="tf中的Batch-Normalization"><a href="#tf中的Batch-Normalization" class="headerlink" title="tf中的Batch Normalization"></a>tf中的Batch Normalization</h2><ul>
<li><p>tf.nn.moments(tensor, axis) 返回两个参数，分别是均值和方差。<br>输入tensor的shape为[batch_size, height, width, kernels]，第二个参数axis是一个int数组，表示要进行计算的维度。</p>
</li>
<li><p>tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def batch_norm(tensor):</span><br><span class="line">    epsilon = 1e-3</span><br><span class="line">    axis = list(range(len(tensor.get_shape()) - 1))</span><br><span class="line">    batch_mean, batch_var = tf.nn.moments(tensor, axis)</span><br><span class="line">    BN_tensor = tf.nn.batch_normalization(tensor, batch_mean, batch_var, offset=None, scale=None, variance_epsilon=epsilon)</span><br><span class="line">    return BN_tensor</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="TensorFlow加载部分模型"><a href="#TensorFlow加载部分模型" class="headerlink" title="TensorFlow加载部分模型"></a>TensorFlow加载部分模型</h3><p>常规保存和加载模型的方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, &apos;model.ckpt&apos;)  # 保存</span><br><span class="line">saver.restore(sess, &apos;model.ckpt&apos;)  # 加载</span><br></pre></td></tr></table></figure></p>
<p>前面的描述相当于是保存了所有的参数，然后加载所有的参数。但是目前的情况有所变化了，不能加载所有的参数，最后一层的参数不一样了，需要随机初始化。<br>首先对每一层添加name scope，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">with name_scope(&apos;conv1&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;conv2&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;fc1&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;output&apos;):</span><br><span class="line">    xxx</span><br></pre></td></tr></table></figure></p>
<p>然后根据变量的名字，选择加载哪些变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#得到该网络中，所有可以加载的参数</span><br><span class="line">variables = tf.contrib.framework.get_variables_to_restore()</span><br><span class="line">#删除output层中的参数</span><br><span class="line">variables_to_resotre = [v for v in varialbes if v.name.split(&apos;/&apos;)[0]!=&apos;output&apos;]</span><br><span class="line">#构建这部分参数的saver</span><br><span class="line">saver = tf.train.Saver(variables_to_restore)</span><br><span class="line">saver.restore(sess,&apos;model.ckpt&apos;)</span><br></pre></td></tr></table></figure></p>
<h3 id="TensorFlow梯度更新"><a href="#TensorFlow梯度更新" class="headerlink" title="TensorFlow梯度更新"></a>TensorFlow梯度更新</h3><ol>
<li><p>根据name_scope选择部分参数进行梯度更新：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&apos;to_train&apos;):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">with tf.name_scope(&apos;not_train&apos;):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">def optimizer():</span><br><span class="line">    train_vars = tf.trainable_variables()</span><br><span class="line">    trainable_vars = [var for var in train_vars if var.name.startswith(&apos;to_train&apos;)]</span><br><span class="line">    untrainable_vars = [var for var in train_vars if var.name.startswith(&apos;not_train&apos;)]</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用tf.gradients(ys, xs)计算梯度：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.get_variable(&apos;w1&apos;, shape=[3])</span><br><span class="line">w2 = tf.get_variable(&apos;w2&apos;, shape=[3])</span><br><span class="line"></span><br><span class="line">w3 = tf.get_variable(&apos;w3&apos;, shape=[3])</span><br><span class="line">w4 = tf.get_variable(&apos;w4&apos;, shape=[3])</span><br><span class="line"></span><br><span class="line">z1 = 2*w1 + 3*w2+ 4*w3</span><br><span class="line">z2 = 6*w3 + 7*w4</span><br><span class="line"></span><br><span class="line">grads1 = tf.gradients([z1], [w1, w2, w3])</span><br><span class="line">grads2 = tf.gradients([z1, z2], [w1, w2, w3, w4])</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    print(sess.run(grads1))</span><br><span class="line">    print(sess.run(grads2))</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用tf.stop_gradient()阻挡节点的BP梯度更新：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(2.0)</span><br><span class="line">w2 = tf.Variable(2.0)</span><br><span class="line"></span><br><span class="line">a = tf.multiply(w1, 3.0)</span><br><span class="line">a_stoped = tf.stop_gradient(a)</span><br><span class="line"></span><br><span class="line"># b=w1*3.0*w2</span><br><span class="line">b = tf.multiply(a_stoped, w2)</span><br><span class="line">gradients = tf.gradients(b, xs=[w1, w2])</span><br><span class="line">print(gradients)</span><br><span class="line">#输出</span><br><span class="line">#[None, &lt;tf.Tensor &apos;gradients/Mul_1_grad/Reshape_1:0&apos; shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>可见，一个节点被 stop之后，这个节点上的梯度，就无法再向前BP了。由于w1变量的梯度只能来自a节点，所以，计算梯度返回的是None。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(1.0)</span><br><span class="line">b = tf.Variable(1.0)</span><br><span class="line"></span><br><span class="line">c = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">c_stoped = tf.stop_gradient(c)</span><br><span class="line"></span><br><span class="line">d = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">e = tf.add(c_stoped, d)</span><br><span class="line"></span><br><span class="line">gradients = tf.gradients(e, xs=[a, b])</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    print(sess.run(gradients))</span><br><span class="line">#输出 [1.0, 1.0]</span><br></pre></td></tr></table></figure></p>
<p>虽然 c节点被stop了，但是a，b还有从d传回的梯度，所以还是可以输出梯度值的。</p>
<h3 id="TensorFlow-top-k相关函数用法"><a href="#TensorFlow-top-k相关函数用法" class="headerlink" title="TensorFlow top_k相关函数用法"></a>TensorFlow top_k相关函数用法</h3><h4 id="tf-nn-in-top-k"><a href="#tf-nn-in-top-k" class="headerlink" title="tf.nn.in_top_k()"></a>tf.nn.in_top_k()</h4><p>在训练过程中输出top k的预测准确率，返回的是布尔值，即label是否在预测的top k中存在。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(16).reshape((4,4)).astype(np.float32)</span><br><span class="line">a = tf.convert_to_tensor(a)</span><br><span class="line">label = [0,1,2,3]</span><br><span class="line">pre = tf.nn.in_top_k(a, label, k=2)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(pre))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[False False  True  True]</span><br></pre></td></tr></table></figure></p>
<h4 id="tf-nn-top-k"><a href="#tf-nn-top-k" class="headerlink" title="tf.nn.top_k()"></a>tf.nn.top_k()</h4><p>根据输入的tensor返回top k个概率最大的预测对应的index和possibility。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([0.1, 0.2, 0.3, 0.4])</span><br><span class="line">a = tf.convert_to_tensor(a)</span><br><span class="line">b = tf.nn.top_k(a, k=3)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">poss, index = sess.run(b)</span><br><span class="line">print(poss, index)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; [0.4 0.3 0.2] [3 2 1]</span><br></pre></td></tr></table></figure></p>
<h4 id="tensorflow-collection"><a href="#tensorflow-collection" class="headerlink" title="tensorflow collection"></a>tensorflow collection</h4><p>tensorflow的collection提供一个全局的存储机制，不会受到变量名生存空间的影响。一处保存，到处可取。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant(1.0)</span><br><span class="line">l1 = tf.nn.l2_loss(x)</span><br><span class="line">y = tf.constant([1.0, 2.0])</span><br><span class="line">l2 = tf.nn.l2_loss(y)</span><br><span class="line"></span><br><span class="line"># 手动指定一个collection并将loss添加进去</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, l1)</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, l2)</span><br><span class="line"></span><br><span class="line"># 可以将同一个变量放到不同的collection中</span><br><span class="line">tf.add_to_collection(&apos;other&apos;, l1)</span><br><span class="line"></span><br><span class="line"># 获取指定的collection中的元素，是一个tensor类型的list</span><br><span class="line">losses = tf.get_collection(&apos;losses&apos;)</span><br><span class="line">final_loss = tf.add_n(losses)</span><br><span class="line">other = tf.get_collection(&apos;other&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(final_loss))</span><br><span class="line">print(sess.run(other))</span><br></pre></td></tr></table></figure></p>
<h4 id="TensorFlow矩阵拼接和分解"><a href="#TensorFlow矩阵拼接和分解" class="headerlink" title="TensorFlow矩阵拼接和分解"></a>TensorFlow矩阵拼接和分解</h4><p>tf.stack() 用于矩阵拼接，tf.unstack()用于矩阵分解<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([1, 2, 3])</span><br><span class="line">b = tf.constant([4 ,5, 6])</span><br><span class="line">c = tf.stack([a ,b], axis=0)</span><br><span class="line">d = tf.unstack(c, axis=0)</span><br><span class="line">e = tf.unstack(c, axis=1)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">    print(sess.run(d))</span><br><span class="line">    print(sess.run(e))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[[1 2 3]</span><br><span class="line">     [4 5 6]]</span><br><span class="line">&gt;&gt;&gt;[array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]</span><br><span class="line">&gt;&gt;&gt;[array([1, 4], dtype=int32), array([2, 5], dtype=int32), array([3, 6], dtype=int32)]</span><br></pre></td></tr></table></figure></p>
<h3 id="tensorflow-布尔掩码"><a href="#tensorflow-布尔掩码" class="headerlink" title="tensorflow 布尔掩码"></a>tensorflow 布尔掩码</h3><h4 id="tf-boolean-mask-tensor-mark"><a href="#tf-boolean-mask-tensor-mark" class="headerlink" title="tf.boolean_mask(tensor, mark)"></a>tf.boolean_mask(tensor, mark)</h4><p>根据mark的真值对tensor进行掩码操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">＃1-D 示例</span><br><span class="line">tensor =  [ 0 ， 1 ， 2 ， 3 ]</span><br><span class="line">mask = np.array（[True，False，True，False] ）</span><br><span class="line">boolean_mask （tensor，mask） == &gt;  [ 0 ， 2 ]</span><br><span class="line"></span><br><span class="line">＃2-D示例</span><br><span class="line">tensor =  [ [ 1 ， 2 ] ， [ 3 ， 4 ] ， [ 5 ， 6 ] ]</span><br><span class="line">mask = np.array（[True，False，True] ）</span><br><span class="line">boolean_mask （tensor，mask） == &gt;  [ [ 1 ， 2 ] ， [ 5 ， 6 ] ]</span><br></pre></td></tr></table></figure></p>
<h4 id="tf-sequence-mask-lengths-maxlen"><a href="#tf-sequence-mask-lengths-maxlen" class="headerlink" title="tf.sequence_mask(lengths, maxlen)"></a>tf.sequence_mask(lengths, maxlen)</h4><p>返回一个表示每个单元的前N个位置的mask张量。</p>
<p>如果lengths的形状为[d_1, d_2, …, d_n]，由此产生的张量mask有dtype类型和形状[d_1, d_2, …, d_n, maxlen]</p>
<p>第二个参数maxlen可以为None，这时将自动用length list中最大长度作为maxlen<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],</span><br><span class="line">                                #  [True, True, True, False, False],</span><br><span class="line">                                #  [True, True, False, False, False]]</span><br><span class="line"></span><br><span class="line">tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],</span><br><span class="line">                                  #   [True, True, True]],</span><br><span class="line">                                  #  [[True, True, False],</span><br><span class="line">                                  #   [False, False, False]]]</span><br></pre></td></tr></table></figure></p>
<h3 id="TensorFlow-graph"><a href="#TensorFlow-graph" class="headerlink" title="TensorFlow graph"></a>TensorFlow graph</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">class Graph():</span><br><span class="line">    def __init__(self, x, y):</span><br><span class="line">        # tf.reset_default_graph()</span><br><span class="line">        self.a = tf.constant(x)</span><br><span class="line">        self.b = tf.constant(y)</span><br><span class="line">        self.c = tf.multiply(self.a, self.b)</span><br><span class="line"></span><br><span class="line">class DoubleGraph():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        gg = tf.Graph()</span><br><span class="line">        with gg.as_default():</span><br><span class="line">            g1 = Graph(1, 2)</span><br><span class="line">        ggg = tf.Graph()</span><br><span class="line">        with ggg.as_default():</span><br><span class="line">            g2 = Graph(3, 4)</span><br><span class="line">        print(g1.a.graph)</span><br><span class="line">        print(g2.a.graph)</span><br><span class="line">        session1 = tf.Session(graph=gg)</span><br><span class="line">        r = session1.run(g1.c)</span><br><span class="line">        print(r)</span><br><span class="line">        session2 = tf.Session(graph=ggg)</span><br><span class="line">        r = session2.run(g2.c)</span><br><span class="line">        print(r)</span><br><span class="line"></span><br><span class="line">DoubleGraph()</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Dongfang Li WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Dongfang Li Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
          
            <a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/12/CUDA/CUDA笔记（五）/" rel="next" title="CUDA 学习笔记（五）">
                <i class="fa fa-chevron-left"></i> CUDA 学习笔记（五）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/04/Project/Kaggle解题步骤/" rel="prev" title="Kaggle解题步骤">
                Kaggle解题步骤 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dongfang Li</p>
              <p class="site-description motion-element" itemprop="description">My blog about programming and machine learning</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">96</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/YHfeather" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/dongfang.li.790" target="_blank" title="FB Page">
                      
                        <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#变量相关"><span class="nav-number">1.</span> <span class="nav-text">变量相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#区别"><span class="nav-number">1.1.</span> <span class="nav-text">区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量共享"><span class="nav-number">1.2.</span> <span class="nav-text">变量共享</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor形状相关操作"><span class="nav-number">2.</span> <span class="nav-text">tensor形状相关操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-shape-input"><span class="nav-number">2.1.</span> <span class="nav-text">tf.shape(input)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-size-input"><span class="nav-number">2.2.</span> <span class="nav-text">tf.size(input)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-rank-input"><span class="nav-number">2.3.</span> <span class="nav-text">tf.rank(input)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-reshape-input-shape"><span class="nav-number">2.4.</span> <span class="nav-text">tf.reshape(input, shape)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-expand-dims-input-axis-None"><span class="nav-number">2.5.</span> <span class="nav-text">tf.expand_dims(input, axis=None)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-squeeze"><span class="nav-number">2.6.</span> <span class="nav-text">tf.squeeze()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-split-value-name-or-size-split-axis-0"><span class="nav-number">2.7.</span> <span class="nav-text">tf.split(value, name_or_size_split, axis=0)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-unstack-value-num-axis"><span class="nav-number">2.8.</span> <span class="nav-text">tf.unstack(value, num, axis)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积操作"><span class="nav-number">3.</span> <span class="nav-text">卷积操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-nn-conv1d"><span class="nav-number">3.1.</span> <span class="nav-text">tf.nn.conv1d()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-nn-conv2d"><span class="nav-number">3.2.</span> <span class="nav-text">tf.nn.conv2d()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-nn-conv3d"><span class="nav-number">3.3.</span> <span class="nav-text">tf.nn.conv3d()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-pad"><span class="nav-number">3.4.</span> <span class="nav-text">tf.pad()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#流程控制"><span class="nav-number">4.</span> <span class="nav-text">流程控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-control-dependencies-和tf-identity"><span class="nav-number">4.1.</span> <span class="nav-text">tf.control_dependencies()和tf.identity()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-assign-x-new-number"><span class="nav-number">4.2.</span> <span class="nav-text">tf.assign(x, new_number)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow优化器方法和梯度裁剪"><span class="nav-number">5.</span> <span class="nav-text">TensorFlow优化器方法和梯度裁剪</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-train-AdamOptimizer"><span class="nav-number">5.1.</span> <span class="nav-text">tf.train.AdamOptimizer()</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#内置函数"><span class="nav-number">5.1.1.</span> <span class="nav-text">内置函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度裁剪"><span class="nav-number">5.2.</span> <span class="nav-text">梯度裁剪</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf中的Batch-Normalization"><span class="nav-number">6.</span> <span class="nav-text">tf中的Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow加载部分模型"><span class="nav-number">6.1.</span> <span class="nav-text">TensorFlow加载部分模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow梯度更新"><span class="nav-number">6.2.</span> <span class="nav-text">TensorFlow梯度更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-top-k相关函数用法"><span class="nav-number">6.3.</span> <span class="nav-text">TensorFlow top_k相关函数用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-in-top-k"><span class="nav-number">6.3.1.</span> <span class="nav-text">tf.nn.in_top_k()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-top-k"><span class="nav-number">6.3.2.</span> <span class="nav-text">tf.nn.top_k()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensorflow-collection"><span class="nav-number">6.3.3.</span> <span class="nav-text">tensorflow collection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow矩阵拼接和分解"><span class="nav-number">6.3.4.</span> <span class="nav-text">TensorFlow矩阵拼接和分解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow-布尔掩码"><span class="nav-number">6.4.</span> <span class="nav-text">tensorflow 布尔掩码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-boolean-mask-tensor-mark"><span class="nav-number">6.4.1.</span> <span class="nav-text">tf.boolean_mask(tensor, mark)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-sequence-mask-lengths-maxlen"><span class="nav-number">6.4.2.</span> <span class="nav-text">tf.sequence_mask(lengths, maxlen)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-graph"><span class="nav-number">6.5.</span> <span class="nav-text">TensorFlow graph</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongfang Li</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://blackfeather.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/';
          this.page.identifier = '2017/11/28/Deep Learning/TensorFlow/TensorFlow常用函数整理/';
          this.page.title = 'TensorFlow常用函数整理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://blackfeather.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
