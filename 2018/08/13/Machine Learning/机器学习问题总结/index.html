<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="62.梯度下降法找到的一定是下降最快的方向么？梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。">
<meta name="keywords" content="Machine Learning, Deep Learning, TensorFlow, Python">
<meta property="og:type" content="article">
<meta property="og:title" content="Black Feather">
<meta property="og:url" content="http://yoursite.com/2018/08/13/Machine Learning/机器学习问题总结/index.html">
<meta property="og:site_name" content="Black Feather">
<meta property="og:description" content="62.梯度下降法找到的一定是下降最快的方向么？梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-09-15T05:51:30.738Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Black Feather">
<meta name="twitter:description" content="62.梯度下降法找到的一定是下降最快的方向么？梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/13/Machine Learning/机器学习问题总结/"/>





  <title> | Black Feather</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Black Feather</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/13/Machine Learning/机器学习问题总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dongfang Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Black Feather">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-13T21:46:01+09:00">
                2018-08-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/13/Machine Learning/机器学习问题总结/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/13/Machine Learning/机器学习问题总结/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>62.梯度下降法找到的一定是下降最快的方向么？<br>梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。</p>
<p>63.牛顿法和梯度下降法有什么不同<br>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。<br>关于牛顿法和梯度下降法的效率对比：<br>a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。<br>b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p>
<p>64.什么是拟牛顿法（Quasi-Newton Methods）<br>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。<br>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p>68.什么最小二乘法？<br>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：<br>使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。</p>
<p>75.简单介绍下logistics回归<br>Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。</p>
<p>86.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是(C)<br>A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小<br>B、在经主分量分解后,协方差矩阵成为对角矩阵<br>C、主分量分析就是K-L变换<br>D、主分量是通过求协方差矩阵的特征值得到<br>@BlackEyes_SGC：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。</p>
<p>92.影响基本K-均值算法的主要因素有(ABD）。<br>A.样本输入顺序；<br>B.模式相似性测度；<br>C.聚类准则；<br>D.初始类中心的选取</p>
<p>93.在统计模式分类问题中，当先验概率未知时，可以使用（BD）。<br>A. 最小损失准则；<br>B. 最小最大损失准则；<br>C. 最小误判概率准则；<br>D. N-P判决</p>
<p>94.如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（BC）。<br>A. 已知类别样本质量； B. 分类准则； C. 特征选取； D. 量纲</p>
<p>95.欧式距离具有（A B ）；马式距离具有（A B C D ）。<br>A. 平移不变性； B. 旋转不变性； C. 尺度缩放不变性； D. 不受量纲影响的特性</p>
<p>111.随机森林如何处理缺失值 方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。<br>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。</p>
<p>112.随机森林如何评估特征重要性 衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：<br>1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。<br>2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。</p>
<p>113.优化Kmeans 使用kd树或者ball tree<br>将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。</p>
<p>114.KMeans初始类簇中心点的选取 k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。</p>
<ol>
<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li>
<li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li>
<li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li>
<li>重复2和3直到k个聚类中心被选出来</li>
<li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ol>
<p>116.如何进行特征选择？ 特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解<br>常见的特征选择方式：</p>
<ol>
<li>去除方差较小的特征</li>
<li>正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。</li>
<li>随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。</li>
<li>稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</li>
</ol>
<p>133.SVD和PCA<br>PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。</p>
<p>134.数据不平衡问题<br>采样，对小样本加噪声采样，对大样本进行下采样<br>进行特殊的加权，如在Adaboost中或者SVM中<br>采用对不平衡数据集不敏感的算法<br>改变评价标准：用AUC/ROC来进行评价<br>采用Bagging/Boosting/ensemble等方法<br>考虑数据的先验分布</p>
<p>Bagging与Boosting的区别：<br>取样方式不同。<br>Bagging采用均匀取样，而Boosting根据错误率取样。<br>Bagging的各个预测函数没有权重，而Boosting是有权重的。<br>Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。</p>
<p>191.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m&lt;n&lt;p&lt;q，以下计算顺序效率最高的是（）<br>A.(AB)C<br>B.AC(B)<br>C.A(BC)<br>D.所以效率都相同<br>正确答案：A<br>首先，根据简单的矩阵知识，因为 A<em>B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项，<br>然后，再看 A 、 C 选项。在 A 选项中，m∗n 的矩阵 A 和n∗p的矩阵 B 的乘积，得到 m∗p的矩阵 A</em>B ，而 A∗B的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 m∗n∗p次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 m∗p∗q次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 m∗n∗p+m∗p∗q 。同理分析， C 选项 A (BC) 需要的乘法次数是 n∗p∗q+m∗n∗q。<br>由于m∗n∗p&lt;m∗n∗q，m∗p∗q&lt;n∗p∗q，显然 A 运算次数更少，故选 A 。</p>
<p>200.特征比数据量还大时，选择什么样的分类器？<br>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。</p>
<p>186.关于 logit 回归和 SVM 不正确的是（）<br>A.Logit回归目标函数是最小化后验概率<br>B. Logit回归可以用于预测事件发生概率的大小<br>C. SVM目标是结构风险最小化<br>D.SVM可以有效避免模型过拟合<br>正确答案： A<br>A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误   B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确    C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。    D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。</p>
<p>227.什么是共线性, 跟过拟合有什么关联?<br>共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。<br>共线性会造成冗余，导致过拟合。<br>解决方法：排除变量的相关性／加入权重正则。</p>
<p>230.机器学习中，有哪些特征选择的工程方法？</p>
<ol>
<li>计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；</li>
<li>构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；</li>
<li>通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；</li>
<li>训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；</li>
<li>通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。</li>
<li>通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。</li>
</ol>
<p>221.带核的SVM为什么能分类非线性问题？<br>核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积</p>
<p>222.常用核函数及核函数的条件：<br>核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。<br>RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。<br>线性核：主要用于线性可分的情况<br>多项式核</p>
<p>223.Boosting和Bagging<br>（1）随机森林<br>　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：<br>1）Boostrap从袋内有放回的抽取样本值<br>2）每次随机抽取一定数量的特征（通常为sqr(n)）。<br>　　分类问题：采用Bagging投票的方式选择类别频次最高的<br>　　回归问题：直接取每颗树结果的平均值。<br>常见参数误差分析优点缺点<br>1、树最大深度<br>2、树的个数<br>3、节点上的最小样本数<br>4、特征数(sqr(n))oob(out-of-bag)<br>将各个树的未采样样本作为预测样本统计误差作为误分率可以并行计算<br>不需要特征选择<br>可以总结出特征重要性<br>可以处理缺失数据<br>不需要额外设计测试集在回归上不能输出连续结果<br>（2）Boosting之AdaBoost<br>　　Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。<br>（3）Boosting之GBDT<br>　　将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。<br>　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。<br>（4）Xgboost<br>这个工具主要有以下几个特点：<br>支持线性分类器<br>可以自定义损失函数，并且可以用二阶偏导<br>加入了正则化项：叶节点数、每个叶节点输出score的L2-norm<br>支持特征抽样<br>在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。</p>
<p>1点到平面距离公式推导</p>
<p>235.对于k折交叉验证, 以下对k的说法正确的是 :<br>A. k越大, 不一定越好, 选择大的k会加大评估时间<br>B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)<br>C. 在选择k时, 要最小化数据集之间的方差<br>D. 以上所有<br>答案：D<br>k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.</p>
<p>255.对于PCA说法正确的是 :</p>
<ol>
<li>我们必须在使用PCA前规范化数据</li>
<li>我们应该选择使得模型有最大variance的主成分</li>
<li>我们应该选择使得模型有最小variance的主成分</li>
<li>我们可以使用PCA在低维度上做数据可视化<br>答案:  1, 2 and 4<br>1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).<br>2）我们总是应该选择使得模型有最大variance的主成分<br>3）有时在低维度上左图是需要PCA的降维帮助的</li>
</ol>
<h3 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h3><p>文章给的定义是： 数据集的标签是不可靠的，如（x，y），y对于x的标记是不可靠的。<br>这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。<br>在实际应用中的学习问题往往以混合形式出现,如多标记多示例、半监督多标记、弱标记多标记等。针对监督信息不完整或不明确对象的学习问题统称为弱监督学习<br>弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或是多个元素的<br>分别表示x没有标记，有一个标记，和有多个标记。在此统一声明为一个标记的集合。<br>在实际的使用中多标记的使用是很常见的，在图像文本语音中是很容易找到多个标记的。<br>举个图像中的例子<br>一般机器学习算法，每一个训练样本都需要类别标号（对于二分类：1/-1）。实际上那样的数据其实已经经过了抽象，实际的数据要获得这样的标号还是很难，图像就是个典型。还有就是数据标记的工作量太大，我们想偷懒了，所以多只是给了正负样本集。负样本集里面的样本都是负的，但是正样本里面的样本不一定都是正的，但是至少有一个样本是正的。比如检测人的问题，一张天空的照片就可以是一个负样本集；一张某某自拍照就是一个正样本集（你可以在N个区域取N个样本，但是只有部分是有人的正样本）。这样正样本的类别就很不明确，传统的方法就没法训练。<br>疑问来了，图像的不是有标注吗？有标注就应该有类别标号啊?这是因为图片是人标的，数据量特大，难免会有些标的不够好,这就是所谓的弱监督集（weakly supervised set）。所以如果算法能够自动找出最优的位置，那分类器不就更精确吗？ 标注位置不是很准确，这个例子不是很明显，还记得前面讲过的子模型的位置吗？比如自行车的车轮的位置，是完全没有位置标注的，只知道在bounding box区域附件有一个车轮。不知道精确位置，就没法提取样本。这种情况下，车轮会有很多个可能的位置，也就会形成一个正样本集，但里面只有部分是包含轮子的。</p>
<p>270.对于维度极低的特征，选择线性还是非线性分类器？ 非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。</p>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</li>
</ol>
<p>237.模型的高bias是什么意思, 我们如何降低它 ?<br>A. 在特征空间中减少特征<br>B. 在特征空间中增加特征<br>C. 增加数据点<br>D. B和C<br>E. 以上所有<br>答案: B<br>bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !</p>
<p>296.一般，k-NN最近邻方法在（ A）的情况下效果较好。<br>A．样本较多但典型性不好<br>B．样本呈团状分布<br>C．样本较少但典型性好<br>D．样本呈链状分布</p>
<p>297.下列哪些方法可以用来对高维数据进行降维（A B C D E F）<br>A LASSO<br>B 主成分分析法<br>C 聚类分析<br>D 小波分析法<br>E 线性判别法<br>F 拉普拉斯特征映射<br>解析：lasso通过参数缩减达到降维的目的；<br>pca就不用说了<br>线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；<br>小波分析有一些变换的操作降低其他干扰可以看做是降维<br>拉普拉斯请看这个<a href="http://f.dataguru.cn/thread-287243-1-1.html" target="_blank" rel="noopener">http://f.dataguru.cn/thread-287243-1-1.html</a></p>
<p>299.以下说法中正确的是（C）<br>A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性<br>B 在adaboost算法中，所有被分错样本的权重更新比例相同<br>C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重<br>D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少</p>
<p>294.机器学习中做特征选择时，可能用到的方法有？ （E）<br>A、卡方<br>B、信息增益<br>C、平均互信息<br>D、期望交叉熵<br>E 以上都有</p>
<p>281.在 k-均值算法中，以下哪个选项可用于获得全局最小？<br>A. 尝试为不同的质心（centroid）初始化运行算法<br>B. 调整迭代的次数<br>C. 找到集群的最佳数量<br>D. 以上所有<br>答案（D）：所有都可以用来调试以找到全局最小。</p>
<p>284.下面哪个选项中哪一项属于确定性算法？<br>A.PCA<br>B.K-Means<br>C. 以上都不是<br>答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。</p>
<p>285.特征向量的归一化方法有哪些？<br>线性函数转换，表达式如下：<br>y=(x-MinValue)/(MaxValue-MinValue)<br>对数函数转换，表达式如下：<br>y=log10 (x)<br>反余切函数转换 ，表达式如下：<br>y=arctan(x)*2/PI<br>减去均值，除以方差：<br>y=(x-means)/ variance</p>
<p>331.在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？<br>A.具有1百万项300个特征的数据集<br>B.具有100000项310个特征的数据集<br>C.具有10,000项8个特征的数据集<br>D.具有10,000项200个特征的数据集<br>解答：（C）<br>t-SNE具有二次时空复杂度。</p>
<p>331题、哪些机器学习算法不需要做归一化处理？<br>概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。<br>我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。<br>至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。</p>
<p>332题、对于树形结构为什么不需要归一化？<br>数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。<br>另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。</p>
<p>333题、在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别<br>欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为：<br>欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。<br>曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：<br>要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。<br>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。<br>曼哈顿距离和欧式距离一般用途不同，无相互替代性。</p>
<p>334题、数据归一化（或者标准化，注意归一化和标准化不同）的原因<br>要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。<br>有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。<br>有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。<br>补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。</p>
<h4 id="adaboost如何并行计算"><a href="#adaboost如何并行计算" class="headerlink" title="adaboost如何并行计算"></a>adaboost如何并行计算</h4><ol>
<li>最优基分类器的选择</li>
<li>每个数据权重的计算和更新<h4 id="Adaboost调参"><a href="#Adaboost调参" class="headerlink" title="Adaboost调参"></a>Adaboost调参</h4></li>
<li>base_estimator: 基分类器的种类，默认决策树</li>
<li>n_estimators：基学习器的最大迭代数，个数</li>
<li>learning_rate：基学习器的权重衰减系数</li>
<li>基学习器参数 max_feature：划分时考虑的最大特征函数</li>
<li>基学习器参数 max_depth：最大深度</li>
</ol>
<h4 id="规则学习"><a href="#规则学习" class="headerlink" title="规则学习"></a>规则学习</h4><ol>
<li>快排，最大堆</li>
<li>给出二叉树的前序和中序序列恢复二叉树的结构</li>
</ol>
<p>密度聚类的方法<br>问CRF、HMM知道不？</p>
<p>关联推荐<br>排序算法的时间空间复杂度<br>WordVec 的原理</p>
<p>圆上三个点组成锐角三角形的概率<br>1/4</p>
<p>将一条线段在两个地方随机切断可以组成三角形的概率<br>假设我们选择的两个点的坐标是x和y（先假设x &lt; y）：<br>那么由三角形两边和大于第三边的性质，有：<br>x+y-x&gt;1-y<br>x+1-y&gt;y-x<br>1-y+y-x&gt;x<br>由上述不等式得到：x &lt; 0.5, 0.5 &lt; y &lt; x+0.5<br>然后画个图就能得到联合概率为1/8，不要忘了这个结果是在假设x &lt; y时得来的，所以再乘以2，得到1/4。</p>
<p>共线性(collinearity)<br>当数据矩阵Xn×(p+1)不是列满秩时，正规方程中的XTX不可逆，任何满足正规方程的参数都能够最小化均方误差。满足要求的解有无限多个，因此得出的解容易过拟合(overfitting)。这个问题又称数据共线性。<br>如果n⩽p，则数据一定共线性。否则，数据很少精确地满足共线性。即使数据不是精确地满足共线性，也可能会非常接近列不满秩。此时XTX虽然可逆，但是数值求解其逆矩阵(inverse matrix)非常不稳定，这样得到的不稳定解也容易产生过拟合。<br>解决共线性的方法主要有两种。第一种是找出共线性的列并且删除其中一列直到数据列满秩。第二种是使用正则化(regularization)。</p>
<p>其中项目是重点，我相信看到这篇文章的你，应该有相关的NLP项目，那么你应该对你简历所写上去的东西负责任（也就是对细节非常熟），对方可能会问到你：</p>
<p>1）具体参数设置，为什么要这样设置（掌握一下调参玄学）<br>2）你的模型，为什么这么做，为什么能work，和xx方法比怎么样<br>3）可能根据你的项目及模型，提出某个可能存在的深藏不露的问题，问你如何解决<br>4）项目难点是什么，又如何解决，从哪几方面解决，效果提升多少<br>5）如今的你再来看从前的这个任务，有没有更好的解决思路<br>6）给你一个新的业务场景，你怎么把你的模型移植上去，怎么重新设计模型，和你之前项目的区别是什么，需要注意哪些问题<br>7）项目分工，你做了哪部分工作<br>8）你这个任务的state of the art</p>
<p>Part II：深度学习</p>
<p>首先关于基础原理，你至少要知道这些：<br>1）CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM<br>2）RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进。<br>3）Word2vec工具，怎么训练词向量，skip-gram和cbow，可以参考一下：一篇通俗易懂的word2vec（也可能并不通俗易懂）<br>4）Attention机制，比较常见的方法，可以参考一下：Attention用于NLP的一些小结<br>5）NLP基础任务，比如分词算法（序列标注任务），分类算法</p>
<p>关于实战部分，你至少也要知道这些：</p>
<p>1）数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练</p>
<p>2）过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？</p>
<p>3）调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。</p>
<p>4）模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估</p>
<p>5）优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法</p>
<p>8）如何处理数据不均衡问题，也是从数据和模型两方面出发解决。</p>
<p>你至少要掌握的算法原理：</p>
<p>5）SVM，核函数选择，不同SVM形式</p>
<p>6）HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？</p>
<p>7）最大熵原理，图解最大熵原理（The Maximum Entropy Principle）</p>
<p>8）KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解</p>
<p>以上列出的算法都需要掌握其基本原理以及优缺点，可以参考：机器学习算法优缺点及其应用领域 - CSDN博客</p>
<p>你必须要会写的公式：</p>
<p>1）BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。</p>
<p>2）softmax和交叉熵推导，分成i=j 和 ij 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导</p>
<p>3）各种Loss函数</p>
<p>4）似然函数，负对数似然函数的推导</p>
<p>5）最小二乘法，利用矩阵的秩进行推导</p>
<p>7）贝叶斯定理，拉普拉斯平滑</p>
<p>你最好也要掌握一下的公式：</p>
<p>1）RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。</p>
<p>2）各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园</p>
<p>3）Batch Normalization，就是个归一化过程，再加一个scale操作</p>
<p>4）SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法</p>
<p>5）最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园</p>
<p>Part IV：算法编程</p>
<p>不管你面试什么公司，请记住coding几乎是必考的，这是工程师的基本功。</p>
<p>编程分成三种：普通算法编程，海量数据编程，模型编程。</p>
<p>普通算法编程，一般用C++，需要掌握数组，链表，二叉树，递归，贪心，动态规划，各种容器，各种排序算法，在时间或者空间上的优化思路，以及复杂度的分析。</p>
<p>容器是个好东西，用vector代替数组，用map实现桶思想，用set排序，用queue写bfs，用stack写dfs等等。</p>
<p>推荐大家刷：剑指offer，这本书两天就可以看完（如果仅仅是看题目以及思路），然后上牛客网做一下题：剑指Offer_编程题_牛客网，66道原题全在这，而且评论区有大神出没，某些题的解法我觉得比书上的要巧妙。或者刷LeetCode也可以。</p>
<p>随手列几道常考的代码题：</p>
<p>1）复杂链表的复制，链表的删除</p>
<p>2）最长公共子序列，逆序对</p>
<p>3）快排，归并排序，堆排序</p>
<p>4）二分查找，以及衍生的题目</p>
<p>5）深度优先搜索</p>
<p>海量数据编程，这种用python写比较方便一点，可以把大文件划分成小文件，或者分治加哈希：十道海量数据处理面试题与十个方法大总结 - CSDN博客</p>
<p>模型编程，有时候可能会让你用某个深度学习框架搭某个模型，不过这种比较少。</p>
<p>做科研比较推荐pytorch，业界用tf 比较多，不过也得看组看个人，如果项目需要上线很有可能就是要用tf了。我个人比较喜欢用pytorch，方便搭模型，对RL也很友好。但tf 还是要掌握一下的，指不定哪天这个项目就是要用tf来上线呢。</p>
<p>总之，刷题即可，多写代码多搭模型。</p>
<p>L2 范数不但可以防止过拟合，提高模型的泛化能力，还可以让我们的优化求解变得稳定和快速。L2 范数对大数和 outlier 更敏感！</p>
<p>填空题<br>经过下列卷积操作后，3×3 conv -&gt; 3×3 conv -&gt; 2×2 maxpool -&gt; 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？<br>100×100×3，3×3 卷积核，输出是 50×50×10，算进行了多少次乘-加操作？</p>
<p>简答题<br>简述梯度下降法和牛顿法的优缺点？<br>正样本 10000，负样本 1000，怎样训练<br>Relu 相对于 sigmoid 函数的优缺点？</p>
<p>编程题<br>输入序列 a, 判断是否存在 i &lt; j &lt; k, 满足 a[i] &lt; a[k] &lt; a[j]，并写出算法复杂度？<br>输入多边形顶点坐标 List，判断是否为凸多边形(如果把一个多边形的所有边中，任意一条边向两方无限延长成为一直线时，其他各边都在此直线的同旁，那么这个多边形就叫做凸多边形)?</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Dongfang Li WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Dongfang Li Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/12/Reinforce Learning/强化学习之Actor Critic/" rel="next" title="强化学习之Actor Critic">
                <i class="fa fa-chevron-left"></i> 强化学习之Actor Critic
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/14/个人项目介绍/" rel="prev" title="">
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dongfang Li</p>
              <p class="site-description motion-element" itemprop="description">My blog about programming and machine learning</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">95</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/YHfeather" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/dongfang.li.790" target="_blank" title="FB Page">
                      
                        <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#弱监督学习"><span class="nav-number">1.</span> <span class="nav-text">弱监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#adaboost如何并行计算"><span class="nav-number">1.1.</span> <span class="nav-text">adaboost如何并行计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaboost调参"><span class="nav-number">1.2.</span> <span class="nav-text">Adaboost调参</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#规则学习"><span class="nav-number">1.3.</span> <span class="nav-text">规则学习</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongfang Li</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://blackfeather.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/08/13/Machine Learning/机器学习问题总结/';
          this.page.identifier = '2018/08/13/Machine Learning/机器学习问题总结/';
          this.page.title = '';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://blackfeather.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
