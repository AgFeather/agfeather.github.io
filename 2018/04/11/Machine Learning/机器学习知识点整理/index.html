<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning," />










<meta name="description" content="本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习常见问题和知识点整理">
<meta property="og:url" content="http://yoursite.com/2018/04/11/Machine Learning/机器学习知识点整理/index.html">
<meta property="og:site_name" content="Black Feather">
<meta property="og:description" content="本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-01T02:25:33.442Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习常见问题和知识点整理">
<meta name="twitter:description" content="本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/11/Machine Learning/机器学习知识点整理/"/>





  <title>机器学习常见问题和知识点整理 | Black Feather</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Black Feather</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/11/Machine Learning/机器学习知识点整理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dongfang Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Black Feather">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习常见问题和知识点整理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T00:00:00+09:00">
                2018-04-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/04/11/Machine Learning/机器学习知识点整理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/04/11/Machine Learning/机器学习知识点整理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。<br>在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。</p>
<a id="more"></a>
<h1 id="模型评估选择"><a href="#模型评估选择" class="headerlink" title="模型评估选择"></a>模型评估选择</h1><ul>
<li>学习器在训练集上的误差称为训练误差或者经验误差，在新样本上的误差称为泛化误差<h4 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h4>详情见我的另一篇总结文章<a href="https://yhfeather.github.io/2017/10/12/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/#more" target="_blank" rel="noopener">生成模型和判别模型</a><h4 id="模型评估方法"><a href="#模型评估方法" class="headerlink" title="模型评估方法"></a>模型评估方法</h4></li>
<li>留出法：直接将数据集分为两个不相交的子集，一个训练，一个测试。一般需要多次划分对评估结果取平均值。</li>
<li>交叉验证：将数据集分成k个大小相同的子集，每次用一个子集i作为测试，用剩下的k-1作为训练，然后遍历k个子集会产生k个训练/测试结果。留一法：让k=数据集大小，也就是说每次直选一个样本进行测试。</li>
<li>自助法：有放回的从原始数据集采样，每次取一个数据复制放入新数据集，然后将该数据放回，重复采样直到新数据集大小为m，初始数据集中会有36%的样本未出现在新数据集中，用新数据训练，用未出现的数据集做测试。 自助法在数据集小，难以有效划分时有用，但改变了初始数据集的分布，引入了估计偏差。<h4 id="模型性能度量"><a href="#模型性能度量" class="headerlink" title="模型性能度量"></a>模型性能度量</h4></li>
<li>错误率和精度</li>
<li>查准率，查全率，F1值</li>
<li>P-R曲线（查准率，查全率曲线）<ol>
<li>将预测结果从最可能到最不可能进行排序，按照此顺序逐个把样本作为正例进行预测，并记录每一刻的P-R值</li>
<li>可以比较PR曲线下的面积来比较模型性能，或者比较平衡点（P==R）的大小</li>
<li>可以根据不同任务需求采用不同的截断点，更重视P则选择排序靠前的位置截断。。。</li>
</ol>
</li>
<li>ROC曲线&amp;AUC<ul>
<li>同样将样本从可能正例到不可能正例排序，首先个将所有样例均预测为反例，这时真正例率和假正例率都为0，然后调整阈值，依次将每个样例划分为正例。每次计算真正例率和假正例率绘制ROC曲线</li>
<li>真正例率：所有真实正例中被预测为正例的比例（对正例预测正确）</li>
<li>假正例率：所有真实反例中被预测为正例的比例（对反例预测错误）</li>
<li>RUC考虑的事样本预测的排序质量，因此它与排序误差有紧密联系</li>
<li>ROC曲线下方围成的面积即为AUC</li>
<li>因为AUC为点连成的非光滑曲线，所以可以通过计算各个相邻点组成的小矩形面积之和得出AUC</li>
</ul>
</li>
</ul>
<ol start="5">
<li>代价敏感误差率与代价曲线<ul>
<li>构建有代价权重的混淆矩阵，然后可以依据此计算表示代价的ROC曲线</li>
</ul>
</li>
</ol>
<h4 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h4><p>在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值的期望之间的差均方。<br><strong>偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。</strong> 噪声表达当前任务在任何学习算法所能达到的泛化误差的下界，通过对泛化误差的进行分解。可以得到：</p>
<ul>
<li>期望泛化误差=方差+偏差+噪声</li>
<li>偏差刻画学习器的拟合能力</li>
<li>方差体现学习器的稳定性</li>
<li>噪音刻画了问题本身的难度<br>方差和偏差具有矛盾性，这就是常说的偏差-方差窘伪境（bias-variance dilamma），<strong>随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。</strong> 换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练要适度辄止。<h5 id="模型复杂程度和偏差-方差权衡："><a href="#模型复杂程度和偏差-方差权衡：" class="headerlink" title="模型复杂程度和偏差/方差权衡："></a>模型复杂程度和偏差/方差权衡：</h5>我们在偏差与方差间都会做出权衡。如果我们的模型过于简单，只有很少的几个参数，那么它可能存在着较大的偏差（但是方差较小）；如果过于复杂而含有非常多的参数，那么模型可能会存在较大的方差（但是偏差较小）。同时，一般而言，高偏差意味着欠拟合，高方差意味着过拟合。<h4 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h4></li>
</ul>
<ol>
<li>假设检验</li>
<li>交叉验证t检验</li>
<li>McNemar检验from</li>
<li>Friedman检验和Nemenyi后续检验</li>
<li>偏差和方差<h3 id="问题补充"><a href="#问题补充" class="headerlink" title="问题补充"></a>问题补充</h3></li>
<li>训练集中类别不均衡，哪个参数最不准确？<br>准确度（Accuracy）。 eg.训练集中class 1的样本数比class 2的样本数是60:1。使用逻辑回归进行分类，最后结果是其忽略了class 2，即其将所有的训练样本都分类为class 1。</li>
</ol>
<h1 id="常见基本问题"><a href="#常见基本问题" class="headerlink" title="常见基本问题"></a>常见基本问题</h1><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>多分类学习的基本思路是“拆解法”，即将多个分类任务拆为若干个二分类任务求解。经典的拆分策略有三种：‘一对一’，‘一对余’，‘多对多’</p>
<ol>
<li>一对一OvO：将N个类别两两配对，产生N*(N-1)/2个二分类任务。在 <strong>测试阶段</strong> ，新样本将同时提交给所有分类器，于是我们得到N(N-1)/2个分类结果，最终结果可以通过投票产生，即把被预测最多的类别作为最终分类结果。</li>
<li>一对余OvR：每次将一个类作为正例，剩下的N-1个类作为反例来训练N个分类器。在 <strong>测试阶段</strong> 若仅有一个分类器预测为正例，则对应的类别标记作为最终分类结果。若有多个分类器预测为正例，则通常考虑各个分类器的预测置信度，选择置信度最大的类别标记为分类结果。</li>
<li>多对多MvM：每次将若干类作为正类，若干类其他类作为反类，显然OvO和OvM是MvM的特例。MvM正反类的构造必须有特殊的设计，不能随意选取，通常使用“纠错输出码（ECOC）”技术。<ul>
<li>ECOE技术详见书第65页</li>
</ul>
</li>
</ol>
<h3 id="样本类别不平衡问题"><a href="#样本类别不平衡问题" class="headerlink" title="样本类别不平衡问题"></a>样本类别不平衡问题</h3><ol>
<li>对于线性分类y=wx+b, 我们通常认为y&gt;0.5为正例y&lt;0.5为反例，说明几率y/(1-y)的阈值为0.5，表示分类器认为正反例可能性相同。所以我们只需要修改分类器预测几率的阈值即可：若y/(1-y) &gt; m+/m-则判定为正例。</li>
<li>解决类别不平衡学习的一个基本策略：<strong>再缩放</strong>，对预测出的几率乘以正负例的比值即为模型的预测输出。</li>
<li>实际应用中共有三种做法： <strong>欠采样，过采样，阈值移动</strong><ul>
<li>欠采样即去除一些反例使得正反例数目接近，由于欠采样可能丢失重要信息，实践中使用EasyEnsemble利用集成学习将反例划分为若干个集合供不同学习器使用，这样对于每个学习器来说都是欠采样，但在全局看来不会丢失信息。</li>
<li>过采样即增加一些正例使得正反例数目接近（不能简单地对正例进行重复否则会产生严重的过拟合，SMOTE算法通过对训练集里的正例进行插值来产生额外的正例。</li>
<li>阈值移动即为开始所述，对预测几率乘以正反例数量的比值</li>
</ul>
</li>
<li>调整正负样本的惩罚权重<br>对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。使用这种方法时不需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。</li>
<li>集成学习，组合学习<br>组合/集成方法指的是：将多数类样本数据集分为几份，每份大小与少数类样本数据集大小相同，每次用一份多数类样本和所有少数类样本组成训练集训练模型，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。</li>
</ol>
<h3 id="过拟合解决办法"><a href="#过拟合解决办法" class="headerlink" title="过拟合解决办法"></a>过拟合解决办法</h3><ol>
<li>L1，L2正则</li>
<li>模型早停</li>
<li>增加样本，数据增强（比如说对图片进行平移旋转剪裁）</li>
<li>针对神经网络：Batch Normalization</li>
<li>针对神经网络：Dropout</li>
</ol>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="最小二乘法和其矩阵形式"><a href="#最小二乘法和其矩阵形式" class="headerlink" title="最小二乘法和其矩阵形式"></a>最小二乘法和其矩阵形式</h2><pre><code>1. 基于均方误差最小化来进行模型求解的方法称为最小二乘法
2. 在线性回归模型中最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小
3. 最小二乘法即对每个参数求偏导，并令其等于0.
4. 最小二乘法的矩阵表示和解法（原理相同，只是变为对矩阵求偏导）
5. **对数线性回归** 实质上已是在求输入空间到输出空间的非线性函数映射：ln(y) = wx + b
6. **广义线性模型** y = g-1(wx + b)
</code></pre><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ul>
<li>优点：实现简单，分类时计算量非常小，速度快，存储资源低</li>
<li>缺点：容易欠拟合，准确度不高。只能处理二分类问题（衍生的softmax可解决），样本必须线性可分。<h4 id="LR推导"><a href="#LR推导" class="headerlink" title="LR推导"></a>LR推导</h4>掌握<h4 id="几率和对数几率"><a href="#几率和对数几率" class="headerlink" title="几率和对数几率"></a>几率和对数几率</h4><ol>
<li>y/(1-y)称为几率，反映了样本作为正例的相对可能性。对几率取对数为对数几率</li>
<li>ln(y/(1-y)) = wx + b 等价于 y = 1/(1+exp(-wx-b))</li>
<li>由上式可看出，LR本质上是训练一个线性模型，逼近真是label的对数几率，也就是数据作为正例的相对可能性。<h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2></li>
</ol>
</li>
<li>书第60页<br>LDA思想：给定训练数据集，设法将样例投影到一条直线上，是的同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。<strong>衡量同类样例距离尽可能小只需要让协方差尽可能小，异类样例距离尽可能大只需要让类中心之间的距离尽可能大。</strong><br>在对新样例进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。</li>
</ul>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="为什么不直接对loss-function求偏导-0，而要用梯度下降？"><a href="#为什么不直接对loss-function求偏导-0，而要用梯度下降？" class="headerlink" title="为什么不直接对loss function求偏导=0，而要用梯度下降？"></a>为什么不直接对loss function求偏导=0，而要用梯度下降？</h3><ol>
<li>逻辑回归没有解析解，就是说无法显式的表现函数的偏导，只能通过数值求解的方法迭代地找到最优解。</li>
<li>即使有解析解，大部分神经网络的loss为非凸函数，KKT条件（偏导为0是其中一项）仅仅是非凸函数最优化的必要非充分条件。</li>
<li>偏导为0可能并非是局部极值</li>
</ol>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>详情请见我另一篇整理文章: <a href="https://yhfeather.github.io/2018/04/21/%E5%90%84%E7%A7%8DBoosting%E6%A0%91/" target="_blank" rel="noopener">各种树: 从决策树到xgboost</a></p>
<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="传统支持向量机"><a href="#传统支持向量机" class="headerlink" title="传统支持向量机"></a>传统支持向量机</h2><p>距离超平面最近的几个训练样本点使距离等式成立，这几个样本点称为支持向量。两个异类支持向量到超平面的距离之和称为“间隔”。</p>
<h2 id="支持向量机对偶形式"><a href="#支持向量机对偶形式" class="headerlink" title="支持向量机对偶形式"></a>支持向量机对偶形式</h2><h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><p>掌握</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>KKT条件是一个具有很强几何意义的结论。需要掌握</p>
<h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO的基本思路是先固定ai之外的所有参数，然后求ai上的极值，由于存在约束sum(aiyi)=0，若固定ai之外的其他变量则ai可以由其他变量导出。于是SMO每次选择两个变量ai，aj。并固定其他参数，这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p>
<pre><code>1. 选取一对需要更新的变量ai和aj
2. 固定剩余的参数，求解SVM对偶式的公式获得更新后的ai和aj
</code></pre><p>注意到只要ai和aj有一个不满足KKT条件，目标函数就会在迭代后增大，KKT条件违背程度越大，变量更新后可能导致的目标函数值增幅越大。<br>于是SMO先选取违背KKT条件程度最大的变量，第二个选取使目标函数值增幅最快的变量，但比较变量复杂度过大，SMO采用启发式方法：选取两个变量所对应样本之间的间隔最大，这样的两个变量有很大差别，对目标函数的更新有更大帮助。   </p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>可将样本从原始空间映射到一个高维看空间，使得样本在这个高维空间线性可分。如果原始空间是有限维，即属性有限，那么一定存在一个高维空间使样本可分。<br>我们发现SVM的对偶式中包含xi和xj的内积运算，使用映射方法只需要将原始空间内的内积运算转换为高维映射空间内的映射运算即可。但映射空间往往维度过高，计算复杂，所以假设存在核函数k(xi, xj) = xi,xj的內积，有了这个核函数在对偶式中只需将內积替换成核函数即可。</p>
<h4 id="核函数条件"><a href="#核函数条件" class="headerlink" title="核函数条件"></a>核函数条件</h4><p>只要一个对称函数所对应的和矩阵半正定，它就能作为核函数使用。</p>
<h4 id="核函数的种类"><a href="#核函数的种类" class="headerlink" title="核函数的种类"></a>核函数的种类</h4><p>核函数选择成了SVM的最大变数，若核函数不适合，样本被映射到了一个不适合的空间，会导致性能不佳。<br>常见的核函数有：</p>
<ol>
<li>线性核</li>
<li>多项式核</li>
<li>高斯核</li>
<li>拉普拉斯核</li>
<li>Sigmoid核<br>此外，还可以通过函数线性组合得到新的核函数。</li>
</ol>
<h2 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h2><p>在现实任务中我们很难恰到好处的找个某个核函数使得训练集在特征空间线性可分，也很难推是否是由过拟合造成的。所以我们允许SVM在一些样本上出错，引入软间隔概念。同时引入松弛变量的概念。</p>
<h4 id="软间隔SVM推导"><a href="#软间隔SVM推导" class="headerlink" title="软间隔SVM推导"></a>软间隔SVM推导</h4><p>掌握（软间隔的对偶形式和硬间隔的对偶形式唯一区别在于对ai的约束，软间隔的约束为’0&lt;=ai&lt;=C’, 硬间隔的约束仅为’0&lt;=ai’</p>
<h4 id="软间隔SVM的KKT条件"><a href="#软间隔SVM的KKT条件" class="headerlink" title="软间隔SVM的KKT条件"></a>软间隔SVM的KKT条件</h4><p>理解</p>
<h2 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h2><p>对于传统回归模型通常直接计算模型输出和真实输出之间的差别来计算损失，只有当两个输出完全相同时loss才为0。SVR容忍预测和label之间最多有e的偏差，即当f预测和label之间的差别大于e才计算损失。相当于以f为中心，构建了一个宽度为2e的间隔带，样本落入该间隔带则被认为是分类正确的。</p>
<h4 id="SVR公式推导"><a href="#SVR公式推导" class="headerlink" title="SVR公式推导"></a>SVR公式推导</h4><p>假设我们能容忍f(x)和y之间最多有e的偏差，即当f(x)与y之间的差别绝对值大于e时才计算损失。相当于以f(x)为中心，构建了一个宽度为2e的间隔带，若样本落入此间隔带则认为被预测正确。</p>
<h2 id="补充考点"><a href="#补充考点" class="headerlink" title="补充考点"></a>补充考点</h2><h4 id="SVM优缺点"><a href="#SVM优缺点" class="headerlink" title="SVM优缺点"></a>SVM优缺点</h4><ul>
<li>优点：<ol>
<li>可用于线性/非线性分类，也可以用于回归</li>
<li>泛化能力强，泛化误差较低</li>
<li>小训练集上往往就可以得到较好的结果</li>
</ol>
</li>
<li>缺点：<ol>
<li>对参数和核函数的选择比较敏感，核函数选取比较难</li>
<li>原始的SVM只比较擅长处理二分类问题</li>
<li>时空开销都比较大<h4 id="SVM和LR的异同"><a href="#SVM和LR的异同" class="headerlink" title="SVM和LR的异同"></a>SVM和LR的异同</h4></li>
</ol>
</li>
</ul>
<ol>
<li>两者都为线性模型</li>
<li>两者都是判别模型</li>
<li>本质上来讲是损失函数的不同，LR为交叉熵损失（对数损失），SVM为hinge损失</li>
<li>由于损失函数不同，LR直接依赖所有数据分布，SVM只关心支持向量</li>
<li>SVM依赖距离测度，数据需要归一化，LR不受影响</li>
<li>SVM自带正则项，LR需要在Loss Function上添加。<h4 id="SVM多分类"><a href="#SVM多分类" class="headerlink" title="SVM多分类"></a>SVM多分类</h4></li>
</ol>
<ul>
<li>一对多，选择预测结果最大值</li>
<li>一对一，然后采用投票</li>
<li>层次SVM，层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。<h4 id="带核的SVM为什么能分类非线性问题？"><a href="#带核的SVM为什么能分类非线性问题？" class="headerlink" title="带核的SVM为什么能分类非线性问题？"></a>带核的SVM为什么能分类非线性问题？</h4>核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积<h4 id="RBF核一定是线性可分的吗"><a href="#RBF核一定是线性可分的吗" class="headerlink" title="RBF核一定是线性可分的吗"></a>RBF核一定是线性可分的吗</h4>不一定，RBF核比较难调参而且容易出现维度灾难，要知道无穷维的概念是从泰勒展开得出的。<h4 id="常用核函数及核函数的条件："><a href="#常用核函数及核函数的条件：" class="headerlink" title="常用核函数及核函数的条件："></a>常用核函数及核函数的条件：</h4>核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。 RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。 线性核：主要用于线性可分的情况 多项式核</li>
</ul>
<h1 id="贝叶斯方法"><a href="#贝叶斯方法" class="headerlink" title="贝叶斯方法"></a>贝叶斯方法</h1><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>公式，多个概率概念掌握</p>
<h4 id="朴素贝叶斯处理连续属性"><a href="#朴素贝叶斯处理连续属性" class="headerlink" title="朴素贝叶斯处理连续属性"></a>朴素贝叶斯处理连续属性</h4><p>对于连续属性可以考虑概率密度函数，假定该属性遵循正态分布，由此得出连续属性的概率</p>
<h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><p>在测试过程中，如果某个属性的取值从未在训练集中出现过，会导致P(x|y)=0,进而让整个贝叶斯公式等于零。通过拉普拉斯平滑可以解决这个问题。通俗的说，拉普拉斯平滑即在计算概率的时候，分别在分子上加1，分母上加属性的取值种类。</p>
<h4 id="朴素贝叶斯优化"><a href="#朴素贝叶斯优化" class="headerlink" title="朴素贝叶斯优化"></a>朴素贝叶斯优化</h4><ol>
<li>可以将涉及到的所有概率先计算好存储起来，在进行预测时只需要“查表”即可进行判别。</li>
<li>当数据有多属性时，涉及到多属性连乘问题，可以将原连乘问题转换为对数连加问题加快计算速度。<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4></li>
</ol>
<ul>
<li>优点：对小规模数据表现很好，适合多分类任务</li>
<li>缺点：对输入数据的表达形式很敏感<h4 id="为什么被称为“朴素”"><a href="#为什么被称为“朴素”" class="headerlink" title="为什么被称为“朴素”"></a>为什么被称为“朴素”</h4>朴素的意思是该分类方法建立在一个较强的独立假设上，即所有特征之间相互独立，所以被称为朴素，具体的说为：P(AB) = P(A)P(B)</li>
</ul>
<h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><ul>
<li>基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较性强的属性依赖关系。</li>
<li>“独依赖估计”是半朴素贝叶斯分类器最常用的一种策略，所谓“独依赖”就是假设每个属性在类别之外最多仅依赖一个其他属性。</li>
<li>半朴素贝叶斯不是很常见，具体细节见书p154</li>
</ul>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>也称“信念网络”，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。</p>
<h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><p>吉布斯采样，EM算法</p>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><p>通过构建并结合多个学习器来完成学习任务，总体思路是：1. 先产生一组“个体学习器”，再用某种策略将他们结合起来。</p>
<ul>
<li>同质的个体学习器也可以称为“基学习器”</li>
<li>异质集成中的个体学习器由不同算法组成，常称为“组件学习器”<br>集成学通过多个学习器的组合，通常会有比单一学习器更显著的泛化性能。<br>如何产生“好而不同”的个体学习器是集成学习的研究核心。<br>目前的集成学习方法大致可以分为两类：</li>
</ul>
<ol>
<li>个体学习器之间存在强依赖关系，必须串行生成序列化方法，代表是Boosting。</li>
<li>个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林</li>
</ol>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting是一族可将弱学习器提升为强学习器的方法：这族算法工作原理类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器的错误样本更受关注然后基于调整后的样本分布训练下一个基学习器。最后将这些基学习器组合到一起。</p>
<ul>
<li>优点：<ol>
<li>低泛化误差</li>
<li>容易实现，准确率较高，不需要大量调参</li>
</ol>
</li>
<li>缺点：<ol>
<li>对异常点过于敏感<br>在基学习器的选择上，大部分Boosting算法都选择决策树作为基学习器。<br>我在另一篇博文中整理了常见Boosting树的原理：<a href="">各种Boosting树</a></li>
</ol>
</li>
</ul>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>基本思想是对训练集进行采样，产生出若干个不同的子集，再从每个子集中训练一个基学习器。常使用的采样方法为“自助采样法”。<br>我们可以采样出T个含有m个样本的采样集，用每个采样集训练出一个基学习器，再将这些基学习器进行结合就是Bagging的基本流程。<br>Bagging对于分类任务采用简单投票法，对于回归任务采用简单平均法。<br>对于每个基学习器的包外样本，可以用其进行泛化性能估计或者辅助剪枝（决策树）</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是在Bagging的基础上进一步在决策树的训练过程中引入了 <strong>随机属性选择</strong>。<br>在RF中，对基决策树的每个节点，<strong>先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度，若k=d，则基决策树的构建和传统决策树相同；若k=1，则随机选择一个属性用于划分，一般情况下推荐k=log2d</strong><br>RF中的基学习器的多样性不仅仅来自于样本扰动（对原始数据集进行自助采样），还来自属性随机。</p>
<h4 id="随机森林进行特征选择"><a href="#随机森林进行特征选择" class="headerlink" title="随机森林进行特征选择"></a>随机森林进行特征选择</h4><p>在随机森林中某个特征X的重要性的计算方法如下：<br>1：对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB1.<br>2: 随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB2.<br>3：假设随机森林中有Ntree棵树,那么对于特征X的重要性=∑(errOOB2-errOOB1)/Ntree,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。</p>
<h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><ol>
<li>对于回归问题：简单平均法、加权平均法</li>
<li>对于分类问题：绝对多数投票法，相对对数投票法，加权投票法。</li>
</ol>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>先从初始数据集训练出初始学习器，然后再生成一个新数据集用于训练次级学习器。在这个新的数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记被当做样例标记。</p>
<h2 id="集成学习的多样性度量"><a href="#集成学习的多样性度量" class="headerlink" title="集成学习的多样性度量"></a>集成学习的多样性度量</h2><p>见书P186页</p>
<h2 id="集成学习通常选用决策树的原因"><a href="#集成学习通常选用决策树的原因" class="headerlink" title="集成学习通常选用决策树的原因"></a>集成学习通常选用决策树的原因</h2><p>决策树可以认为是 if-then 规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征。<br>不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。因此，梯度提升方法和决策树学习算法可以互相取长补短</p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>对于一些依赖于距离测度的学习算法，在训练算法之前对数据进行预处理是非常有必要的</p>
<h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>指将属于映射到指定范围内，用于去除不同维度数据的量纲以及量纲单位。一般会映射到[0, 1]或者[-1, 1]</p>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>x = (x-u)/s. 其中u为均值，s为方差。标准化改变了数据的分布，某种程度上加快模型训练速度。</p>
<h3 id="为什么需要归一化"><a href="#为什么需要归一化" class="headerlink" title="为什么需要归一化"></a>为什么需要归一化</h3><ul>
<li>对于需要用距离度量的算法，如果不同特征的量纲不同会导致对预测结果的权重影响不同（如房价预测，kNN）</li>
<li>本质上是loss函数不同造成的，对于梯度下降来说，量纲不同会导致收敛速度过慢等问题。</li>
</ul>
<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><ul>
<li>用来评估聚类效果的好坏<ul>
<li>同一簇的样本尽可能的彼此相似，不同簇的样本尽可能不同。</li>
<li>簇内相似度尽可能高，簇间相似度尽可能低</li>
</ul>
</li>
<li>两类性能度量：<ol>
<li>将聚类结果与某个参考模型进行比较，称为”外部目标”</li>
<li>直接考察聚类结果而不利用任何参考模型，称为“内部目标”·</li>
</ol>
</li>
<li>三个聚类性能度量外部指标：<ol>
<li>Jaccard系数</li>
<li>FM系数</li>
<li>Rand指数</li>
</ol>
<ul>
<li>上述性能度量都在零一之间，且越大越好</li>
</ul>
</li>
<li>两个聚类性能度量内部指标<ol>
<li>DB指数</li>
<li>Dunn指数</li>
</ol>
<ul>
<li>DBI越小越好，DI越大越好</li>
</ul>
</li>
</ul>
<h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><ul>
<li>闵可夫斯基距离度量：<ol>
<li>当p=2时，即为欧氏距离</li>
<li>当p=1时，即为曼哈顿距离</li>
</ol>
</li>
<li>使用闵可夫斯基距离时，注意只能针对有序属性特征</li>
<li>对于无序属性，可以采用VDM</li>
<li>对于一般问题，可以将闵可夫斯基度量和VDM相结合，前者处理有序属性，后者处理无序属性</li>
</ul>
<h2 id="k均值算法"><a href="#k均值算法" class="headerlink" title="k均值算法"></a>k均值算法</h2><ul>
<li>优点：思想简单，理论成熟，既可以回归也可以分类</li>
<li>缺点：计算量大，样本不平衡问题，需要大量内存<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4></li>
</ul>
<ol>
<li>设定聚类簇数为k，随机选取k个样本作为初始均值向量。</li>
<li>考察剩余样本，分别计算剩余样本到这k个均值向量的距离，并将其加入距离最近的聚类簇中</li>
<li>计算每个新聚类簇的均值向量，返回第二步直至收敛。</li>
</ol>
<h2 id="k近邻学习"><a href="#k近邻学习" class="headerlink" title="k近邻学习"></a>k近邻学习</h2><p>k近邻是最简单直接的算法之一，对于每个输入实例，找到在空间上距离该实例最近的k个数据，根据这k个数据的label对输入实例进行分类/回归。通常情况下，分类任务中使用投票法，回归任务中使用平均法，也可以使用加权平均法。<br>常见的距离测度方法为：闵可夫斯基距离，包括欧氏距离，曼哈顿距离等。<br>k近邻算法是懒惰学习 lazy learning的著名代表，没有显式的训练过程。（相应的，那些在训练阶段就对样本进行学习处理的方法称为急切学习 eager learning）</p>
<ul>
<li>k值越大，模型越简单</li>
</ul>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Dongfang Li WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Dongfang Li Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/25/Machine Learning/xgboost实战/" rel="next" title="xgboost实战">
                <i class="fa fa-chevron-left"></i> xgboost实战
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/12/Programming Language/Python/Python装饰器/" rel="prev" title="python装饰器">
                python装饰器 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dongfang Li</p>
              <p class="site-description motion-element" itemprop="description">My blog about programming and machine learning</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/YHfeather" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/dongfang.li.790" target="_blank" title="FB Page">
                      
                        <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#模型评估选择"><span class="nav-number">1.</span> <span class="nav-text">模型评估选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#生成模型和判别模型"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">生成模型和判别模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型评估方法"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">模型评估方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型性能度量"><span class="nav-number">1.0.0.3.</span> <span class="nav-text">模型性能度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差"><span class="nav-number">1.0.0.4.</span> <span class="nav-text">偏差与方差</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#模型复杂程度和偏差-方差权衡："><span class="nav-number">1.0.0.4.1.</span> <span class="nav-text">模型复杂程度和偏差/方差权衡：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#比较检验"><span class="nav-number">1.0.0.5.</span> <span class="nav-text">比较检验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问题补充"><span class="nav-number">1.0.1.</span> <span class="nav-text">问题补充</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常见基本问题"><span class="nav-number">2.</span> <span class="nav-text">常见基本问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多分类问题"><span class="nav-number">2.0.1.</span> <span class="nav-text">多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#样本类别不平衡问题"><span class="nav-number">2.0.2.</span> <span class="nav-text">样本类别不平衡问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合解决办法"><span class="nav-number">2.0.3.</span> <span class="nav-text">过拟合解决办法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性模型"><span class="nav-number">3.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘法和其矩阵形式"><span class="nav-number">3.1.</span> <span class="nav-text">最小二乘法和其矩阵形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑斯蒂回归"><span class="nav-number">3.2.</span> <span class="nav-text">逻辑斯蒂回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LR推导"><span class="nav-number">3.2.0.1.</span> <span class="nav-text">LR推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#几率和对数几率"><span class="nav-number">3.2.0.2.</span> <span class="nav-text">几率和对数几率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性判别分析（LDA）"><span class="nav-number">3.3.</span> <span class="nav-text">线性判别分析（LDA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#补充"><span class="nav-number">3.4.</span> <span class="nav-text">补充</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么不直接对loss-function求偏导-0，而要用梯度下降？"><span class="nav-number">3.4.1.</span> <span class="nav-text">为什么不直接对loss function求偏导=0，而要用梯度下降？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树"><span class="nav-number">4.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#支持向量机"><span class="nav-number">5.</span> <span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#传统支持向量机"><span class="nav-number">5.1.</span> <span class="nav-text">传统支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机对偶形式"><span class="nav-number">5.2.</span> <span class="nav-text">支持向量机对偶形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式推导"><span class="nav-number">5.2.0.1.</span> <span class="nav-text">公式推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KKT条件"><span class="nav-number">5.2.0.2.</span> <span class="nav-text">KKT条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO算法"><span class="nav-number">5.2.0.3.</span> <span class="nav-text">SMO算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数"><span class="nav-number">5.3.</span> <span class="nav-text">核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#核函数条件"><span class="nav-number">5.3.0.1.</span> <span class="nav-text">核函数条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核函数的种类"><span class="nav-number">5.3.0.2.</span> <span class="nav-text">核函数的种类</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#软间隔支持向量机"><span class="nav-number">5.4.</span> <span class="nav-text">软间隔支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#软间隔SVM推导"><span class="nav-number">5.4.0.1.</span> <span class="nav-text">软间隔SVM推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#软间隔SVM的KKT条件"><span class="nav-number">5.4.0.2.</span> <span class="nav-text">软间隔SVM的KKT条件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量回归"><span class="nav-number">5.5.</span> <span class="nav-text">支持向量回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SVR公式推导"><span class="nav-number">5.5.0.1.</span> <span class="nav-text">SVR公式推导</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#补充考点"><span class="nav-number">5.6.</span> <span class="nav-text">补充考点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM优缺点"><span class="nav-number">5.6.0.1.</span> <span class="nav-text">SVM优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM和LR的异同"><span class="nav-number">5.6.0.2.</span> <span class="nav-text">SVM和LR的异同</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM多分类"><span class="nav-number">5.6.0.3.</span> <span class="nav-text">SVM多分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#带核的SVM为什么能分类非线性问题？"><span class="nav-number">5.6.0.4.</span> <span class="nav-text">带核的SVM为什么能分类非线性问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RBF核一定是线性可分的吗"><span class="nav-number">5.6.0.5.</span> <span class="nav-text">RBF核一定是线性可分的吗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用核函数及核函数的条件："><span class="nav-number">5.6.0.6.</span> <span class="nav-text">常用核函数及核函数的条件：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#贝叶斯方法"><span class="nav-number">6.</span> <span class="nav-text">贝叶斯方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">6.1.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯处理连续属性"><span class="nav-number">6.1.0.1.</span> <span class="nav-text">朴素贝叶斯处理连续属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拉普拉斯平滑"><span class="nav-number">6.1.0.2.</span> <span class="nav-text">拉普拉斯平滑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯优化"><span class="nav-number">6.1.0.3.</span> <span class="nav-text">朴素贝叶斯优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优缺点"><span class="nav-number">6.1.0.4.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么被称为“朴素”"><span class="nav-number">6.1.0.5.</span> <span class="nav-text">为什么被称为“朴素”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#半朴素贝叶斯分类器"><span class="nav-number">6.2.</span> <span class="nav-text">半朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#贝叶斯网络"><span class="nav-number">6.3.</span> <span class="nav-text">贝叶斯网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构"><span class="nav-number">6.3.0.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#学习"><span class="nav-number">6.3.0.2.</span> <span class="nav-text">学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#推断"><span class="nav-number">6.3.0.3.</span> <span class="nav-text">推断</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#集成学习"><span class="nav-number">7.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">7.1.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-number">7.2.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林"><span class="nav-number">7.3.</span> <span class="nav-text">随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机森林进行特征选择"><span class="nav-number">7.3.0.1.</span> <span class="nav-text">随机森林进行特征选择</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结合策略"><span class="nav-number">7.4.</span> <span class="nav-text">结合策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stacking"><span class="nav-number">7.5.</span> <span class="nav-text">Stacking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集成学习的多样性度量"><span class="nav-number">7.6.</span> <span class="nav-text">集成学习的多样性度量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集成学习通常选用决策树的原因"><span class="nav-number">7.7.</span> <span class="nav-text">集成学习通常选用决策树的原因</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据处理"><span class="nav-number">8.</span> <span class="nav-text">数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化"><span class="nav-number">8.0.1.</span> <span class="nav-text">归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#标准化"><span class="nav-number">8.0.2.</span> <span class="nav-text">标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么需要归一化"><span class="nav-number">8.0.3.</span> <span class="nav-text">为什么需要归一化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#聚类"><span class="nav-number">9.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#性能度量"><span class="nav-number">9.1.</span> <span class="nav-text">性能度量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#距离计算"><span class="nav-number">9.2.</span> <span class="nav-text">距离计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k均值算法"><span class="nav-number">9.3.</span> <span class="nav-text">k均值算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#流程"><span class="nav-number">9.3.0.1.</span> <span class="nav-text">流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k近邻学习"><span class="nav-number">9.4.</span> <span class="nav-text">k近邻学习</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongfang Li</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://blackfeather.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/04/11/Machine Learning/机器学习知识点整理/';
          this.page.identifier = '2018/04/11/Machine Learning/机器学习知识点整理/';
          this.page.title = '机器学习常见问题和知识点整理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://blackfeather.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
