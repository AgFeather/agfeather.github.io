<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning," />










<meta name="description" content="在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习中的树算法总结">
<meta property="og:url" content="http://yoursite.com/2018/04/21/Machine Learning/各种Boosting树/index.html">
<meta property="og:site_name" content="Black Feather">
<meta property="og:description" content="在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/gbdt_alg.png">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/decision_tree.jpg">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/boosting_tree.jpg">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/xgboost_fomula.jpg">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/xgboost_math1.jpg">
<meta property="og:image" content="http://yoursite.com/images/other_ML_knowledge/xgboost_math2.png">
<meta property="og:updated_time" content="2018-09-01T06:19:18.311Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习中的树算法总结">
<meta name="twitter:description" content="在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost">
<meta name="twitter:image" content="http://yoursite.com/images/other_ML_knowledge/gbdt_alg.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/21/Machine Learning/各种Boosting树/"/>





  <title>机器学习中的树算法总结 | Black Feather</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Black Feather</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/21/Machine Learning/各种Boosting树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dongfang Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Black Feather">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习中的树算法总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-21T00:00:00+09:00">
                2018-04-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/04/21/Machine Learning/各种Boosting树/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/04/21/Machine Learning/各种Boosting树/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p>
<a id="more"></a>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><ul>
<li>优点：计算量小，可解释性强，比较适合处理有缺失值的样本，能够处理不相关特征</li>
<li>容易过拟合（集成树方法减少了过拟合现象）<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4></li>
<li>信息熵的计算</li>
<li>信息熵值越小，纯度越高。<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4></li>
<li>信息增益的计算</li>
<li>一般而言，信息增益越大，意味着使用属性a来进行划分所获得的“纯度提升”越大。</li>
<li>缺点：信息增益倾向于选择可取值数目较多的属性。<h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4></li>
<li>增益率为：分割属性的信息增益/分割属性的信息熵</li>
<li>缺点：增益率倾向于选择取值数目较少的属性<h4 id="ID3决策树算法"><a href="#ID3决策树算法" class="headerlink" title="ID3决策树算法"></a>ID3决策树算法</h4></li>
</ul>
<ol>
<li>ID3决策树生成算法采用信息增益生成决策树（见P75）</li>
<li>基于信息增益的划分法会偏向选取取值多的属性（如果用ID进行划分，也就是每个分治只有一个样本，每个分治纯度最大，但这样显然不具备泛化能力）<h4 id="ID4-5决策树算法"><a href="#ID4-5决策树算法" class="headerlink" title="ID4.5决策树算法"></a>ID4.5决策树算法</h4></li>
<li>为克服基于信息增益的算法偏向选择属性多的缺点，ID4.5基于信息增益率生成决策树</li>
<li>增益率准则倾向选择取值少的属性，因此C4.5算法并不是简单选用增益率划分，而是使用了一个启发式方法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<h4 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h4></li>
</ol>
<ul>
<li>CART树使用基尼系数进行属性划分</li>
<li>如果是二分类问题，基尼系数为：Gini(p) = 2p(1-p)</li>
<li>直观来说，基尼系数反应了从数据集随机抽取两个样本，其类别标记不一致的概率，因此Gini系数越小，数据集的纯度越高。</li>
</ul>
<h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><ul>
<li>决策树剪枝的基本策略有“预剪枝”和“后剪枝”。<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4></li>
</ul>
<ol>
<li>预剪枝是指在决策树生成过程中，首先从数据集中分割出验证集，然后对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；<strong>比较划分前后在验证集的精度是否提高</strong></li>
<li>预剪枝有欠拟合风险<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4></li>
<li>后剪枝则是先训练一颗完整的决策树，然后自底向上的对非叶节点进行考察，若将该节点对应的子节点替换为叶节点能带来决策树泛化性能的提升（在验证集上准确率提高），则将该子树替换为叶节点。 <strong>比较划分前后在验证集的精度是否提高</strong></li>
<li>后剪枝也可以通过构建决策树loss function，最小化loss实现。</li>
<li>后剪枝欠拟合风险小，泛化性能优于预剪枝，但需要先生成再剪枝，开销大。</li>
</ol>
<h2 id="连续值和缺失值"><a href="#连续值和缺失值" class="headerlink" title="连续值和缺失值"></a>连续值和缺失值</h2><h4 id="连续值"><a href="#连续值" class="headerlink" title="连续值"></a>连续值</h4><ul>
<li>对于取值为连续值的属性，考察该属性的所有取值，并从小到大排列。分别以任意相邻的取值的中值作为分割点，计算信息增益，并选取信息增益最大的点作为分割点。（其中可以将原节点的信息熵改为期望，因为每个取值的概率都为1/n） （具体公式见书84页）</li>
<li>对于回归树的连续型属性，考察该属性的所有取值，分别以任意相邻节点取值的中值作为分割点，<strong>计算平方损失</strong>，找到平方损失最小的连续型属性和该属性的分割节点。</li>
<li>与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。<h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4>三种情况：</li>
</ul>
<ol>
<li>如何在属性值缺失的情况下进行划分属性的选择（如何重构信息增益）<br>假如使用ID3算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的熵增。然后结果乘0.9（新样本占raw样本的比例），就是a属性最终的熵。<strong>可以理解为对各个属性的信息增益进行加权计算</strong></li>
<li>分类属性选择完成，对训练样本分类，发现属性缺失怎么办？<br>比如该节点是根据a属性划分，但是待分类样本a属性缺失，就把该样本分配到两个子节点中去，但是权重由1变为（每个子节点中实例数目/父节点实例数目），直观的看，就是让同一个样本以不同的概率划入到不同的子节点中去。在子节点的继续分裂时，计算比例时缺失值的使用权重而不是为1.</li>
<li>训练完成，给测试集样本分类，有缺失值怎么办？<br>这时候不能按比例分配，因为必须给该样本一个确定的label。这时候可以 <strong>根据投票来确定，或者填充缺失值。</strong></li>
</ol>
<h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><ul>
<li>通常决策树相当于用多个平行于轴线的线段对特征空间进行分割。这样的分类边界使得学习结果具有较好的可解释性，因为每一段划分都直接对应了某个属性的取值，<strong>但在分类边界比较复杂时，对于同一个属性，必须使用很多段划分才能获得较好的近似，此时决策树会相当复杂</strong>。</li>
<li><strong>若能使用斜的划分边界，则决策树模型会大大简化</strong>，“多变量决策树” 就是能实现这样斜划分甚至更复杂划分的决策树。</li>
<li>以斜划分为例，在此类决策树中，<strong>非节点不再是仅针对某个属性，而是对属性的线性组合进行测试，换言之，每个非叶节点是一个形如 wa=t 的线性分类器，w表示属性a的权重，w和t可以在节点所含的样本集和属性集上学得</strong></li>
<li>于是，与传统的单变量决策树不同，在多变量决策树的学习过程中，不是为每个节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</li>
</ul>
<h2 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h2><p>CART树是分类决策树的缩写，有如下特点：</p>
<ol>
<li>一定是二叉树</li>
<li>既可用于分类也可以用于回归</li>
<li>对于分类树，使用 <strong>基尼指数</strong> 作为划分标准</li>
<li>对于回归树，使用 <strong>平方损失最小</strong> 作为划分标准，叶节点的预测值为叶节点所有label的平均值。</li>
<li>CART剪枝方法：从生成的决策树上不断剪枝，直到根节点，形成一个子树序列。然后用交叉验证集对每个子树进行验证，找出泛化误差最小的子树。</li>
</ol>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p>Adaboost本质上是一个加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类算法。<br>基本思想是不断调整所有训练数据的训练权重，增加上一个基学习器分类错误数据的权重。<br>主要步骤如下：</p>
<ol>
<li>初始化训练数据的权重分布为1/n</li>
<li>训练一个基分类器，对所有数据进行分类</li>
<li>根据每个数据的分类结果和权重，计算分类误差率。</li>
<li>计算当前基分类器的权重：’a = 1/2 log((1-e)/e)’ (误差率越小，a越大)</li>
<li>更新所有数据的权重</li>
<li>重复2~5步，知道模型收敛。</li>
</ol>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>Gradient Boosting主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。 <strong>GBDT树内部是多个回归CART树</strong><br>主要由三个概念组成：回归决策树，Gradient Boosting，Shrinkage。</p>
<h4 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h4><p>GBDT的核心在于每个决策树是对上一个决策树预测“误差”的学习，然后累加所有树的结果作为最终结果，而 <strong>分类树的结果累加是没有意义的， 所以GBDT中的树都是回归树，不是分类树</strong>。<br>GBDT调整后可以用于分类问题，但内部还是回归树。<br><strong>回归树在每个节点（不一定是叶子节点）都会得到一个预测值。</strong> 该预测值为属于这个节点所有人label值的平均值。构建回归树进行分割属性选择时按照回归树的属性选择方式。</p>
<h4 id="梯度迭代"><a href="#梯度迭代" class="headerlink" title="梯度迭代"></a>梯度迭代</h4><p><img src="/images/other_ML_knowledge/gbdt_alg.png" alt="GBDT"><br>GBDT学习过程如上图所示，主要流程是：</p>
<ol>
<li>初始化f0(x) = 0</li>
<li>对于m=1,2,..M<ol>
<li>计算之前所有树的残差</li>
<li>拟合残差学习一颗回归树（选用平方损失函数时，对其求偏导的负梯度等于残差近似值）</li>
<li>更新回归树集合（进行相加时通常会使用Shrinkage）</li>
</ol>
</li>
<li>M次迭代后，得到GBDT模型，为M个回归树的加性模型<br>从GBDT的算法过程可以发现 <strong>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，而当损失函数为平方损失函数时，梯度值正好等于这个残差值，也就是说残差向量都是它的全局最优方向，这就是 Gradient。</strong><br>比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。<h4 id="GBDT-工作实例"><a href="#GBDT-工作实例" class="headerlink" title="GBDT 工作实例"></a>GBDT 工作实例</h4>还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下所示结果：<br><img src="/images/other_ML_knowledge/decision_tree.jpg" alt="decision_tree"><br>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果：<br><img src="/images/other_ML_knowledge/boosting_tree.jpg" alt="decision_tree"><br>在GBDT的第一棵树中，(A,B)和(C,D)被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。<h5 id="GBDT优点"><a href="#GBDT优点" class="headerlink" title="GBDT优点"></a>GBDT优点</h5>正在上述例子中，决策树和GBDT可以达到同样的效果，而GBDT的优点就是减小 <strong>过拟合问题。</strong><br>图1为了达到100%精度使用了3个feature（上网时长、时段、网购金额），其中分枝“上网时长&gt;1.1h” 很显然已经过拟合了，这个数据集上A,B也许恰好A每天上网1.09h, B上网1.05小时，但用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的；<br>相对来说图2的boosting虽然用了两棵树 ，但其实只用了2个feature就搞定了，后一个feature是问答比例，显然图2的依据更靠谱。 <strong>Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。</strong><h4 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h4>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即<h6 id="没用Shrinkage"><a href="#没用Shrinkage" class="headerlink" title="没用Shrinkage"></a>没用Shrinkage</h6>y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) =  y真实值 - y(1 ~ i)<br>y(1 ~ i) = SUM(y1, …, yi)<h6 id="使用Shrinkage"><a href="#使用Shrinkage" class="headerlink" title="使用Shrinkage"></a>使用Shrinkage</h6>Shrinkage不改变第一个方程，只把第二个方程改为：<br>y(1 ~ i) = y(1 ~ i-1) + step <em> yi<br>**Shrinkage对于残差学习出来的结果，只累加一小部分（step</em>残差）逐步逼近目标，step一般都比较小，如0.01~0.001，导致各个树的残差是渐变的而不是陡变的。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight**。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。<h4 id="GBDT的适用范围"><a href="#GBDT的适用范围" class="headerlink" title="GBDT的适用范围"></a>GBDT的适用范围</h4></li>
</ol>
<ul>
<li>GBDT 可以适用于回归问题（线性和非线性）；</li>
<li>GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。<h4 id="GBDT和随机森林比较"><a href="#GBDT和随机森林比较" class="headerlink" title="GBDT和随机森林比较"></a>GBDT和随机森林比较</h4><h5 id="GBDT和随机森林的相同同点"><a href="#GBDT和随机森林的相同同点" class="headerlink" title="GBDT和随机森林的相同同点"></a>GBDT和随机森林的相同同点</h5></li>
<li>都是由多棵树组成，最终的结果都由多棵树共同决定。<h5 id="GBDT和随机森林的不同点"><a href="#GBDT和随机森林的不同点" class="headerlink" title="GBDT和随机森林的不同点"></a>GBDT和随机森林的不同点</h5></li>
<li>组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；</li>
<li>组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；</li>
<li>对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；</li>
<li>随机森林对异常值不敏感，GBDT 对异常值非常敏感；</li>
<li>随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；<h4 id="GBDT可调参数"><a href="#GBDT可调参数" class="headerlink" title="GBDT可调参数"></a>GBDT可调参数</h4><h5 id="框架参数"><a href="#框架参数" class="headerlink" title="框架参数"></a>框架参数</h5></li>
</ul>
<ol>
<li>n_estimators：基学习器的个数</li>
<li>learning_rate: 即每个基学习器的缩减权重。</li>
<li>subsample：不放回子采样</li>
<li>init：初始弱学习器</li>
<li>loss：损失函数，对于分类模型可以选择对数损失和指数损失。对于回归模型有均方差，绝对损失等<h5 id="基学习器参数"><a href="#基学习器参数" class="headerlink" title="基学习器参数"></a>基学习器参数</h5></li>
<li>max_feature：划分时考虑的最大特征数（类似于随机森林的随机特征选择）</li>
<li>max_depth：决策树最大深度，可以不输入，但这样在构建子树时不会限制子树的深度。通常情况下推荐限制该值</li>
<li>min_sample_split：内部节点再划分所需要最小样本数</li>
<li>min_samples_leaf：叶节点最少样本数<br>…<h4 id="GBDT常见问题"><a href="#GBDT常见问题" class="headerlink" title="GBDT常见问题"></a>GBDT常见问题</h4></li>
<li>GBDT 相比于决策树有什么优点<br>泛化性能更好！GBDT 的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于 0。这样后面就更加专注于那些分错的样本。</li>
<li>Gradient 体现在哪里？<br>可以理解为残差是全局最优的绝对方向，类似于求梯度。</li>
</ol>
<h2 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h2><p>目前最好最快的boosting tree</p>
<p>如果不考虑工程实现、解决问题上的一些差异，xgboost与gbdt比较大的不同就是目标函数的定义。<br><img src="/images/other_ML_knowledge/xgboost_fomula.jpg" alt="xgboost_fomula"><br>注：红色箭头指向的l即为损失函数；红色方框为正则项，包括L1、L2正则项；红色圆圈为常数项。xgboost将损失函数做二阶泰勒展开做一个近似，我们可以看到，最终的目标函数只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。</p>
<h4 id="原理详解"><a href="#原理详解" class="headerlink" title="原理详解"></a>原理详解</h4><p><img src="/images/other_ML_knowledge/xgboost_math1.jpg" alt="gbdt_math"><br>对GBDT的目标函数（平方损失）进行泰勒2阶展开，得到如上图推导过程。相当于对每个叶节点的数据计算一阶二阶导数。</p>
<p><img src="/images/other_ML_knowledge/xgboost_math2.png" alt="gbdt_math2"><br>如上图所示，然后obj(t)对w求偏导得0，最后得到obj(t)的表示。wi为当前树对数据i的预测值。<br>推导上述目标函数公式后，单棵决策树的学习过程如下：</p>
<ol>
<li>枚举所有可能的树结构q（<strong>xgboost对该步骤进行了并行</strong>）</li>
<li>用上述目标函数为每个q计算目标函数分数，分数越小说明对应的树结构越好</li>
<li>根据上一步的结果，找到最佳树结构，并计算每个叶节点的预测值wj<h4 id="xgboost的并行"><a href="#xgboost的并行" class="headerlink" title="xgboost的并行"></a>xgboost的并行</h4>xgboost支持并行。xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<h4 id="xgboost对比GBDT"><a href="#xgboost对比GBDT" class="headerlink" title="xgboost对比GBDT"></a>xgboost对比GBDT</h4></li>
</ol>
<ul>
<li>GBDT缺点明显：boost是一个串行过程，不好并行化，计算复杂度高。而XGB在每一个基决策树的构造时并行遍历所有可能的树，计算obj(t)的值找到最有树。</li>
<li>GBDT是直接拟合负梯度（等于残差），xgboost对目标函数进行泰勒展开，引入二阶导。</li>
<li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>
<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</li>
<li>两者都用到了Shrinkage（缩减），</li>
<li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>
</ul>
<h2 id="lightgbm"><a href="#lightgbm" class="headerlink" title="lightgbm"></a>lightgbm</h2><p>GBDT 虽然是个强力的模型，但却有着一个致命的缺陷，不能用类似 mini batch 的方式来训练，需要对数据进行无数次的遍历。如果想要速度，就需要把数据都预加载在内存中，但这样数据就会受限于内存的大小；如果想要训练更多的数据，就要使用外存版本的决策树算法。虽然外存算法也有较多优化，SSD 也在普及，但在频繁的 IO 下，速度还是比较慢的。为了能让 GBDT 高效地用上更多的数据，我们把思路转向了分布式 GBDT， 然后就有了 LightGBM。设计的思路主要是两点，1. 单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据；2.多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。</p>
<ol>
<li>histogram决策树算法替换了传统的Pre-Sorted，histogram在内存消耗和计算代价上优势不小</li>
<li>带有深度限制的按叶子生长算法代替了传统的决策树生长策略，提升精度同时避免过拟合。</li>
</ol>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Dongfang Li WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Dongfang Li Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/18/Hadoop/Hadoop入门/" rel="next" title="Hadoop入门">
                <i class="fa fa-chevron-left"></i> Hadoop入门
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/27/Reading/SICP第二章笔记/" rel="prev" title="SICP第二章:构造数据抽象">
                SICP第二章:构造数据抽象 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dongfang Li</p>
              <p class="site-description motion-element" itemprop="description">My blog about programming and machine learning</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/YHfeather" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/dongfang.li.790" target="_blank" title="FB Page">
                      
                        <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树"><span class="nav-number">1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#划分选择"><span class="nav-number">1.1.</span> <span class="nav-text">划分选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#信息熵"><span class="nav-number">1.1.0.1.</span> <span class="nav-text">信息熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益"><span class="nav-number">1.1.0.2.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#增益率"><span class="nav-number">1.1.0.3.</span> <span class="nav-text">增益率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3决策树算法"><span class="nav-number">1.1.0.4.</span> <span class="nav-text">ID3决策树算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ID4-5决策树算法"><span class="nav-number">1.1.0.5.</span> <span class="nav-text">ID4.5决策树算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基尼系数"><span class="nav-number">1.1.0.6.</span> <span class="nav-text">基尼系数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树剪枝"><span class="nav-number">1.2.</span> <span class="nav-text">决策树剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#预剪枝"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">预剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#后剪枝"><span class="nav-number">1.2.0.2.</span> <span class="nav-text">后剪枝</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#连续值和缺失值"><span class="nav-number">1.3.</span> <span class="nav-text">连续值和缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#连续值"><span class="nav-number">1.3.0.1.</span> <span class="nav-text">连续值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺失值"><span class="nav-number">1.3.0.2.</span> <span class="nav-text">缺失值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多变量决策树"><span class="nav-number">1.4.</span> <span class="nav-text">多变量决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART树"><span class="nav-number">1.5.</span> <span class="nav-text">CART树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaboost"><span class="nav-number">1.6.</span> <span class="nav-text">Adaboost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT"><span class="nav-number">1.7.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归决策树"><span class="nav-number">1.7.0.1.</span> <span class="nav-text">回归决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度迭代"><span class="nav-number">1.7.0.2.</span> <span class="nav-text">梯度迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT-工作实例"><span class="nav-number">1.7.0.3.</span> <span class="nav-text">GBDT 工作实例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT优点"><span class="nav-number">1.7.0.3.1.</span> <span class="nav-text">GBDT优点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shrinkage"><span class="nav-number">1.7.0.4.</span> <span class="nav-text">Shrinkage</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#没用Shrinkage"><span class="nav-number">1.7.0.4.0.1.</span> <span class="nav-text">没用Shrinkage</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#使用Shrinkage"><span class="nav-number">1.7.0.4.0.2.</span> <span class="nav-text">使用Shrinkage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT的适用范围"><span class="nav-number">1.7.0.5.</span> <span class="nav-text">GBDT的适用范围</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT和随机森林比较"><span class="nav-number">1.7.0.6.</span> <span class="nav-text">GBDT和随机森林比较</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT和随机森林的相同同点"><span class="nav-number">1.7.0.6.1.</span> <span class="nav-text">GBDT和随机森林的相同同点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT和随机森林的不同点"><span class="nav-number">1.7.0.6.2.</span> <span class="nav-text">GBDT和随机森林的不同点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT可调参数"><span class="nav-number">1.7.0.7.</span> <span class="nav-text">GBDT可调参数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#框架参数"><span class="nav-number">1.7.0.7.1.</span> <span class="nav-text">框架参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基学习器参数"><span class="nav-number">1.7.0.7.2.</span> <span class="nav-text">基学习器参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT常见问题"><span class="nav-number">1.7.0.8.</span> <span class="nav-text">GBDT常见问题</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost"><span class="nav-number">1.8.</span> <span class="nav-text">xgboost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原理详解"><span class="nav-number">1.8.0.1.</span> <span class="nav-text">原理详解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xgboost的并行"><span class="nav-number">1.8.0.2.</span> <span class="nav-text">xgboost的并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xgboost对比GBDT"><span class="nav-number">1.8.0.3.</span> <span class="nav-text">xgboost对比GBDT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lightgbm"><span class="nav-number">1.9.</span> <span class="nav-text">lightgbm</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongfang Li</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://blackfeather.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/04/21/Machine Learning/各种Boosting树/';
          this.page.identifier = '2018/04/21/Machine Learning/各种Boosting树/';
          this.page.title = '机器学习中的树算法总结';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://blackfeather.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
