<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>SVD分解在推荐系统中的应用</title>
      <link href="/2018/08/25/Machine%20Learning/SVD%E5%88%86%E8%A7%A3%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/08/25/Machine%20Learning/SVD%E5%88%86%E8%A7%A3%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>整理SVD在推荐系统中的应用<br><a id="more"></a></p><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>我们知道，任何一个’M * N’的矩阵，可以被分解成三个矩阵的乘积：</p><ol><li>U矩阵：M行M列的列正交矩阵</li><li>S矩阵：M*N的对角矩阵，矩阵元素非负</li><li>V矩阵：N<em>N的正交矩阵的倒置<br>即 A = U </em> S * V’</li></ol><h2 id="SVD在推荐系统中"><a href="#SVD在推荐系统中" class="headerlink" title="SVD在推荐系统中"></a>SVD在推荐系统中</h2><p>我们知道，在推荐系统中，user-item表是非常关键的数据，这个二维表存储了每个用户对每个item的喜欢程度。而在实际应用中，user-item表往往是一个极其巨大，且极其稀疏的表。而这个稀疏性往往也导致推荐系统很难直接应用user-item数据。<br>通过SVD分解，可以将一个大的矩阵降维，进行有损压缩。</p><h3 id="SVD对user-item矩阵进行分解"><a href="#SVD对user-item矩阵进行分解" class="headerlink" title="SVD对user-item矩阵进行分解"></a>SVD对user-item矩阵进行分解</h3><p>通过上文对SVD分解的定义，对于user-item矩阵A，我们可以将其拆分成三个矩阵的乘积。<br>其中S矩阵，是一个对角矩阵，对角上的每个值就是特征值， <strong>特征值用来表示该矩阵向着该特征值对应的特征向量方向的变化权重。</strong> S矩阵对角线上的值依次减小。值越大，说明矩阵在该特征向量方向上变化越大，该特征向量也就越大。也就是说，我们可以选择前k个大的特征值和特征向量来表示原始矩阵，抛弃剩余的小特征值以及对应的特征向量。这样虽然损失了一定的信息，但大大压缩了矩阵的大小。<br>举例：</p><blockquote><p>user-item矩阵A大小为6<em>4， 经过SVD分解，得到三个矩阵U= 6\</em>6，S= 6*4，V=4*4。通过对S矩阵的观察发现，S对角上前两个值特别大，所以选择k=2对矩阵进行压缩，压缩后的形状为：U= 6*2，S= 2*2，V= 2*4。</p></blockquote><p>得到压缩后的三个矩阵后，将三个矩阵相乘，可以得到压缩后的user-item矩阵。经过对比可以发现，原矩阵和压缩后的矩阵每个元素都非常接近。</p><h3 id="SVD矩阵数据相关性"><a href="#SVD矩阵数据相关性" class="headerlink" title="SVD矩阵数据相关性"></a>SVD矩阵数据相关性</h3><p>如上文例子所示，原始的user-item矩阵大小为6<em>4，即一共有6个user和4个item。经过SVD分解，产生的矩阵U=6</em>2，V=2*4。也就是说，矩阵U的每一行可以表示一个user，矩阵V的每一行表示一个item。<br>可以在坐标系中画出每个user和item的位置。</p><p><img src="/images/recommender_practice/svd_graph.png" alt="svd_graph"><br>从图中可以看出，Season5和Season6距离很近，用户Ben和用户Fred距离也很近。</p><p>至此我们通过SVD分解，将每个user和item的特征向量表示压缩到更小的维度空间，然后计算user之间或者item之间的相似度，进行推荐。</p><h2 id="SVD推荐流程"><a href="#SVD推荐流程" class="headerlink" title="SVD推荐流程"></a>SVD推荐流程</h2><ol><li>将原始的user-item矩阵进行SVD分解，得到三个矩阵U，S，V</li><li>选定压缩维度k，将对角矩阵S进行压缩。</li><li>矩阵U和矩阵V也根据压缩维度k进行对应的压缩</li><li>压缩后的矩阵U每一行都代表着一个用户的特征向量</li><li>矩阵V的每一列都表示一个item的特征向量</li><li>得到特征向量后，可以在用户之间或者item之间计算相似度，进而完成推荐任务</li></ol><h2 id="应用SVD的注意事项"><a href="#应用SVD的注意事项" class="headerlink" title="应用SVD的注意事项"></a>应用SVD的注意事项</h2><ol><li>在user-item矩阵较大时，SVD的时间消耗很大，可以使用梯度下降等方法进行近似计算，减少时间消耗。</li><li>SVD分解后，仍然需要进行相似度的计算，各种相似度计算方法的选择直接影响模型准确率。</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python变量作用域</title>
      <link href="/2018/08/21/Programming%20Language/Python%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F/"/>
      <url>/2018/08/21/Programming%20Language/Python%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F/</url>
      <content type="html"><![CDATA[<p>Python的作用域和其他常见的编程语言作用域不同，特记录于此。</p><a id="more"></a><h2 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(5):</span><br><span class="line">    res = 0</span><br><span class="line">print(i)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p>在上段代码中，在如C++和Java语言中，两个print会报错，很简单，因为变量i和变量res都是for循环体内的局部变量，外部无法引用。<br>而在Python中：</p><ol><li>能改变作用域的代码段是：’def’, ‘class’, ‘lambda’</li><li>‘if/elif/else’, ‘try/except/finally’, ‘for/while’并不能涉及作用域的更改，也就是代码块中的变量在外部也是可以访问的。</li><li>变量的搜索路径是：本地变量-&gt;全局变量<br>在Python中，以下代码可以正常运行：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    newvar=8</span><br><span class="line">    print(newvar)</span><br><span class="line">    break;</span><br><span class="line">print(newvar)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    newlocal=7</span><br><span class="line">    raise Exception</span><br><span class="line">except:</span><br><span class="line">    print(newlocal)</span><br></pre></td></tr></table></figure></li></ol><h2 id="变量搜索过程"><a href="#变量搜索过程" class="headerlink" title="变量搜索过程"></a>变量搜索过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def scopetest():</span><br><span class="line">    var=6;</span><br><span class="line">    print(var)#</span><br><span class="line">    def innerFunc():</span><br><span class="line">        print(var)#look here</span><br><span class="line">    innerFunc()</span><br><span class="line"></span><br><span class="line">var=5</span><br><span class="line">print(var)</span><br><span class="line">scopetest()</span><br><span class="line">print(var)</span><br></pre></td></tr></table></figure><p>输出结果：5 6 6 5<br>根据调用顺序反向搜索，先本地变量再全局变量，例如搜先在innerFunc中搜索本地变量，没有，好吧，找找调用关系上一级scopetest，发现本地变量var=6，所以打印var=6.</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Programming Language </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LFM隐语义模型</title>
      <link href="/2018/08/21/Machine%20Learning/LFM%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/08/21/Machine%20Learning/LFM%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>隐语义模型LFM和LSI，LDA，Topic Model其实都属于隐含语义分析技术，本质上是找出潜在的主题或分类。这些技术一开始都是在文本挖掘领域中提出来的，近些年它们也被不断应用到其他领域中，并得到了不错的应用效果。比如，在推荐系统中它能够基于用户的行为对item进行自动聚类，也就是把item划分到不同类别/主题，这些主题/类别可以理解为用户的兴趣。</p><a id="more"></a><h2 id="商品分类问题"><a href="#商品分类问题" class="headerlink" title="商品分类问题"></a>商品分类问题</h2><p>在推荐系统中，最直接的想法就是将物品分类，如果用户经常购买某个类别的商品，就可以向其推荐该类别的其他商品。但在实际应用中，“分类”本身就是一个比较难的东西，因为同一样物品，根据分类粒度不同，可以被分到多种类别中。<br>比如说，一本书《离散数学》，既可以被分到数学类，也可以被分到计算机类，或者按照出版社分类，按照作者分类。都是可以的，而用户可能对其中的一类感兴趣，比如说用户A特别喜欢这个作者的书，但其实对这类书不感兴趣。这就让“人工分类”变得非常难。</p><h3 id="两个问题"><a href="#两个问题" class="headerlink" title="两个问题"></a>两个问题</h3><ol><li>我们可以根据用户的数据对喜欢物品进行归类，但不等于该用户就只喜欢这几类类，对其他类别的商品就一点兴趣也没有。也就是说，我们需要了解用户对于所有类别的兴趣度。</li><li>对于一个给定的类来说，我们需要确定这个类中每个商品属于该类别的权重。权重有助于我们确定该推荐哪些商品给用户。</li></ol><h2 id="LFM"><a href="#LFM" class="headerlink" title="LFM"></a>LFM</h2><p>对于一个给定的用户行为数据集（数据集包含的是所有的user, 所有的item，以及每个user有过行为的item列表），使用LFM对其建模后，我们可以得到如下图所示的模型：（假设数据集中有3个user, 4个item, LFM建模的分类数为4）<br><img src="/images/recommender_practice/lfm_matrix.png" alt="lfm_matrix"></p><h3 id="LFM效果"><a href="#LFM效果" class="headerlink" title="LFM效果"></a>LFM效果</h3><p>在使用了LFM后：</p><ol><li>不需要手动对物品进行分类，分类结果是基于用户行为统计自动聚类的</li><li>不需要关注分类粒度，通过设置LFM的最终分类数可以自动控制粒度</li><li>对于一个item，不需要明确划分到某各类，而是计算属于各个类的权值，是一种软分类</li><li>对于一个user，可以得到他对每一个类的兴趣度，而不是仅关心几个类</li><li>对于每个class，可以得到类中每个item的权重，越能代表这个class，这个item 的权重就越高。</li></ol><h3 id="LFM参数计算"><a href="#LFM参数计算" class="headerlink" title="LFM参数计算"></a>LFM参数计算</h3><p>有了如上定义，接下来的问题就是如何计算矩阵P和矩阵Q中的参数值。一般做法是最优化损失函数。<br>在数据集上， 我们假定用户其有过行为的item为正样本，规定兴趣度RUI=1，没有行为的item为负样本，兴趣度RUI=0。<br><img src="/images/recommender_practice/lfm_loss_function.png" alt="lfm_loss_function"></p><h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def LFM(user_item, F, N, alpha, lambda_value):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    user_item 矩阵记录所有user对所有item的行为</span><br><span class="line">    F 表示分类数</span><br><span class="line">    N 表示更新迭代次数</span><br><span class="line">    alpha 学习速率</span><br><span class="line">    lambda_value 表示正则化参数</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 初始化P，Q矩阵</span><br><span class="line">    [P, Q] = InitModel(user_item, F)</span><br><span class="line">    # 开始更新迭代</span><br><span class="line">    for step in range(0, N):</span><br><span class="line">        # 从数据集中依次取出一个user和该user所有喜欢的item集合</span><br><span class="line">        for user, items in user_item.items():</span><br><span class="line">            # 从该user为发生行为的item中随机采样，取得item的正样本和负样本</span><br><span class="line">            samples = randomSelectSamples(items)</span><br><span class="line">            # sample为一个字典，key是各个item，value是该user对该item的rui，也就是喜爱程度</span><br><span class="line">            for item, rui in samples.items():</span><br><span class="line">                # 计算LFM对user和item的预测结果，并计算误差</span><br><span class="line">                error = rui - predict(P[user,:], Q[:,item])</span><br><span class="line">                # 更新参数</span><br><span class="line">                for f in range(0, F):</span><br><span class="line">                    P[user][f] += alpha * (eui * Q[f][item] - lambda_value * P[user][f])</span><br><span class="line">                    Q[f][item] += alpha * (eui * P[user][f] - lambda_value * Q[f][item])</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python中的Magic Method</title>
      <link href="/2018/08/16/Programming%20Language/Python%E4%B8%AD%E7%9A%84Magic%20Method/"/>
      <url>/2018/08/16/Programming%20Language/Python%E4%B8%AD%E7%9A%84Magic%20Method/</url>
      <content type="html"><![CDATA[<p>在Python中，所有以“__”双下划线包起来的方法，都统称为“Magic Method”</p><a id="more"></a><h2 id="对象构造和初始化"><a href="#对象构造和初始化" class="headerlink" title="对象构造和初始化"></a>对象构造和初始化</h2><h3 id="new-方法"><a href="#new-方法" class="headerlink" title="new 方法"></a><strong>new</strong> 方法</h3><p>当我们创建一个对象时，最开始调用的并不是 <strong>init</strong> 而是 <strong>new</strong> 方法，该方法会创建类并返回这个类的实例。</p><h3 id="init-self"><a href="#init-self" class="headerlink" title="init(self,)"></a><strong>init</strong>(self,)</h3><p>在new方法被调用并创建一个实例后，init函数会将参数传入到实例中，初始化该实例。</p><h3 id="del-self"><a href="#del-self" class="headerlink" title="del(self)"></a><strong>del</strong>(self)</h3><p>当对象生命周期结束时，del方法会被调用</p><h2 id="控制属性访问"><a href="#控制属性访问" class="headerlink" title="控制属性访问"></a>控制属性访问</h2><p>由于Python语言没办法定义私有变量，但Python可以通过magic method完成封装</p><h3 id="getattr-self-name"><a href="#getattr-self-name" class="headerlink" title="getattr(self, name)"></a><strong>getattr</strong>(self, name)</h3><p>获取输入参数对应变量的值</p><h3 id="setattr-self-name-value"><a href="#setattr-self-name-value" class="headerlink" title="setattr(self, name, value)"></a><strong>setattr</strong>(self, name, value)</h3><p>set方法，需要注意的是，setattr无论属性是否存在，它都允许赋值。</p><h2 id="可调用对象"><a href="#可调用对象" class="headerlink" title="可调用对象"></a>可调用对象</h2><h3 id="call-self-args"><a href="#call-self-args" class="headerlink" title="call(self, [args..])"></a><strong>call</strong>(self, [args..])</h3><p>call允许一个类的实例像函数一样被调用</p><h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h2><p>有时候，尤其是当你在处理可变对象时，你可能想要复制一个对象，然后对其做出一些改变而不希望影响原来的对象。这就是Python的copy所发挥作用的地方。</p><h3 id="copy-self"><a href="#copy-self" class="headerlink" title="copy(self)"></a><strong>copy</strong>(self)</h3><p>定义了当对你的类的实例调用copy.copy()时所产生的行为。copy.copy()返回了你的对象的一个浅拷贝——这意味着，当实例本身是一个新实例时，它的所有数据都被引用了——例如，当一个对象本身被复制了，它的数据仍然是被引用的（因此，对于浅拷贝中数据的更改仍然可能导致数据在原始对象的中的改变）。</p><h3 id="deepcopy-self-memodict"><a href="#deepcopy-self-memodict" class="headerlink" title="deepcopy(self, memodict={})"></a><strong>deepcopy</strong>(self, memodict={})</h3><p>定义了当对你的类的实例调用copy.deepcopy()时所产生的行为。copy.deepcopy()返回了你的对象的一个深拷贝——对象和其数据都被拷贝了。memodict是对之前被拷贝的对象的一个缓存——这优化了拷贝过程并且阻止了对递归数据结构拷贝时的无限递归。当你想要进行对一个单独的属性进行深拷贝时，调用copy.deepcopy()，并以memodict为第一个参数。</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Programming Language </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Actor Critic</title>
      <link href="/2018/08/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BActor%20Critic/"/>
      <url>/2018/08/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BActor%20Critic/</url>
      <content type="html"><![CDATA[<p>Actor Critic 是目前来说表现最好的强化学习方法， 它结合了value_based和policy_based两种模型的特点。<br><a id="more"></a><br>我们知道value_based算法有一个缺点就是不能对连续值进行预测，只能预测每一个状态每一个动作对应的Q值。而传统 policy_based 虽然可以对连续值进行预测，却是回合更新，没办法按步更新，回合更新的学习小于要低很多。</p><h2 id="Actor和Critic"><a href="#Actor和Critic" class="headerlink" title="Actor和Critic"></a>Actor和Critic</h2><p>我们可以用两个不同的神经网络构造Actor-Critic，一个网络是Actor（Policy Network），一个网络是Critic（Q_Learning）。<strong>Actor_Net用于实际做各种动作，输入状态，预测出每个动作的概率。Critic Net会执行的动作的好坏进行预测，并将预测结果告知Actor Net，然后Actor Net根据Critic Net的结果计算loss函数，进而更新模型。</strong> 如果这个状态-动作是一个有益的动作就加大幅度更新，否则减小更新。</p><p>Critic Net输入状态，输出是每个动作的值，然后根据Q_Learning的Q值更新方法来计算Actor Net在状态s执行的动作a的Q(s, a)。并把这个值传递给Actor Net。<br>Critic Net传递给Actor Net的值就是Policy Network中的loss function：’log(poss)*f(s,a)’中的f(s,a)表示在当前状态s下执行动作a的优劣。</p><p>一句话概括Actor Critic方法：<strong>结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.</strong></p><h3 id="优势和劣势"><a href="#优势和劣势" class="headerlink" title="优势和劣势"></a>优势和劣势</h3><p>相比于传统policy gradient方法，Actor Critic方法的优势是：可以进行单步更新，比传统policy gradient要快。<br>劣势：取决于Critic的价值判断，但Critic难收敛，再加上actor的更新会更难收敛。<br>基于上面的劣势，提出了新的方法DDPG</p><h2 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient(DDPG)"></a>Deep Deterministic Policy Gradient(DDPG)</h2><p>因为Actor-Critic是在连续状态上进行更新，状态之间的相关性很高，会导致模型在连续动作上无法学习的问题。DeepMind团队将Actor-Critic和DQN结合到一起，成功解决了在连续动作预测上学不到东西的问题。</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>利用字典树过滤敏感词</title>
      <link href="/2018/08/11/Algorithms/%E5%88%A9%E7%94%A8%E5%AD%97%E5%85%B8%E6%A0%91%E8%BF%87%E6%BB%A4%E6%95%8F%E6%84%9F%E8%AF%8D/"/>
      <url>/2018/08/11/Algorithms/%E5%88%A9%E7%94%A8%E5%AD%97%E5%85%B8%E6%A0%91%E8%BF%87%E6%BB%A4%E6%95%8F%E6%84%9F%E8%AF%8D/</url>
      <content type="html"><![CDATA[<p>字典树，又称单词查找树，广泛应用于搜索引擎的词频统计和敏感词过滤。字典树的原理是：利用字符串的公共前缀来减少查询时间，最大限度的减少无谓的比较，运行效率很高。</p><a id="more"></a><h2 id="字典树特性"><a href="#字典树特性" class="headerlink" title="字典树特性"></a>字典树特性</h2><ol><li>树形结构，每个节点有多个子节点</li><li>每个节点仅仅保存一个字符</li><li>根节点不包含任何字符</li><li>节点表示的字符串是从根节点到该节点所经历节点路径对应字符连接在一起的字符串</li><li>每个节点子节点所包含的字符都不相同</li><li>每个路径保存的字符串不相同</li></ol><h2 id="算法运行原理"><a href="#算法运行原理" class="headerlink" title="算法运行原理"></a>算法运行原理</h2><ol><li>通过敏感词集合构建一个字典树，并在之后用该字典树进行敏感词过滤</li><li>创建字典树节点<ol><li>使用一个boolean变量isEnd表示该节点是否为子节点，也就是说该节点经过的路径是否一个完整的敏感词</li><li>每个节点包含一个Map成员表示所有子节点</li><li>对外提供方法</li></ol></li></ol><ul><li><p>节点类的java代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class TrieNode&#123;</span><br><span class="line">    public boolean isEnd = false; // 表示是否为叶节点</span><br><span class="line">    private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); // 该节点所有子节点的表示</span><br><span class="line">    public void addSubNode(Character key, TrieNode node)&#123; // 向指定位置添加子树</span><br><span class="line">        subNodes.put(key, node);</span><br><span class="line">    &#125;</span><br><span class="line">    public TrieNode getSubNode(Character key)&#123;</span><br><span class="line">        return subNodes.get(key);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>字典树类的java代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class TrimTree&#123;</span><br><span class="line">    private static final String DEFAULT_REPLACEMENT = &quot;NotAllow&quot;;</span><br><span class="line">    private TrieNode rootNode = new TrieNode();</span><br><span class="line">    //判断是否是一个符号</span><br><span class="line">    private boolean isSymbol(char c) &#123;</span><br><span class="line">        int ic = (int) c;</span><br><span class="line">        // 0x2E80-0x9FFF 东亚文字范围</span><br><span class="line">        return !((c &gt;= &apos;0&apos; &amp;&amp; c &lt;= &apos;9&apos;) || (c &gt;= &apos;a&apos; &amp;&amp; c &lt;= &apos;z&apos;)|| (c &gt;= &apos;A&apos; &amp;&amp; c &lt;= &apos;Z&apos;)) &amp;&amp; (ic &lt; 0x2E80 || ic &gt; 0x9FFF);</span><br><span class="line">    &#125;</span><br><span class="line">    public void addDirTreeNode(String textLine)&#123;</span><br><span class="line">        // 构建字典树方法， 根据输入字符串，逐步构建字典树</span><br><span class="line">    &#125;</span><br><span class="line">    public String filterWords(String text)&#123;</span><br><span class="line">        // 过滤文本中的敏感词</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>字典树里面有两个核心方法，一个是根据输入敏感字符串，构建字典树：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public void addDirTreeNode(String textLine)&#123;</span><br><span class="line">    // 构建字典树方法， 根据输入字符串，逐步构建字典树</span><br><span class="line">    if(textLine == null)&#123;return;&#125;</span><br><span class="line">    TrieNode tempNode = this.rootNode; // 临时节点指向根节点</span><br><span class="line">    for (int i = 0; i&lt;textLine.length(); i++) &#123;</span><br><span class="line">        char word = textLine.charAt(i);</span><br><span class="line">        if (isSymbol(c)) &#123;contiue;&#125; // 直接删除掉非法字符</span><br><span class="line">        TrieNode node = tempNode.getSubNode(c);</span><br><span class="line">        if (node == null) &#123; // 当前节点没有对应char的子节点</span><br><span class="line">            node = new TrieNode()</span><br><span class="line">            tempNode.addSubNode(c, node);</span><br><span class="line">        &#125;</span><br><span class="line">        tempNode = node;</span><br><span class="line">        if (i == textLine.length() - 1) &#123; // 当前敏感词已经遍历结束，将当前节点设为叶节点</span><br><span class="line">            tempNode.isEnd = true;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>另一个核心方法是对给定的一些输入文档，将文档中的敏感词过滤掉</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public String filterWords(String text)&#123;</span><br><span class="line">    // 过滤文本中的敏感词</span><br><span class="line">    if (text.trim().length == 0) &#123;return text;&#125;</span><br><span class="line">    StringBuilder result = new StringBuilder();</span><br><span class="line">    TrieNode tempNode = rootNode;</span><br><span class="line">    int begin = 0; // 回滚数</span><br><span class="line">    int position = 0; // 当前比较的位置</span><br><span class="line">    while (position &lt; text.length())&#123;</span><br><span class="line">        char c = text.charAt(position);</span><br><span class="line">        // 直接跳过空格</span><br><span class="line">        if (isSymbol(c)) &#123;</span><br><span class="line">            if (tempNode == rootNode) &#123;</span><br><span class="line">                result.append(c);</span><br><span class="line">                begin++;</span><br><span class="line">            &#125;</span><br><span class="line">            position++;</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        tempNode = tempNode.getSubNode(c);</span><br><span class="line">        if (tempNode == null) &#123; // 以begin开始的当前字符串匹配结束，并不是敏感词</span><br><span class="line">            result.append(text.charAt(begin));</span><br><span class="line">            // 调到下一个字符开始测试</span><br><span class="line">            position = begin + 1;</span><br><span class="line">            begin = position;</span><br><span class="line">            tempNode = rootNode; // 回到树的初始节点</span><br><span class="line">        &#125;else if (tempNode.isEnd) &#123; //从begin开始的字符串为敏感词，用replace替换掉</span><br><span class="line">            result.append(this.DEFAULT_REPLACEMENT);</span><br><span class="line">            position = position + 1;</span><br><span class="line">            begin = position;</span><br><span class="line">            tempNode = tempNode;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            position++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(text.substring(begin));</span><br><span class="line">    return result.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>从背包问题理解动态规划</title>
      <link href="/2018/07/28/Algorithms/%E4%BB%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>/2018/07/28/Algorithms/%E4%BB%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      <content type="html"><![CDATA[<p>动态规划是程序设计中非常重要的一个编程思想，基本上所有大厂的笔试面试都会遇到对动态规划的考察。而因为动态规划的思想和我们日常思考问题的角度不太相同，因此动态规划也一直是一个比较难掌握的思想。本文是对经典的入门教程：背包十讲的学习笔记。</p><a id="more"></a><p>动态规划最关键的解题步骤就是找到所有状态，以及状态转移方程。找到这两个东西，基本上问题也就解决了。</p><h2 id="背包问题基本定义"><a href="#背包问题基本定义" class="headerlink" title="背包问题基本定义"></a>背包问题基本定义</h2><p>背包问题主要是给定背包大小，可以将一些物品放入背包中，让背包中物品的总价值最大。这里定义背包类如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public class Knapsack&#123;</span><br><span class="line">    private int[] value; // 每个商品的价值</span><br><span class="line">    private int[] weight; // 每个商品所占的体积</span><br><span class="line">    private int[] nums;  // 每个商品的数量</span><br><span class="line">    private int variety;  // 商品种类数量</span><br><span class="line">    private int volume;  // 背包大小</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>背包问题涉及到的变量如上注释。</p><h2 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h2><p>01背包是最简单的背包问题，但其后所有复杂背包问题本质上都是对01背包问题的转化。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 件物品和一个容量为 V 的背包。放入第 i 件物品耗费的费用是 Ci，得到的 价值是 Wi。求解将哪些物品装入背包可使价值总和最大。</p><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>最基本的背包问题，特点是：<strong>每种物品仅有一件，可以选择放或者不放。</strong><br>直接上状态转移方程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX(table[i-1][j], table[i-1][j-Ci]+Wi)</span><br></pre></td></tr></table></figure></p><p>table[i][j]表示表示前 i 种物品恰放入一个容量为 j 的背包时，背包里物品总价值的最大值。<br>这样，状态转移方程就可以解释为：<strong>当前总价值的最大值在物品i在背包大小为j时的取值，等于不放入第i件物品(table[i-1][j])，和放入第i件物品(table[i-1][j-Ci]+Wi) 的最大值。</strong></p><p>若只考虑第 i 件物品的策略(放或不放)，那么就可以转化为一个只和前 i − 1 件物品相关的问题。<br>如果不放第 i 件物品，那么问题就转化为“前 i − 1 件物品放入容量为 v 的背包中”，价值为 F[i−1,v];<br>如果放第 i 件物品，那么问题就转化为“前 i − 1 件物品放入剩下的容量为v−Ci 的背包中”，<br>此时能获得的最大价值就是F[i−1,v−Ci]再加上通过放入第 i 件物品获得的价值 Wi</p><p>Java代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int zeroOnePackV1()&#123;</span><br><span class="line">    int[][] table = new int[variety+1][volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = 1; j&lt;=volume; j++) &#123;</span><br><span class="line">            table[i][j] = Math.max(table[i-1][j], table[i-1][j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume][variety];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="空间复杂度优化"><a href="#空间复杂度优化" class="headerlink" title="空间复杂度优化"></a>空间复杂度优化</h3><p>通过上面的状态转移方程我们发现，考察物品i时背包的价值仅取决于考察物品i-1时的各个背包大小时的值。所以，我们可以将原01背包的空间复杂度O(NV)优化成O(V)。<br>我们知道循环更新每个table[i][j]时需要用到之前的值table[i-1][j]和table[i-1][j-c]<br>所以可以通过从后向前遍历容量来计算table[v]，这样就可以保证在计算f[v]是f[v-c]是f[i-1, v-c]的值。</p><p>Java代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int zeroOnePackV2()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1;i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume;j&gt;=weight[i-1];j--) &#123;</span><br><span class="line">            table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 种物品和一个容量为 V 的背包，每种物品都有无限件可用。放入第 i 种物品 的费用是 Ci，价值是 Wi。 和01背包最大的不同是：<strong>每个物品可以使用无限次</strong></p><h3 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h3><p>仍然按照01背包的解题思路，状态转移方程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX&#123;table[i-1][j-k*Ci]+k*Wi&#125;   0&lt;=kCi&lt;=V</span><br></pre></td></tr></table></figure></p><p>将前i件物品放入大小为j的背包时的最大价值，就等于：1.不将第i件商品放入；2. 将第i件商品放入1,2,3,4…个的最大值。</p><h3 id="简单优化"><a href="#简单优化" class="headerlink" title="简单优化"></a>简单优化</h3><p>完全背包问题有一个很简单有效的优化，是这样的:若两件物品 i、j 满足 Ci ≤ Cj 且 Wi ≥ Vj，则将可以将物品 j 直接去掉，不用考虑。这个优化思想是非常显而易见的。</p><p>Java实现代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public int completePackV1()&#123;</span><br><span class="line">    int[] table = new int[volume];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume;j&gt;=weight[i-1];j-- ) &#123;</span><br><span class="line">            for (int k = 0; volume &gt; k*weight[i-1];k++ ) &#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-weight[i-1]*k] + value[i-1]*k);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面的代码用了三层循环，时间复杂度为O(NCK)，我们不妨转换一下思想，可以将时间复杂度变为O(VN).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int completePackV2()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = weight[i-1];j&lt;=volume ; j++) &#123;</span><br><span class="line">            table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个伪代码与 01 背包问题的伪代码只有 v 的循环次序不同而已。 为什么这个算法就可行呢?首先想想为什么 01 背包中要按照 v 递减的次序来循环。 让 v 递减是为了保证第 i 次循环中的状态 table[i][j] 是由状态 F[i−1][j−Ci] 递推而来。<br>换句话说，这正是为了保证每件物品只选一次，保证在考虑“选入第 i 件物品”这件策略时，依据的是一个绝无已经选入第 i 件物品的子结果 F [i − 1, v − Ci]。而现在完全背包的特点恰是每种物品可选无限件，所以在考虑“加选一件第 i 种物品”这种策略时， 却正需要一个可能已选入第 i 种物品的子结果 F [i, v − Ci ]，所以就可以并且必须采用 v 递增的顺序循环。这就是这个简单的程序为何成立的道理。<br>值得一提的是，上面的伪代码中两层 for 循环的次序可以颠倒。这个结论有可能会 带来算法时间常数上的优化。<br>这个算法也可以由另外的思路得出。例如，将基本思路中求解 table[i][j-Ci] 的状态转移方程显式地写出来，代入原方程中，会发现该方程可以等价地变形成这种形式:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX(table[i−1][j], table[i][j−Ci] + Wi)</span><br></pre></td></tr></table></figure></p><h2 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h2><p>有 N 种物品和一个容量为 V 的背包。第 i 种物品最多有 Mi 件可用，每件耗费的空间是 Ci，价值是 Wi。 与完全背包不同的是，<strong>每个物品的个数是有限的</strong></p><h3 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h3><p>这题目和完全背包问题很类似。基本的方程只需将完全背包问题的方程略微一改 即可。<br>因为对于第 i 种物品有 Mi +1 种策略:取 0 件，取 1 件……取 Mi 件。<br>状态转移方程:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX&#123;table[i-1][j-k*Ci]+k*Wi&#125;   0&lt;=k&lt;=Mi</span><br></pre></td></tr></table></figure></p><p>原始解法的Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public int multiplePackV1()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume; j&gt;=weight[i-1];j-- ) &#123;</span><br><span class="line">            for (int k = 0; volume&gt;weight[i-1] &amp;&amp; k&lt;nums[i-1];k++ ) &#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-k*weight[i-1]]+k*value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h3><p>仍然考虑二进制的思想，我们考虑把第 i 种物品换成若干件物品，使得原问题中第 i 种物品可取的每种策略——取 0…Mi 件——均能等价于取若干件代换以后的物品。 另外，取超过 Mi 件的策略必不能出现。<br>方法是:将第 i 种物品分成若干件 01 背包中的物品，其中每件物品有一个系数。这件物品的费用和价值均是原来的费用和价值乘以这个系数。令这些系数分别为 1,2,22 …2k−1,Mi −2k +1，且 k 是满足 Mi −2k +1 &gt; 0 的最大整数。例如，如果 Mi 为 13，则相应的 k = 3，这种最多取 13 件的物品应被分成系数分别为 1, 2, 4, 6 的四件物品。分成的这几件物品的系数和为 Mi，表明不可能取多于 Mi 件的第 i 种物品。另外 这种方法也能保证对于0…Mi 间的每一个整数，均可以用若干个系数的和表示。这里 算法正确性的证明可以分0…2k−1 和2k…Mi 两段来分别讨论得出。</p><h2 id="混合三种背包问题"><a href="#混合三种背包问题" class="headerlink" title="混合三种背包问题"></a>混合三种背包问题</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>如果将前面1、2、3中的三种背包问题混合起来。也就是说，有的物品只可以取一 次(01 背包)，有的物品可以取无限次(完全背包)，有的物品可以取的次数有一个上限<br>(多重背包)。</p><h3 id="01背包和完全背包混合"><a href="#01背包和完全背包混合" class="headerlink" title="01背包和完全背包混合"></a>01背包和完全背包混合</h3><p>考虑到 01 背包和完全背包中给出的伪代码只有一处不同，故如果只有两类物品: 一类物品只能取一次，另一类物品可以取无限次，那么只需在对每个物品应用转移方程 时，根据物品的类别选用顺序或逆序的循环即可，复杂度是 O(V N )。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public int mixturePackV1(boolean[] nums)&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for(int i = 1;i&lt;=variety; i++)&#123;</span><br><span class="line">        if(nums[i-1])&#123; // 当前物品只许取一次</span><br><span class="line">            for(int j = volume; j&gt;=weight[i-1];j--)&#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            for(int j = weight[i-1]; j&lt;=volume; j++)&#123;</span><br><span class="line">                table[i] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="加入多重背包"><a href="#加入多重背包" class="headerlink" title="加入多重背包"></a>加入多重背包</h3><p>很简单，只需要多加入一个判断即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fori ←1toN</span><br><span class="line">    if第 i 件物品属于01背包</span><br><span class="line">        ZeroOnePack(F,Ci,Wi)</span><br><span class="line">    else if 第 i 件物品属于完全背包</span><br><span class="line">        CompletePack(F,Ci,Wi)</span><br><span class="line">    else if 第 i 件物品属于多重背包</span><br><span class="line">        MultiplePack(F,Ci,Wi,Ni)</span><br></pre></td></tr></table></figure></p><h2 id="二维费用背包问题"><a href="#二维费用背包问题" class="headerlink" title="二维费用背包问题"></a>二维费用背包问题</h2><h3 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h3><p>对于每件物品，具有两种不同的费用，选择这件物品必 须同时付出这两种费用。对于每种费用都有一个可付出的最大值(背包容量)。<br>设第 i 件物品所需的两种费用分别为 Ci 和 Di。两种费用可付出的最大值(也即两 种背包容量)分别为 V 和 U。物品的价值为 Wi。</p><h3 id="解题思路-3"><a href="#解题思路-3" class="headerlink" title="解题思路"></a>解题思路</h3><p>费用加了一维，只需状态也加一维即可。设 F [i, v, u] 表示前 i 件物品付出两种费用<br>分别为 v 和 u 时可获得的最大价值。状态转移方程就是:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F[i,v,u] = max&#123;F[i − 1,v,u],F[i − 1,v − Ci,u − Di] + Wi&#125;</span><br></pre></td></tr></table></figure></p><p>如前述优化空间复杂度的方法，可以只使用二维的数组:当每件物品只可以取一次 时变量 v 和 u 采用逆序的循环，当物品有如完全背包问题时采用顺序的循环，当物品 有如多重背包问题时拆分物品。</p><h3 id="物品总个数的限制"><a href="#物品总个数的限制" class="headerlink" title="物品总个数的限制"></a>物品总个数的限制</h3><p>有时，“二维费用”的条件是以这样一种隐含的方式给出的:最多只能取 U 件物品。 这事实上相当于每件物品多了一种“件数”的费用，每个物品的件数费用均为 1，可以付出的最大件数费用为 U。换句话说，设 F[v,u] 表示付出费用 v、最多选 u 件时可得到的最大价值，则根据物品的类型(01、完全、多重)用不同的方法循环更新，最后在 f[0…V,0…U] 范围内寻找答案。</p><h2 id="分组背包问题"><a href="#分组背包问题" class="headerlink" title="分组背包问题"></a>分组背包问题</h2><h3 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 件物品和一个容量为 V 的背包。第 i 件物品的费用是 Ci，价值是 Wi。这些 物品被划分为 K 组，每组中的物品互相冲突，最多选一件。求解将哪些物品装入背包 可使这些物品的费用总和不超过背包容量，且价值总和最大。</p><h3 id="解题思路-4"><a href="#解题思路-4" class="headerlink" title="解题思路"></a>解题思路</h3><p>这个问题变成了每组物品有若干种策略:是选择本组的某一件，还是一件都不选。<br>也就是说设 F [k, v] 表示前 k 组物品花费费用 v 能取得的最大权值，则有:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F[k,v]=max&#123;F[k−1,v],F[k−1,v−Ci]+Wi |itemi∈groupk&#125;</span><br></pre></td></tr></table></figure></p><p>核心思想是：<strong>将该分组背包问题拆解为k个01背包问题</strong><br>使用一维数组的伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fork ←1toK</span><br><span class="line">    forv ←V to0</span><br><span class="line">        for all item i in group k</span><br><span class="line">            F[v] ←max&#123;F[v],F[v−Ci]+Wi&#125;</span><br></pre></td></tr></table></figure></p><h2 id="有依赖背包问题"><a href="#有依赖背包问题" class="headerlink" title="有依赖背包问题"></a>有依赖背包问题</h2><h3 id="问题描述-5"><a href="#问题描述-5" class="headerlink" title="问题描述"></a>问题描述</h3><p>这种背包问题的物品间存在某种“依赖”的关系。也就是说，物品 i 依赖于物品 j， 表示若选物品 i，则必须选物品 j。为了简化起见，我们先设没有某个物品既依赖于别 的物品，又被别的物品所依赖;另外，没有某件物品同时依赖多件物品。我们将不依赖于别的物品的物品称为“主件”，依赖于某主件的物品称为“附件”。</p><h2 id="泛化物品"><a href="#泛化物品" class="headerlink" title="泛化物品"></a>泛化物品</h2><h3 id="问题描述-6"><a href="#问题描述-6" class="headerlink" title="问题描述"></a>问题描述</h3><p>考虑这样一种物品，它并没有固定的费用和价值，而是它的价值随着你分配给它的 费用而变化。这就是泛化物品的概念。<br>更严格的定义之。在背包容量为V 的背包问题中，泛化物品是一个定义域为0…V 中的整数的函数 h，当分配给它的费用为 v 时，能得到的价值就是 h(v)。<br>这个定义有一点点抽象，另一种理解是一个泛化物品就是一个数组 h[0 . . . V ]，给它 费用 v，可得到价值 h[v]。</p>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>自制编程语言</title>
      <link href="/2018/07/27/Reading/%E8%87%AA%E5%88%B6%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
      <url>/2018/07/27/Reading/%E8%87%AA%E5%88%B6%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</url>
      <content type="html"><![CDATA[<p>本文是通过对《2週間でできる! スクリプト言語の作り方》这本书的学习，完成一个简单的编译器。</p><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>身在Programming Language Research Lab，虽然自身的研究是深度学习&amp;programming environment，但也有义务对PL有一定了解，虽然看了《SICP》，自己也试着写了一个特别小的Scheme解释器，对编程语言设计整体有了个大概的认识，但因为大三学的《编译原理》早早的就都还给了老师，所以对PL还是缺少一个清晰的认识。正好看到东工大编程语言研究室前任教授：千葉滋教授的编译器入门书籍：《2週間でできる! スクリプト言語の作り方》，而且书评不错，花时间把书中代码实现了一遍。</p><h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><p>本章是关于编程设计语言和语言处理器的一些基本概念。</p><h2 id="机器语言和汇编语言的区别"><a href="#机器语言和汇编语言的区别" class="headerlink" title="机器语言和汇编语言的区别"></a>机器语言和汇编语言的区别</h2><p>两者经常混淆，其实机器语言只由一串很长的01二进制组成，而汇编语言就是用来表示这串01数字的，便于理解。所以要想执行汇编语言程序仍然需要将其转换为机器语言。</p><h2 id="编译器和解释器的区别"><a href="#编译器和解释器的区别" class="headerlink" title="编译器和解释器的区别"></a>编译器和解释器的区别</h2><ul><li>解释器就是根据程序中的算法执行运算并返回结果。简单来说，计算器就是一个解释器，我们输入一串数字和运算符到计算器中，计算器返回计算结果，这就是个简单的解释器。</li><li>编译器能将某种语言的程序转换为另一种语言的程序，通常会转换为机器语言程序。</li><li>现代编程语言经常会混用二者，比如Java语言会首先通过编译器将源码转换为java二进制码，并将这种虚拟的语言保存在文件中。之后，Java虚拟机的解释器会执行这段代码。</li><li>大多数Java虚拟机为了提高性能，会在执行过程中通过编译器将一部分Java二进制代码直接转换为机器语言，在执行过程中进行的机器语言转换称为冬天编译或者JIT编译，转换后得到的机器语言程序将被载入内存，由硬件执行，不需要使用解释器。</li></ul><h2 id="语言处理器的架构"><a href="#语言处理器的架构" class="headerlink" title="语言处理器的架构"></a>语言处理器的架构</h2><p>无论是解释器还是编译器，语言处理器的前半部分大同小异，首先源码会进行词法分析，被分割成多个小字符串单元称为单词，之后处理器会执行语法分析，把单词排列转换为AST。之后编译器会将AST转换为其他语言，而解释器会一边分析AST，一边执行运算。</p><h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><p>本章是关于一种名叫Stone语言的语法设计。</p><h2 id="Stone语言语法"><a href="#Stone语言语法" class="headerlink" title="Stone语言语法"></a>Stone语言语法</h2><ol><li>包含数的四则运算</li><li>提供变量支持</li><li>基本的循环判断控制语句</li><li>动态语言（不需要显式声明变量类型）</li></ol><h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><p>本章是关于如何将源码分割成token，也是语言处理器的第一个组成部分：词法分析器（Lexer）</p><p>分为如下几步：</p><ol><li>定义token类</li><li>借助regex构建词法分析器</li><li>搭建UI</li></ol><h1 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h1><p>本章是语法分析，关于如何从sequence of token构建AST。</p><p>语法分析的主要任务是分析token之间的关系，如判断哪些token属于同一个表达式或语句，以及处理左右括号配对问题等。同时也会检查程序中是否含有语法错误。</p><h2 id="构建ASTree类"><a href="#构建ASTree类" class="headerlink" title="构建ASTree类"></a>构建ASTree类</h2><ol><li>构建抽象类ASTree，</li><li>并声明两个实现ASTLeaf和ASTList，前者表示AST的叶节点，后者表示AST的中间节点。</li><li>Name类继承自ASTLeaf，表示变量token</li><li>NameLiteral类继承自ASTLeaf，表示整数token</li><li>BinaryExpr类继承自ASTList，表示二元操作符</li></ol><ul><li>要构造AST，处理器首先需要知道会接收到那些token sequence，并确定希望构造出怎样的AST。语法规定了token的组合规则。本章讨论的语法较为浅显，进通过判断语句从哪个token开始，中途能够出现哪些token，又以什么token结束。</li><li>使用BNF巴科斯范式，与上下文无关文法等价。</li></ul><h2 id="BNF"><a href="#BNF" class="headerlink" title="BNF"></a>BNF</h2><p>BNF(Backus-Naur Form)是描述编程语言的文法。巴科斯范式是一种用于表示上下文无关文法的语言，上下文无关文法描述了一类形式语言。</p><p>自然语言存在不同程度的二义性。这种模糊、不确定的方式无法精确定义一门程序设计语言。必须设计一种准确无误地描述程序设计语言的语法结构，这种严谨、简洁、易读的形式规则描述的语言结构模型称为文法。</p><h4 id="BNF语法"><a href="#BNF语法" class="headerlink" title="BNF语法"></a>BNF语法</h4><p>&lt; &gt;     : 内包含的为必选项。<br>[ ]     : 内包含的为可选项。<br>{ }     : 内包含的为可重复0至无数次的项。<br>|       : 表示在其左右两边任选一项，相当于”OR”的意思。<br>::=     : 是“被定义为”的意思<br>“…”   : 术语符号<br>[…]   : 选项，最多出现一次<br>{…}   : 重复项，任意次数，包括 0 次<br>(…)   : 分组<br>|       : 并列选项，只能选一个</p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>Java语言总的for语句的BNF范式定义如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FOR_STATEMENT ::=</span><br><span class="line">    &quot;for&quot; &quot;(&quot; ( variable_declaration |</span><br><span class="line">    ( expression &quot;;&quot; ) | &quot;;&quot; )</span><br><span class="line">    [ expression ] &quot;;&quot;</span><br><span class="line">    [ expression ]</span><br><span class="line">    &quot;)&quot; statement</span><br></pre></td></tr></table></figure></p><h1 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h1><p>本章是关于如何设计语法分析器，即将token sequence与语法规则定义的模式进行匹配，并构造AST。</p><h4 id="Stone语言语法规则"><a href="#Stone语言语法规则" class="headerlink" title="Stone语言语法规则"></a>Stone语言语法规则</h4><p>利用BNF定义Stone语言的语法规则。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Java </tag>
            
            <tag> Project </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Dueling DQN</title>
      <link href="/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDueling%20DQN/"/>
      <url>/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDueling%20DQN/</url>
      <content type="html"><![CDATA[<p>对于传统的DQN，存在训练时间太长，收敛速度较慢的问题，Dueling DQN是一个新的方法来提升学习速度，甚至提高模型准确率。</p><a id="more"></a><h2 id="Dueling-DQN结构"><a href="#Dueling-DQN结构" class="headerlink" title="Dueling DQN结构"></a>Dueling DQN结构</h2><p>在传统的DQN中，输入一个state到Q_Net中，Q_Net会预测在该state每个动作的Q值大小。而在Dueling DQN中，Q_Net的输出会被分为两部分：1. 输入的state的价值；2. 输入state中每个动作的advantage。而Dueling DQN的Q值是用下面的公式计算得出：<br><a href="images/reinforce_learning/dueling_DQN.png">dueling_DQN</a><br>Dueling DQN这么设计的直观意义是对于某些state，在当前的state无论做什么动作对下一个state都没有多大影响，这个时候动作的Q值更多的受advantage的影响。具体解释请看这篇paper:<a href="https://arxiv.org/abs/1511.06581" target="_blank" rel="noopener"><br>Dueling Network Architectures for Deep Reinforcement Learning</a></p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>参考的<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-7-dueling-DQN/" target="_blank" rel="noopener">莫烦Python</a>中的例子，具体代码URL：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Dueling_DQN" target="_blank" rel="noopener">Dueling DQN</a></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Policy gradient</title>
      <link href="/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPolicy%20Gradients/"/>
      <url>/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPolicy%20Gradients/</url>
      <content type="html"><![CDATA[<p>我们已经知道DQN是一个基于价值value的方法。换句话说就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法。Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 那样学习每个 action 的 value, 而是给定每个action对应的概率，直接输出动作. 而且 Policy gradient 有一个优势是: 输出的这个 action 可以是一个连续的值, 而 value-based 方法输出的都是不连续的值。<br><a id="more"></a><br>举一个例子，车辆的速度选择，我们知道速度是一个连续值，而value-based是针对每一个动作给出reward进行学习，这样就不得不将速度离散化。而如果使用policy gradient方法则可以学习连续动作，直接给出车辆速度。</p><h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>想要使用梯度下降算法训练策略网络，就需要一个目标损失函数。策略网络使用策略梯度作为更新，具体的意思就是 <strong>直接改变动作的出现概率</strong>。仅从概率的角度来思考问题：我们有一个策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到reward，或者result。那么这个时候，我们有个非常简单的想法： <strong>如果某一个动作得到reward多，那么我们就使其出现的概率增大，如果某一个动作得到的reward少，那么我们就使其出现的概率减小。</strong></p><p>如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！通常情况下，使用对数似然来表示动作的评价指标。<br>因此，可以构造一个损失函数如下：<br><img src="images/reinforce_learning/policy_gradient_loss.png" alt="policy_gradient_loss"><br>这里f(s,a)表示最后的结果，也就是说一个游戏回合结束后，如果赢了，就认为这轮游戏中执行的每个动作都是好的，f(s,a)就为1，如果输了，每个动作都是不好的，f(s, a)为-1.</p><h2 id="Policy-Network"><a href="#Policy-Network" class="headerlink" title="Policy Network"></a>Policy Network</h2><p>什么是策略网络？Policy Network就是一个神经网络，输入是状态，输出直接就是动作（而不是Q值）。或者输出每个动作的概率。<br>以下是Policy Network的组成部分：</p><ol><li>一个神经网络（区别于DQN），网络的输入是状态，输出是每个动作的概率</li><li>损失函数时plicy gradient（log(poss)*f(s,a))</li><li>基于回合更新，也就是在整个回合结束后再更新，而不像value_based每一步一更新</li></ol><h2 id="Policy-Network工作流程"><a href="#Policy-Network工作流程" class="headerlink" title="Policy Network工作流程"></a>Policy Network工作流程</h2><ol><li>初始化神经网络模型</li><li>输入一个状态s到网络中，网络预测状态时每个动作的执行概率，并根据该概率返回一个动作a</li><li>将s和a输入到环境中，得到对应的reward，下一个状态s_，以及终止符t</li><li>如果t为未终止<ol><li>将[s, a, r]存入到网络中（可以理解为网络的记忆库）</li><li>将新的状态s_输入到网络中，重复第2步到第4步</li></ol></li><li>如果t为终止<ol><li>表示游戏结束，Policy Network根据之前存储的[s, a, r]数据对网络进行训练</li><li>重置环境，重复2~5步</li></ol></li></ol><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>使用OpenAI Gym的爬山小车作为环境，训练一个Policy Network模型进行训练。URL:<img src="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/policy%20gradient" alt="Policy Network"></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Prioritized Experience Replay DQN</title>
      <link href="/2018/07/24/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPrioritized%20Experience%20Preplay%20DQN/"/>
      <url>/2018/07/24/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPrioritized%20Experience%20Preplay%20DQN/</url>
      <content type="html"><![CDATA[<p>DQN是一个非常强大的模型，成功的将强化学习和深度学习结合到了一起，但在实际应用中，DQN往往会遇到很多问题，Prioritized Experience Replay即是一种用来解决DQN记忆库中样本不均衡的问题。<br><a id="more"></a></p><h2 id="记忆库样本不均衡问题"><a href="#记忆库样本不均衡问题" class="headerlink" title="记忆库样本不均衡问题"></a>记忆库样本不均衡问题</h2><p>想想一下个简单的RL场景：机器人在走迷宫。我们设定没有成功到达出口的奖励值为-1， 成功到达出口的奖励值为100，同时只有一个出口。这时我们会发现DQN的训练过程非常缓慢，因为在DQN的记忆库中，只有非常少的几条记忆对应的奖励值时100，而且因为传统DQN是从记忆库中随机采样，DQN会很难学习到这几条“有价值”的记忆。也就是说，因为记忆库中的正负样本不均衡，会导致DQN训练时间过长。</p><p>而Prioritized Replay通过更加重视这些少量的记忆样本，达到加快学习速度的目的。</p><h2 id="Prioritized-Replay原理"><a href="#Prioritized-Replay原理" class="headerlink" title="Prioritized Replay原理"></a>Prioritized Replay原理</h2><p>PR的重点在于训练Q_Net时，每次从记忆库中选取batch个记忆是不再使用随机抽样，而是按照Memory中的样本优先级来抽取，这样可以更加高效的进行学习。</p><h4 id="优先级"><a href="#优先级" class="headerlink" title="优先级"></a>优先级</h4><p>而样本优先级p的设定可以根据TD-error来计算，每次更新Q_Net时计算的误差值（具体计算方法请查看之前的DQN笔记或者Double DQN笔记）。TD-error值越大，说明Q_Net的估计误差越大，越需要用这个样本进行学习，优先级p也就越高。</p><h4 id="SumTree"><a href="#SumTree" class="headerlink" title="SumTree"></a>SumTree</h4><p>在有了优先级p的计算方式之后，下一个问题就是如何根据p进行采样。如果Q_Net每个batch的训练都对记忆库根据p进行排序会非常消耗计算力。<a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener">Prioritized Experience Replay</a>这篇论文提出了SumTree结构，可以不需要对记忆库中的记忆排序而获取高优先级的训练记忆。</p><p>SumTree是一个二叉树，节点保存一个int值，该值等于左右子树节点的和。每个叶节点表示一个记忆样本。容易推断出根节点的值是所有数据p值的和。<br>一个SumTree的例子如下：<br><a href="/images/reinforce_Learning/sumtree.png">SumTree</a></p><p>进行抽样时， 将p的总和（也就是根节点的值）除batch size。也就是将p值分为batch size个区间。拿上图举例, 假设batch size为6, 这时的区间拥有的 priority 可能是这样.<br>[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]<br>然后在每个区间里随机选取一个数. 比如在第区间 [21-28] 里选到了24, 就按照这个 24 从最顶上的42开始向下搜索. 首先看到最顶上 42 下面有两个 child nodes, 拿着手中的24对比左边的 child 29, 如果 左边的 child 比自己手中的值大, 那我们就走左边这条路, 接着再对比 29 下面的左边那个点 13, 这时, 手中的 24 比 13 大, 那我们就走右边的路, 并且将手中的值根据 13 修改一下, 变成 24-13 = 11. 接着拿着 11 和 13 左下角的 12 比, 结果 12 比 11 大, 那我们就选 12 当做这次选到的 priority, 并且也选择 12 对应的数据.</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里是一个应用Prioritized Replay DQN的小车爬山例子，代码来自于<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/" target="_blank" rel="noopener">莫烦Python</a>。代码的URL如下：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/DQN_Prioritied_Replay" target="_blank" rel="noopener">DQN with Prioritized Replay</a></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Double DQN</title>
      <link href="/2018/07/23/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDouble%20DQN/"/>
      <url>/2018/07/23/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDouble%20DQN/</url>
      <content type="html"><![CDATA[<p>在实际应用DQN的过程中，会出现各种问题，其中一个问题就是过估计（Overestimate），如果在DQN训练过程中试着输出Q值，可能发现Q值会非常大，这就是过估计。而Double DQN就是对传统DQN的一个改进方式，可以很好的解决过估计问题。<br><a id="more"></a></p><h2 id="过估计的原因"><a href="#过估计的原因" class="headerlink" title="过估计的原因"></a>过估计的原因</h2><p>在DQN中，我们知道是用Target_Net的输出值中的最大值作为Q_Learning更新公式中的Max(Q(s’, a’))来得到Q(s, a)的真实label值，进而和Q_Net的预测值做比较更新神经网络。而Target_Net本身就是一个没有训练完成的，包含误差的神经网络，这就导致了Q值的过估计。</p><h2 id="DoubleDQN的解决方法"><a href="#DoubleDQN的解决方法" class="headerlink" title="DoubleDQN的解决方法"></a>DoubleDQN的解决方法</h2><p>Double DQN的想法是引入另一个神经网络来打消这些最大误差的影响。而DQN中本身就有两个神经网络，所以我们直接Q_Net估计Target_Net中的最大动作值，然后用这个被Q_Net估计出来的动作选择Target_Net的Q(s’,).<br>对比一下DQN的更新方法和Double的更新方法会更好理解：</p><ul><li>传统DQN：gamma<em>Q_next = gamma </em> Max(Q(s’, a’))   //取Target_Net对s’状态预测的所有动作最大值用来更新</li><li>Double DQN：gamma<em>Q_next = gamma </em> Q(s’, argmax(Q_Net(s’, a’))) //先用Q_Net对s’状态进行估计，找到Q值最大的动作，再用Target_Net中对应的最大动作的值Target_Net(s’, a’)来更新。<br>Double DQN的更新公式也就变成了：<br><img src="/images/reinforce_learning/double_DQN_update.png" alt="double_DQN_update"></li></ul><h2 id="DoubleDQN更新流程"><a href="#DoubleDQN更新流程" class="headerlink" title="DoubleDQN更新流程"></a>DoubleDQN更新流程</h2><p>DoubleDQN在整体上和DQN没有任何区别，只是在训练Q_Net时计算真实label值进而计算误差反向传播时有区别，我们只着眼于这个区别：<br>更新流程如下：</p><ol><li>每次训练有数据：[s, a, r, s_]，分别表示当前状态s，要采取的行动a，获得的奖励r，以及到达的新状态s_</li><li>对于输入状态s，Q_Net预测所有动作的Q值，并找到动作a对应的Q值设为 q_pre</li><li>对于输入状态s_, Q_Net预测所有动作的Q值，并找到最大Q值对应的动作 a_next</li><li>将状态s_输入到Target_Net中，得到所有动作的Q值。我们找到在第2步得到的动作a_next对应的q值, 设为q_up。</li><li>根据更新公式计算真实label的值：q_label = r + gamma * q_up</li><li>计算误差：sqrt( (q_pre - q_label)^2 )，对Q_Net进行反向传播</li><li>重复2至6步，不断训练Q_Net</li></ol><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里借用了OpenAI的gym中的一个Pendulum环境。主要参考的是<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-5-double_DQN/" target="_blank" rel="noopener">莫烦Python</a>的教学实例。</p><p>该项目在我的GitHub上。具体项目URL：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Double_DQN" target="_blank" rel="noopener">Double DQN Example</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DQN是一个非常强大的算法，它成功的将深度学习和强化学习相结合。但在具体应用中会遇到各种各样的问题，而Double DQN就是一种解决Q值过大导致过估计的方法。Double DQN在结构和运行机制上和DQN误差，主要区别就在于训练Q_Net时对真实label的计算不同于传统DQN。</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之DQN</title>
      <link href="/2018/07/02/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/"/>
      <url>/2018/07/02/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/</url>
      <content type="html"><![CDATA[<p>DQN全称Deep Q Network，是一种将传统强化学习算法Q-Learning和Deep Learning相结合的算法。最有名的DQN应用应该就是在围棋上打败全人类的AlphaGo了。</p><a id="more"></a><h2 id="传统强化学习"><a href="#传统强化学习" class="headerlink" title="传统强化学习"></a>传统强化学习</h2><p>在<a href="">强化学习之Q_Learning</a>中我们已经了解到强化学习的基本概念以及Q_Learning的思想和。而Q_Learning算法面临一个巨大的问题：当State个数过于庞大时，Q_table会变得特别大，以至于每次执行学习循环时table的查找会消耗大量时间，严重影响效率，同时过大的Q_table也会造成存储问题。对于这个问题，DQN是一个非常好的解决办法。</p><h2 id="Q-Learning和Deep-Neural-Network"><a href="#Q-Learning和Deep-Neural-Network" class="headerlink" title="Q_Learning和Deep Neural Network"></a>Q_Learning和Deep Neural Network</h2><p>我们可以通过一个神经网络学习并预测每个State和Action对应的Q值，这样我们就不在需要记录巨大的Q表，只需要训练一个神经网络即可。具体流程如下：输入一个状态，神经网络输出所有动作对应的预测Q值，然后按照Q Learning的原则，使用梯度下降方法对网络进行更新。<br>形象的理解，就是Agent接收当前State的种种信息，然后在神经网络大脑中对自己可以做的每个Action默默评估，选择自认为最优的行为执行，做出动作后然后通过环境的反馈不断更新神经网络。</p><p>整个DQN算法结构如下：<br><img src="/images/reinforce_learning/DQN_alg.jpg" alt="DQN_Algorithm"></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>在DQN中，一共有三个组件：包括两个神经网络Target_Net、Q_Net和用来存储训练数据的记忆库（ReplayMemory）。</p><h4 id="Q-Net和Target-Net"><a href="#Q-Net和Target-Net" class="headerlink" title="Q_Net和Target_Net"></a>Q_Net和Target_Net</h4><p><strong>结构上这两个神经网络完全相同</strong>（可以包含卷积层，全连接层）。<strong>两个神经网络的输入都是当前状态的一些信息，输出是每一个action对应的预测Q值</strong>。<br>在最开始学习DQN的时候，一定会有这样的困惑，为什么要有两个神经网络？这两个神经网络的作用是什么？在这里我做个简单的不严谨说明便于理解：Q_Net是一个实时的网络（每次输入状态，就是用Q_Net的预测作为输出动作）。而Target_Net是用来计算Q更新公式求训练label的（将s的下一个状态s_输入Target_Net，输出的最大值作为Q(s’, a’)，计算更新公式得到Q(s, a)作为真实label）。<br>所以要想理解DQN两个网络工作机制的核心，只需要记住这一句话：<strong>Q_Net做出预测，Target_Net计算真实label，然后计算loss反向传播Q_Net</strong><br>在训练时，我们只对Q_Net做反向传播更新权重，Target_Net的权重更新来自于对Q_Net的直接复制。</p><h4 id="记忆库"><a href="#记忆库" class="headerlink" title="记忆库"></a>记忆库</h4><p>（ReplayMemeory）记忆库本质上是一个二维表，表的每一行都是一个马尔科夫节点表示一条记忆。举个例子：比如当前Agent处在状态s_t下，并执行action a_t到达下一个状态s_t_1，同时获得奖励reward值r_t。那么由：[s_t, a_t, r_t, s_t_1]构成的一个list就是一个马尔科夫节点。说明当前状态和下一个状态之间的关系。<br>记忆库是作为对神经网络的训练数据集存在，可以理解为一小段Q表信息。每次从环境获得一个[状态，动作，奖励，新状态]，就将其更新到记忆库中。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>在训练时我们只训练Q_Net，并不训练Target_Net。Target_Net权值的更新来自于对Q_Net权值的直接复制。</p><ol><li>每一次对Q_Net的迭代训练，就会从记忆库中随机抽取一个batch的马尔科夫节点[s_t, a_t, s_t_1, r_t, t_t]记忆。</li><li>然后将s_t输入到Q_Net中，得到Q_Net对s_t状态下每个action预测的Q值，然后取出 action a_t 对应的Q值这就是Q_Net的预测值Q(a_t|s_t)。</li><li>然后将s_t_1喂入Target-Net, 得到Target_Net在状态s_t_1下，对每一个action对应的Q值，然后取最大的Q值作为Max(Q(s’, a’))</li><li>根据贝尔曼公式（Q Learning更新方法）计算对应的Q(s, a)作为真实label值。</li><li>最后用上一步计算的Q(s, a)减去Q_Net的预测值Q(a_t|s_t)作为误差反向传播，更新Q_Net。</li><li>然后再每次Q_Net的训练迭代到一定次数的时候更新Target_Net（将Q_Net的参数完全复制到Target_Net）中。</li></ol><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol><li>初始化状态、环境以及DQN</li><li>DQN通过当前状态s的观测值选择行为a<ol><li>将输入状态s的种种观测值observation输入到Q_Net，Q_Net预测每个动作对应的Q值</li><li>大概率返回预测Q值最大的动作，小概率随机返回一个动作。增加DQN的探索性</li></ol></li><li>环境根据s和a，返回下一个新状态s_，奖励值r以及是否结束t</li><li>DQN存储记忆，将[当前状态s，行为a，奖励r，新状态s_]存储到记忆库中<ol><li>将[s, a, r, s_]存储到DQN的记忆库中</li><li>若已经达到记忆库的大小，则不断删除旧记忆，实现记忆库更新</li></ol></li><li>每隔指定步骤对神经网络进行一次训练<ol><li>从记忆库中随机选取batch个记忆</li><li>将每个记忆的s输入到Q_Net中，每个记忆的s_输入到Target_Net中。从Q_Net得到在s时每个动作的预测Q值（一个list），从Target_Net中得到s_的预测Q值（一个list）。</li><li>从记忆[s, a, r, s_]中找到动作a，同时找到该动作在Q_Net的预测输出中对应的Q值q_pre，以及Target_Net的预测输出中最大的Q值q_next。</li><li>根据更新公式：Q(s, a) = r + gamma * Max(Q(s_, a_))，该值作为实际的label值</li><li>计算误差，对Q_Net进行反向传播：loss = q_pre - (r + gamma*q_next)</li><li>重复1至5步，实现对Q_Net的训练，每训练指定个step后对Target_Net进行更新。更新方法是将Q_Net的参数直接copy给Target_Net。</li></ol></li><li>更新状态s = s_</li><li>循环2至5步直至环境返回结束状态</li></ol><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>在我的GitHub上实现了一个简单的DQN走迷宫的例子，（参考<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">莫烦Python</a>教程的例子）。为了便于理解，我在代码中尽可能详细的写了注释。这是项目地址：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/DQN" target="_blank" rel="noopener">DQN走迷宫</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DQN是将传统的强化学习方法Q_Learning和深度学习相结合，得到的一个强力RL模型。DQN在很多任务上取得了非常优秀的表现，著名的AlphaGo即使DQN的代表应用之一，当然AlphaGo的模型要复杂的多的多。</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop家族各个组件的关系</title>
      <link href="/2018/06/27/Hadoop/Hadoop%E5%AE%B6%E6%97%8F%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
      <url>/2018/06/27/Hadoop/Hadoop%E5%AE%B6%E6%97%8F%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
      <content type="html"><![CDATA[<p>Hadoop是一个庞大的家族，用于多个组件，新手刚入门时会搞不懂各个组件是干什么的，它们之间的关系如何，这篇文章介绍了Hadoop家族中主要组件的功能。</p><a id="more"></a><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>大数据，首先你要能存的下大数据。<br>传统的文件系统是单机的，不能横跨不同的机器。HDFS的设计本质上是为了大量的数据能横跨成百上千台机器，但是你看到的是一个文件系统而不是很多文件系统。比如你说我要获取/hdfs/tmp/file1的数据，你引用的是一个文件路径，但是实际的数据存放在很多不同的机器上。你作为用户，不需要知道这些，就好比在单机上你不关心文件分散在什么磁道什么扇区一样。HDFS为你管理这些数据。</p><h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><p>存的下数据之后，你就开始考虑怎么处理数据。虽然HDFS可以为你整体管理不同机器上的数据，但是这些数据太大了。一台机器慢慢跑也许需要好几天甚至好几周。对于很多公司来说，单机处理是不可忍受的，比如微博要更新24小时热博，它必须在24小时之内跑完这些处理。那么我如果要用很多台机器处理，我就面临了如何分配工作，机器之间如何互相通信交换数据以完成复杂的计算等等。这就是MapReduce / Tez / Spark的功能。MapReduce是第一代计算引擎，Tez和Spark是第二代。MapReduce的设计，采用了很简化的计算模型，只有Map和Reduce两个计算过程（中间用Shuffle串联），用这个模型，已经可以处理大数据领域很大一部分问题了。</p><h4 id="什么是Map什么是Reduce？"><a href="#什么是Map什么是Reduce？" class="headerlink" title="什么是Map什么是Reduce？"></a>什么是Map什么是Reduce？</h4><p>考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。Map阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的Pair；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个Reducer都如上处理，你就得到了整个文件的词频结果。</p><h4 id="Tez-amp-Spark"><a href="#Tez-amp-Spark" class="headerlink" title="Tez &amp; Spark"></a>Tez &amp; Spark</h4><p>Map＋Reduce的简单模型很黄很暴力，虽然好用，但是很笨重。第二代的Tez和Spark除了内存Cache之类的新feature，本质上来说，是让Map/Reduce模型更通用，让Map和Reduce之间的界限更模糊，数据交换更灵活，更少的磁盘读写，以便更方便地描述复杂算法，取得更高的吞吐量。</p><h4 id="Pig-amp-Hive"><a href="#Pig-amp-Hive" class="headerlink" title="Pig &amp; Hive"></a>Pig &amp; Hive</h4><p>有了MapReduce，Tez和Spark之后，程序员发现，MapReduce的程序写起来真麻烦。他们希望简化这个过程。希望有个更高层更抽象的语言层来描述算法和数据处理流程。于是就有了Pig和Hive。 <strong>Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL。</strong> 它们把脚本和SQL语言翻译成MapReduce程序，丢给计算引擎去计算，而你就从繁琐的MapReduce程序中解脱出来，用更简单更直观的语言去写程序了。</p><p>有了Hive之后，人们发现SQL对比Java有巨大的优势。一个是它太容易写了。刚才词频的东西，用SQL描述就只有一两行，MapReduce写起来大约要几十上百行。Hive逐渐成长成了大数据仓库的核心组件。甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。</p><h4 id="Impala-amp-Presto-amp-Drill"><a href="#Impala-amp-Presto-amp-Drill" class="headerlink" title="Impala &amp; Presto &amp; Drill"></a>Impala &amp; Presto &amp; Drill</h4><p>自从数据分析人员开始用Hive分析数据之后，它们发现，Hive在MapReduce上跑，慢！流水线作业集也许没啥关系，比如24小时更新的推荐，反正24小时内跑完就算了。但是数据分析，人们总是希望能跑更快一些。</p><p>于是Impala，Presto，Drill诞生了（当然还有无数非著名的交互SQL引擎，就不一一列举了）。三个系统的核心理念是，MapReduce引擎太慢，因为它太通用，太强壮，太保守，我们SQL需要更轻量，更激进地获取资源，更专门地对SQL做优化，而且不需要那么多容错性保证（因为系统出错了大不了重新启动任务，如果整个处理时间更短的话，比如几分钟之内）。这些系统让用户更快速地处理SQL任务，牺牲了通用性稳定性等特性。如果说MapReduce是大砍刀，砍啥都不怕，那上面三个就是剔骨刀，灵巧锋利，但是不能搞太大太硬的东西。</p><h4 id="Hive-on-Tez-Spark-amp-SparkSQL"><a href="#Hive-on-Tez-Spark-amp-SparkSQL" class="headerlink" title="Hive on Tez/Spark &amp; SparkSQL"></a>Hive on Tez/Spark &amp; SparkSQL</h4><p>这些系统，说实话，一直没有达到人们期望的流行度。因为这时候又两个异类被造出来了。他们是Hive on Tez / Spark和SparkSQL。它们的设计理念是，MapReduce慢，但是如果我用新一代通用计算引擎Tez或者Spark来跑SQL，那我就能跑的更快。而且用户不需要维护两套系统。</p><p>上面的介绍，基本就是一个数据仓库的构架了。底层HDFS，上面跑MapReduce／Tez／Spark，再上面跑Hive，Pig。或者HDFS上直接跑Impala，Drill，Presto。这解决了中低速数据处理的要求。</p><h4 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h4><p>那如果我要更高速的处理呢？</p><p>如果我是一个类似微博的公司，我希望显示不是24小时热博，我想看一个不断变化的热播榜，更新延迟在一分钟之内，上面的手段都将无法胜任。于是又一种计算模型被开发出来，这就是Streaming（流）计算。Storm是最流行的流计算平台。流计算的思路是，如果要达到更实时的更新，我何不在数据流进来的时候就处理了？比如还是词频统计的例子，我的数据流是一个一个的词，我就让他们一边流过我就一边开始统计了。流计算很牛逼，基本无延迟，但是它的短处是，不灵活，你想要统计的东西必须预先知道，毕竟数据流过就没了，你没算的东西就无法补算了。因此它是个很好的东西，但是无法替代上面数据仓库和批处理系统。</p><h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><p>还有一个有些独立的模块是KVStore，比如Cassandra，HBase，MongoDB等。所以KV Store就是说，我有一堆键值，我能很快速获取与这个Key绑定的数据。比如我用身份证号，能取到你的身份数据。这个动作用MapReduce也能完成，但是很可能要扫描整个数据集。而KV Store专用来处理这个操作，所有存和取都专门为此优化了。从几个P的数据中查找一个身份证号，也许只要零点几秒。这让大数据公司的一些专门操作被大大优化了。比如我网页上有个根据订单号查找订单内容的页面，而整个网站的订单数量无法单机数据库存储，我就会考虑用KV Store来存。KV Store的理念是，基本无法处理复杂的计算，大多没法JOIN，也许没法聚合，没有强一致性保证（不同数据分布在不同机器上，你每次读取也许会读到不同的结果，也无法处理类似银行转账那样的强一致性要求的操作）。但是丫就是快。极快。</p><p>每个不同的KV Store设计都有不同取舍，有些更快，有些容量更高，有些可以支持更复杂的操作。必有一款适合你。</p><h4 id="Mahout-amp-Protobuf-amp-ZooKeeper"><a href="#Mahout-amp-Protobuf-amp-ZooKeeper" class="headerlink" title="Mahout &amp; Protobuf &amp; ZooKeeper"></a>Mahout &amp; Protobuf &amp; ZooKeeper</h4><p>除此之外，还有一些更特制的系统／组件，比如Mahout是分布式机器学习库，Protobuf是数据交换的编码和库，ZooKeeper是高一致性的分布存取协同系统，等等。</p><h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><p>有了这么多乱七八糟的工具，都在同一个集群上运转，大家需要互相尊重有序工作。所以另外一个重要组件是，调度系统。现在最流行的是Yarn。你可以把他看作中央管理，好比你妈在厨房监工，哎，你妹妹切菜切完了，你可以把刀拿去杀鸡了。只要大家都服从你妈分配，那大家都能愉快滴烧菜。</p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP流水线</title>
      <link href="/2018/06/25/Project/NLP%E6%B5%81%E6%B0%B4%E7%BA%BF/"/>
      <url>/2018/06/25/Project/NLP%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
      <content type="html"><![CDATA[<p>NLP项目是一个庞大且复杂的项目，在机器学习中任何复杂的事情通常意味着需要建立一条流水线（pipeline），把复杂问题分解成各个小部分，然后用机器学习模型分别解决每个部分，最后通过吧几个互相馈送结果的模型连接在一起，就可以用来解决非常复杂的问题。<br><a id="more"></a></p><h2 id="NLP流水线"><a href="#NLP流水线" class="headerlink" title="NLP流水线"></a>NLP流水线</h2><h3 id="步骤1：句子分割"><a href="#步骤1：句子分割" class="headerlink" title="步骤1：句子分割"></a>步骤1：句子分割</h3><p>pipeline的第一步是把原始文本拆分成单独的句子。编写一个程序来理解一个句子比理解整个段落要容易的多。<br>编码一个句子分割模型可以很简单地在任何看到标点符号的时候拆分句子。但是，现代 NLP 流水线通常使用更为复杂的技术，以应对那些没有被格式化干净的文件。</p><h3 id="步骤2：词汇标记化"><a href="#步骤2：词汇标记化" class="headerlink" title="步骤2：词汇标记化"></a>步骤2：词汇标记化</h3><p>对于已经处理完的句子，下一步就是将句子拆分成不同的单词或者标记，这个步骤叫做标记化，通常也将标点符号当做单独的标记对待。<br>对于英文莱索标记化很容易做到，只因为英文单词之间有空格。</p><h3 id="步骤3：预测标记的词性"><a href="#步骤3：预测标记的词性" class="headerlink" title="步骤3：预测标记的词性"></a>步骤3：预测标记的词性</h3><p>对于每个标记，需要尝试猜测他的词性：名词，动词，形容词等等。知道每个单词的词性可以帮助计算机理解句子的意思。<br>每个单词（和它周围的一些额外的单词用于上下文）输入预先训练的词性分类模型：<br><img src="/images/NLP/cixingbiaoji.png" alt="NLP_cixingyuce"><br>词性模型最初是通过给它提供数以百万计的英语句子来训练的，每一个单词的词性都已经标注出来，并让它学会复制这种行为。<br>最后，对于句子中每个标记，我们可以得到它对应的词性预测。如：is-Verb；china-Noun</p><h3 id="步骤4：文本词形还原"><a href="#步骤4：文本词形还原" class="headerlink" title="步骤4：文本词形还原"></a>步骤4：文本词形还原</h3><p>在如英语等大多数语言中，单词会以不同形式出现，如动词的单三形式，过去式，过去分词等。在计算机中处理文本时，了解每个单词的基本形式是有意义的，这样才可以知道两个句子都在讨论一个概念。<br>所以，在NLP中，对单词的各种变换进行词形还原是非常重要的：<strong>找出句子中每个单词的基本形式</strong><br><strong>词形还原通常是通过基于词性的词条形式查表直接替换完成的</strong></p><h3 id="步骤5：识别停止词"><a href="#步骤5：识别停止词" class="headerlink" title="步骤5：识别停止词"></a>步骤5：识别停止词</h3><p>接下来，我们要考虑句子中每个词的重要性。英语有很多填充词，它们经常出现，如「and」、「the」和「a」。当对文本进行统计时，这些词引入了大量的噪声，因为它们比其他词更频繁地出现。一些 NLP 流水线将它们标记为「停止词」，也就是说，在进行任何统计分析之前，这可能是你想要过滤掉的单词。</p><h3 id="步骤6a：依赖解析"><a href="#步骤6a：依赖解析" class="headerlink" title="步骤6a：依赖解析"></a>步骤6a：依赖解析</h3><p>下一步是弄清楚我们句子中的所有单词是如何相互关联的，这叫做依赖解析。<br>我们的目标是构建一棵树，它给句子中的每个单词分配一个单一的父词。树的根结点是句子中的主要动词。下面是我们的句子的解析树一开始的样子：<br><img src="/images/NLP/yilaishu.png" alt="依赖树"><br>作者：机器之心<br>链接：<a href="https://zhuanlan.zhihu.com/p/41850756" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/41850756</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。</p><p>这棵解析树告诉我们，句子的主语是名词「London」，它与「capital」有「be」关系。我们终于知道了一些有用的东西——伦敦是一个首都！如果我们遵循完整的解析树的句子（除上方所示），我们甚至会发现，伦敦是英国的首都。</p><p>就像我们先前使用机器学习模型预测词性一样，依赖解析也可以通过将单词输入机器学习模型并输出结果来工作。但是解析单词的依赖项是一项特别复杂的任务，需要一篇完整的文章来详细说明。如果你想知道它是如何工作的，一个很好的开始阅读的地方是 Matthew Honnibal 的优秀文章「Parsing English in 500 Lines of Python」。</p><p>但是，尽管作者在 2015 的一篇文章中说这种方法在现在是标准的，但它实际上已经过时了，甚至不再被作者使用。在 2016，谷歌发布了一个新的依赖性分析器，称为 Parsey McParseface，它使用了一种新的深度学习方法并超越了以前的基准，它迅速地遍及整个行业。一年后，他们发布了一种新的叫做 ParseySaurus 的模型，它改进了更多的东西。换句话说，解析技术仍然是一个活跃的研究领域，在不断地变化和改进。</p><p>同样需要记住的是，很多英语句子都是模棱两可的，难以解析的。在这种情况下，模型将根据该句子的解析版本进行猜测，但它并不完美，有时该模型将导致令人尴尬的错误。但随着时间的推移，我们的 NLP 模型将继续以更好的方式解析文本。</p><h3 id="步骤6b：寻找名词短语"><a href="#步骤6b：寻找名词短语" class="headerlink" title="步骤6b：寻找名词短语"></a>步骤6b：寻找名词短语</h3><p>到目前为止，我们把句子中的每个词都看作是独立的实体。但是有时候把代表一个想法或事物的单词组合在一起更有意义。我们可以使用依赖解析树中的相关信息自动将所有讨论同一事物的单词组合在一起。是否做这一步取决于我们的最终目标。如果我们不需要更多的细节来描述哪些词是形容词，而是想更多地关注提取完整的想法，那么这是一种快速而简单的方法。</p><h3 id="步骤7：命名实体识别（NER）"><a href="#步骤7：命名实体识别（NER）" class="headerlink" title="步骤7：命名实体识别（NER）"></a>步骤7：命名实体识别（NER）</h3><p>命名实体识别（NER）的目标是用它们所代表的真实世界的概念来检测和标记这些名词。以下是我们在使用 NER 标签模型运行每个标签之后的句子：<br><img src="/images/NLP/ner.png" alt="NER"><br>NER 系统不仅仅是简单的字典查找。相反，他们使用的是一个单词如何出现在句子中的上下文和一个统计模型来猜测单词代表的是哪种类型的名词。一个好的 NER 系统可以通过上下文线索来区分「Brooklyn Decker」这个人名和「Brooklyn」这个位置。<br>下面是一些典型的 NER 系统可以标记的对象类型：</p><ul><li>人名</li><li>公司名称</li><li>地理位置（物理和政治）</li><li>产品名称</li><li>日期与时间</li><li>金钱数量</li><li>事件名称<br>NER 有大量的用途，因为它可以很容易地从文本中获取结构化数据。这是从 NLP 流水线中快速获取有价值信息的最简单方法之一。<h3 id="步骤8：共指解析"><a href="#步骤8：共指解析" class="headerlink" title="步骤8：共指解析"></a>步骤8：共指解析</h3>到此，我们对句子已经有了一个很好的表述。我们知道每个单词的词性、单词如何相互关联、哪些词在谈论命名实体。<br>然而，我们还有一个大问题。英语里充满了人称代词，比如他、她，还有它。这些是我们使用的快捷表述方法，而不需要在每个句子中一遍又一遍地写名字。人类可以根据上下文来记录这些词所代表的内容。但是我们的 NLP 模型不知道人称代词是什么意思，因为它一次只检查一个句子。<blockquote><p>「It was founded by the Romans, who named it Londinium.」</p></blockquote></li></ul><p>如果我们用 NLP 流水线来解析这个句子，我们就会知道「it」是由罗马人建立的。但知道「London」是由罗马人建立的则更为有用。</p><p>人类阅读这个句子时，可以很容易地理解「it」的意思是「London」。共指解析的目的是通过追踪句子中的代词来找出相同的映射。我们想找出所有提到同一个实体的单词。<br>共指解析是 NLP 流水线实现中最困难的步骤之一。这比句子分析更困难。深度学习的最新进展研究出了更精确的新方法，但还不完善。</p>]]></content>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Q_Learning</title>
      <link href="/2018/06/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BQ_Learning/"/>
      <url>/2018/06/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BQ_Learning/</url>
      <content type="html"><![CDATA[<p>Q_Learning是最典型，最简单的强化学习算法，但其蕴含的思想却影响了很多其它的RL算法，现将Q_Learning的思路和特点做以总结。</p><a id="more"></a><p>Agent，Environment，State，Reward，Action 是强化学习中最基本的几个概念，Agent在当前的State下执行Action，并从Environment处获得反馈和更新的State，Agent接收reward反馈，更新自己的策略模式并进入到下个State，进而不断循环上述过程打到学习的目的。这就是强化学习最一般的概念。</p><h2 id="Q-Learning概念"><a href="#Q-Learning概念" class="headerlink" title="Q_Learning概念"></a>Q_Learning概念</h2><p>Q-Learning是最直接反应上述过程的学习算法，通过构建Q_table，记录每个State状态下每个Action的Q值（相当于在该状态下采取该动作的奖惩值），并通过环境返回的Reward不断更新Q_table直至算法可以快速准确的在每个State下做出相对正确的Action。Q_Table的更新算法如下：<br><img src="/images/q_learning/q_update.png" alt="q_learning"><br>其中：</p><ul><li>Q(s, a) 表示在s状态下动作a的Q值</li><li>r 表示如果在s状态实行a，环境返回的reward值</li><li>gamma 表示衰减率，用来衡量当前状态的更新和之后状态的关系。gamma的存在让Q表的更新有了”前瞻性”。（如果下个状态s’最大的q值很低，说明新状态s’是一个很差的状态，Q(s, a)对应的q值就会很小，让agent尽可能不在s执行a动作到达差状态s’。</li><li>Max(Q(s’, a’)) 表示在状态s执行动作a后，到达新状态s’，并返回新状态s’时对应所有状态Q值的最大值。</li></ul><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>现在我们已经知道了Q_Learning的运行原理本质上是从Environment的反馈更新Q表，同时也理解了更新方法。下面看一下整个Q_Learning算法的具体流程：</p><ol><li>初始化一个Q表，行对应着所有的state，列对应着所有的action</li><li>初始化state</li><li>根据当前state选择动作action。（贪心选择当前state对应Q值最大的action，同时有一定概率随机选择action）</li><li>将当前state和选择的action输入到环境中，环境返回下一个状态s_ 、本次执行动作的奖励r、以及游戏是否终止t</li><li>找到新状态s_下所有动作q值最大的值，根据更新公式对Q(s, a)进行更新。</li><li>从新状态s_不断重复3至5步，直到游戏结束完成一个epoch。循环数个epoch会发现模型收敛。</li></ol><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里是两个我用Python写的Q_Learning简单例子，主要参考了<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">莫烦Python</a>的教程，为了便于理解，在代码里我写了尽可能多的注释以便将整个Q_Learning的运行原理解释清楚。具体例子请看下面的URL。<br><a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Q_learning" target="_blank" rel="noopener">Q_Learning Example</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上就是我整理的Q_Learning的简单教程，Q_Learning和很多强化学习算法的基础，比如Sarsa，DQN等。理解Q Learning在我看来只需要记住这句话： <strong>在状态s执行动作a到达新状态s_并获得奖励r，根据r和新状态最大的q值对目前状态s和动作a的q值Q(s, a)进行更新</strong></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop之MapReduce</title>
      <link href="/2018/06/11/Hadoop/Hadoop%E4%B9%8BMapReduce/"/>
      <url>/2018/06/11/Hadoop/Hadoop%E4%B9%8BMapReduce/</url>
      <content type="html"><![CDATA[<p>介绍整理MapReduce的结构和使用方法<br><a id="more"></a></p><h1 id="MapReduce计算模型"><a href="#MapReduce计算模型" class="headerlink" title="MapReduce计算模型"></a>MapReduce计算模型</h1><p>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个JobTracker，一个TaskTracker。前者是用于调度工作的，一个Hadoop集群只有一个JobTracker。后者是用于执行工作的。</p><h2 id="MapReduce-Job"><a href="#MapReduce-Job" class="headerlink" title="MapReduce Job"></a>MapReduce Job</h2><p>每个MapReduce任务会被初始化为一个Job，每个Job分为两个阶段：map阶段和reduce阶段。</p><h4 id="InputFormat-和InputSplit"><a href="#InputFormat-和InputSplit" class="headerlink" title="InputFormat()和InputSplit"></a>InputFormat()和InputSplit</h4><p>InputSplit是Hadoop定义的用来传送给每个单独map数据，InputSplit存储的并非数据本身，而还是一个分片长度和一个记录数据位置的数组。当数据传送给map时，map会将输入分片传送到InputFormat上，InputFormat会创建可供map处理的key-value对。也就是说， <strong>InputFormat()方法是用来生成可供map处理的key-value对的</strong>。<br>Hadoop预定义了多种将不同类型的输入数据转化为key-value对的方法， 他们都继承自InputFormat。<br>其中FileInputFormat又有多个子类。<br>在key-value对中，<strong>key值时每个数据记录在数据分片中的字节偏移量，数据类型是LongWritable，value值时每行的内容，数据类型是Text</strong></p><h4 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h4><p>每个输入格式都有一种输出格式与其对应，outputFormat会将每条记录以一行的形式存入文本文件中，它的key-value对可以试任意的。</p><h4 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h4><p>Map函数接收InputFormat处理所产生的key-value对，经过map函数的处理后输出key-value对，Hadoop会将map输出的中间结果存储在磁盘上而不是HDFS上。Reduce会读取map的输出数据，这时的key-value数据中key相同的value已经被合并为一个list，reduce函数会对输入的数据进行处理并将结果存储在HDFS上。<br>自定义Map类时需要继承自Mapper类。自定义Reduce类继承自Reducer类。并分别在类中重写map()和reduce()函数。以下为WordCount的map和reduce实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountMapper</span><br><span class="line">        extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">        throws IOException, InterruptedException&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] words = line.split(&quot; &quot;);</span><br><span class="line">        for(String word: words)&#123;</span><br><span class="line">            context.write(new Text(word), new IntWritable(1));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Reduce类实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iteterable&lt;IntWritable&gt; values, Context context)</span><br><span class="line">            throws IOException, InterruptedException&#123;</span><br><span class="line">        Integer count = 0;</span><br><span class="line">        for(IntWritable value:values)&#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, new IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意：</p><ol><li>reduce task的数量可以通过程序制定，当有多个reduce task时，每个task都会生成一个输出文件</li><li>没有reduce任务的时候，系统会直接将map task的输出作为最终输出，有多少个map task文件就有多少个输出文件。</li></ol><h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><ol><li>编写代码</li><li>编译成.class文件</li><li>将所有class文件打包成jar文件</li><li>上传到Hadoop</li><li>运行jar文件</li></ol><p>Hadoop运行MapReduce任务时，JobTracker调度任务给TaskTracker，TaskTracker执行任务时会返回进度报告，JobTracker会记录进度的进行状况，如果某个TaskTracker上的任务失败，JobTracker会把这个任务分配给另一台TaskTracker。</p><h2 id="MapReduce优化"><a href="#MapReduce优化" class="headerlink" title="MapReduce优化"></a>MapReduce优化</h2><p>MapReduce模型的优化设计到很多方面，主要为：1. 计算性能优化；2. IO操作优化</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>MapReduce擅长处理少量的大数据，不擅长处理大量的小数据。所以可以在使用MapReduce前对数据进行预处理合并。当一个map任务只需要几秒就可以运行结束时，应该给他分配更多任务。一般而言，一个map任务的运行时间在一分钟比较好。</p><h3 id="map和reduce的数量"><a href="#map和reduce的数量" class="headerlink" title="map和reduce的数量"></a>map和reduce的数量</h3><p>定义两个概念：map任务槽和reduce任务槽，表示这个Hadoop集群能够同时运行的map/reduce任务的数量。</p><h3 id="combine函数"><a href="#combine函数" class="headerlink" title="combine函数"></a>combine函数</h3><p>combine函数用于在本地合并数据，也就是说，在map处理数据后，先将map的输出进行一个预合并，然后再将合并的结果传送给reduce，这样会大大减少网络IO的消耗。例如：在wordcount中，每个map可能都会生成大量的(the, 1)输出，如果将这些输出一个个的都传递个reduce，会浪费很多IO，可以在combine函数中，先将map的输出进行合并（the，10）。</p><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>可以对map的输出和最终的输出进行压缩。通常情况下map的输出会很多大，对其进行压缩可以有效的减少数据在网络的传输量。</p><h3 id="自定义comparator"><a href="#自定义comparator" class="headerlink" title="自定义comparator"></a>自定义comparator</h3><p>可以通过自定义comparator实现数据的二进制比较，这样可以省去数据序列化和反序列化的时间，提高运行效率。</p><h1 id="Hadoop中的IO"><a href="#Hadoop中的IO" class="headerlink" title="Hadoop中的IO"></a>Hadoop中的IO</h1><h2 id="IO操作中的数据检查"><a href="#IO操作中的数据检查" class="headerlink" title="IO操作中的数据检查"></a>IO操作中的数据检查</h2><p>Hadoop是由上千台主机集成的，这么多主机同时运行时，难免会有主机损坏，所以对于Hadoop来说，进行数据完整性检查是非常重要的。</p><h3 id="校验和方式"><a href="#校验和方式" class="headerlink" title="校验和方式"></a>校验和方式</h3><p>通过对比新旧校验和来确定数据是否损坏。循环冗余校验被广泛应用在Hadoop网络IO检查，数据完整性检查等方面。</p><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><p>对于HDFS来说，文件压缩是必须的，它会带来两个好处：1.减少文件所需的存储空间；2.加快文件在网络上的传输速度。<br>Hadoop文件压缩模块都在’package org.apache.hadoop.io.compress’中。<br>Hadoop支持多种压缩格式和压缩算法。</p><h3 id="MapReduce中使用压缩"><a href="#MapReduce中使用压缩" class="headerlink" title="MapReduce中使用压缩"></a>MapReduce中使用压缩</h3><p>只需要在它的Job配置时配置好conf就可以了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 设置map处理后数据的压缩代码</span><br><span class="line">JobConf conf = new Jobconf();</span><br><span class="line">conf.setBoolean(&quot;mapred.compress.map.output&quot;, true);</span><br><span class="line"></span><br><span class="line">// 设置output输出压缩代码</span><br><span class="line">JobConf conf = new JobConf();</span><br><span class="line">conf.setBoolean(&quot;mapred.output.compress&quot;, true);</span><br><span class="line">conf.setClass(&quot;mapred.output.compression.codec&quot;, GzipCodec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习补充知识</title>
      <link href="/2018/05/22/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86/"/>
      <url>/2018/05/22/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<p>深度学习知识点复杂繁多，同时各个点之间往往有很深的联系，这篇文章总结了我在学习CNN和RNN过程中遇到的小知识点的补充。</p><a id="more"></a><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h4 id="BP算法推导过程"><a href="#BP算法推导过程" class="headerlink" title="BP算法推导过程"></a>BP算法推导过程</h4><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization是将分散的数据统一的一种方法。<br>神经网络的训练效率受数据分布影响，如果输入神经元的数值过大或过小，该输入值会在激活函数的饱和区，也就是说这个时候的梯度几乎为0，导致梯度消失，很难更新权重。<br>Batch Normalization相当于在每层hidden layer之间对数据进行归一化，让数据分布集中在（-1， 1）之间，这样下一层神经元的输入大部分会落到激活函数的工作区，BP时更新梯度较大，模型可以更快收敛。<br><img src="/images/nn_knowledge/batch_normalization.jpg" alt="batch_normalization"></p><p>一般应用Batch Normalization时往往还进行反向Normalization，将Normalized的数据用可训练的参数再扩展和平移回去，目的是让神经网络自己学习扩展平移参数，进而检测此次的Batch Normalization是否起到优化作用。</p><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>层数比较多的神经网络模型在训练时会出现一些问题，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。</p><h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><h4 id="梯度消失原因"><a href="#梯度消失原因" class="headerlink" title="梯度消失原因"></a>梯度消失原因</h4><p>对于多层神经网络，当梯度消失发生时，接近输出层的hidden layer权重正常更新，但接近输入层的权重更新会非常缓慢，导致前几层的权值几乎不变，这就导致了前几层就相当于一个映射层。这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p><p>对于梯度消失的具体原因，我们可以将反向传播的公式进行展开，这里激活函数我们用Sigmoid举例:<br><img src="/images/deep_learning/sigmoid_bp.png" alt="sigmoid_bp"><br>可以看到，整个反向传播过程是由几个sigmoid函数的导数和权重相乘组成的，而对sigmoid求导发现，sigmoid的导数最大值为0.25，同时我们初始化网络权值时通常也会小于1，也就是说对于上面的链式法则求导，层数越多，求导结果越小，对前几个隐藏层权重的更新也越小。从而导致了梯度消失。</p><h4 id="梯度消失解决办法"><a href="#梯度消失解决办法" class="headerlink" title="梯度消失解决办法"></a>梯度消失解决办法</h4><ul><li>放弃sigmoid函数，使用Relu是最直接的方法</li><li>在网络中加入Batch Normalization</li></ul><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><h4 id="梯度爆炸原因"><a href="#梯度爆炸原因" class="headerlink" title="梯度爆炸原因"></a>梯度爆炸原因</h4><p>在深层网络或递归神经网络中，误差梯度在更新中累积得到一个非常大的梯度，这样的梯度会大幅度更新网络参数，进而导致网络不稳定。在极端情况下，权重的值变得特别大，以至于权重溢出（NaN值）。当梯度爆炸发生时，网络层之间反复乘以大于1.0的梯度值使得梯度值成倍增长，网络不稳定，无法收敛。</p><p>根据链式法则我们知道，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大</p><h4 id="梯度爆炸现象"><a href="#梯度爆炸现象" class="headerlink" title="梯度爆炸现象"></a>梯度爆炸现象</h4><ul><li>模型无法在训练数据上收敛（比如，损失函数值非常差）；</li><li>模型不稳定，在更新的时候损失有较大的变化；</li><li>模型的损失函数值在训练过程中变成NaN值；</li><li>模型在训练过程中，权重变化非常大；</li><li>模型在训练过程中，权重变成NaN值；<h4 id="梯度爆炸解决办法"><a href="#梯度爆炸解决办法" class="headerlink" title="梯度爆炸解决办法"></a>梯度爆炸解决办法</h4></li></ul><ol><li>重新设计网络。通过减少模型隐藏层数或者训练时使用较小的batch。</li><li>使用Relu。</li><li>对于RNN使用LSTM。虽然LSTM无法完全消除梯度爆炸，但有效的减小了发生的概率</li><li>使用梯度裁剪。当梯度大于某一值时将其裁剪成固定值。</li><li>使用L1或L2正则化</li><li>也可以使用Batch Normalization</li></ol><h2 id="为什么Dropout可以解决过拟合"><a href="#为什么Dropout可以解决过拟合" class="headerlink" title="为什么Dropout可以解决过拟合"></a>为什么Dropout可以解决过拟合</h2><ul><li>解除特定神经网络元之间的依赖 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。</li><li>类集成学习， Dropout本质上是在训练多个子神经网络，而在测试时，完整神经网络的输出相当于是对多个子神经网络预测结果的组合。</li></ul><h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><p>激活函数给神经元引入了非线性因素，如果不用激活函数的话，无论神经网络有多少层，输出都是输入的线性组合。<br>激活函数的发展经历了Sigmoid -&gt; Tanh -&gt; ReLU -&gt; Leaky ReLU -&gt; Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。</p><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><ul><li>最具代表性的激活函数，可以将输入数据压缩到0~1之间，但现在已经很少有人使用sigmoid</li><li>缺点1：sigmoid饱和使得梯度消失</li><li>缺点2：sigmoid求导复杂</li><li>缺点3：输出不是0中心<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3></li><li>将输入数据压缩到[-1, 1]之间</li><li>解决了sigmoid输出不是零中心的问题。但仍然存在饱和导致梯度消失的问题。<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3></li><li>现在的主流激活函数</li><li>因为大于0的部分导数恒等于1，不会出现梯度消失并且可以大大加快模型训练速度。</li><li>函数简单，不需要指数运算</li><li>缺点：可能会导致神经元死掉。<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3></li><li>在ReLU的基础上做了改进，让函数的非正数部分导数恒等于一个非常小的正数。</li><li>解决了神经元死掉的问题。</li></ul><h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><ol><li>01损失</li><li>平方损失</li><li>绝对值损失</li><li>对数损失</li><li>交叉熵损失</li></ol><h2 id="TensorFlow的优缺点"><a href="#TensorFlow的优缺点" class="headerlink" title="TensorFlow的优缺点"></a>TensorFlow的优缺点</h2><ul><li>缺点<ol><li>API接口更新过快，不断挪位置。接口不灵活</li><li>调试困难，tensorflow是静态图框架，打印中间结果必须借助Session，或者额外的debug工具    </li></ol></li><li>优点<ol><li>文档最全，资源最多，用户基数最大</li><li>可视化组件tensorboard</li><li>静态图结构虽然调试困难，但部署方便，可以部署到移动端</li><li>和Numpy完美结合</li><li>支持多GPU</li></ol></li></ul>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Scheme基本语法</title>
      <link href="/2018/05/21/Programming%20Language/Scheme%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2018/05/21/Programming%20Language/Scheme%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>因为研究室要进行EOPL（《Essentials of Programming Languages》）的轮讲，而这本书是用Scheme作为教学语言，以至于不得不学习一下Scheme的基本语法。作为一种教学性质的函数式编程语言，Scheme语法简单，非常通俗易懂，可以帮助程序员理解函数式编程思想，还是值得一学的。而且著名的编程神书SICP（《Structure and Interpretation of Computer Programs》）也是用Scheme作为教学语言。所以为了方便更好的理解这两本书，对Scheme的基本语法有一个简单的了解还是有必要的。</p><a id="more"></a><h2 id="Scheme-Basic"><a href="#Scheme-Basic" class="headerlink" title="Scheme Basic"></a>Scheme Basic</h2><p>与一般的编程语言最大的区别，Scheme使用前缀表达式。什么意思呢，看如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2)</span><br><span class="line">(+ 1 2 3 4 5)</span><br></pre></td></tr></table></figure></p><p>上面这行代码就是一句最简单的Scheme程序，返回1+2的结果。可以看到Scheme使用前缀表达式，先输入运算符‘+’，然后输入运算符的两个参数‘1’，‘2’。使用前缀表达式的一个最直接的优点就是运算符的运算对象个数不受限制，我们可以用一个‘+’计算任意个数的和。</p><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><ul><li><p>Scheme使用分号”;”表示单行注释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">; this is a scheme comment</span><br></pre></td></tr></table></figure></li><li><p>标准Scheme中未定义多行注释方法。</p></li></ul><h3 id="块-form"><a href="#块-form" class="headerlink" title="块(form)"></a>块(form)</h3><ul><li><p>块是Scheme中最小单元，用”( )”表示一个form，一个form可以是一个表达式，一个过程，一个变量声明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2)</span><br><span class="line">(define x 2)</span><br></pre></td></tr></table></figure></li><li><p>块可以通过嵌套完成复杂的表达式计算。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ (* 2 3) (/ 5 (- 3 2)))</span><br><span class="line">; == 2*3 + 5/(3-2)</span><br></pre></td></tr></table></figure></li></ul><h3 id="基本数值运算"><a href="#基本数值运算" class="headerlink" title="基本数值运算"></a>基本数值运算</h3><p>+, -, *, 和/分别代表加、减、乘、除。由于Scheme使用的是前缀表达式，所以这些操作都接受任意多的参数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2 3 4) ;→ 10</span><br><span class="line">(- 10 1 2)  ;→ 7</span><br><span class="line">(* 2 3 4)   ;→ 24</span><br><span class="line">(/ 29 3 7)  ;→ 29/21</span><br></pre></td></tr></table></figure></p><h3 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h3><p>Scheme有很多扩展库定义了一些有用的过程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(max 1 2 3 4)</span><br><span class="line">(min 1 2 3 4)</span><br><span class="line">(abs -2)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>高级编程语言的一个重要特性就是变量，我们可以通过定义变量从而记录对应的数据，进而可以进行更复杂的计算。在Scheme中使用’define’定义一个变量：</p><ul><li><p>定义一个变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define x 2)</span><br></pre></td></tr></table></figure></li><li><p>使用set!来改变变量的值:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(set! x &quot;hello&quot;)</span><br></pre></td></tr></table></figure></li><li><p>Scheme和python一样，它的变量类型是不固定的，可以随时改变。</p></li></ul><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="布尔型"><a href="#布尔型" class="headerlink" title="布尔型"></a>布尔型</h3><ul><li>Scheme使用“#t”表示真(True)，使用“#f”表示假(False)</li><li><p>“not” 表示取反:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(not #f) -&gt;True</span><br><span class="line">(not 1)  -&gt;False</span><br></pre></td></tr></table></figure></li><li><p>“not”后面的参数只要不是布尔型，都返回#f</p></li></ul><h3 id="数值型"><a href="#数值型" class="headerlink" title="数值型"></a>数值型</h3><p>Scheme支持四种数值类型：整型(integer)，有理数型(rational)，实型(real)，复数型(complex)</p><h3 id="字符型"><a href="#字符型" class="headerlink" title="字符型"></a>字符型</h3><p>Scheme语言中的字符型数据均以符号组合 “#\” 开始，表示单个字符，可以是字母、数字或”[ ! $ % &amp; * + - . / : %lt; = &gt; ? @ ^ _ ~ ]”等等其它字符，如：</p><p>#\A 表示大写字母A，#\0表示字符0，<br>其中特殊字符有：#\space 表示空格符和 #\newline 表示换行符。</p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>字符串(string) 由多个字符组成的数据类型，可以直接写成由双引号括起的内容，如：”hello” 。</p><h3 id="点对-pair"><a href="#点对-pair" class="headerlink" title="点对(pair)"></a>点对(pair)</h3><p>pair是Scheme中非常重要的一个数据结构，它是由一个点和被它分隔开的两个所值组成的。形如： (1 . 2) 或 (a . b) ，注意的是点的两边有空格。<br>它用cons来定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(cons 8 9) =&gt;(8 . 9)</span><br></pre></td></tr></table></figure></p><p>其中在点前面的值被称为 car ，在点后面的值被称为 cdr ，car和cdr同时又成为取pair的这两个值的过程，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(define p (cons 4 5))   =&gt; (4 . 5)</span><br><span class="line">(car p)         =&gt; 4</span><br><span class="line">(cdr p)         =&gt; 5</span><br></pre></td></tr></table></figure></p><p>还可以用set-car! 和 set-cdr! 来分别设定这两个值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(set-car! p &quot;hello&quot;)</span><br><span class="line">(set-cdr! p &quot;good&quot;)</span><br></pre></td></tr></table></figure></p><h3 id="列表-list"><a href="#列表-list" class="headerlink" title="列表(list)"></a>列表(list)</h3><p>Scheme中的列表结构和数据结构中的链表比较相似具体方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(define la (list 1 2 3 4 ))   -&gt;(1 2 3 4)</span><br><span class="line">(length la)  ; 取得列表的长度</span><br><span class="line">(list-ref la 3)  ; 取得列表第3项的值（从0开始）</span><br><span class="line">(list-set! la 2 99)  ; 设定列表第2项的值为99</span><br><span class="line">(define y (make-list 5 6))  ;创建列表</span><br><span class="line">(6 6 6 6 6)</span><br></pre></td></tr></table></figure></p><ul><li>make-list用来创建列表，第一个参数是列表的长度，第二个参数是列表中添充的内容；还可以实现多重列表，即列表的元素也是列表，如：(list (list 1 2 3) (list 4 5 6))。</li><li>列表是在点对的基础上形成的一种特殊格式，所以可以通过pair构建list。用于pair的操作过程大多可以用于list。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(cadr ls)   ; 此&quot;点对&quot;对象的cdr的car</span><br><span class="line">(cddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr</span><br><span class="line">(caddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr的car</span><br><span class="line">(cdddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr的cdr</span><br></pre></td></tr></table></figure></li></ul><h3 id="类型判断-转换"><a href="#类型判断-转换" class="headerlink" title="类型判断/转换"></a>类型判断/转换</h3><ul><li>Scheme语言中所有判断都是用类型名加问号再加相应的常量或变量构成：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(type? variable)</span><br><span class="line">(boolean? #f)</span><br><span class="line">(char? #\a)</span><br><span class="line">(integer? 1)</span><br><span class="line">(real? 4/5)</span><br><span class="line">(null? &apos;())</span><br></pre></td></tr></table></figure><ul><li>Scheme中可以使用 “=”, “eq? “, “eqv? “, “equal? “判断是否相等</li><li>Scheme使用”-&gt;”表示类型转换：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(number-&gt;string 123)  ; 数字转换为字符串</span><br><span class="line">(string-&gt;number &quot;456&quot;)  ; 字符串转换为数字</span><br><span class="line">(char-&gt;integer #\a)   ;字符转换为整型数，小写字母a的ASCII码值为97</span><br><span class="line">(char-&gt;integer #\A)  ;大写字母A的值为65</span><br><span class="line">(integer-&gt;char 97)  ;整型数转换为字符#\a</span><br></pre></td></tr></table></figure></li></ul><h2 id="过程-Procedure"><a href="#过程-Procedure" class="headerlink" title="过程(Procedure)"></a>过程(Procedure)</h2><p>在Scheme语言中，过程相当于C语言中的函数，不同的是Scheme语言过程是一种数据类型，这也是为什么Scheme语言将程序和数据作为同一对象处理的原因。也正是因为如此，define不仅可以定义变量，还可以定义过程，不过标准过程定义要使用lambda关键字来标识。</p><h3 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h3><p>Scheme语言中可以用lambda来定义过程，其格式如下：<br>(define 过程名 ( lambda (参数 …) (操作过程 …)))<br>例：定义自增1的过程 add1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define add1 (lambda (x) (+ x 1)))</span><br></pre></td></tr></table></figure></p><p>该过程需要传入一个参数x，返回x+1的值。</p><h3 id="另一种过程定义方法"><a href="#另一种过程定义方法" class="headerlink" title="另一种过程定义方法"></a>另一种过程定义方法</h3><p>在Scheme语言中，也可以不用lambda，而直接用define来定义过程，它的格式为：<br>(define (过程名 参数) (过程内容 …))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define (add1 x) (+ x 1))</span><br></pre></td></tr></table></figure></p><h3 id="过程的嵌套定义"><a href="#过程的嵌套定义" class="headerlink" title="过程的嵌套定义"></a>过程的嵌套定义</h3><p>在Scheme语言中，过程定义也可以嵌套，一般情况下，过程的内部过程定义只有在过程内部才有效，相当C语言中的局部变量。</p><h2 id="常用结构"><a href="#常用结构" class="headerlink" title="常用结构"></a>常用结构</h2><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h3><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p>用法如同常规语言，Scheme的具体语法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(if (predicate)</span><br><span class="line">    (consequent)</span><br><span class="line">    (alternative))</span><br></pre></td></tr></table></figure></p><h4 id="cond"><a href="#cond" class="headerlink" title="cond"></a>cond</h4><p>用法如同C语言中的switch关键字，具体语法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(cond</span><br><span class="line">  (predicate1) (consequent1)</span><br><span class="line">  (predicate2) (consequent2)</span><br><span class="line">  ...</span><br><span class="line">  else (consequentn))</span><br><span class="line">; else 可写可不写</span><br></pre></td></tr></table></figure></p><h3 id="逻辑判断词"><a href="#逻辑判断词" class="headerlink" title="逻辑判断词"></a>逻辑判断词</h3><p>在Scheme中，使用逻辑关键字and，or，not表示布尔逻辑判断与或非：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(not #f)</span><br><span class="line">(and (#f) (#t))</span><br><span class="line">(or (#f) (#t))</span><br></pre></td></tr></table></figure></p><h3 id="case"><a href="#case" class="headerlink" title="case"></a>case</h3><p>case类似于枚举型的cond，具体用法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(case (* 2 4)</span><br><span class="line">  ((1 3 5 7) &apos;odd)</span><br><span class="line">  ((2 4 6 8) &apos;even))</span><br><span class="line">; 结果返回 even</span><br></pre></td></tr></table></figure></p><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>在Scheme中， 并没有定义循环关键字，但如同其他高级语言一样，Scheme支持递归调用，所以一般情况下Scheme通过递归来实现循环</p><h2 id="变量和过程的绑定"><a href="#变量和过程的绑定" class="headerlink" title="变量和过程的绑定"></a>变量和过程的绑定</h2><p>使用let, let*, letrec在lambda里面定义局部变量。</p><h3 id="let"><a href="#let" class="headerlink" title="let"></a>let</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(let (x 2) (y 5) (* x y))</span><br></pre></td></tr></table></figure><ul><li>letrec<br>letrec是将内部定义的过程或变量间的相互引用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(letrec ((countdown</span><br><span class="line">          (lambda (i)</span><br><span class="line">          (if (= i 0)</span><br><span class="line">              &apos;()</span><br><span class="line">              (countdown (- i 1)))))))</span><br></pre></td></tr></table></figure></li></ul><h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><p>apply的功能是为数据赋予某一个操作过程，他的第一个参数必须是一个过程，随后的阐述必须是一个列表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(apply + (list 1 2 3))</span><br><span class="line">&gt;&gt; 6</span><br><span class="line"></span><br><span class="line">(define scrum</span><br><span class="line">  (lambda (x)</span><br><span class="line">    (apply + x)))</span><br></pre></td></tr></table></figure></p><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>map的功能和apply有些相似，它的第一个参数也必需是一个过程，随后的参数必需是多个列表，返回的结果是此过程来操作列表后的值</p><h2 id="顺序结构"><a href="#顺序结构" class="headerlink" title="顺序结构"></a>顺序结构</h2><h3 id="begin"><a href="#begin" class="headerlink" title="begin"></a>begin</h3><p>通过begin过程实现顺序结构，用begin来将多个form放在一对小括号内，最终形成一个form。格式为：(begin form1 form2 …)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(begin</span><br><span class="line">    (display &quot;hello world&quot;)</span><br><span class="line">    (display &quot;goodbye world&quot;)</span><br><span class="line">    (newline))</span><br></pre></td></tr></table></figure></p><h3 id="scheme-quote函数"><a href="#scheme-quote函数" class="headerlink" title="scheme quote函数"></a>scheme quote函数</h3><p>引用（Quotation）</p><p>语法：(quote obj) 或者简写为 ‘obj</p><p>(+ 2 3)      ; 返回 5<br>‘(+ 2 3)     ; 返回列表 (+ 2 3)<br>(quote (+ 2 3)) ; 返回列表 (+ 2 3)</p>]]></content>
      
      <categories>
          
          <category> Programming Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop之HDFS</title>
      <link href="/2018/05/21/Hadoop/Hadoop%E4%B9%8BHDFS/"/>
      <url>/2018/05/21/Hadoop/Hadoop%E4%B9%8BHDFS/</url>
      <content type="html"><![CDATA[<p>介绍HDFS的结构和相关操作<br><a id="more"></a></p><h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><p>HDFS是Hadoop的分布式文件系统。HDFS具有如下特点：</p><ol><li>处理超大文件</li><li>流式地访问数据</li><li>运行于廉价的商用机器集群上<br>由于以上设计的种种考虑，HDFS存在着一定的缺点：</li><li>不适合低延迟数据访问</li><li>无法高效存储大量小文件（由于NameNode大小限制）</li><li>不支持多用户写入及任意修改文件（在HDFS的一个文件只有一个写入者，而且只能在文件末尾执行追加操作）<h2 id="HDFS体系结构"><a href="#HDFS体系结构" class="headerlink" title="HDFS体系结构"></a>HDFS体系结构</h2><h3 id="HDFS相关概念"><a href="#HDFS相关概念" class="headerlink" title="HDFS相关概念"></a>HDFS相关概念</h3><h4 id="块（Block）"><a href="#块（Block）" class="headerlink" title="块（Block）"></a>块（Block）</h4>在HDFS中，文件是以block的形式存储在各个主机上，HDFS默认一个文件块大小为64MB。而且当一个文件存储在HDFS时，该文件是按照逻辑块来进行分割存储的，而不是物理块。也就是说，一个文件的不同块可能被存储在不同的主机上，但逻辑上他们是存储在一起的。<br>在HDFS中，为了处理节点故障，默认将文件块副本数设定为3份，分别存储在集群的不同节点上，当一个块损坏时，NameNode会在另外的主机上读取一个副本并存储。<h4 id="NameNode和DataNode"><a href="#NameNode和DataNode" class="headerlink" title="NameNode和DataNode"></a>NameNode和DataNode</h4>NameNode和DataNode分别承担Master和Worker任务。NameNode管理文件系统的命名空间，维护整个文件系统的文件目录树以及文件的索引目录。从NameNode中你可以获得每个文件的每块所在的DataNode。<br>DataNode是文件系统的Worker节点，用来执行具体任务：存储文件块，被客户端和NameNode调用。同时，它通过心跳通信定时向NameNode发送存储文件的信息。<br>一个HDFS是由一个NameNode和一定数目的DataNode组成的。</li></ol><h2 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h2><ol><li><p>从本地将一个文件复制到HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal localPath hdfsPath</span><br></pre></td></tr></table></figure></li><li><p>从HDFS复制到本机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyToLocal hdfsPath localPath</span><br></pre></td></tr></table></figure></li><li><p>创建文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir DirName</span><br></pre></td></tr></table></figure></li><li><p>用命令行查看HDFS文件列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr DirName // 查看文件夹下的所有文件详细信息</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS常用Java-API"><a href="#HDFS常用Java-API" class="headerlink" title="HDFS常用Java API"></a>HDFS常用Java API</h2><h3 id="使用Hadoop-URL读取数据"><a href="#使用Hadoop-URL读取数据" class="headerlink" title="使用Hadoop URL读取数据"></a>使用Hadoop URL读取数据</h3><p>想从HDFS中读取数据，最简单的办法就是使用java.net.URL对象打开一个数据流，从中读取数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">InputStream in = null;</span><br><span class="line">try&#123;</span><br><span class="line">    in = new URL(&quot;hdfs://NameNodeIP/path&quot;).openStream();</span><br><span class="line">&#125;finally&#123;</span><br><span class="line">    IOUtils.closeStream(in);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之算法种类</title>
      <link href="/2018/05/13/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A7%8D%E7%B1%BB/"/>
      <url>/2018/05/13/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A7%8D%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>本小节主要介绍了强化学习的分类</p><a id="more"></a><h2 id="Model-free-amp-Model-based"><a href="#Model-free-amp-Model-based" class="headerlink" title="Model_free &amp; Model_based"></a>Model_free &amp; Model_based</h2><p>不理解环境和理解环境<br>Model_free是指Agent不会尝试理解环境，环境返回什么就是什么，然后根据环境返回的状态和奖励进行学习，而不试图去理解环境本身。Model_based值Agent理解环境如何运行，本质上是学习出一个模型来表示环境。<br>Model_free的代表算法有：Q_Learning，Sarsa，Policy Gradients。<br>Model_based RL只是多了一道程序：为真实世界建模，model_based RL可以通过想象来预判接下来将发生的情况，并选择这些想象中最优解。而Model_free只能按部就班等待环境的反馈。</p><h2 id="Policy-Based-amp-Value-Based"><a href="#Policy-Based-amp-Value-Based" class="headerlink" title="Policy_Based &amp; Value_Based"></a>Policy_Based &amp; Value_Based</h2><p>基于概率和基于价值<br>基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同. 而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作。<br>我们现在说的动作都是一个一个不连续的动作, 而对于选取连续的动作, 基于价值的方法是无能为力的. 我们却能用一个概率分布在连续动作中选取特定动作, 这也是基于概率的方法的优点之一.<br><br>Policy_Based代表算法：Policy Gradients<br>Value_Based代表算法那：Q_learning, Sarsa</p><h2 id="回合更新和逐步更新"><a href="#回合更新和逐步更新" class="headerlink" title="回合更新和逐步更新"></a>回合更新和逐步更新</h2><p>回合制更新是在本轮学习过程结束后，对学习过程中的每个决策进行更新. 而逐步更新则是学习过程中的每一步都在更新, 不用等待本轮学习结束，这样的好处是可以边学习边更新。7因为逐步更新更有效率, 所以现在大多方法都是基于逐步更新. 而且有的强化学习问题并不属于回合问题.</p><p>回合制更新算法：Monte-carlo learning 和基础版的policy gradients<br>逐步更新算法： Q_learning, Sarsa, 升级版的 policy gradients 等都是单步更新制.</p><h2 id="在线学习和离线学习"><a href="#在线学习和离线学习" class="headerlink" title="在线学习和离线学习"></a>在线学习和离线学习</h2><p>所谓在线学习, 就是指Agent必须“亲自”从环境的反馈中学习, 而离线学习则是可以通过观察别的Agent的学习过程来学习。同样是从过往的经验中学习, 离线学习中的过往经历没必要是自己的经历, 任何人的经历都能被学习.</p><p>在线学习算法：Sarsa 以及优化版Sarsa：Sarsa lambda<br>离线学习算法：Q_learning, Deep-Q-Network.</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SICP第二章:构造数据抽象</title>
      <link href="/2018/04/27/Reading/SICP%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/27/Reading/SICP%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>SICP的第二章：构造数据抽象<br><a id="more"></a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="为什么需要复合数据？"><a href="#为什么需要复合数据？" class="headerlink" title="为什么需要复合数据？"></a>为什么需要复合数据？</h3><p>与我们需要复合过程一样的原因一样：同样是为了提升我们在设计程序时所位于的概念层次，提高设计的模块性，增强语言的表达能力。正如定义过程的能力使我们有可能在更高的概念层次上处理计算工作一样，复合数据的能力，也将使我们得以在比语言提供的基本数据对象更高的概念层次上，处理与数据有关的各种问题。</p><h3 id="复合数据提高模块性"><a href="#复合数据提高模块性" class="headerlink" title="复合数据提高模块性"></a>复合数据提高模块性</h3><p>如果我们可以直接在将有理数本身当作对象的方式下操作他们，那么也就可能把处理有理数的那些程序部分，与有理数如何表示的细节隔离开。也就是说：将程序中处理数据对象的表示部分，和处理数据对象的使用的部分相互隔离。数据抽象技术能使程序更容易设计，维护和修改。</p><h2 id="2-1-数据抽象索引"><a href="#2-1-数据抽象索引" class="headerlink" title="2.1 数据抽象索引"></a>2.1 数据抽象索引</h2><ul><li><p>数据抽象使一种方法学，它使我们能将一个 <strong>复合数据对象的使用</strong>，与该数据对象怎样由 <strong>更基本的数据对象构造</strong> 起来的细节隔离开。（对于过程抽象，我们可以理解为：构造一个抽象，它将这一过程的 <strong>使用方式</strong>，和该过程究竟如何通过 <strong>更基本的过程实现的具体细节</strong> 相互分离）  </p></li><li><p>数据抽象的基本思想，就是设法构造出一些使用复合数据对象的程序。我们的程序使用数据的方式应该是这样的：除了完成当前工作所必要的东西之外，它们不对所有数据作任何假设，与此同时，一种‘具体’数据表示的定义，也应该与程序中使用数据的方式无关。这两个部分之间的界面是一组过程，称为 <strong>选择函数和构造函数</strong></p></li></ul><h3 id="2-1-1-实例：有理数的运算"><a href="#2-1-1-实例：有理数的运算" class="headerlink" title="2.1.1 实例：有理数的运算"></a>2.1.1 实例：有理数的运算</h3><p>假定我们要作有理数的算术，包括加减乘除等<br>假定已经有了一种从分子和分母构造有理数的方法，并进一步假定如果有了一个有理数，我们有一种方法取得他的分子和分母。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(make-rat &lt;n&gt; &lt;d&gt;) 返回一个有理数， 分子是n，分母是d</span><br><span class="line">(numer &lt;x&gt;) 返回分子</span><br><span class="line">(denom &lt;x&gt;) 返回分母</span><br></pre></td></tr></table></figure></p><p>目前，我们并不考虑上面的三个过程的具体实现，我们仅根据他们的功能，完成有理数的加减乘除：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(define (add-rat x y)</span><br><span class="line">  (make-rat (+ (* (numer x) (denom y))</span><br><span class="line">               (* (numer y) (denom x)))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (sub-rat x y)</span><br><span class="line">  (make-rat (- (* (numer x) (denom y))</span><br><span class="line">               (* (numer y) (denom x)))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (mul-rat x y)</span><br><span class="line">  (make-rat (* (numer x) (numer y))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (div-rat x y)</span><br><span class="line">  (make-rat (* (numer x) (denom y))</span><br><span class="line">            (* (numer y) (denom x))))</span><br><span class="line"></span><br><span class="line">(define (equal-rat? x y)</span><br><span class="line">  (let ((a (* (numer x) (denom y)))</span><br><span class="line">        (b (* (numer y) (denom x))))</span><br><span class="line">    (if (eqv? a b)</span><br><span class="line">        #t</span><br><span class="line">        #f)))</span><br></pre></td></tr></table></figure></p><p>这样，我们有了定义在三个构造过程基础之上的各种运算。而这些基础还没有定义。我们可以使用序对cons进行构造：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(define (make-rat n d) (cons n d))</span><br><span class="line">(define (numer x) (car x))</span><br><span class="line">(define (denom x) (cdr x))</span><br><span class="line">(define (print-rat x))</span><br></pre></td></tr></table></figure></p><h3 id="2-1-2-抽象屏障"><a href="#2-1-2-抽象屏障" class="headerlink" title="2.1.2 抽象屏障"></a>2.1.2 抽象屏障</h3><ul><li>一般而言，数据抽象的基本思想就是为每一类数据对象标示出一组 <strong>基本操作</strong>，使得对这类数据对象的所有操作都可以基于他们表述，而且在操作这些数据对象时也只能使用他们。</li><li>对于数据，可以有多个抽象层次，也就是多个抽象屏障，如有理数的定义：<blockquote><p>使用有理数的程序将仅仅通过有理数包提供的“API”（add-rat, sub-rat, mul-rat, div-rat..)去完成有理数操作；而这些过程转而又是完全基于构造函数和选择函数make-rat, numer, denom实现的。而这些函数又是基于序对实现的，只要序对可以通过cons,car和cdr操作。，有关序对如何实现的细节与有理数包的其余部分都完全没有关系。</p></blockquote></li></ul><h3 id="2-1-3-数据意味着什么"><a href="#2-1-3-数据意味着什么" class="headerlink" title="2.1.3 数据意味着什么"></a>2.1.3 数据意味着什么</h3><ul><li>可以考虑 <strong>序对</strong> ，我们从来没有说过序对是什么，只是说所有的语言为序对的操作提供了三个过程cons，car，cdr。有关这三个操作，我们需要知道的全部东西就是，三个过程的功能。</li><li><p>进一步思考，我们能发现一个令人吃惊的事实：我们完全可以不用任何数据结构，只使用过程就可以实现序对：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(define (cons x y)</span><br><span class="line">  (define (dispatch m)</span><br><span class="line">    (cond ((= m 0) x)</span><br><span class="line">          ((= m 1) y)</span><br><span class="line">          (else (error &quot;argument no 0 or 1&quot; m))))</span><br><span class="line">  dispatch)</span><br><span class="line">(define (car z) (z 0))</span><br><span class="line">(define (cdr z) (z 1))</span><br><span class="line"></span><br><span class="line">(car (cons 0 1)) -&gt; 0</span><br><span class="line">(cdr (cons 0 1)) -&gt; 1</span><br><span class="line"></span><br><span class="line">; cons过程会返回一个过程，而过程car，cdr会使用cons返回的过程并传入；参数，而在cons返回的过程中针对传入的参数进行操作。</span><br></pre></td></tr></table></figure></li><li><p>可以看出，cons返回一个过程，而对这三个方法的定义，完全满足了序对的定义。从这个例子可以看出，我们无法把这一实现和“真正的”数据结构区分开。</p></li><li><strong>数据的过程性表示</strong> 将在我们的程序设计中扮演一种核心角色。有关的程序设计风格通常称为 <strong>消息传递</strong></li></ul><h2 id="2-2-层次性数据和闭包性质"><a href="#2-2-层次性数据和闭包性质" class="headerlink" title="2.2 层次性数据和闭包性质"></a>2.2 层次性数据和闭包性质</h2><ul><li>序对为我们提供了一种用于构造复合数据的基本“粘合剂”。我们可以建立元素本身也是序对的序对，这就是表结构得以作为一种表示工具的根本基础。我们将这种能力成为cons的闭包性质。</li><li>一般的说，某种组合数据对象的操作满足闭包性质，那就是说，通过它组合起来的数据对象得到的结果本身还可以通过同样的操作再进行组合。</li></ul><h3 id="2-2-1-序列的表示"><a href="#2-2-1-序列的表示" class="headerlink" title="2.2.1 序列的表示"></a>2.2.1 序列的表示</h3><ul><li><p>可以使用cons构造序列，同时Scheme提供过程list用以快速构造序列：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(list &lt;a1&gt; &lt;a2&gt; ..) = (cons (a1 (cons a2 (cons ..))))</span><br></pre></td></tr></table></figure></li><li><p>使用map过程，对一个list的所有元素进行操作映射。</p></li></ul><h3 id="2-2-2-层次性结构"><a href="#2-2-2-层次性结构" class="headerlink" title="2.2.2 层次性结构"></a>2.2.2 层次性结构</h3><p>使用list和cons可以构建复合形式的数据，我们可以将它看作树结构，进行递归访问。</p><h3 id="2-2-3-序列作为一种约定的界面"><a href="#2-2-3-序列作为一种约定的界面" class="headerlink" title="2.2.3 序列作为一种约定的界面"></a>2.2.3 序列作为一种约定的界面</h3><p>对于一个复杂的过程，我们可以将其拆分成一个信号流系统，将这个复杂的过程分割成不同的子过程，并且各个子过程之间用信号表示流动。<br>例：给定一个树，计算值为奇数的叶子的平方和。最原始的思想解题过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define (sum-odd-squares tree)</span><br><span class="line">  (cond ((null? tree) 0)</span><br><span class="line">        ((not (pair? tree))</span><br><span class="line">         (if (odd? tree)</span><br><span class="line">             (square tree)</span><br><span class="line">             0))</span><br><span class="line">        (else (+ (sum-odd-squares (car tree))</span><br><span class="line">                 (sum-odd-squares (cdr tree))))))</span><br></pre></td></tr></table></figure></p><p>我们可以将上面这个复杂的过程归纳为信号流结构：</p><ol><li>枚举出一棵树的每个树叶</li><li>过滤它们，选出其中的奇数树叶</li><li>对选出的每一个数求平方</li><li>用+累加起来得到结果，从0开始</li></ol><p>但在上面的原始过程中，我们并没有体现出信号流结构。我们需要重新组织这些程序，使之能够清晰的反应上面信号流的结构，其中最关键的一点就是将注意力集中在处理过程中从一个步骤流向下一个步骤的“信号”。   </p><ul><li><p>枚举一棵树的所有树叶：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define (enumerate-tree tree)</span><br><span class="line">  (cond ((null? tree) nil)</span><br><span class="line">        ((not (pair? tree)) (list tree))</span><br><span class="line">        (else (append (enumerate-tree (car tree))</span><br><span class="line">                      (enumerate-tree (cdr tree))))))</span><br><span class="line"></span><br><span class="line">(enumerate-tree (list 1 (list 2 (list 3 4)) 5))</span><br><span class="line">(1 2 3 4 5)</span><br></pre></td></tr></table></figure></li><li><p>按照给定谓词过滤元素：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(define (filter predicate sequence)</span><br><span class="line">  (cond ((null? sequence) nil)</span><br><span class="line">        ((predicate (car sequence))</span><br><span class="line">         (cons (car sequence)</span><br><span class="line">               (filter predicate (cdr sequence))))</span><br><span class="line">        (else (filter predicate (cdr sequence)))))</span><br><span class="line"></span><br><span class="line">(filter odd? (list 1 2 3 4 5))</span><br><span class="line">(1 3 5)</span><br></pre></td></tr></table></figure></li><li><p>实现信号流图中的映射步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(map square (list 1 2 3 4 5))</span><br></pre></td></tr></table></figure></li><li><p>实现流图中的累加过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(define (accumulate op initial sequence)</span><br><span class="line">  (if (null? sequence)</span><br><span class="line">      initial</span><br><span class="line">      (op (car sequence)</span><br><span class="line">          (accumulate op initial (cdr sequence)))))</span><br></pre></td></tr></table></figure></li><li><p>最后，将整个流程整合起来形成一个过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(define (sum-odd-squares tree)</span><br><span class="line">  (accumulate +</span><br><span class="line">              0</span><br><span class="line">              (map square</span><br><span class="line">                   (filter odd?</span><br><span class="line">                           (enumerate-tree tree)))))</span><br></pre></td></tr></table></figure></li></ul><p>将程序表示为一些针对序列的操作，这样做的价值就在于能帮助我们得到模块化的程序设计，也就是说，得到由一些比较独立的片段的组合构成的设计。在工业设计中，模块化结构是控制复杂性的一种威力强大的策略。</p><h3 id="2-2-4-实例分析：一个图形语言"><a href="#2-2-4-实例分析：一个图形语言" class="headerlink" title="2.2.4 实例分析：一个图形语言"></a>2.2.4 实例分析：一个图形语言</h3><p>在描述一种语言时，应该将注意力集中到语言的基本原语，它的组合手段以及它的抽象手段。详细内容见书P86.</p><h2 id="2-3-符号数据"><a href="#2-3-符号数据" class="headerlink" title="2.3 符号数据"></a>2.3 符号数据</h2><p>到目前为止，我们已经使用过的所有复合数据都是从数值出发构造起来的，在这一节，我们要扩充所用语言的表述能力，引进将任意符号作为数据的功能。</p><h3 id="2-3-1-使用引号表示字符"><a href="#2-3-1-使用引号表示字符" class="headerlink" title="2.3.1 使用引号表示字符"></a>2.3.1 使用引号表示字符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define a 1)</span><br><span class="line">(define b 2)</span><br><span class="line">(list a b)</span><br><span class="line">&gt;&gt;(1 2)</span><br><span class="line">(list &apos;a &apos;b)</span><br><span class="line">&gt;&gt;(a b)</span><br><span class="line">(list &apos;a b)</span><br><span class="line">&gt;&gt;(a 2)</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> SICP </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN模型演化过程</title>
      <link href="/2018/04/27/Deep%20Learning/CNN%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8C%96%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/04/27/Deep%20Learning/CNN%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8C%96%E8%BF%87%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>本文介绍了从LeNet到ResNet的CNN模型演化过程</p><a id="more"></a><h2 id="几个经典CNN模型"><a href="#几个经典CNN模型" class="headerlink" title="几个经典CNN模型"></a>几个经典CNN模型</h2><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>卷积神经网络的鼻祖，由两个卷积层、两个池化层和两个全连接层组成，卷积都是5*5的模板，stride=1，池化都是MAX。</p><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>AlexNet最大的贡献是证明了CNN在复杂模型上的有效性。</p><ul><li>在LeNet的基础上使用了5个卷积层，五个最大池化层和三个全连接层。输出端是一个softmax。</li><li>为了加快训练速度，使用了ReLU和GPU并行</li><li>为了减少过拟合，采用了dropout和data augmentation（数据增强）以及LRN（局部相应归一化）</li><li>AlexNet确立了CNN在CV中的统治地位</li></ul><h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p>相当于AlexNet的增强版，有着结构简单但深度更深，精度更强的优势。特点是连续卷积多，计算量巨大。</p><ul><li>VGG 与 AlexNet 最鲜明的对比是卷积层、卷积核设计的变化。VGGNet 探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠 3x3 的小型卷积核和 2x2 的最大池化层，成功构筑了 16~19 层深的卷积神经网络。</li><li>证明了可以用多个小卷积核堆叠等价于一个大卷积核，而多个小卷积核的参数量要更少</li><li>VGG性能优异：同 AlexNet 提升明显，同 GoogleNet, ResNet 相比，表现相近</li><li>VGG是选择最多的基本模型，方便进行结构的优化、设计，SSD, RCNN，等其他任务的基本模型(base model)</li></ul><h4 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h4><ul><li>GoogleNet的特点是结构复杂，多分辨率融合，证明了用更多的卷积，更深的层次可以得到更好的效果</li><li>GoogleNet主要目标是找到最优的稀疏结构单元，也就是 Inception module。Inception 结构是将不同的卷积层通过并联的方式结合在一起，它主要改进了网络内部计算资源的利用率，让我们能在固定计算资源下增加神经网络深度和宽度。</li><li>GooLeNet 的 Inception 对特征图进行了三种不同的卷积(1x1, 3x3, 5x5)来提取多个尺度的信息，也就是提取更多的特征。举个例子，一张图片有两个人，近处一个远处一个，如果只用 5x5，可能对近处的人的学习比较好，而对远处那个人，由于尺寸的不匹配，达不到理想的学习效果，而采用不同卷积核来学习，相当于融合了不同的分辨率，可以较好的解决这个问题。把这些卷积核卷积后提取的 feature map进行聚合操作合并作为输出，但会发现这样结构下的参数暴增，耗费大量的计算资源。</li><li>改进方案，在 3x3，5x5 之前，以及 pooling 以后都跟上一个 1x1 的卷积用以降维，就可以在提取更多特征的同时，大量减少参数，降低计算量。1x1 的卷积核性价比很高，很小的计算量就能增加一层特征变换和非线性化(如果后面接 ReLU)，另外，这也是一种降维的方式，可以减少过拟合。</li><li>GoogleNet实现了参数数量更少，但准确率更高的效果。</li><li>提出了Batch Normalization</li></ul><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><h2 id="ResNet网络"><a href="#ResNet网络" class="headerlink" title="ResNet网络"></a>ResNet网络</h2><p>深度卷积网络自然的整合了低中高不同层次的特征，特征的层次可以靠加深网络的层次来丰富。从而，在构建卷积网络时，网络的深度越高，可抽取的特征层次就越丰富。<br>但是当使用更深层的网络时，会发生梯度消失、爆炸问题，这个问题很大程度通过标准的初始化和正则化层来基本解决，这样可以确保几十层的网络能够收敛，但是随着网络层数的增加，梯度消失或者爆炸的问题仍然存在。</p><h3 id="网络退化问题"><a href="#网络退化问题" class="headerlink" title="网络退化问题"></a>网络退化问题</h3><p><strong>网络退化问题</strong>，举个例子，假设已经有了一个最优化的网络结构，是18层。当我们设计网络结构的时候，并不知道多少层的网络最优化网络结构，假设设计了34层网络结构。那么多出来的16层其实是冗余的，我们希望训练网络的过程中，模型能够自己训练这五层为恒等映射，也就是经过这层时的输入与输出完全一样。但是往往模型很难将这16层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。它不是由过拟合产生的，而是由冗余的网络层学习了不是恒等映射的参数造成的。</p><h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>ResNet希望网络可以自动训练出哪些隐藏层是冗余的，并将冗余层的输入输出相等。具体方法是将原网络的基层改成一个残差块，残差块的构造如下：<br><img src="images/deep_learning/resnet_block.png" alt="ResNet_block"><br>X是这一层残差块的输入，也称作F(x)为残差，x为输入值，F（X）是经过第一层线性变化并激活后的输出，该图表示在残差网络中，第二层进行线性变化之后激活之前，F(x)加入了这一层输入值X，然后再进行激活后输出。在第二层输出值激活前加入X，这条路径称作shortcut连接。</p><h3 id="残差块原理"><a href="#残差块原理" class="headerlink" title="残差块原理"></a>残差块原理</h3><p>假设该层是冗余的，我们想让该层的映射h(x)=x，但这是比较困难的。ResNet想到避免去学习该层恒等映射的参数，使用了如上图的结构，让h(x)=F(x)+x;这里的F(x)我们称作残差项，我们发现，要想让该冗余层能够恒等映射，我们只需要学习F(x)=0。学习F(x)=0比学习h(x)=x要简单，因为一般每层网络中的参数初始化偏向于0，这样在相比于更新该网络层的参数来学习h(x)=x，该冗余层学习F(x)=0的更新参数能够更快收敛。</p><h3 id="ResNet使用技巧"><a href="#ResNet使用技巧" class="headerlink" title="ResNet使用技巧"></a>ResNet使用技巧</h3><p>如果遇到了h(x) = F(x)+x中，F(x)和x维度不同，需要对x进行线性变换。可以使用zero-padding方法增加维度。或者采用一个新的1*1卷积映射但这样会增加参数。<br>ResNet由于自身网络结构原因，进行反向传播时的梯度恒大于1，解决了梯度消失问题。</p><h3 id="ResNet特点"><a href="#ResNet特点" class="headerlink" title="ResNet特点"></a>ResNet特点</h3><ul><li>由微软提出。ResNet的层数最多，训练用了8个GPU三周完成。</li><li>最大特性是允许原始输入信息传输到后面的层中，可以将一个卷积层的输出和输入相融合后再输入到下一个卷积层。这样使得后面的层可以直接学习残差，整个神经网络只需要学习输入输出差别的那部分，简化了学习目标和难度。</li></ul>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之GBDT</title>
      <link href="/2018/04/22/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/"/>
      <url>/2018/04/22/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>Gradient Boosting主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。 <strong>GBDT树内部是多个回归CART树</strong><br>主要由三个概念组成：回归决策树，Gradient Boosting，Shrinkage。</p><h3 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h3><p>GBDT的核心在于每个决策树是对上一个决策树预测“误差”的学习，然后累加所有树的结果作为最终结果，而 <strong>分类树的结果累加是没有意义的， 所以GBDT中的树都是回归树，不是分类树</strong>。<br>GBDT调整后可以用于分类问题，但内部还是回归树。<br><strong>回归树在每个节点（不一定是叶子节点）都会得到一个预测值。</strong> 该预测值为属于这个节点所有人label值的平均值。构建回归树进行分割属性选择时按照回归树的属性选择方式。</p><h3 id="梯度迭代"><a href="#梯度迭代" class="headerlink" title="梯度迭代"></a>梯度迭代</h3><p><img src="/images/other_ML_knowledge/gbdt_alg.png" alt="GBDT"><br>GBDT学习过程如上图所示，主要流程是：</p><ol><li>初始化f0(x) = 0</li><li>对于m=1,2,..M<ol><li>计算之前所有树的残差</li><li>拟合残差学习一颗回归树（选用平方损失函数时，对其求偏导的负梯度等于残差近似值）</li><li>更新回归树集合（进行相加时通常会使用Shrinkage）</li></ol></li><li>M次迭代后，得到GBDT模型，为M个回归树的加性模型<br>从GBDT的算法过程可以发现 <strong>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，而当损失函数为平方损失函数时，梯度值正好等于这个残差值，也就是说残差向量都是它的全局最优方向，这就是 Gradient。</strong><br>比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。<h3 id="GBDT-工作实例"><a href="#GBDT-工作实例" class="headerlink" title="GBDT 工作实例"></a>GBDT 工作实例</h3>还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下所示结果：<br><img src="/images/other_ML_knowledge/decision_tree.jpg" alt="decision_tree"><br>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果：<br><img src="/images/other_ML_knowledge/boosting_tree.jpg" alt="decision_tree"><br>在GBDT的第一棵树中，(A,B)和(C,D)被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。<h3 id="GBDT优点"><a href="#GBDT优点" class="headerlink" title="GBDT优点"></a>GBDT优点</h3>正在上述例子中，决策树和GBDT可以达到同样的效果，而GBDT的优点就是减小 <strong>过拟合问题。</strong><br>图1为了达到100%精度使用了3个feature（上网时长、时段、网购金额），其中分枝“上网时长&gt;1.1h” 很显然已经过拟合了，这个数据集上A,B也许恰好A每天上网1.09h, B上网1.05小时，但用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的；<br>相对来说图2的boosting虽然用了两棵树 ，但其实只用了2个feature就搞定了，后一个feature是问答比例，显然图2的依据更靠谱。 <strong>Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。</strong><h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即<h4 id="没用Shrinkage"><a href="#没用Shrinkage" class="headerlink" title="没用Shrinkage"></a>没用Shrinkage</h4>y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) =  y真实值 - y(1 ~ i)<br>y(1 ~ i) = SUM(y1, …, yi)<h4 id="使用Shrinkage"><a href="#使用Shrinkage" class="headerlink" title="使用Shrinkage"></a>使用Shrinkage</h4>Shrinkage不改变第一个方程，只把第二个方程改为：<br>y(1 ~ i) = y(1 ~ i-1) + step <em> yi<br>**Shrinkage对于残差学习出来的结果，只累加一小部分（step</em>残差）逐步逼近目标，step一般都比较小，如0.01~0.001，导致各个树的残差是渐变的而不是陡变的。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight**。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。<h3 id="GBDT的适用范围"><a href="#GBDT的适用范围" class="headerlink" title="GBDT的适用范围"></a>GBDT的适用范围</h3></li></ol><ul><li>GBDT 可以适用于回归问题（线性和非线性）；</li><li>GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。</li></ul><h3 id="GBDT用于分类"><a href="#GBDT用于分类" class="headerlink" title="GBDT用于分类"></a>GBDT用于分类</h3><p>首先，无论是回归任务还是分类任务，GBDT内部的决策树都是CART回归树。而由于GBDT是不断拟合上一棵树的残差，而在分类任务中类别的相减是没有意义的。所以GBDT应用于分类任务时，需要对模型进行一定的修改。</p><h4 id="为每一个类构建GBDT"><a href="#为每一个类构建GBDT" class="headerlink" title="为每一个类构建GBDT"></a>为每一个类构建GBDT</h4><p>GBDT用于分类任务的方法核心思想是：<strong>对每一个类都构建一个GBDT</strong>。<br>举个例子：分类任务一共有三个类，我们可以用one-hot-encoding来表示每个样本的类别，设样本x属于第二类，则x对应的label为：[0, 1, 0]。</p><h3 id="GBDT和随机森林比较"><a href="#GBDT和随机森林比较" class="headerlink" title="GBDT和随机森林比较"></a>GBDT和随机森林比较</h3><h4 id="GBDT和随机森林的相同点"><a href="#GBDT和随机森林的相同点" class="headerlink" title="GBDT和随机森林的相同点"></a>GBDT和随机森林的相同点</h4><ul><li>都是由多棵树组成，最终的结果都由多棵树共同决定。<h4 id="GBDT和随机森林的不同点"><a href="#GBDT和随机森林的不同点" class="headerlink" title="GBDT和随机森林的不同点"></a>GBDT和随机森林的不同点</h4></li><li>组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；</li><li>组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；</li><li>对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；</li><li>随机森林对异常值不敏感，GBDT 对异常值非常敏感；</li><li>随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；<h3 id="GBDT可调参数"><a href="#GBDT可调参数" class="headerlink" title="GBDT可调参数"></a>GBDT可调参数</h3><h4 id="框架参数"><a href="#框架参数" class="headerlink" title="框架参数"></a>框架参数</h4></li></ul><ol><li>n_estimators：基学习器的个数</li><li>learning_rate: 即每个基学习器的缩减权重。</li><li>subsample：不放回子采样</li><li>init：初始弱学习器</li><li>loss：损失函数，对于分类模型可以选择对数损失和指数损失。对于回归模型有均方差，绝对损失等<h4 id="基学习器参数"><a href="#基学习器参数" class="headerlink" title="基学习器参数"></a>基学习器参数</h4></li><li>max_feature：划分时考虑的最大特征数（类似于随机森林的随机特征选择）</li><li>max_depth：决策树最大深度，可以不输入，但这样在构建子树时不会限制子树的深度。通常情况下推荐限制该值</li><li>min_sample_split：内部节点再划分所需要最小样本数</li><li>min_samples_leaf：叶节点最少样本数<br>…<h3 id="GBDT常见问题"><a href="#GBDT常见问题" class="headerlink" title="GBDT常见问题"></a>GBDT常见问题</h3></li><li>GBDT 相比于决策树有什么优点<br>泛化性能更好！GBDT 的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于 0。这样后面就更加专注于那些分错的样本。</li><li>Gradient 体现在哪里？<br>可以理解为残差是全局最优的绝对方向，类似于求梯度。<h3 id="GBDT用作特征选择"><a href="#GBDT用作特征选择" class="headerlink" title="GBDT用作特征选择"></a>GBDT用作特征选择</h3>GBDT用于特征选择时，主要通过计算特征j在单棵树中重要度的平均值：</li></ol><p><img src="/images/other_ML_knowledge/gbdt_feature_select.png" alt="feature_select"><br>其中，M是树的数量，L为树的叶子节点数量，L-1为树的非叶子节点数量，vt是和节点t相关联的特征，hat(ti2)是节点t分裂后平凡损失的减少值。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之决策树</title>
      <link href="/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><ul><li>优点：计算量小，可解释性强，比较适合处理有缺失值的样本，能够处理不相关特征</li><li>容易过拟合（集成树方法减少了过拟合现象）<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3></li><li>信息熵的计算</li><li>信息熵值越小，纯度越高。<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3></li><li>信息增益的计算</li><li>一般而言，信息增益越大，意味着使用属性a来进行划分所获得的“纯度提升”越大。</li><li>缺点：信息增益倾向于选择可取值数目较多的属性。</li><li>ID3决策树生成算法采用信息增益生成决策树（见P75）</li><li>基于信息增益的划分法会偏向选取取值多的属性（如果用ID进行划分，也就是每个分治只有一个样本，每个分治纯度最大，但这样显然不具备泛化能力）<h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3></li><li>增益率为：分割属性的信息增益/分割属性的信息熵</li><li>缺点：增益率倾向于选择取值数目较少的属性</li><li>为克服基于信息增益的算法偏向选择属性多的缺点，ID4.5基于信息增益率生成决策树</li><li>增益率准则倾向选择取值少的属性，因此C4.5算法并不是简单选用增益率划分，而是使用了一个启发式方法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<h3 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h3></li><li>CART树使用基尼系数进行属性划分</li><li>如果是二分类问题，基尼系数为：Gini(p) = 2p(1-p)</li><li>直观来说，基尼系数反应了从数据集随机抽取两个样本，其类别标记不一致的概率，因此Gini系数越小，数据集的纯度越高。</li></ul><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><ul><li>决策树剪枝的基本策略有“预剪枝”和“后剪枝”。<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3></li></ul><ol><li>预剪枝是指在决策树生成过程中，首先从数据集中分割出验证集，然后对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；<strong>比较划分前后在验证集的精度是否提高</strong></li><li>预剪枝有欠拟合风险<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3></li><li>后剪枝则是先训练一颗完整的决策树，然后自底向上的对非叶节点进行考察，若将该节点对应的子节点替换为叶节点能带来决策树泛化性能的提升（在验证集上准确率提高），则将该子树替换为叶节点。 <strong>比较划分前后在验证集的精度是否提高</strong></li><li>后剪枝也可以通过构建决策树loss function，最小化loss实现。</li><li>后剪枝欠拟合风险小，泛化性能优于预剪枝，但需要先生成再剪枝，开销大。</li></ol><h2 id="连续值和缺失值"><a href="#连续值和缺失值" class="headerlink" title="连续值和缺失值"></a>连续值和缺失值</h2><h3 id="连续值"><a href="#连续值" class="headerlink" title="连续值"></a>连续值</h3><ul><li>对于取值为连续值的属性，考察该属性的所有取值，并从小到大排列。分别以任意相邻的取值的中值作为分割点，计算信息增益，并选取信息增益最大的点作为分割点。（其中可以将原节点的信息熵改为期望，因为每个取值的概率都为1/n） （具体公式见书84页）</li><li>对于回归树的连续型属性，考察该属性的所有取值，分别以任意相邻节点取值的中值作为分割点，<strong>计算平方损失</strong>，找到平方损失最小的连续型属性和该属性的分割节点。</li><li>与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3>三种情况：</li></ul><ol><li>如何在属性值缺失的情况下进行划分属性的选择（如何重构信息增益）<br>假如使用ID3算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的熵增。然后结果乘0.9（新样本占raw样本的比例），就是a属性最终的熵。<strong>可以理解为对各个属性的信息增益进行加权计算</strong></li><li>分类属性选择完成，对分类属性缺失的样本，如何进行划分？<br>比如该节点是根据a属性划分，但是待分类样本a属性缺失，就把该样本分配到两个子节点中去，但是权重由1变为（每个子节点中实例数目/父节点实例数目），直观的看，就是让同一个样本以不同的概率划入到不同的子节点中去。在子节点的继续分裂时，计算比例时缺失值的使用权重而不是为1.</li><li>训练完成，给测试集样本分类，有缺失值怎么办？<br>这时候不能按比例分配，因为必须给该样本一个确定的label。这时候可以 <strong>根据投票来确定，或者填充缺失值。</strong></li></ol><h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><ul><li>通常决策树相当于用多个平行于轴线的线段对特征空间进行分割。这样的分类边界使得学习结果具有较好的可解释性，因为每一段划分都直接对应了某个属性的取值，<strong>但在分类边界比较复杂时，对于同一个属性，必须使用很多段划分才能获得较好的近似，此时决策树会相当复杂</strong>。</li><li><strong>若能使用斜的划分边界，则决策树模型会大大简化</strong>，“多变量决策树” 就是能实现这样斜划分甚至更复杂划分的决策树。</li><li>以斜划分为例，在此类决策树中，<strong>非节点不再是仅针对某个属性，而是对属性的线性组合进行测试，换言之，每个非叶节点是一个形如 wa=t 的线性分类器，w表示属性a的权重，w和t可以在节点所含的样本集和属性集上学得</strong></li><li>于是，与传统的单变量决策树不同，在多变量决策树的学习过程中，不是为每个节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</li></ul><h2 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h2><p>CART树是分类决策树的缩写，有如下特点：</p><ol><li>一定是二叉树</li><li>既可用于分类也可以用于回归</li><li>对于分类树，使用 <strong>基尼指数</strong> 作为划分标准</li><li>对于回归树，使用 <strong>平方损失最小</strong> 作为划分标准，叶节点的预测值为叶节点所有label的平均值。</li><li>CART剪枝方法：从生成的决策树上不断剪枝，直到根节点，形成一个子树序列。然后用交叉验证集对每个子树进行验证，找出泛化误差最小的子树。</li></ol><h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p>Adaboost本质上是一个加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类算法。<br>基本思想是不断调整所有训练数据的训练权重，增加上一个基学习器分类错误数据的权重。<br>主要步骤如下：</p><ol><li>初始化训练数据的权重分布为1/n</li><li>训练一个基分类器，对所有数据进行分类</li><li>根据每个数据的分类结果和权重，计算分类误差率。</li><li>计算当前基分类器的权重：’a = 1/2 log((1-e)/e)’ (误差率越小，a越大)</li><li>更新所有数据的权重</li><li>重复2~5步，直至模型收敛。</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之xgboost</title>
      <link href="/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8Bxgboost/"/>
      <url>/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8Bxgboost/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h2 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h2><p>目前最好最快的boosting tree</p><p>如果不考虑工程实现、解决问题上的一些差异，xgboost与gbdt比较大的不同就是目标函数的定义。<br><img src="/images/other_ML_knowledge/xgboost_fomula.jpg" alt="xgboost_fomula"><br>注：红色箭头指向的l即为损失函数；红色方框为正则项，包括L1、L2正则项；红色圆圈为常数项。xgboost将损失函数做二阶泰勒展开做一个近似，我们可以看到，最终的目标函数只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。</p><h3 id="原理详解"><a href="#原理详解" class="headerlink" title="原理详解"></a>原理详解</h3><p><img src="/images/other_ML_knowledge/xgboost_math1.jpg" alt="gbdt_math"><br>对GBDT的目标函数（平方损失）进行泰勒2阶展开，得到如上图推导过程。相当于对每个叶节点的数据计算一阶二阶导数。</p><p><img src="/images/other_ML_knowledge/xgboost_math2.png" alt="gbdt_math2"><br>如上图所示，然后obj(t)对w求偏导得0，最后得到obj(t)的表示。wi为当前树对数据i的预测值。<br>推导上述目标函数公式后，单棵决策树的学习过程如下：</p><ol><li>枚举所有可能的树结构q（<strong>xgboost对该步骤进行了并行</strong>）</li><li>用上述目标函数为每个q计算目标函数分数，分数越小说明对应的树结构越好</li><li>根据上一步的结果，找到最佳树结构，并计算每个叶节点的预测值wj<h3 id="xgboost的并行"><a href="#xgboost的并行" class="headerlink" title="xgboost的并行"></a>xgboost的并行</h3>xgboost支持并行。xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<h3 id="xgboost对比GBDT"><a href="#xgboost对比GBDT" class="headerlink" title="xgboost对比GBDT"></a>xgboost对比GBDT</h3></li></ol><ul><li>GBDT缺点明显：boost是一个串行过程，不好并行化，计算复杂度高。而XGB在每一个基决策树的构造时并行遍历所有可能的树，计算obj(t)的值找到最有树。</li><li>GBDT是直接拟合负梯度（等于残差），xgboost对目标函数进行泰勒展开，引入二阶导。</li><li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</li><li>两者都用到了Shrinkage（缩减），</li><li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。<h3 id="xgboost进行特征选择"><a href="#xgboost进行特征选择" class="headerlink" title="xgboost进行特征选择"></a>xgboost进行特征选择</h3>XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+…)。</li></ul>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop入门</title>
      <link href="/2018/04/18/Hadoop/Hadoop%E5%85%A5%E9%97%A8/"/>
      <url>/2018/04/18/Hadoop/Hadoop%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<p>Hadoop如今已经成为了大数据处理的代名词，无论是云计算，机器学习还是后端开发都离不开大数据的支持，这篇文章就是我整理的关于Hadoop的入门概念。<br><a id="more"></a><br>Hadoop是一个分布式数据存储和分析系统，存储是由HDFS实现，分析是由MapReduce实现。纵然Hadoop还有其他功能，但这些功能是它的核心所在。</p><h4 id="大数据的三个‘V’："><a href="#大数据的三个‘V’：" class="headerlink" title="大数据的三个‘V’："></a>大数据的三个‘V’：</h4><ul><li>volume 数据量非常大</li><li>variety 数据有各种来源，也就导致了数据以各种形式存储</li><li>velocity 系统可能需要以非常快的速度接受数据，所以存储速度和处理速度都非常重要</li></ul><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>MapReduce是一种用于数据处理的编程模型，MapReduce支持多种编程语言，MapReduce本质上是并行的。</p><h3 id="map和reduce"><a href="#map和reduce" class="headerlink" title="map和reduce"></a>map和reduce</h3><p>MapReduce的工作过程分为两个阶段：<strong>map阶段和reduce阶段</strong>，每个阶段都有键值对作为输入和输出，并且他们的类型可以由程序员选择，程序员还需要定义两个函数map函数和reduce函数。</p><p>用一个列子说明MapReduce的工作过程：从海量的天气数据中找到气温最高的记录。<br>运行过程如下：</p><ol><li>在map阶段输入原始数据，对数据进行处理，使得reduce可以在此基础上进行工作：找出每年的最高气温。在map过程中，将筛选掉缺失的，不可靠的或者错误的气温数据。</li><li>map经过对数据的处理，输出（年份，气温）键值对。</li><li>map函数的输出先由MapReduce框架处理，然后再发送到reduce函数，这一处理过程根据键值对进行排序和分组。</li><li>reduce函数会看到（年份，[气温1， 气温2，。。。]）的输入，每个年份后都跟着一系列的气温。</li><li>reduce函数重复这个列表并找出每年中最高温度。<br><img src="iamges/hadoop_introduce/example1.png" alt="temperature"><h3 id="Java-MapReduce"><a href="#Java-MapReduce" class="headerlink" title="Java MapReduce"></a>Java MapReduce</h3>在理解MapReduce工作流程后，尝试使用Java来实现这个流程，我们需要实现三样东西：map函数，reduce函数以及一些运行作业的代码。map函数时由一个Mapper接口来实现的，声明了一个map()方法。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperatureMapper extends MapReduceBase</span><br><span class="line">    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    private static final int MISSING = 9999;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void map(LongWritable key, Text value, Context context) throws IOException&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String year = line.substring(15, 19);</span><br><span class="line">        int airTemperature;</span><br><span class="line">        if (line.charAt(87) == &apos;+&apos;) //parseInt doesnot like leading plus signs</span><br><span class="line">            airTemperature = Integer.parseInt(line.substring(88, 92));</span><br><span class="line">        else</span><br><span class="line">            airTemperature = Integer.parseInt(line.substring(87, 92));</span><br><span class="line">        String quality = line.substring(92, 93);</span><br><span class="line">        if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;))</span><br><span class="line">            context.write(new Text(year), new IntWritable(airTemperature));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>Mapper是一个泛型类型，有四个参数，分别为输入键，输入值，输出键和输出值的类型<br>map方法需要传入一个键一个值。我们将Text转换成Java的String类型，然后利用substring提取感兴趣的列<br>map方法还提供了一个OutputCollector实例来写入输出内容。<br>Hadoop 规定了一套可用于网络序列优化的基本类型，不同于Java内置类型，可以在org.apache.hadoop.io包找到<br>LongWritable相当于Java的Long型，Text相当于String，IntWritable相当于Integer</p><p>接下来reduce方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void reduce(Text key, Interator&lt;IntWritable&gt; values, Context context)</span><br><span class="line">        throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        int maxValue = Integer.MIN_VALUE;</span><br><span class="line">        while(values.hasNext())&#123;</span><br><span class="line">            maxValue = Math.max(maxValue, values.next().get());</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, new IntWritable(maxValue));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后是主函数，负责设定和启动整个MapReduce<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperature &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        if (args.length != 2)&#123;</span><br><span class="line">            System.err.println(&quot;Usage: MaxTemperature &lt;input path&gt;</span><br><span class="line">                &lt;output path&gt;&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = new Job();</span><br><span class="line">        //JobConf对象指定了作业执行规范，授予你对整个作业如何运行的控制权</span><br><span class="line">        job.setJarByClass(MaxTemperature.class);</span><br><span class="line">        job.setJobName(&quot;Max temperature&quot;);</span><br><span class="line"></span><br><span class="line">        //指定文件的输入和输出路径,可以多次调用addInputPath添加多路径，addOutputPath只能有一个</span><br><span class="line">        FileInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.addOutputPath(job, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        //指定要使用的map和reduce类</span><br><span class="line">        job.setMapperClass(MaxTemperatureMapper.class);</span><br><span class="line">        job.setReduceClass(MaxTemperatureReducer.class);</span><br><span class="line"></span><br><span class="line">        //指定map和reduce函数的输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0:1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>具体解释已经在代码中注明。<br>在Hadoop上运行这个作业时，要把代码打包成一个JAR文件，不必明确指定JAR文件的名称，在Job对象setJarByClass()方法中传递一个类即可，Hadoop利用这个类来查找包含它的JAR文件。</p><h4 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h4><ol><li>客户端提交编写好的MapReduce代码到Hadoop，Hadoop会先到HDFS中查看目标文件的大小，了解要获取数据的规模，然后形成任务分配的规划，例如：<blockquote><p>a.txt 0-128M交给一个task，128-256M 交给一个task，b.txt 0-128M交给一个task，128-256M交给一个task …，形成规划文件job.split。</p></blockquote></li></ol><p>然后把规划文件job.split、jar、配置文件xml提交给yarn（Hadoop集群资源管理器，负责为任务分配合适的服务器资源）</p><ol start="2"><li>启动appmaster<br>appmaster是本次job的主管，负责maptask和reducetask的启动、监控、协调管理工作。<br>yarn找一个合适的服务器来启动appmaster，并把job.split、jar、xml交给它。</li><li>启动maptask<br>Appmaster启动后，根据固化文件job.split中的分片信息启动maptask，<strong>一个分片对应一个maptask。</strong><br>分配maptask时，会尽量让maptask在目标数据所在的datanode上执行。</li><li>执行maptask<br>maptask会一行行地读目标文件，交给我们写的map程序，读一行就调一次map方法进行数据统计处理。<br>然后map调用context.write把处理结果写出去，保存到本机的一个结果文件，这个文件中的内容是分区且有序的。<br>分区的作用就是定义哪些key在一组，一个分区对应一个reducer。</li><li>启动reducetask<br>maptask都运行结束后，appmaster再启动reducetask，maptask的结果中有几个分区，就启动几个reducetask。</li><li>执行reducetask<br>reducetask读取maptask的结果文件自己对应的需要处理的数据。reducetask把读取的数据按照key组织好，传给reduce方法进行处理，最后把处理结果写到指定的输出路径中。</li></ol><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>这里我们只是使用MapReduce，现在我们需要把数据存在HDFS上，由此允许Hadoop将MapReduce计算转移到存储有部分数据的各台机器上。</p><h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h3><p>MapReduce是在客户端需要执行的一个工作单元，包括多个任务。</p><p>有两类节点控制MapReduce作业的执行过程：一个jobtracker一个tasktracker。前者通过调度后者上运行的任务来协调所有运行在系统上的作业。后者在运行任务的同时将运行进度报告发给jobtracker，jobtracker由此记录每项作业任务的整体进度情况。如果其中一个任务失败，jobtrracker可以在另一个tasktracker节点上重启该任务。</p><h4 id="map过程"><a href="#map过程" class="headerlink" title="map过程"></a>map过程</h4><p><strong>Hadoop将MaprReduce的输入数据划分成等长的数据块，称为‘分片’。Hadoop为每一个分片构建一个map任务，并由该任务来运行用户自定义的map函数从而处理分片中的任务。 因为Hadoop并行运行多个map处理每个分片，整体时间花费较小</strong></p><p>一个合理的分片大小趋向于HDFS的一个块大小，默认是64MB。Hadoop在存储有输入数据的节点上运行map任务，可以获得最佳性能，因为它不需要集群带宽资源。</p><p>map任务将其输出写入本地硬盘，而非HDFS，因为map的输出是中间结果，该结果还需要由reduce任务处理才产生最终结果，而一旦整体任务完成，中间结果就可以被删除，所以存在HDFS上有点小题大做。</p><h4 id="reduce过程"><a href="#reduce过程" class="headerlink" title="reduce过程"></a>reduce过程</h4><p>reduce的输入往往来自所有mapper的输出。因此，排过序的map输出需要通过网络传输发送到运行reduce任务的节点，数据再reduce端合并，然后由用户定义的reduce函数处理。reduce的输出通常存储在HDFS中。<br><img src="/images/hadoop_introduce/mapreduce_flow.png" alt="mapreduce_flow"><br>上图是整个MapReduce的数据流。<br>reduce任务的数量并非由输入数据大小决定，是需要独立指定的。</p><p>如果有多个reduce任务，map任务会针对输出进行分区（partition），即为每个reduce任务建一个分区。分区由用户定义的partition函数控制。<br><img src="/images/hadoop_introduce/mapreduce_flow2.png" alt="mapreduce_flow"></p><h3 id="combiner函数"><a href="#combiner函数" class="headerlink" title="combiner函数"></a>combiner函数</h3><p>Hadoop允许用户对map任务的输出指定一个combiner函数，该函数的输出作为reduce的输入。由于combiner属于优化方案，所以Hadoop无法确定map任务输出记录调用多少次combiner。</p><p>我们可以理解combiner函数是对map函数输出数据的进一步处理，通过这种方法优化map和reduce任务之间的数据传递效率。<br>我们可以再之前的程序中修改Job的设置来指定combiner类<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setJarByClass(MaxTemperatureWithCombiner.class);</span><br><span class="line">job.setCombinerClass(MaxTemperatureReducer.class);</span><br></pre></td></tr></table></figure></p><h1 id="Hadoop分布式文件系统"><a href="#Hadoop分布式文件系统" class="headerlink" title="Hadoop分布式文件系统"></a>Hadoop分布式文件系统</h1><h2 id="HDFS概念"><a href="#HDFS概念" class="headerlink" title="HDFS概念"></a>HDFS概念</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>HDFS定义每个数据块大小为64MB，HDFS上的文件被划分为块大小的多个分块(chunk)，作为独立的存储单元。<br>这么做的优点是文件的所有块并不需要存在同一个磁盘上，可以利用集群上任意个磁盘进行存储。</p><h3 id="NameNode和DataNode"><a href="#NameNode和DataNode" class="headerlink" title="NameNode和DataNode"></a>NameNode和DataNode</h3><p>HDFS集群有两类节点以 <strong>一个管理者(NameNode)多个工作者(DataNode)模式运行。</strong></p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode管理文件系统的命名空间，维护者文件系统树及整棵树内所有文件和目录。同时NameNode也记录着每个文件中各个块所在数据节点信息。<br>NameNode在内存中保存文件系统中每个文件和数据块的引用关系，如果NameNode损坏，意味着我们失去了整个HDFS的存储信息。<br>如果集群过大，NameNode会成为限制系统横向扩展的瓶颈，Hadoop引入联邦HDFS允许系统通过添加NameNode实现扩展，每个NameNode管理HDFS中的一部分。</p><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode是HDFS的工作节点，它们根据需要存储并检索数据块并定期向NameNode发送它们所存储块的列表。</p><h3 id="通过FileSystem-API读取数据"><a href="#通过FileSystem-API读取数据" class="headerlink" title="通过FileSystem API读取数据"></a>通过FileSystem API读取数据</h3><h3 id=""><a href="#" class="headerlink" title="???"></a>???</h3><p>Hadoop文件系统通过Hadoop Path对象来表示文件，可以将路径视为一个Hadoop文件系统的URI，如’hdfs://localhost/user/tom/text.txt’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class FileSystemCat&#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String uri = args[0];</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        FIleSystem fs = FileSystem.get(URI.create(uri), conf);</span><br><span class="line">        InputStream in = null;</span><br><span class="line">        try &#123;</span><br><span class="line">            in = fs.open(new Path(uri));</span><br><span class="line">            IOUtils.copyBytes(in, System.out, 4096, false);</span><br><span class="line">        &#125;finally&#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sklearn学习笔记</title>
      <link href="/2018/04/16/Machine%20Learning/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/16/Machine%20Learning/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>Sklearn是机器学习领域最知名的python模块之一，广泛的应用到各种机器学习项目中。Sklearn使用方便，大多数ML算法的调用形式相同，极其易于入门。</p><a id="more"></a><p><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">http://scikit-learn.org/stable/</a><br>Sklearn包含了多种机器学习方式：</p><ul><li>Classification分类</li><li>Regression回归</li><li>Clustering非监督聚类</li><li>Dimensionality reduction数据降维</li><li>Preprocessing数据预处理</li></ul><h2 id="Sklearn安装"><a href="#Sklearn安装" class="headerlink" title="Sklearn安装"></a>Sklearn安装</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>Linux可以直接使用pip进行安装，方便快捷。需要说明的是，sklearn需要Numpy和Scipy模块的支持。也就是说，在安装sklearn前，需要确定电脑已经安装了依赖模块。</p><h3 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h3><p>windows安装建议通过Anaconda来安装所有科学计算需要的模块，方便快捷。</p><h2 id="Sklearn模型选择"><a href="#Sklearn模型选择" class="headerlink" title="Sklearn模型选择"></a>Sklearn模型选择</h2><p>在官网上，有一个机器学习模型选择流程图，在进行机器学习模型选择时可以用作参考：<br><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a></p><h2 id="Sklearn初探"><a href="#Sklearn初探" class="headerlink" title="Sklearn初探"></a>Sklearn初探</h2><p>Sklearn把所有及其模型的使用模式整合在了一起，所有模型的调用方式都是一样的。也就是说，学会了一种机器学习模型的使用方法，也就掌握了所有模型的使用方式。</p><h3 id="K近邻方法"><a href="#K近邻方法" class="headerlink" title="K近邻方法"></a>K近邻方法</h3><p>让我们使用Sklearn自带的数据集，使用k近邻算法，探索Sklearn模块的使用模式：</p><ol><li><p>首先导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br></pre></td></tr></table></figure></li><li><p>载入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_X = iris.data</span><br><span class="line">iris_y = iris.target</span><br><span class="line"></span><br><span class="line">print(iris_X[:3,:])</span><br><span class="line">print(iris_y[:3])</span><br></pre></td></tr></table></figure></li><li><p>训练数据集测试数据集分离并训练K近邻模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)</span><br><span class="line"></span><br><span class="line">kNN = KNeighborsClassifier()</span><br><span class="line">kNN.fit(X_train, y_train)</span><br></pre></td></tr></table></figure></li><li><p>预测和评分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prediction = kNN.predict(X_test)</span><br><span class="line">score = kNN.score(X_test, y_test)#分类准确率</span><br><span class="line">probability = kNN.predict_proba(X_test)</span><br><span class="line">neighborpoint=knn.kneighbors(iris_x_test[-1],5,False)</span><br><span class="line">#计算与最后一个测试样本距离在最近的5个点，返回的是这些样本的序号组成的数组</span><br></pre></td></tr></table></figure></li></ol><h2 id="模型常用属性和功能"><a href="#模型常用属性和功能" class="headerlink" title="模型常用属性和功能"></a>模型常用属性和功能</h2><h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><p>如上文代码所示：</p><ul><li>model.fit(x_train, y_labels)可以对模型进行训练</li><li>model.predict(x_test)使用训练好的模型进行预测</li><li>每个模型在训练和预测时，都有非常多的参数可以调整，对于新手来说，可以直接使用默认参数。当熟练掌握模型后，可以通过对参数的调整来优化模型。</li></ul><h3 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h3><p>Sklearn对每个模型都提供方法，用以取出模型的具体参数信息：</p><ul><li>model.coef_, model.intercept_就属于model的属性。例如对线性回归来说，这两个参数分别代表模型的斜率和截距。</li><li>model.get_params()函数可以返回在模型定义时，定义的参数。</li></ul><h3 id="预测评分"><a href="#预测评分" class="headerlink" title="预测评分"></a>预测评分</h3><ul><li>model.score(x_test, y_test)可以输出模型对测试集合的预测评分。</li></ul><p>以上就是最常见的模型使用方法：1.创建模型 2.训练模型 3.预测/测试模型</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>Sklearn提供便捷强大的数据预处理模块，方便工程师对数据进行快速处理</p><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line">data = preprocessing.scale(data)</span><br></pre></td></tr></table></figure><h2 id="常用机器学习模型"><a href="#常用机器学习模型" class="headerlink" title="常用机器学习模型"></a>常用机器学习模型</h2><h3 id="k近邻"><a href="#k近邻" class="headerlink" title="k近邻"></a>k近邻</h3><p>sklearn.neighbors封装了所有与k近邻相关的算法模型，包括非监督kNN，分类kNN，回归kNN，以及kNN等几个实现算法等</p><h4 id="常用参数："><a href="#常用参数：" class="headerlink" title="常用参数："></a>常用参数：</h4><ul><li>n_neighbors：kNN中的k值</li><li>radius：限定半径最近邻中的半径</li><li>algorithm：实现算法，提供’ball_tree’, ‘kd_tree’, ‘auto’, ‘brute’</li><li>metric：距离度量方法，提供’euclidean’, ‘manhattan’, ‘chebyshev’, ‘minkowski’等，一般使用默认欧式距离<h4 id="非监督k近邻"><a href="#非监督k近邻" class="headerlink" title="非监督k近邻"></a>非监督k近邻</h4>‘from sklearn.neighbors import NearestNeighbors’</li></ul><h4 id="分类kNN"><a href="#分类kNN" class="headerlink" title="分类kNN"></a>分类kNN</h4><p>‘from sklearn.neighbors import KNeighborsClassifier’</p><h4 id="回归kNN"><a href="#回归kNN" class="headerlink" title="回归kNN"></a>回归kNN</h4><p>‘from sklearn.neighbors import KNeighborsRegressor’</p><h4 id="实现算法"><a href="#实现算法" class="headerlink" title="实现算法"></a>实现算法</h4><p>‘from sklearn.neighbors import KDTree’<br>‘from sklearn.neighbors import BallTree’</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python装饰器</title>
      <link href="/2018/04/12/Programming%20Language/Python%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
      <url>/2018/04/12/Programming%20Language/Python%E8%A3%85%E9%A5%B0%E5%99%A8/</url>
      <content type="html"><![CDATA[<p>python装饰器类似于java中的面向切面编程，是python的高级语法， 可以将各个函数中重复的操作（插入日志，性能测试，事务处理，缓存等）抽离出来，将这些于函数原本功能无关的雷同代码定义在装饰器中，概括的讲： <strong>装饰器的作用就是为已经存在的对象添加额外的功能</strong><br><a id="more"></a></p><h2 id="装饰器简单应用"><a href="#装饰器简单应用" class="headerlink" title="装饰器简单应用"></a>装饰器简单应用</h2><p>看如下代码：decorator函数即定义一个装饰器，foo为一个常规的函数。该代码即为用decorator对f函数进行装饰。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper():</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func()</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">def func():</span><br><span class="line">print(&apos;i am a normal function&apos;)</span><br><span class="line"></span><br><span class="line">foo = decorator(foo)</span><br><span class="line">foo()</span><br><span class="line"># 调用装饰过得foo函数会首先运行wrapper()中的内容，然后返回foo函数</span><br></pre></td></tr></table></figure></p><h2 id="装饰器的语法糖"><a href="#装饰器的语法糖" class="headerlink" title="装饰器的语法糖"></a>装饰器的语法糖</h2><p>在上面对装饰器的简单实用中，我们发现每次想要对函数进行装饰时，都需要进行一步赋值操作 ‘foo = decorator(foo)’，这种操作让程序变得更为复杂难以调试，所以我们可以使用语法糖完成对函数进行装饰的过程。</p><p>将装饰器的语法糖‘@’放到函数定义的地方，这样就可以省略最后一步的赋值操作。可以将上面的例子改写为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper():</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func()</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo():</span><br><span class="line">print(&apos;i am a normal func&apos;)</span><br><span class="line"></span><br><span class="line">foo()</span><br></pre></td></tr></table></figure></p><p>如上所示，使用语法糖，我们就可以省略最后一步的赋值操作。foo函数不需要作任何修改，只需要在定义函数的地方加上装饰器的语法糖，调用的时候还是和以前一样。</p><h2 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h2><p>以上只是对装饰器最简单的应用，在现实应用中，业务逻辑函数会更加复杂，如果业务逻辑函数foo需要参数，这样在装饰器中我们似乎是需要修改参数，但通过在装饰器定义可变参数，可以更加便捷的实现该功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper(*args):</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func(*args)</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo(name, age)</span><br><span class="line">  print(&apos;....&apos;)</span><br></pre></td></tr></table></figure></p><p>这样以来，不管业务函数需要多少个参数，我们都不需要对装饰器进行过多的更改。<br>同时，如果在业务逻辑函数中存在指定关键字参数。我们可以在wrapper中定义指定关键字参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper(*args, **kargs):</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func(*args, **kargs)</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo(name, age, height=None)</span><br><span class="line">  print(&apos;....&apos;)</span><br></pre></td></tr></table></figure></p><h2 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h2><p>在装饰器中，如果对传入的不同业务逻辑函数需要作不同的装饰，可以在业务逻辑函数定义使用装饰器语法糖时指定参数，从而达到在装饰器中分别处理的目的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def use_logging(level):</span><br><span class="line">    def decorator(func):</span><br><span class="line">        def wrapper(*args, **kwargs):</span><br><span class="line">            if level == &quot;warn&quot;:</span><br><span class="line">                logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class="line">            elif level == &quot;info&quot;:</span><br><span class="line">                logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class="line">            return func(*args)</span><br><span class="line">        return wrapper</span><br><span class="line"></span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line">@use_logging(level=&quot;warn&quot;)</span><br><span class="line">def foo(name=&apos;foo&apos;):</span><br><span class="line">    print(&quot;i am %s&quot; % name)</span><br></pre></td></tr></table></figure></p><p>上面的 use logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=”warn”)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。<br>@use_logging(level=”warn”)等价于@decorator</p><h2 id="类装饰器"><a href="#类装饰器" class="headerlink" title="类装饰器"></a>类装饰器</h2><p>装饰器不仅仅是函数，也可以是类，使用类作为装饰器更加灵活强大。类装饰器主要依靠类的’<strong>call</strong>‘方法，当使用@形式将装饰器附加到函数上时，就会调用该方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Foo(object):</span><br><span class="line">  def __init__(self, func):</span><br><span class="line">    self._func = func</span><br><span class="line">  def __call__(self):</span><br><span class="line">    print(&apos;class decorator runing&apos;)</span><br><span class="line">    self._func()</span><br><span class="line">    print(&apos;class decorator ending&apos;)</span><br><span class="line"></span><br><span class="line">@Foo</span><br><span class="line">def bar():</span><br><span class="line">  print(&apos;bar&apos;)</span><br></pre></td></tr></table></figure></p><h2 id="装饰器顺序"><a href="#装饰器顺序" class="headerlink" title="装饰器顺序"></a>装饰器顺序</h2><p>对于一个函数，我们当然可以定义多个装饰器：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@a</span><br><span class="line">@b</span><br><span class="line">@c</span><br><span class="line">def f():</span><br><span class="line">  pass</span><br></pre></td></tr></table></figure></p><p>这个函数的执行顺序是从里向外的，最先调用定义在最后的装饰器，等效于：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f = a(b(c(f)))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Programming Language </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习常见问题和知识点整理</title>
      <link href="/2018/04/11/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"/>
      <url>/2018/04/11/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。<br>在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。</p><a id="more"></a><h1 id="模型评估选择"><a href="#模型评估选择" class="headerlink" title="模型评估选择"></a>模型评估选择</h1><ul><li>学习器在训练集上的误差称为训练误差或者经验误差，在新样本上的误差称为泛化误差<h4 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h4>详情见我的另一篇总结文章<a href="https://yhfeather.github.io/2017/10/12/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/#more" target="_blank" rel="noopener">生成模型和判别模型</a><h4 id="模型评估方法"><a href="#模型评估方法" class="headerlink" title="模型评估方法"></a>模型评估方法</h4></li><li>留出法：直接将数据集分为两个不相交的子集，一个训练，一个测试。一般需要多次划分对评估结果取平均值。</li><li>交叉验证：将数据集分成k个大小相同的子集，每次用一个子集i作为测试，用剩下的k-1作为训练，然后遍历k个子集会产生k个训练/测试结果。留一法：让k=数据集大小，也就是说每次直选一个样本进行测试。</li><li>自助法：有放回的从原始数据集采样，每次取一个数据复制放入新数据集，然后将该数据放回，重复采样直到新数据集大小为m，初始数据集中会有36%的样本未出现在新数据集中，用新数据训练，用未出现的数据集做测试。 自助法在数据集小，难以有效划分时有用，但改变了初始数据集的分布，引入了估计偏差。<h4 id="模型性能度量"><a href="#模型性能度量" class="headerlink" title="模型性能度量"></a>模型性能度量</h4></li><li>错误率和精度</li><li>查准率，查全率，F1值</li><li>P-R曲线（查准率，查全率曲线）<ol><li>将预测结果从最可能到最不可能进行排序，按照此顺序逐个把样本作为正例进行预测，并记录每一刻的P-R值</li><li>可以比较PR曲线下的面积来比较模型性能，或者比较平衡点（P==R）的大小</li><li>可以根据不同任务需求采用不同的截断点，更重视P则选择排序靠前的位置截断。。。</li></ol></li><li>ROC曲线&amp;AUC<ul><li>同样将样本从可能正例到不可能正例排序，首先个将所有样例均预测为反例，这时真正例率和假正例率都为0，然后调整阈值，依次将每个样例划分为正例。每次计算真正例率和假正例率绘制ROC曲线</li><li>真正例率：所有真实正例中被预测为正例的比例（对正例预测正确）</li><li>假正例率：所有真实反例中被预测为正例的比例（对反例预测错误）</li><li>RUC考虑的事样本预测的排序质量，因此它与排序误差有紧密联系</li><li>ROC曲线下方围成的面积即为AUC</li><li>因为AUC为点连成的非光滑曲线，所以可以通过计算各个相邻点组成的小矩形面积之和得出AUC</li></ul></li></ul><ol start="5"><li>代价敏感误差率与代价曲线<ul><li>构建有代价权重的混淆矩阵，然后可以依据此计算表示代价的ROC曲线<h4 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h4></li></ul></li><li>假设检验</li><li>交叉验证t检验</li><li>McNemar检验from</li><li>Friedman检验和Nemenyi后续检验</li><li>偏差和方差<h3 id="问题补充"><a href="#问题补充" class="headerlink" title="问题补充"></a>问题补充</h3></li><li>训练集中类别不均衡，哪个参数最不准确？<br>准确度（Accuracy）。 eg.训练集中class 1的样本数比class 2的样本数是60:1。使用逻辑回归进行分类，最后结果是其忽略了class 2，即其将所有的训练样本都分类为class 1。</li></ol><h1 id="常见基本问题"><a href="#常见基本问题" class="headerlink" title="常见基本问题"></a>常见基本问题</h1><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>多分类学习的基本思路是“拆解法”，即将多个分类任务拆为若干个二分类任务求解。经典的拆分策略有三种：‘一对一’，‘一对余’，‘多对多’</p><ol><li>一对一OvO：将N个类别两两配对，产生N*(N-1)/2个二分类任务。在 <strong>测试阶段</strong> ，新样本将同时提交给所有分类器，于是我们得到N(N-1)/2个分类结果，最终结果可以通过投票产生，即把被预测最多的类别作为最终分类结果。</li><li>一对余OvR：每次将一个类作为正例，剩下的N-1个类作为反例来训练N个分类器。在 <strong>测试阶段</strong> 若仅有一个分类器预测为正例，则对应的类别标记作为最终分类结果。若有多个分类器预测为正例，则通常考虑各个分类器的预测置信度，选择置信度最大的类别标记为分类结果。</li><li>多对多MvM：每次将若干类作为正类，若干类其他类作为反类，显然OvO和OvM是MvM的特例。MvM正反类的构造必须有特殊的设计，不能随意选取，通常使用“纠错输出码（ECOC）”技术。<ul><li>ECOE技术详见书第65页</li></ul></li></ol><h3 id="样本类别不平衡问题"><a href="#样本类别不平衡问题" class="headerlink" title="样本类别不平衡问题"></a>样本类别不平衡问题</h3><ol><li>对于线性分类y=wx+b, 我们通常认为y&gt;0.5为正例y&lt;0.5为反例，说明几率y/(1-y)的阈值为0.5，表示分类器认为正反例可能性相同。所以我们只需要修改分类器预测几率的阈值即可：若y/(1-y) &gt; m+/m-则判定为正例。</li><li>解决类别不平衡学习的一个基本策略：<strong>再缩放</strong>，对预测出的几率乘以正负例的比值即为模型的预测输出。</li><li>实际应用中共有三种做法： <strong>欠采样，过采样，阈值移动</strong><ul><li>欠采样即去除一些反例使得正反例数目接近，由于欠采样可能丢失重要信息，实践中使用EasyEnsemble利用集成学习将反例划分为若干个集合供不同学习器使用，这样对于每个学习器来说都是欠采样，但在全局看来不会丢失信息。</li><li>过采样即增加一些正例使得正反例数目接近（不能简单地对正例进行重复否则会产生严重的过拟合，SMOTE算法通过对训练集里的正例进行插值来产生额外的正例。</li><li>阈值移动即为开始所述，对预测几率乘以正反例数量的比值</li></ul></li><li>调整正负样本的惩罚权重<br>对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。使用这种方法时不需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。</li><li>集成学习，组合学习<br>组合/集成方法指的是：将多数类样本数据集分为几份，每份大小与少数类样本数据集大小相同，每次用一份多数类样本和所有少数类样本组成训练集训练模型，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。</li></ol><h3 id="过拟合解决办法"><a href="#过拟合解决办法" class="headerlink" title="过拟合解决办法"></a>过拟合解决办法</h3><ol><li>L1，L2正则</li><li>模型早停</li><li>增加样本，数据增强（比如说对图片进行平移旋转剪裁）</li><li>针对神经网络：Batch Normalization</li><li>针对神经网络：Dropout</li></ol><h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="最小二乘法和其矩阵形式"><a href="#最小二乘法和其矩阵形式" class="headerlink" title="最小二乘法和其矩阵形式"></a>最小二乘法和其矩阵形式</h2><pre><code>1. 基于均方误差最小化来进行模型求解的方法称为最小二乘法2. 在线性回归模型中最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小3. 最小二乘法即对每个参数求偏导，并令其等于0.4. 最小二乘法的矩阵表示和解法（原理相同，只是变为对矩阵求偏导）5. **对数线性回归** 实质上已是在求输入空间到输出空间的非线性函数映射：ln(y) = wx + b6. **广义线性模型** y = g-1(wx + b)</code></pre><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ul><li>优点：实现简单，分类时计算量非常小，速度快，存储资源低</li><li>缺点：容易欠拟合，准确度不高。只能处理二分类问题（衍生的softmax可解决），样本必须线性可分。<h4 id="LR推导"><a href="#LR推导" class="headerlink" title="LR推导"></a>LR推导</h4>掌握<h4 id="几率和对数几率"><a href="#几率和对数几率" class="headerlink" title="几率和对数几率"></a>几率和对数几率</h4><ol><li>y/(1-y)称为几率，反映了样本作为正例的相对可能性。对几率取对数为对数几率</li><li>ln(y/(1-y)) = wx + b 等价于 y = 1/(1+exp(-wx-b))</li><li>由上式可看出，LR本质上是训练一个线性模型，逼近真是label的对数几率，也就是数据作为正例的相对可能性。<h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2></li></ol></li><li>书第60页<br>LDA思想：给定训练数据集，设法将样例投影到一条直线上，是的同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。<strong>衡量同类样例距离尽可能小只需要让协方差尽可能小，异类样例距离尽可能大只需要让类中心之间的距离尽可能大。</strong><br>在对新样例进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。</li></ul><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="为什么不直接对loss-function求偏导-0，而要用梯度下降？"><a href="#为什么不直接对loss-function求偏导-0，而要用梯度下降？" class="headerlink" title="为什么不直接对loss function求偏导=0，而要用梯度下降？"></a>为什么不直接对loss function求偏导=0，而要用梯度下降？</h3><ol><li>逻辑回归没有解析解，就是说无法显式的表现函数的偏导，只能通过数值求解的方法迭代地找到最优解。</li><li>即使有解析解，大部分神经网络的loss为非凸函数，KKT条件（偏导为0是其中一项）仅仅是非凸函数最优化的必要非充分条件。</li><li>偏导为0可能并非是局部极值</li></ol><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>详情请见我另一篇整理文章: <a href="https://yhfeather.github.io/2018/04/21/%E5%90%84%E7%A7%8DBoosting%E6%A0%91/" target="_blank" rel="noopener">各种树: 从决策树到xgboost</a></p><h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="传统支持向量机"><a href="#传统支持向量机" class="headerlink" title="传统支持向量机"></a>传统支持向量机</h2><p>距离超平面最近的几个训练样本点使距离等式成立，这几个样本点称为支持向量。两个异类支持向量到超平面的距离之和称为“间隔”。</p><h2 id="支持向量机对偶形式"><a href="#支持向量机对偶形式" class="headerlink" title="支持向量机对偶形式"></a>支持向量机对偶形式</h2><h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><p>掌握</p><h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>KKT条件是一个具有很强几何意义的结论。需要掌握</p><h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO的基本思路是先固定ai之外的所有参数，然后求ai上的极值，由于存在约束sum(aiyi)=0，若固定ai之外的其他变量则ai可以由其他变量导出。于是SMO每次选择两个变量ai，aj。并固定其他参数，这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><pre><code>1. 选取一对需要更新的变量ai和aj2. 固定剩余的参数，求解SVM对偶式的公式获得更新后的ai和aj</code></pre><p>注意到只要ai和aj有一个不满足KKT条件，目标函数就会在迭代后增大，KKT条件违背程度越大，变量更新后可能导致的目标函数值增幅越大。<br>于是SMO先选取违背KKT条件程度最大的变量，第二个选取使目标函数值增幅最快的变量，但比较变量复杂度过大，SMO采用启发式方法：选取两个变量所对应样本之间的间隔最大，这样的两个变量有很大差别，对目标函数的更新有更大帮助。   </p><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>可将样本从原始空间映射到一个高维看空间，使得样本在这个高维空间线性可分。如果原始空间是有限维，即属性有限，那么一定存在一个高维空间使样本可分。<br>我们发现SVM的对偶式中包含xi和xj的内积运算，使用映射方法只需要将原始空间内的内积运算转换为高维映射空间内的映射运算即可。但映射空间往往维度过高，计算复杂，所以假设存在核函数k(xi, xj) = xi,xj的內积，有了这个核函数在对偶式中只需将內积替换成核函数即可。</p><h4 id="核函数条件"><a href="#核函数条件" class="headerlink" title="核函数条件"></a>核函数条件</h4><p>只要一个对称函数所对应的和矩阵半正定，它就能作为核函数使用。</p><h4 id="核函数的种类"><a href="#核函数的种类" class="headerlink" title="核函数的种类"></a>核函数的种类</h4><p>核函数选择成了SVM的最大变数，若核函数不适合，样本被映射到了一个不适合的空间，会导致性能不佳。<br>常见的核函数有：</p><ol><li>线性核</li><li>多项式核</li><li>高斯核</li><li>拉普拉斯核</li><li>Sigmoid核<br>此外，还可以通过函数线性组合得到新的核函数。</li></ol><h2 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h2><p>在现实任务中我们很难恰到好处的找个某个核函数使得训练集在特征空间线性可分，也很难推是否是由过拟合造成的。所以我们允许SVM在一些样本上出错，引入软间隔概念。同时引入松弛变量的概念。</p><h4 id="软间隔SVM推导"><a href="#软间隔SVM推导" class="headerlink" title="软间隔SVM推导"></a>软间隔SVM推导</h4><p>掌握（软间隔的对偶形式和硬间隔的对偶形式唯一区别在于对ai的约束，软间隔的约束为’0&lt;=ai&lt;=C’, 硬间隔的约束仅为’0&lt;=ai’</p><h4 id="软间隔SVM的KKT条件"><a href="#软间隔SVM的KKT条件" class="headerlink" title="软间隔SVM的KKT条件"></a>软间隔SVM的KKT条件</h4><p>理解</p><h2 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h2><p>对于传统回归模型通常直接计算模型输出和真实输出之间的差别来计算损失，只有当两个输出完全相同时loss才为0。SVR容忍预测和label之间最多有e的偏差，即当f预测和label之间的差别大于e才计算损失。相当于以f为中心，构建了一个宽度为2e的间隔带，样本落入该间隔带则被认为是分类正确的。</p><h4 id="SVR公式推导"><a href="#SVR公式推导" class="headerlink" title="SVR公式推导"></a>SVR公式推导</h4><p>假设我们能容忍f(x)和y之间最多有e的偏差，即当f(x)与y之间的差别绝对值大于e时才计算损失。相当于以f(x)为中心，构建了一个宽度为2e的间隔带，若样本落入此间隔带则认为被预测正确。</p><h2 id="补充考点"><a href="#补充考点" class="headerlink" title="补充考点"></a>补充考点</h2><h4 id="SVM优缺点"><a href="#SVM优缺点" class="headerlink" title="SVM优缺点"></a>SVM优缺点</h4><ul><li>优点：<ol><li>可用于线性/非线性分类，也可以用于回归</li><li>泛化能力强，泛化误差较低</li><li>小训练集上往往就可以得到较好的结果</li></ol></li><li>缺点：<ol><li>对参数和核函数的选择比较敏感，核函数选取比较难</li><li>原始的SVM只比较擅长处理二分类问题</li><li>时空开销都比较大<h4 id="SVM和LR的异同"><a href="#SVM和LR的异同" class="headerlink" title="SVM和LR的异同"></a>SVM和LR的异同</h4></li></ol></li></ul><ol><li>两者都为线性模型</li><li>两者都是判别模型</li><li>本质上来讲是损失函数的不同，LR为交叉熵损失（对数损失），SVM为hinge损失</li><li>由于损失函数不同，LR直接依赖所有数据分布，SVM只关心支持向量</li><li>SVM依赖距离测度，数据需要归一化，LR不受影响</li><li>SVM自带正则项，LR需要在Loss Function上添加。<h4 id="SVM多分类"><a href="#SVM多分类" class="headerlink" title="SVM多分类"></a>SVM多分类</h4></li></ol><ul><li>一对多，选择预测结果最大值</li><li>一对一，然后采用投票</li><li>层次SVM，层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。<h4 id="带核的SVM为什么能分类非线性问题？"><a href="#带核的SVM为什么能分类非线性问题？" class="headerlink" title="带核的SVM为什么能分类非线性问题？"></a>带核的SVM为什么能分类非线性问题？</h4>核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积<h4 id="RBF核一定是线性可分的吗"><a href="#RBF核一定是线性可分的吗" class="headerlink" title="RBF核一定是线性可分的吗"></a>RBF核一定是线性可分的吗</h4>不一定，RBF核比较难调参而且容易出现维度灾难，要知道无穷维的概念是从泰勒展开得出的。<h4 id="常用核函数及核函数的条件："><a href="#常用核函数及核函数的条件：" class="headerlink" title="常用核函数及核函数的条件："></a>常用核函数及核函数的条件：</h4>核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。 RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。 线性核：主要用于线性可分的情况 多项式核</li></ul><h1 id="贝叶斯方法"><a href="#贝叶斯方法" class="headerlink" title="贝叶斯方法"></a>贝叶斯方法</h1><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>公式，多个概率概念掌握</p><h4 id="朴素贝叶斯处理连续属性"><a href="#朴素贝叶斯处理连续属性" class="headerlink" title="朴素贝叶斯处理连续属性"></a>朴素贝叶斯处理连续属性</h4><p>对于连续属性可以考虑概率密度函数，假定该属性遵循正态分布，由此得出连续属性的概率</p><h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><p>在测试过程中，如果某个属性的取值从未在训练集中出现过，会导致P(x|y)=0,进而让整个贝叶斯公式等于零。通过拉普拉斯平滑可以解决这个问题。通俗的说，拉普拉斯平滑即在计算概率的时候，分别在分子上加1，分母上加属性的取值种类。</p><h4 id="朴素贝叶斯优化"><a href="#朴素贝叶斯优化" class="headerlink" title="朴素贝叶斯优化"></a>朴素贝叶斯优化</h4><ol><li>可以将涉及到的所有概率先计算好存储起来，在进行预测时只需要“查表”即可进行判别。</li><li>当数据有多属性时，涉及到多属性连乘问题，可以将原连乘问题转换为对数连加问题加快计算速度。<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4></li></ol><ul><li>优点：对小规模数据表现很好，适合多分类任务</li><li>缺点：对输入数据的表达形式很敏感<h4 id="为什么被称为“朴素”"><a href="#为什么被称为“朴素”" class="headerlink" title="为什么被称为“朴素”"></a>为什么被称为“朴素”</h4>朴素的意思是该分类方法建立在一个较强的独立假设上，即所有特征之间相互独立，所以被称为朴素，具体的说为：P(AB) = P(A)P(B)</li></ul><h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><ul><li>基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较性强的属性依赖关系。</li><li>“独依赖估计”是半朴素贝叶斯分类器最常用的一种策略，所谓“独依赖”就是假设每个属性在类别之外最多仅依赖一个其他属性。</li><li>半朴素贝叶斯不是很常见，具体细节见书p154</li></ul><h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>也称“信念网络”，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><p>吉布斯采样，EM算法</p><h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><p>通过构建并结合多个学习器来完成学习任务，总体思路是：1. 先产生一组“个体学习器”，再用某种策略将他们结合起来。</p><ul><li>同质的个体学习器也可以称为“基学习器”</li><li>异质集成中的个体学习器由不同算法组成，常称为“组件学习器”<br>集成学通过多个学习器的组合，通常会有比单一学习器更显著的泛化性能。<br>如何产生“好而不同”的个体学习器是集成学习的研究核心。<br>目前的集成学习方法大致可以分为两类：</li></ul><ol><li>个体学习器之间存在强依赖关系，必须串行生成序列化方法，代表是Boosting。</li><li>个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林</li></ol><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting是一族可将弱学习器提升为强学习器的方法：这族算法工作原理类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器的错误样本更受关注然后基于调整后的样本分布训练下一个基学习器。最后将这些基学习器组合到一起。</p><ul><li>优点：<ol><li>低泛化误差</li><li>容易实现，准确率较高，不需要大量调参</li></ol></li><li>缺点：<ol><li>对异常点过于敏感<br>在基学习器的选择上，大部分Boosting算法都选择决策树作为基学习器。<br>我在另一篇博文中整理了常见Boosting树的原理：<a href="">各种Boosting树</a></li></ol></li></ul><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>基本思想是对训练集进行采样，产生出若干个不同的子集，再从每个子集中训练一个基学习器。常使用的采样方法为“自助采样法”。<br>我们可以采样出T个含有m个样本的采样集，用每个采样集训练出一个基学习器，再将这些基学习器进行结合就是Bagging的基本流程。<br>Bagging对于分类任务采用简单投票法，对于回归任务采用简单平均法。<br>对于每个基学习器的包外样本，可以用其进行泛化性能估计或者辅助剪枝（决策树）</p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是在Bagging的基础上进一步在决策树的训练过程中引入了 <strong>随机属性选择</strong>。<br>在RF中，对基决策树的每个节点，<strong>先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度，若k=d，则基决策树的构建和传统决策树相同；若k=1，则随机选择一个属性用于划分，一般情况下推荐k=log2d</strong><br>RF中的基学习器的多样性不仅仅来自于样本扰动（对原始数据集进行自助采样），还来自属性随机。</p><h4 id="随机森林进行特征选择"><a href="#随机森林进行特征选择" class="headerlink" title="随机森林进行特征选择"></a>随机森林进行特征选择</h4><p>在随机森林中某个特征X的重要性的计算方法如下：<br>1：对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB1.<br>2: 随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB2.<br>3：假设随机森林中有Ntree棵树,那么对于特征X的重要性=∑(errOOB2-errOOB1)/Ntree,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。</p><h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><ol><li>对于回归问题：简单平均法、加权平均法</li><li>对于分类问题：绝对多数投票法，相对对数投票法，加权投票法。</li></ol><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>先从初始数据集训练出初始学习器，然后再生成一个新数据集用于训练次级学习器。在这个新的数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记被当做样例标记。</p><h2 id="集成学习的多样性度量"><a href="#集成学习的多样性度量" class="headerlink" title="集成学习的多样性度量"></a>集成学习的多样性度量</h2><p>见书P186页</p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>对于一些依赖于距离测度的学习算法，在训练算法之前对数据进行预处理是非常有必要的</p><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>指将属于映射到指定范围内，用于去除不同维度数据的量纲以及量纲单位。一般会映射到[0, 1]或者[-1, 1]</p><h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>x = (x-u)/s. 其中u为均值，s为方差。标准化可以将数据压缩到一定范围，某种程度上加快模型训练速度。</p><h3 id="为什么需要归一化"><a href="#为什么需要归一化" class="headerlink" title="为什么需要归一化"></a>为什么需要归一化</h3><ul><li>对于需要用距离度量的算法，如果不同特征的量纲不同会导致对预测结果的权重影响不同（如房价预测，kNN）</li><li>本质上是loss函数不同造成的，对于梯度下降来说，量纲不同会导致收敛速度过慢等问题。</li></ul><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><ul><li>用来评估聚类效果的好坏<ul><li>同一簇的样本尽可能的彼此相似，不同簇的样本尽可能不同。</li><li>簇内相似度尽可能高，簇间相似度尽可能低</li></ul></li><li>两类性能度量：<ol><li>将聚类结果与某个参考模型进行比较，称为”外部目标”</li><li>直接考察聚类结果而不利用任何参考模型，称为“内部目标”·</li></ol></li><li>三个聚类性能度量外部指标：<ol><li>Jaccard系数</li><li>FM系数</li><li>Rand指数</li></ol><ul><li>上述性能度量都在零一之间，且越大越好</li></ul></li><li>两个聚类性能度量内部指标<ol><li>DB指数</li><li>Dunn指数</li></ol><ul><li>DBI越小越好，DI越大越好</li></ul></li></ul><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><ul><li>闵可夫斯基距离度量：<ol><li>当p=2时，即为欧氏距离</li><li>当p=1时，即为曼哈顿距离</li></ol></li><li>使用闵可夫斯基距离时，注意只能针对有序属性特征</li><li>对于无序属性，可以采用VDM</li><li>对于一般问题，可以将闵可夫斯基度量和VDM相结合，前者处理有序属性，后者处理无序属性</li></ul><h2 id="k均值算法"><a href="#k均值算法" class="headerlink" title="k均值算法"></a>k均值算法</h2><ul><li>优点：思想简单，理论成熟，既可以回归也可以分类</li><li>缺点：计算量大，样本不平衡问题，需要大量内存<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4></li></ul><ol><li>设定聚类簇数为k，随机选取k个样本作为初始均值向量。</li><li>考察剩余样本，分别计算剩余样本到这k个均值向量的距离，并将其加入距离最近的聚类簇中</li><li>计算每个新聚类簇的均值向量，返回第二步直至收敛。</li></ol><h1 id="k近邻学习"><a href="#k近邻学习" class="headerlink" title="k近邻学习"></a>k近邻学习</h1><ul><li>分类任务中使用投票法，回归任务中使用平均法，也可以使用加权平均法</li><li>是懒惰学习 lazy learning的著名代表，没有显式的训练过程。（相应的，那些在训练阶段就对样本进行学习处理的方法称为急切学习 eager learning）</li><li>k值越大，模型越简单</li></ul><h1 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h1><p>最大熵模型基于最大熵原理：<strong>学习概率模型时，所有可能的概率模型中熵最大的模型时最好的模型</strong></p><h4 id="最大熵模型和逻辑回归区别"><a href="#最大熵模型和逻辑回归区别" class="headerlink" title="最大熵模型和逻辑回归区别"></a>最大熵模型和逻辑回归区别</h4><p>简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应为二类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。</p><h4 id="最大熵模型总结"><a href="#最大熵模型总结" class="headerlink" title="最大熵模型总结"></a>最大熵模型总结</h4><ol><li>最大熵模型的约束函数随着样本量的增大而增大，导致训练时间和计算力的花费非常巨大。scklearn中甚至没有最大熵模型的库。</li><li>优点1：</li></ol><h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p>对于给定的数据，想要求解这些数据服从的分布，一个方法就是极大似然估计。但如果这些数据是来自不同的概率分布（抽取的到的每个样本都不知道是从哪个分布抽取的），在预测分布是就有两个问题：1. 该数据来自哪个分布，2. 该分布的是什么样的。<br>只有当我们知道了哪些人属于同一个高斯分布的时候，我们才能够对这个分布的参数作出靠谱的预测，例如刚开始的最大似然所说的，但现在两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个分布。<br>为了解决这个循环问题，提出了EM算法：先随便选出一个随机值来，根据这个值来不断调整，如此循环迭代相互推导最后EM算法收敛到一个解。</p><p> EM算法就是这样，假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。</p><p>  EM的意思是“Expectation Maximization”，在我们上面这个问题里面，我们是先随便猜一下男生（身高）的正态分布的参数：如均值和方差是多少。例如男生的均值是1米7，方差是0.1米（当然了，刚开始肯定没那么准），然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布），这个是属于Expectation一步。有了每个人的归属，或者说我们已经大概地按上面的方法将这200个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整E步……如此往复，直到参数基本不再发生变化为止。</p><p>有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：</p><p>　　E步：选取一组参数，求出在该参数下隐含变量的条件概率值；</p><p>　　M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。</p><p>　　重复上面2步直至收敛。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>xgboost实战</title>
      <link href="/2018/03/25/Machine%20Learning/xgboost%E5%AE%9E%E6%88%98/"/>
      <url>/2018/03/25/Machine%20Learning/xgboost%E5%AE%9E%E6%88%98/</url>
      <content type="html"><![CDATA[<p>xgboost是一个非常强大的梯度提升树，在之前的文章中，详细介绍了xgboost对传统GBDT的优化。这篇文章介绍如何在项目中使用xgboost。<br><a id="more"></a></p><h2 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h2><p>xgboost支持三种数据格式的载入</p><ol><li>libsvm格式的文本数据</li><li>numpy的二维数组数据</li><li>xgboost的二进制缓存文件</li><li>scipy.sparse格式的数据<br>只需要将libsvm/numpy/scipy.sparse数据对象作为参数传递给函数’xgb.DMatrix(“fileObject”)’，就可以将原始格式数据转换为xgboost的DMatrix数据格式<br>例：numpy二维数组转换<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.rand(5,10) # 随机生成5个样本， 每个样本包含10个特征属性</span><br><span class="line">label = np.random.randint(2, size=5)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br></pre></td></tr></table></figure></li></ol><h3 id="样本设置权重参数"><a href="#样本设置权重参数" class="headerlink" title="样本设置权重参数"></a>样本设置权重参数</h3><p>xgboost可以对训练数据进行加权，使用方式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight = np.random.rand(5,1)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, weight=weight)</span><br></pre></td></tr></table></figure></p><h3 id="保存为二进制文件"><a href="#保存为二进制文件" class="headerlink" title="保存为二进制文件"></a>保存为二进制文件</h3><p>可以将DMatrix格式的数据保存为xgboost的二进制文件，这样下次加载时可以提高加载速度：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(&apos;train.svm.txt&apos;)</span><br><span class="line">dtrain.save_binary(&quot;train.buffer&quot;)</span><br></pre></td></tr></table></figure></p><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><p>对于训练完成的模型，可以将其保存在本地：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst.save_model(&apos;xgb.model&apos;)</span><br></pre></td></tr></table></figure></p><p>也可以导出训练好的模型到txt文件，或者导出模型的特征映射<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bst.dump_model(&apos;dump.raw.txt&apos;) # dump model</span><br><span class="line">bst.dump_model(&apos;dump.raw.txt&apos;,&apos;featmap.txt&apos;) # dump model with feature map</span><br></pre></td></tr></table></figure></p><p>加载模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bst = bgb.Booster(&#123;&apos;nthread&apos;:4&#125;) # init model</span><br><span class="line">bst.load_model(&quot;model.bin&quot;) # load model</span><br></pre></td></tr></table></figure></p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>xgboost是通过字典来存储参数的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    &apos;booster&apos;: &apos;gbtree&apos;,           # 使用的基学习器</span><br><span class="line">    &apos;objective&apos;: &apos;multi:softmax&apos;,  # 指定问题类型，multi:softmax表示多分类问题；reg:gamma表示回归问题</span><br><span class="line">    &apos;num_class&apos;: 10,               # 类别数，与 multisoftmax 并用</span><br><span class="line">    &apos;gamma&apos;: 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2</span><br><span class="line">    &apos;max_depth&apos;: 12,               # 构建树的深度，越大越容易过拟合</span><br><span class="line">    &apos;lambda&apos;: 2,                   # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span><br><span class="line">    &apos;subsample&apos;: 0.7,              # 随机采样训练样本</span><br><span class="line">    &apos;colsample_bytree&apos;: 0.7,       # 生成树时进行的列采样</span><br><span class="line">    &apos;min_child_weight&apos;: 3,</span><br><span class="line">    &apos;silent&apos;: 1,                   # 设置成1则没有运行信息输出，最好是设置为0.</span><br><span class="line">    &apos;eta&apos;: 0.007,                  # 如同学习率</span><br><span class="line">    &apos;seed&apos;: 1000,</span><br><span class="line">    &apos;nthread&apos;: 4,                  # cpu 线程数</span><br><span class="line">&#125;</span><br><span class="line">params_list = params.items()</span><br><span class="line">model = xgb.train(dtrain, params_list, num_rounds)</span><br></pre></td></tr></table></figure></p><p>xgboost包含三种参数：general parameters，booster parameters和task parameters。由于xgboost参数众多，所以这里只考虑常见的，需要调参的参数。</p><ul><li>General parameters<br>该参数参数控制在提升（boosting）过程中使用哪种booster，常用的booster有树模型（tree）和线性模型（linear model）。</li><li>Booster parameters<br>这取决于使用哪种booster。</li><li>Task parameters<br>控制学习的场景，例如在回归问题中会使用不同的参数控制排序。<h3 id="General-Parameters"><a href="#General-Parameters" class="headerlink" title="General Parameters"></a>General Parameters</h3></li><li>booster [default=gbtree]<br>有两中模型可以选择gbtree和gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。缺省值为gbtree</li><li>silent [default=0]<br>取0时表示打印出运行时信息，取1时表示以缄默方式运行，不打印运行时信息。缺省值为0。建议打印运行状况</li><li>nthread<br>XGBoost运行时的线程数。缺省值是当前系统可以获得的最大线程数<h3 id="Parameters-for-TreeBooster"><a href="#Parameters-for-TreeBooster" class="headerlink" title="Parameters for TreeBooster"></a>Parameters for TreeBooster</h3></li><li>eta [default=0.3]<br>为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3 取值范围为：[0,1]</li><li>gamma [default=0]<br>用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2。 取值范围为：[0,∞]</li><li>max_depth [default=6]<br>树的最大深度。缺省值为6 取值范围为：[1,∞]</li><li>min_child_weight [default=1]<br>孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该值越大，越保守，不容易过拟合。 取值范围为：[0,∞]</li><li>subsample [default=1]<br>用于训练模型的子样本占整个样本集合的比例。如果设置为0.5则意味着XGBoost将随机的从整个样本集合中随机的抽取出50%的子样本建立树模型，这能够防止过拟合。 取值范围为：(0,1]</li><li>colsample_bytree [default=1]<br>在建立树时对特征采样的比例。缺省值为1 取值范围为：(0,1]<h3 id="Parameters-for-LinearBooster"><a href="#Parameters-for-LinearBooster" class="headerlink" title="Parameters for LinearBooster"></a>Parameters for LinearBooster</h3></li><li>lambda [default=0]<br>L2 正则的惩罚系数</li><li>alpha [default=0]<br>L1 正则的惩罚系数</li><li>lambda_bias<br>在偏置上的L2正则。缺省值为0（在L1上没有偏置项的正则，因为L1时偏置不重要）<h3 id="Task-Parameters"><a href="#Task-Parameters" class="headerlink" title="Task Parameters"></a>Task Parameters</h3></li><li>objective [ default=reg:linear ]<br>定义学习任务及相应的学习目标，可选的目标函数如下：<ol><li>“reg:linear” —— 线性回归。</li><li>“reg:logistic”—— 逻辑回归。</li><li>“binary:logistic”—— 二分类的逻辑回归问题，输出为概率。</li><li>“binary:logitraw”—— 二分类的逻辑回归问题，输出的结果为wTx。</li><li>“count:poisson”—— 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7。(used to safeguard optimization)</li><li>“multi:softmax” –让XGBoost采用softmax目标函数处理多分类问题，同时需要设置参数num_class（类别个数）</li><li>“multi:softprob” –和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。</li><li>“rank:pairwise” rank任务，方法是最小化pairwise loss</li></ol></li><li>eval_metric [ default according to objective ]<br>校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标（rmse for regression, and error for classification, mean average precision for ranking, ‘auc’, ‘logloss’, ‘merror’, and so on…）<br>用户可以添加多种评价指标，对于Python用户要以list传递参数对给程序，而不是map参数</li></ul><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="显示重要特征"><a href="#显示重要特征" class="headerlink" title="显示重要特征"></a>显示重要特征</h3><p>GBDT的一大应用就是特征选择，xgboost提供’plot_importance(model)’方法， 可以基于F-score绘制每个特征的值，值越大，表示该属性越重要。</p><h2 id="xgboost实战应用"><a href="#xgboost实战应用" class="headerlink" title="xgboost实战应用"></a>xgboost实战应用</h2><p>XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现 分类 和 回归 两种任务。</p><h3 id="基于原生接口的分类"><a href="#基于原生接口的分类" class="headerlink" title="基于原生接口的分类"></a>基于原生接口的分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># read in the iris data</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;booster&apos;: &apos;gbtree&apos;,</span><br><span class="line">    &apos;objective&apos;: &apos;multi:softmax&apos;,</span><br><span class="line">    &apos;num_class&apos;: 3,</span><br><span class="line">    &apos;gamma&apos;: 0.1,</span><br><span class="line">    &apos;max_depth&apos;: 6,</span><br><span class="line">    &apos;lambda&apos;: 2,</span><br><span class="line">    &apos;subsample&apos;: 0.7,</span><br><span class="line">    &apos;colsample_bytree&apos;: 0.7,</span><br><span class="line">    &apos;min_child_weight&apos;: 3,</span><br><span class="line">    &apos;silent&apos;: 1,</span><br><span class="line">    &apos;eta&apos;: 0.1,</span><br><span class="line">    &apos;seed&apos;: 1000,</span><br><span class="line">    &apos;nthread&apos;: 4,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plst = params.items()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train)</span><br><span class="line">num_rounds = 500</span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds)</span><br><span class="line"></span><br><span class="line"># 对测试集进行预测</span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">ans = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">cnt1 = 0</span><br><span class="line">cnt2 = 0</span><br><span class="line">for i in range(len(y_test)):</span><br><span class="line">    if ans[i] == y_test[i]:</span><br><span class="line">        cnt1 += 1</span><br><span class="line">    else:</span><br><span class="line">        cnt2 += 1</span><br><span class="line"></span><br><span class="line">print(&quot;Accuracy: %.2f %% &quot; % (100 * cnt1 / (cnt1 + cnt2)))</span><br><span class="line"></span><br><span class="line"># 显示重要特征</span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="基于sklearn的分类"><a href="#基于sklearn的分类" class="headerlink" title="基于sklearn的分类"></a>基于sklearn的分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># read in the iris data</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=160, silent=True, objective=&apos;multi:softmax&apos;)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 对测试集进行预测</span><br><span class="line">ans = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">cnt1 = 0</span><br><span class="line">cnt2 = 0</span><br><span class="line">for i in range(len(y_test)):</span><br><span class="line">    if ans[i] == y_test[i]:</span><br><span class="line">        cnt1 += 1</span><br><span class="line">    else:</span><br><span class="line">        cnt2 += 1</span><br><span class="line"></span><br><span class="line">print(&quot;Accuracy: %.2f %% &quot; % (100 * cnt1 / (cnt1 + cnt2)))</span><br><span class="line"></span><br><span class="line"># 显示重要特征</span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SICP第一章:构造过程抽象</title>
      <link href="/2018/03/22/Reading/SICP%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/03/22/Reading/SICP%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>SICP（《Structure and Interpretation of Computer Science》）被无数外国程序员誉为最值得读的神书之一，同时身在PL研究室的我，EOPL和SICP都是不得不读的书。在这将这本书的读书笔记整理下来。</p><a id="more"></a><h2 id="1-1-程序设计的基本元素"><a href="#1-1-程序设计的基本元素" class="headerlink" title="1.1 程序设计的基本元素"></a>1.1 程序设计的基本元素</h2><p>  一个优秀的程序设计语言，应该是一种框架，可以让程序员通过它自由的组织自己的计算思想。每一种强有力的语言都应该提供三个机制：</p><ul><li>基本表达形式：用于表示语言所关心的最简单的个体</li><li>组合的抽象方法：通过他们可以从较为简单的东西出发构造复杂的元素</li><li>抽象的方法：通过他们可以为复合对象命名，并将他们当作单元去操作<br>在程序设计中，我们需要处理两类要素：过程和数据，数据是一种我们希望去操作的东西，而过程就是有关操作浙西数据的规则描述。<h3 id="1-1-1-正则序求值和应用序求值"><a href="#1-1-1-正则序求值和应用序求值" class="headerlink" title="1.1.1 正则序求值和应用序求值"></a>1.1.1 正则序求值和应用序求值</h3><ul><li>对于一个复合计算过程，“完全展开而后归约”的求值模型称为正则序求值</li><li>“先求值参数而后应用”的方式，被称为应用序求值</li></ul></li></ul><h3 id="1-1-2-牛顿法求平方根"><a href="#1-1-2-牛顿法求平方根" class="headerlink" title="1.1.2 牛顿法求平方根"></a>1.1.2 牛顿法求平方根</h3><p>如果对x的平方根的值有了一个猜测y，那么只需要求出y和x/y的平均值（它一定更接近实际的平方根值）。然后将得到的平均值作为下一个猜测，不断递归。</p><h3 id="1-1-3-过程作为黑箱的抽象"><a href="#1-1-3-过程作为黑箱的抽象" class="headerlink" title="1.1.3 过程作为黑箱的抽象"></a>1.1.3 过程作为黑箱的抽象</h3><p>对于一个复合过程，我们可以将其拆分成多个更小的子过程，而对于父过程而言，子过程即使一个个的黑箱，我们不需要知道子过程的具体实现过程，只需要知道子过程可以完成父过程需要的功能即可。<br><strong>按照过程抽象的思想编程，可以让我们更关注过程本身，至于子过程可以推后实现，相当于将过程解偶</strong></p><h3 id="1-1-4-局部子过程"><a href="#1-1-4-局部子过程" class="headerlink" title="1.1.4 局部子过程"></a>1.1.4 局部子过程</h3><p>对于一般子过程的定义，为了防止该子过程定义后与其他过程重名导致的混淆，应该将只被父过程调用的子过程定义在父过程的内部，即“块结构”。 在其它语言中，可以使用匿名函数，包命名管理等方法。</p><h2 id="1-3-用高阶函数作抽象"><a href="#1-3-用高阶函数作抽象" class="headerlink" title="1.3 用高阶函数作抽象"></a>1.3 用高阶函数作抽象</h2><p>如果一个编程语言将过程限制为只能作为以数作为抽象，那也会严重的限制我们建立抽象的能力。我们需要构造出这样的过程，让它们能以过程作为参数，或者以过程作为返回值。这类能操作过程的过程称为 <strong>高阶过程</strong></p><blockquote><p>函数式语言的设计的核心并不是仅仅想要将函数作为一切操作的核心，而是为了建立更高级，更广泛的过程抽象</p></blockquote><p><strong>let 表达式的一般形式</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(let ((&lt;var1&gt; &lt;exp1&gt;)</span><br><span class="line">      (&lt;var2&gt; &lt;exp2&gt;)</span><br><span class="line">      ...</span><br><span class="line">      )</span><br><span class="line">  &lt;body&gt;)</span><br></pre></td></tr></table></figure></p><h3 id="1-3-4-过程作为返回值"><a href="#1-3-4-过程作为返回值" class="headerlink" title="1.3.4 过程作为返回值"></a>1.3.4 过程作为返回值</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(define (yahaha f)</span><br><span class="line">  (lambda (x) (f x x)))</span><br><span class="line"></span><br><span class="line">&gt;&gt;((yahaha *) 2)</span><br><span class="line">&gt;&gt; 4</span><br></pre></td></tr></table></figure><p>定义过程‘yahaha’，该过程会返回一个由lambda定义的过程，在该返回的过程中，会接收参数x，并返回(f x x)</p>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> SICP </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于深度学习的电影推荐系统</title>
      <link href="/2018/03/11/Project/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2018/03/11/Project/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
      <content type="html"><![CDATA[<p>本项目是使用python3 + tensorflow，搭建一个基于卷积神经网络模型的离线电影推荐系统，电影数据集用的是MovieLens。核心思想是对于电影和用户的不同属性，构建多个神经网络进而获得每个属性的特征表示。使用这些特征表示构建用户特征矩阵和电影特征矩阵，进而完成：TopK电影推荐，相似用户查找等功能。<br><a id="more"></a></p><p>该项目是我关于深度学习基于推荐系统实践的练习项目，具体代码在我的GitHub上可以找到：<br><a href="https://github.com/YHfeather/Movie_Recommendation_System" target="_blank" rel="noopener">Movie_Recommendation_System</a></p><p>模型的整体架构如下图所示：<br><img src="/images/movie_recommender/overview.jpg" alt="overview"></p><h2 id="技术说明"><a href="#技术说明" class="headerlink" title="技术说明"></a>技术说明</h2><p>该项目主要使用的技术版本如下：</p><ul><li>python：3.5.2</li><li>tensorflow: 1.7.0</li><li>numpy: 1.14.0</li><li>pandas: 0.22</li></ul><h1 id="关于数据集"><a href="#关于数据集" class="headerlink" title="关于数据集"></a>关于数据集</h1><p>本项目使用的是MovieLens的ml-1m数据集，该数据集合包含6,040名用户对3,900个电影的1,000,209个匿名评论。<br><img src="/images/movie_recommender/dataset.jpeg" alt="DataSet"><br>数据集包括movies.dat, ratings.dat, users.dat三个文件</p><h4 id="movies-dat"><a href="#movies-dat" class="headerlink" title="movies.dat"></a>movies.dat</h4><p>该数据集存储了电影信息，包含字段：MovieID，Title，Genres</p><ul><li>MovieID: 电影ID(1-3952)</li><li>Title：电影标题，包括出版年份</li><li>Genres：电影类别（包括喜剧，动作剧，纪录片等..)</li></ul><p>详细内容可以参照ml-1m/README</p><h4 id="users-dat"><a href="#users-dat" class="headerlink" title="users.dat"></a>users.dat</h4><p>该数据集包含了对电影进行评分的用户信息，包括字段：UserID，Gender，Age，Occupation，Zip-code</p><ul><li>UserID：用户ID(1-6040)</li><li>Gender：性别（“M” or “F”）</li><li>Age：年龄，该年龄不是连续变量，而是被分为7个年龄集合（under 18；18-24；25-34；35-44…）</li><li>Occupation：职业，这里用数字0-20表示各个职业</li></ul><p>详细内容可以参照ml-1m/README</p><h4 id="ratings-dat"><a href="#ratings-dat" class="headerlink" title="ratings.dat"></a>ratings.dat</h4><p>该数据集是用户对电影的评分，包括字段：UserID::MovieID::Rating::Timestamp。<br>其中rating取值为：0，1，2，3，4，5<br>Timestamp表示时间戳<br>每个用户有最少20个评分</p><p>详细内容可以参照ml-1m/README</p><h1 id="文件组成"><a href="#文件组成" class="headerlink" title="文件组成"></a>文件组成</h1><p>项目共有三部分组成：1：数据下载和处理，2：模型构建和训练，3：推荐测试</p><h2 id="数据下载和处理"><a href="#数据下载和处理" class="headerlink" title="数据下载和处理"></a>数据下载和处理</h2><p>分别为<code>data_download.py</code>和<code>data_processing.py</code>文件。</p><h3 id="data-download"><a href="#data-download" class="headerlink" title="data_download"></a>data_download</h3><p>运行<code>data_download.py</code>会自动下载ml-1m数据集并解压到当前目录。<br>该文件包含<code>downl_data</code> <code>extract_data</code>两个函数和用来显示下载进度的类DLProgress</p><h3 id="data-processing"><a href="#data-processing" class="headerlink" title="data_processing"></a>data_processing</h3><p>该文件主要包含多个对原始数据进行处理的函数，将原始数据载入并进行处理，然后将处理后的数据和对应的映射和参数保存到本地。使用pickle保存为<code>.p</code>文件</p><ol><li><code>user_data_processing</code>函数对user数据进行处理，其中：<ul><li>UserID 不做处理</li><li>JobID 不做处理</li><li>Gender 将‘F’和‘M’转换0和1</li><li>Age 转换为0~6七个离散数字分别代表不同年龄段</li><li>zip-code 舍弃</li></ul></li><li><code>movie_data_processing</code>函数对movie数据进行处理，其中：<ul><li>movie_ID 不做处理</li><li>Genres 表示电影的类别，每个电影可能有多个genres标签，需要构建genre_to_index映射将每个电影的genres映射成一个定长的integer list表示</li><li>Title 首先去除掉title中的year，然后以word为粒度构建word_to_index映射将每个电影的title映射成一个定长的integer list表示</li></ul></li><li><code>rating_data_processing</code>函数对rating数据进行处理，其中：<ul><li>保留MovieID， UserID，ratings。将timestamps删除</li></ul></li><li><code>get_feature</code>函数将上述函数串联在一起，并将处理过后的数据进行拼接和切割，生成features和target两个数据，并保存在本地磁盘中。</li></ol><h2 id="模型构建和训练"><a href="#模型构建和训练" class="headerlink" title="模型构建和训练"></a>模型构建和训练</h2><p>分为<code>movie_nn.py</code> <code>user_nn.py</code> <code>traing.py</code>文件</p><h3 id="movie-nn"><a href="#movie-nn" class="headerlink" title="movie_nn"></a>movie_nn</h3><p>在该文件中，主要包含构建和movie相关的神经网络模型，主要包括对movie特征的embedding和使用卷积神经网络和常规神经网络进行特征提取。</p><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p><img src="/images/movie_recommender/movie_embedding.jpeg" alt="movie_embedding"><br>对电影的三个特征进行embedding操作。</p><h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>针对movieId和 movie genres分别构建两个神经网络进行训练，对于movie title构建 CNN with sliding window。然后将三个神经网络的输出拼接到一起输入到另一个神经网络，该神经网络的输出就是movie feature representation. 具体结构如下图所示：<br><img src="/images/movie_recommender/movie_nn.jpeg" alt="movie_nn"></p><h4 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h4><ol><li><code>get_inputs</code>定义movie属性：id，genres，title，dropout的placeholder</li><li><code>get_movie_id_embed_layer</code>构建movie id的embedding layer，并返回输入id的embedding</li><li><code>get_movie_categories_embed_layer</code>构建对movie genres的embedding layer，由于movie的genres是一个integer的list，经过embedding后会产生一个二维特征矩阵。目前仅实现对movie所有genre feature representation vector的直接加法。进而对于电影得到一个vector 的genre表示。</li><li><p><code>get_movie_cnn_layer</code>该函数对movie的title进行卷积操作。</p><ul><li>首先对title构建embedding layer。然后将生成的2维representation 扩展到三维。</li><li>然后定义一个sliding_window为一个list，包含多个integer表示窗口大小。该窗口大小即卷积过程中对一个word的卷积大小（卷积核size为：window_size * embedding_size)。</li><li>进行最大池化操作。</li><li>将多个feature map(由多个卷积核产生)连接到一起，表示movie title的representation。</li></ul></li><li><p><code>get_movie_feature_layer</code>构建一个DNN，将上述函数生成的movie属性对应的feature作为输入，训练出movie整体的feature representation。</p></li></ol><h3 id="user-nn"><a href="#user-nn" class="headerlink" title="user_nn"></a>user_nn</h3><p>对user的属性构建神经网络，主要包括对user创建embedding layers，以及创建神经网络进行进行user 特征提取。</p><h4 id="embedding-layer"><a href="#embedding-layer" class="headerlink" title="embedding layer"></a>embedding layer</h4><p>创建四个embedding layers。<br><img src="/images/movie_recommender/user_embedding.jpeg" alt="user_embedding"></p><h4 id="神经网络结构-1"><a href="#神经网络结构-1" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>分别将user的四个embedding representation vector输入到四个神经网络中，然后将四个神经网络的输出进行连接，输入到另一个神经网络中，该神经网络的输出即为user feature representation.具体结构如下图所示：<br><img src="/images/movie_recommender/user_nn.jpeg" alt="user_nn"></p><h4 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a>函数说明</h4><ol><li><code>get_inputs</code> 获取user特征的input placeholder</li><li><code>get_user_embedding</code> 对user属性：user_id, gender, age, job进行embedding</li><li><code>get_user_feature_layer</code>对于user的各个属性的embedding representation，分别构建一个小型的神经网络。然后将每个神经网络的输出，也就是各个属性的feature representation进行顺序连接，然后用一个全连接神经网络对连接后的user total feature进行训练，得到user整体的feature representation。</li><li><code>user_feature</code> 顺序连接上述方法，返回user的feature representation。</li></ol><h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p>对定义的movie_nn 和 user_nn进行训练:<br><img src="/images/movie_recommender/training.jpeg" alt="training"></p><h4 id="流程说明"><a href="#流程说明" class="headerlink" title="流程说明"></a>流程说明</h4><ol><li>首先从user_nn和movie_nn两个文件处获得各自对应的feature representation。</li><li>然后将user feature vector和movie feature vector进行矩阵乘法。得到的数据即为模型的预测输出，表示该user对该movie的预测评分</li><li>使用平方损失函数，AdamgradOptimizer进行训练。</li><li>使用tensorboard保存训练数据</li></ol><h4 id="损失曲线"><a href="#损失曲线" class="headerlink" title="损失曲线"></a>损失曲线</h4><p>训练过程中的loss curve 如下图所示：<br><img src="/images/movie_recommender/loss.png" alt="loss"></p><h2 id="推荐测试"><a href="#推荐测试" class="headerlink" title="推荐测试"></a>推荐测试</h2><p>对训练好的模型从多个方面进行测试，包含在<code>recommendation.py</code>文件中。</p><p><img src="/images/movie_recommender/recommender.jpeg" alt="recommender"></p><h4 id="函数说明："><a href="#函数说明：" class="headerlink" title="函数说明："></a>函数说明：</h4><ol><li><code>get_tensors</code> 获取测试用的placeholder</li><li><code>rating_movie</code> 给定user和movie，对模型进行正向传播，得到的分数即为预测评分</li><li><code>save_movie_feature_matrix</code> 生成电影特征矩阵，对movie部分的神经网络进行正向传播，得到每个movie的feature representation vector，并以矩阵形式保存到本地。</li><li><code>save_user_feature_matrix</code> 生成用户特征矩阵，对user部分的神经网络进行正向传播，得到每个user的feature representation vector，并以矩阵形式保存到本地。</li><li><code>recommend_save_type_movie</code> 给定movie，返回top k个和该movie最相似的movies。使用给定movie的feature vector，与整个movie feature matrix中所有其他的movie计算余弦相似度，返回相似度最大的top k个电影。</li><li><code>recommend_user_favorite_movie</code> 给定user，推荐其可能喜欢的top k个电影。用该给定user的feature vector和movie feature matrix中所有电影计算预测评分，返回top k个预测评分最大的电影。</li><li><code>recommend_other_favorite_movie</code> 给定movie，返回看过这个电影的人还可能喜欢哪些电影。首先选出top k个最喜欢给定movie的人（用给定movie的feature vector和整个user feature matrix相乘），得到这top k个人的user feature。然后分别求出每个user最喜欢的n个电影（分别用每个user的feature vector和movie feature matrix相乘，找到预测得分最高的n个电影）</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>该系统总体上来说实现了整个推荐系统的流程，包括数据处理，模型构建，模型训练以及推荐测试。但仍然有很多可以提高的地方。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Project </tag>
            
            <tag> Recommender System </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSTM和GRU运行原理</title>
      <link href="/2018/03/11/Deep%20Learning/LSTM%E5%92%8CGRU%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
      <url>/2018/03/11/Deep%20Learning/LSTM%E5%92%8CGRU%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/</url>
      <content type="html"><![CDATA[<p>在学习RNN过程中，一定会遇到LSTM和GRU，网上有很多LSTM的入门教程，内容非常详细但在我看来有些过于繁杂，没有突出重点，导致每次看懂LSTM的内在原理后过一段时间都会忘掉。这里我仅仅对LSTM和GRU的特性做出简要总结，力求简短明了，便于记忆。</p><a id="more"></a><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>我们知道RNN是在时间概念上将神经网络串联起来，当前时刻t的输入不仅仅是时刻t对应的数据，还包括RNN隐藏层在t-1时刻的输出。通过这样的方式RNN可以很好的结合时间概念上的上下文信息，这给了RNN对时序数据处理的能力。<br><img src="/images/deep_learning/rnn_basic.png" alt="rnn"><br>RNN的结构如上图所示。</p><h3 id="RNN常规变种"><a href="#RNN常规变种" class="headerlink" title="RNN常规变种"></a>RNN常规变种</h3><p>对于标准RNN，有很多变种：</p><h5 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h5><p>Deep RNN概念非常好理解，就是拥有多个隐藏层的RNN，同样对应的隐藏层的输入不仅仅包含了当前时刻上一个隐藏层的输出，还包括上一个时刻同一隐藏层的输出。</p><h5 id="Bi-Directional-RNN"><a href="#Bi-Directional-RNN" class="headerlink" title="Bi_Directional RNN"></a>Bi_Directional RNN</h5><p>有的时候我们对当前时刻的预测不仅仅根据之前时刻的信息，往往还包括之后时刻的信息。这就是Bi_Directional RNN的主要思想：不仅仅利用前一时刻RNN隐藏层的输出，还利用下一时刻RNN隐藏层的输出。<br>也就是说，当前隐藏层的输入不仅仅包含了当前时刻上一个隐藏层的输出，还包括上一个时刻同一隐藏层的输出，以及下一个时刻同一隐藏层的输出。<br>这样RNN可以更好的结合前后时刻的信息，在充满“上下文语境”的数据上往往会有更好的效果。</p><h3 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h3><p>但RNN在应用的时候往往会遇到两个问题：1.梯度消失； 2.梯度爆炸</p><ol><li>梯度消失<br>梯度消失的具体数学原理不在这里重复，主要原因是在进行反向传播更新参数时，偏导数的值小于1，当多个小于1的值相乘时传递到之前的梯度会变得非常小几乎接近于0，也就是说模型“训练不动了”。<br>对应到RNN中，梯度消失往往会导致长期记忆问题，也就是说RNN只会“着眼”于最近的几个相邻时刻的信息，而对于更远的信息RNN几乎没有办法获取。</li><li>梯度爆炸<br>与梯度消失对应的是梯度爆炸，数学原理同样略去，在网上有很多非常详细的教程。梯度爆炸主要指的是偏导数的值远大于1，多个大于1的值相乘会导致参数的更新变化幅度非常大甚至达到NaN。</li></ol><h2 id="LSTM的原理"><a href="#LSTM的原理" class="headerlink" title="LSTM的原理"></a>LSTM的原理</h2><p>本质上是增加了一个叫cell的结构，用来保存神经网络的“记忆”。每次训练时，一个LSTM中共发生三种变换：遗忘门，记忆门，输出门。<br><img src="/images/deep_learning/lstm.jpg" alt="lstm"></p><ul><li><strong>遗忘门</strong> 首先连接当前时刻输入xt和上一时刻的输出h(t-1)，经过sigmoid，计算遗忘门，与cell相乘，相当于对cell内容进行选择性遗忘。<br><img src="/images/deep_learning/forget_gate.jpg" alt="forget_gate"></li><li><strong>记忆门</strong> 对xt和h(t-1)进行tanh变换得到想要记忆到cell中的信息h’，同时对xt&amp;h(t-1)进行sigmoid变换得到记忆门阀。将h’和记忆门相乘后与cell相加，作为cell当前时刻的state<br><img src="/images/deep_learning/update_gate.png" alt="update_gate"></li><li><strong>输出门</strong> 对x&amp;h(t-1)进行sigmoid得到输出门，同时对当前cell的state进行tanh变换，然后与输出门相乘，作为当前时刻LSTM的输出。<br><img src="/images/deep_learning/output_gate.jpg" alt="output_gate"><h3 id="LSTM运行过程"><a href="#LSTM运行过程" class="headerlink" title="LSTM运行过程"></a>LSTM运行过程</h3>LSTM内部主要有三个阶段，分别对应着三个门：</li></ul><ol><li>遗忘阶段：这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。</li><li>选择记忆阶段：这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。</li><li>输出阶段：这个阶段将决定哪些将会被当成当前状态的输出。</li></ol><h3 id="LSTM总结"><a href="#LSTM总结" class="headerlink" title="LSTM总结"></a>LSTM总结</h3><p>通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。<br>但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p><h2 id="GRU-门循环单元"><a href="#GRU-门循环单元" class="headerlink" title="GRU 门循环单元"></a>GRU 门循环单元</h2><p>GRU是LSTM的一种变体，保持了LSTM的效果同时又使结构更加简单，相比LSTM，GRU能够达到几乎相同的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><h3 id="GRU结构"><a href="#GRU结构" class="headerlink" title="GRU结构"></a>GRU结构</h3><p>GRU的结构和普通的RNN一样，不同于LSTM使用了cell和hidden state两种状态。GRU只有hidden state。</p><h4 id="GRU门结构"><a href="#GRU门结构" class="headerlink" title="GRU门结构"></a>GRU门结构</h4><p>在GRU中，将LSTM的输入门，遗忘门，输出门合并成了两个门：<strong>更新门z和重置门r，两个门都是对当前时刻输入和上一个hidden state结合后的向量进行sigmoid。</strong></p><h4 id="GRU流程"><a href="#GRU流程" class="headerlink" title="GRU流程"></a>GRU流程</h4><h5 id="1-重置hidden-state"><a href="#1-重置hidden-state" class="headerlink" title="1 重置hidden state"></a>1 重置hidden state</h5><p>首先使用重置门r和上一个hidden state h(t-1)相乘，即对state进行重置后，得到重置后的状态h(t-1)’。</p><h5 id="2-计算h’"><a href="#2-计算h’" class="headerlink" title="2 计算h’"></a>2 计算h’</h5><p>然后将其与当前时刻输入xt进行拼接，在通过一个tanh函数将数据压缩到-1~1之间，得到h’。</p><h5 id="3-更新记忆阶段"><a href="#3-更新记忆阶段" class="headerlink" title="3 更新记忆阶段"></a>3 更新记忆阶段</h5><p>GRU的最后一个阶段叫“更新记忆”阶段，在这个阶段，同时进行“遗忘”和“记忆”两个步骤<br>此时，h’主要包含了当前时刻的输入xt数据。接下来，需要有针对的将h’添加到当前时刻的hidden state中。相当于“记忆了当前时刻的状态”。<br>我们用更新门z和上一个时刻的hidden state：h(t-1)以及上一步计算得到的h’计算当前时刻的hidden state：ht。</p><blockquote><p>ht = z*h(t-1) + (1-z)h’</p></blockquote><p>更新门（这里的 z ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。<br><img src="/images/deep_learning/gru_2.jpg" alt="gru_2"></p><h3 id="GRU特点"><a href="#GRU特点" class="headerlink" title="GRU特点"></a>GRU特点</h3><p>GRU最大的特点就是使用了同一个门控 z 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。</p><ul><li>z*h(t-1) 表示对上一个hidden state的选择性遗忘，这时更新门可以看做遗忘门</li><li>(1-z)h’ 表示对包含当前时刻输入信息的h’进行选择性记忆，(1-z)也会选择性的忘记h’中一些不重要的信息。</li><li>综上，ht即包含了对上一个时刻t-1信息的选择遗忘，又包含了对当前时刻的选择性记忆</li><li>这里z和(1-z)是联动的，也就是说，对t-1遗忘的越多，对t的记忆就越多，反之亦然。</li></ul><p><img src="/images/deep_learning/gru.png" alt="gru"><br>图中zt为更新门，rt为重置门</p><h3 id="GRU和LSTM的关系"><a href="#GRU和LSTM的关系" class="headerlink" title="GRU和LSTM的关系"></a>GRU和LSTM的关系</h3><p>GRU提出于2014年，LSTM是1997年。GRU的本质结构是和LSTM相似的。<br>我们知道重置门r可以将h(t-1)和xt转换成h’。<br>这里的h’实际上就是LSTM中的hidden state；而GRU中上一个节点传下来的h(t-1)则对应LSTM中的cell state。 z对应着LSTM中的遗忘门，(1-z)对应LSTM中的更新门。<br><img src="/images/deep_learning/compare.png" alt="compare"></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>推荐系统实战笔记</title>
      <link href="/2018/02/18/Reading/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/02/18/Reading/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="Chapter-One好的推荐系统"><a href="#Chapter-One好的推荐系统" class="headerlink" title="Chapter One好的推荐系统"></a>Chapter One好的推荐系统</h1><h2 id="1-1-什么是推荐系统"><a href="#1-1-什么是推荐系统" class="headerlink" title="1.1 什么是推荐系统"></a>1.1 什么是推荐系统</h2><p>在用户并没有一个明确的需求时，就需要一个自动化工具，它可以分析你的历史兴趣，从庞大的物品中找到可能符合你口味的商品，这个工具就是个性化推荐系统。<br>推荐系统和搜索引擎对于用户来说是两个互补的工具，搜索引擎满足了用户有明确目的时的主动查找需求，而推荐系统能够在用户没有明确目的的时候帮助他们发现感兴趣的新内容。</p><h2 id="1-2-个性化推荐系统的应用"><a href="#1-2-个性化推荐系统的应用" class="headerlink" title="1.2 个性化推荐系统的应用"></a>1.2 个性化推荐系统的应用</h2><p>几乎所有推荐系统应用都是由以下三部分组成：</p><ol><li>前台的展示页面</li><li>后台的日志系统</li><li>推荐算法系统</li></ol><h2 id="1-3-推荐系统评测"><a href="#1-3-推荐系统评测" class="headerlink" title="1.3 推荐系统评测"></a>1.3 推荐系统评测</h2><p>一个完整的推荐系统一般存在3个参与方：</p><ol><li>用户</li><li>物品提供者</li><li>提供推荐系统的网站<br>以图书推荐为例，首先需要满足用户的需求，给用户推荐令他们感兴趣的书。其次，需要让各种小众书籍都被推荐给相应的用户，而不是只推荐大出版社的热门书籍。同时，好的推荐系统设计能够让推荐系统不断收集高质量的用户反馈，不断完善推荐质量。因此，一个好的推荐系统是一个三方共赢的产物。</li></ol><h3 id="1-3-1-推荐系统实验方法"><a href="#1-3-1-推荐系统实验方法" class="headerlink" title="1.3.1 推荐系统实验方法"></a>1.3.1 推荐系统实验方法</h3><p>在推荐系统中，主要有3中评测推荐效果的试验方法：</p><ol><li>离线实验</li><li>用户调查</li><li>在线实验</li></ol><h5 id="1-离线实验"><a href="#1-离线实验" class="headerlink" title="1. 离线实验"></a>1. 离线实验</h5><p>离线实验的方法由一下几个步骤构成：</p><ol><li>通过日志系统获得用户行为数据，并按照一定格式生成标准数据集</li><li>将数据集按照一定的规则分成训练集和测试集</li><li>在训练集上训练用户兴趣模型，并在测试集上进行测试</li><li>通过事先定义的离线指标评测算法在测试集上的效果<br>离线实验可以说是一个静态的试验方法，不需要用户参与，只需要静态数据即可，有 <strong>不需要用户参与；方便快捷；可以测试大量算法的优点</strong> ，缺点是 <strong>无法获得很多商业上关注的指标，如点击率转化率等</strong></li></ol><h5 id="2-用户调查"><a href="#2-用户调查" class="headerlink" title="2. 用户调查"></a>2. 用户调查</h5><p>在离线实验中，高预测准确率并不等于高用户满意度，因此用户调查就成了推荐系统测评的一个重要工具。<br>用户调查的优缺点也很明显，优点是：<strong>可以获得用户主观指标</strong>， 缺点是： <strong>募集测试用户代价较大，成本高，而测试用户如果过少也会失去意义</strong></p><h5 id="3-在线实验"><a href="#3-在线实验" class="headerlink" title="3. 在线实验"></a>3. 在线实验</h5><p>AB测试是一种常见在线实验方法，通过将用户随机分为几组，并对每组采用不同算法，然后通过计算每组不同的指标来评价算法性能。 <strong>优点是准确，缺点是实验复杂，周期长</strong>，所以只会对在1，2步骤表现优秀的算法进行AB实验。</p><h3 id="1-3-2-评测指标"><a href="#1-3-2-评测指标" class="headerlink" title="1.3.2 评测指标"></a>1.3.2 评测指标</h3><h5 id="1-用户满意度"><a href="#1-用户满意度" class="headerlink" title="1 用户满意度"></a>1 用户满意度</h5><p>用户满意度只可以通过用户调查和在线实验获得。需要注意的是，设计调查问卷时需要一些技巧，而不仅仅是简单的询问用户对结果是否满意。在线实验中，可以对一些用户行为进行计算的到满意度。如点击率，停留时间，转化率等指标。</p><h5 id="2-预测准确度"><a href="#2-预测准确度" class="headerlink" title="2 预测准确度"></a>2 预测准确度</h5><p>该指标度量的是算法预测用户行为的能力。是非常重要的离线评测指标。预测准确度的计算方法同传统机器学习方法一样（训练模型，测试模型）。在预测准确度中，也有不同的度量指标：</p><ol><li>评分预测：<ol><li>预测用户对物品的评分，</li><li>预测评分的误差计算可以使用平方根误差RMSE和绝对值误差MAE。</li><li>RMSE被认为加大了对预测不准的用户物品评分的惩罚，因而对系统的评测更加苛刻。</li></ol></li><li>TopN推荐<ol><li>对于一个用户，系统会给该用户生成一个推荐列表，即TopN推荐</li><li>TopN推荐一般通过准确率和召回率来度量</li></ol></li><li>覆盖率<ol><li>简单的定义为系统能够推荐出来的物品占总物品集合的比例。</li><li>覆盖率是一个内容提供商会关心的指标</li><li>覆盖率为100%的系统可以将每个物品都推荐给至少一个用户</li><li>好的系统不仅仅需要较高的用户满意度，还要较高的覆盖率</li><li>可以用每个商品被推荐的数量用来计算覆盖率</li><li>可以用 <strong>覆盖率</strong> 和 <strong>基尼系数</strong> 衡量</li></ol></li><li>多样性<ol><li>推荐列表需要比较多样，覆盖用户绝大多数的兴趣点</li><li>可以用相似度算法计算推荐列表items之间的相似度</li><li>相似度最好的情况是满足用户的喜爱程度。</li></ol></li><li>新颖性</li><li>惊喜度<ol><li>如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果惊喜度很高。而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果</li></ol></li><li>信任度<ol><li>让用户对推荐结果产生信任是非常重要的</li><li>同样结果以信任方式推荐给用户会让用户满意，而如果以广告的形式用户会些许不满。</li><li>首先可以增加推荐系统透明度，主要方法是提供推荐解释。也可以利用好友信息给用户做推荐，并用好友进行推荐解释。</li></ol></li><li>实时性<ol><li>在有些网站中，推荐的实时性会非常重要。</li><li>首先是对用户的实时性，需要对推荐列表的item更新更加准确</li><li>然后是对商品的实时性，对于新加入的商品，如何正确推荐给用户，这涉及到考验系统 <strong>冷启动</strong> 的能力</li></ol></li><li>健壮性<ol><li>大型网站的推荐系统会面临攻击，健壮性指标衡量了一个推荐系统抗击作弊的能力。</li><li>可以通过模拟攻击来计算算法的健壮性</li><li>设计推荐系统时尽量使用代价比较高的用户行为</li><li>在使用数据前，进行攻击检测，从而对攻击数据进行清理</li></ol></li><li>商业指标<ol><li>不同的网站具有不同的商业指标，和盈利能力息息相关。</li></ol></li></ol><h3 id="1-3-3-评价维度"><a href="#1-3-3-评价维度" class="headerlink" title="1.3.3 评价维度"></a>1.3.3 评价维度</h3><p>一个推荐算法，可能整体性能不是很好，但在某种情况下性能比较好，一般来说，评测维度主要是以下三种：</p><pre><code>1. 用户维度2. 物品维度3. 时间维度</code></pre><h1 id="Chapter-Two-利用用户行为数据"><a href="#Chapter-Two-利用用户行为数据" class="headerlink" title="Chapter Two 利用用户行为数据"></a>Chapter Two 利用用户行为数据</h1><p>基于用户行为分析的推荐算法被称为协同过滤算法。</p><h2 id="用户行为数据"><a href="#用户行为数据" class="headerlink" title="用户行为数据"></a>用户行为数据</h2><ol><li>用户行为被保存在服务器上的种种日志文件中</li><li>用户行为在个性推荐系统中一般分为两种：显性反馈行为和隐性反馈行为<ul><li>显性反馈行为包括用户明确表示对物品喜好的行为</li><li>隐性反馈虽然不能明确反应用户喜好，但数量庞大，获取容易<h2 id="基于邻域的算法"><a href="#基于邻域的算法" class="headerlink" title="基于邻域的算法"></a>基于邻域的算法</h2>基于邻域的算法是推荐系统中最基本的算法。基于邻域的算法分为两大类，一类是基于用户的协同过滤算法，另一类是 基于物品的协同过滤算法。<h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3>基于用户的协同过滤算法本质上是 <strong>找到与需要推荐的用户A有相似兴趣的其他用户，然后把那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法称为基于用户的协同过滤算法。</strong><br>从上面的描述中可以看到，基于用户的协同过滤算法主要包括两个步骤。</li></ul></li><li>找到和目标用户兴趣相似的用户集合。</li><li>找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。<br>步骤1的关键在于计算两个用户的相似度。这里协同过滤算法主要利用 <strong>行为的相似度计算兴趣的相似度</strong>。<br>我们可以通过Jaccard公式简单计算u，v两个用户的相似度。<strong>Jaccard公式计算方法为：找到两个用户分别有过正反馈的物品集合，计算两个集合的交集和集合并集的比值即为相似度。</strong> Jaccard相似度的取值为[0, 1]，越大两个用户越相似。<br>还有一种方法是计算余弦相似度。<br>常规UserCF对两两用户都利用余弦相似度计算相似度。这种方法的时间复杂度是O(|U|*|U|)，这在用户数很大时非常耗时。事实上，很多用户相互之间并没有对同样的物品产生过行为。所以对于最基本的UserCF的一个优化就是：<strong>首先找到所有兴趣集合并集不为空的用户对，然后只计算这些用户之间的余弦相似度。</strong><br>为此，可以首先建立 <strong>物品到用户的倒排表</strong>，对于每个物品都保存对该物品产生过行为的用户列表。然后建立稀疏矩阵C[u][v]表示用户u和用户v兴趣集合的并集。最后扫描倒排列表中每个物品对应的用户列表，将用户列表中的两两用户对应的C[u][v]加一。最后可以得到所有有共同兴趣的用户以及共同行为的个数。<br><img src="/images/recommender_practice/daopaibiao.png" alt="用户行为倒排表"><br><img src="/images/recommender_practice/UserCF.png" alt="UserCF算法"><h5 id="用户相似度计算的改进"><a href="#用户相似度计算的改进" class="headerlink" title="用户相似度计算的改进"></a>用户相似度计算的改进</h5>两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度。在普通的UserCF基础上提出了User-IIF算法：<br><img src="/images/recommender_practice/UserIIF.png" alt="User-IIF算法"><h5 id="UserCF实际应用"><a href="#UserCF实际应用" class="headerlink" title="UserCF实际应用"></a>UserCF实际应用</h5>相比基于物品的协同过滤算法(ItemCF)， UserCF在目前的实际应用中使用并不多。主要原因是：首先，随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。其次，基于用户的协同过滤很难对推荐结果作出解释。</li></ol><h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p>基于物品的协同过滤算法是目前业界应用最多的算法。 无论是亚马逊网，还是Netflix、Hulu、YouTube，其推荐算法的基础都是该算法。 <strong>基于物品的协同过滤算法(简称ItemCF)给用户推荐那些和他们之前喜欢的物品相似的物品。</strong> 不过，ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。<strong>该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品 B。</strong></p><h4 id="ItemCF算法步骤"><a href="#ItemCF算法步骤" class="headerlink" title="ItemCF算法步骤"></a>ItemCF算法步骤</h4><p>基于物品的协同过滤算法主要分为两步。</p><ol><li>计算物品之间的相似度。</li><li>根据物品的相似度和用户的历史行为给用户生成推荐列表。<br>两个物品的相似度Wij可以用如下公式计算：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wij = (N(i)交N(j)) / sqrt(N(i)*N(j))</span><br></pre></td></tr></table></figure></li></ol><p>Wij可以解释为：<strong>同时喜欢商品i和商品j的人的交集的人数 / 喜欢商品i的人数*喜欢商品j的人数</strong></p><p>ItemCF通过如下公式计算用户u对一个物品j的兴趣<br><img src="images/recommender_practice/ItemCF.png" alt="ItemCF"><br>这里N(u)是用户喜欢的物品的集合，S(j,K)是和物品j最相似的K个物品的集合，wji是物品j和i 的相似度，rui是用户u对物品i的兴趣。<br>该公式的核心思想是：<strong>找出和商品j最相近的k个商品中被用户喜欢的商品is，计算所有is和j的相似度的和</strong><br>该公式的含义是，<strong>和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</strong></p><h6 id="参数K"><a href="#参数K" class="headerlink" title="参数K"></a>参数K</h6><p>参数K是ItemCF算法的重要参数，它对推荐算法的各种指标都会产生一些列的影响：</p><ul><li>精度（准确率和召回率）：准确率和召回率与参数k并不呈正相关或者负相关，但是选择合适的K对于获得推荐系统高的精度比较重要。</li><li>流行度：随着K的增大，推荐结果的流行度会逐渐提高，但是当K增加到一定的程度，流行度就不会再有明显变化。</li><li>覆盖率：K越大，覆盖率会相应地降低</li></ul><h4 id="用户活跃度对物品相似度的影响"><a href="#用户活跃度对物品相似度的影响" class="headerlink" title="用户活跃度对物品相似度的影响"></a>用户活跃度对物品相似度的影响</h4><p>假设有这么一个用户，他是开书店的，假设当当网有100万本书，他买了80万本准备开店卖。从前面ItemCF算法可知，这意味着因为存在这么一个用户，有80万本书两两之间就产生了相似度，也就是说，内存里即将诞生一个80万乘80万的稠密矩阵。但这个人的存在对于ItemCF是没有意义的。</p><h5 id="ItemCF-IUF-Inverse-User-Frequence"><a href="#ItemCF-IUF-Inverse-User-Frequence" class="headerlink" title="ItemCF-IUF(Inverse User Frequence)"></a>ItemCF-IUF(Inverse User Frequence)</h5><p>IUF即用户活跃度对数的倒数的参数，IUF认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF 参数来修正物品相似度的计算公式:<br><img src="images/recommender_practice/IUF.png" alt="IUF"></p><p>在实际应用中，对于上面这种过于活跃的用户，一般直接将其删除。</p><h4 id="物品相似度的归一化"><a href="#物品相似度的归一化" class="headerlink" title="物品相似度的归一化"></a>物品相似度的归一化</h4><p>如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率。1 其研究表明，如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度 矩阵w’:<br><img src="images/recommender_practice/itemcf_guiyihua.png" alt="guiyihua"><br>归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。 一般来说，物品总是属于很多不同的类，每一类中的物品联系比较紧密。</p><h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p>举一个例子，假设物品分为两类——A和B，A类物品之间的相似 度为0.5，B类物品之间的相似度为0.6，而A类物品和B类物品之间的相似度是0.2。在这种情况下， 如果一个用户喜欢了5个A类物品和5个B类物品，用ItemCF给他进行推荐，推荐的就都是B类物品， 因为B类物品之间的相似度大。但如果归一化之后，A类物品之间的相似度变成了1，B类物品之 间的相似度也是1，那么这种情况下，用户如果喜欢5个A类物品和5个B类物品，那么他的推荐列 表中A类物品和B类物品的数目也应该是大致相等的。从这个例子可以看出，相似度的归一化可 以提高推荐的多样性。</p><h4 id="物品过于热门问题"><a href="#物品过于热门问题" class="headerlink" title="物品过于热门问题"></a>物品过于热门问题</h4><p>如果一个物品过于热门，仅仅在分母上乘该喜欢该物品的人数仍然过于热门，可以在分母的上加上α的幂加大惩罚。通过这种方法可以在适当牺牲准确率和召回率的情况下显著提升结果的覆盖率和新颖性(降低流行度即提高了新颖性)。</p><h3 id="ItemCF和UserCF的对比"><a href="#ItemCF和UserCF的对比" class="headerlink" title="ItemCF和UserCF的对比"></a>ItemCF和UserCF的对比</h3><p>UserCF给用户推荐那些和他有共同兴 趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算 法的原理可以看到，UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF 的推荐结果着重于维系用户的历史兴趣。换句话说，UserCF的推荐更社会化，反映了用户所在的 6 小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。</p><p><img src="images/recommender_practice/UserCompareItem.png" alt="UserCompareItem"></p><h3 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h3><p>LFM（latent factor model）隐语义模型在推荐系统中表现了强大的威力，LFM主要用于找到文本的隐含语义，可以应用在TopK推荐中。</p><h4 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h4><p>LFM的核心思想是：<strong>通过隐含特征联系用户兴趣和物品</strong>，LFM采用基于用户行为统计的自动聚类，LFM有很多衍生的手段，包括pLSA，LDA，隐含类别模型，隐含主题模型，矩阵分解等。<br>LFM通过如下公式计算用户u对物品i的兴趣：<br><img src="images/recommender_practice/lfm_format.png" alt="lfm_format"></p><p>所以，想使用LFM模型，需要一个训练集，对于每个用户u，训练集里都包含了用户u喜欢的物品和不感兴趣的物品，通过学习这个数据集，就可以获得上面的模型参数。而对于隐性反馈数据集来说，只有正样本没有负样本，所以LFM的第一个问题就是如何给每个用户生成负样本。负样本的生成可以遵循以下原则：</p><ol><li>对每个用户，正负样本保持平衡</li><li>对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品。<br>一般认为，很热门而用户却没有行为更加代表用户对这个物品不感兴趣。因为对于冷门的物 品，用户可能是压根没在网站中发现这个物品，所以谈不上是否感兴趣。</li></ol><p>经过采样，可 以得到一个用户—物品集 K = {(u, i)} ，其中如果(u, i)是正样本，则有 rui = 1 ，否则有 rui = 0 。然后，需要优化如下的损失函数来找到最合适的参数p和q:</p><p><img src="images/recommender_practice/lfm_format2.png" alt="lfm_format2"><br>这里面的K即是隐类，LFM通过隐类将User和Item关联起来。</p><h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><ol><li>隐特征的个数K</li><li>梯度下降的学习速率alpha</li><li>正则化参数lambda</li><li>每个用户的正负样本比例ratio<br>通过实验发现，ratio参数对LFM的性能影响最大。随着负样本数目的增加，覆盖率不 断降低，而推荐结果的流行度不断增加，说明ratio参数控制了推荐算法发掘长尾的能力。<h4 id="LFM缺点"><a href="#LFM缺点" class="headerlink" title="LFM缺点"></a>LFM缺点</h4></li></ol><ul><li>当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</li><li>LFM很难实现实时的推荐。经典的LFM模型每次训练时都需要扫描所有的用户行为记录，这样才能计算出用户隐类向量(pu)和物品隐类向量(qi)。而且LFM的训练需要在用户行为记录上反复迭代才能获得比较好的性能。因此，LFM的每次训练都很耗时，一般在实际应用中只能每天训练一次，并且计算出所有用户的推荐结果。</li><li>在新闻推荐中，冷启动问题非常明显。每天都会有大量新的新闻。这些新闻会在很短的时间内获得很多人的关注，但也会在很短的时间内失去用户的关注。因此，它们的生命周期很短，而推荐算法需要在它们短暂的生命周期内就将其推荐给对它们感兴趣的用户。</li></ul><h3 id="LFM和基于邻域方法的比较"><a href="#LFM和基于邻域方法的比较" class="headerlink" title="LFM和基于邻域方法的比较"></a>LFM和基于邻域方法的比较</h3><p>LFM是一种基于机器学习的方法，具有比较好的理论基础。这个方法和基于邻域的方法(比 如UserCF、ItemCF)相比，各有优缺点。下面将从不同的方面对比LFM和基于邻域的方法。</p><ol><li>理论基础： LFM具有较好的理论基础，他是一种学习方法，而基于邻域的方法是一种基于统计的方法。</li><li>离线计算的空间复杂度：基于邻域的方法需要维护一张离线的相关表。假设有M个用户和N个物品， 在计算相关表的过程中，我们可能会获得一张比较稠密的临时相关表， 那么假设是用户相关表，则需要O(M<em>M)的空间，而对于物品相关表，则需要O(N</em>N)的空间。而LFM在建模过程中，如果是F个隐类，那么它需要的存储空间是O(F*(M+N))，这在 M和N很大时可以很好地节省离线计算的内存。</li><li>离线计算的时间复杂度：假设有M个用户、N个物品、K条用户对物品的行为记录。那么， UserCF计算用户相关表的时间复杂度是O(N <em> (K/N)^2)，而ItemCF计算物品相关表的时间 复杂度是O(M</em>(K/M)^2)。而对于LFM，如果用F个隐类，迭代S次，那么它的计算复杂度 是O(K <em> F </em> S)。在一般情况下，LFM的时间复杂度要 稍微高于UserCF和ItemCF，这主要是因为该算法需要多次迭代。但总体上，这两种算法 在时间复杂度上没有质的差别。</li><li>在线实时推荐：UserCF和ItemCF在线服务算法需要将相关表缓存在内存中，然后可以在线进行实时的预测。以ItemCF算法为例，一旦用户喜欢了新的物品，就可以通过查询内存中的相关表将和该物品相似的其他物品推荐给用户。因此，一旦用户有了新的行为， 而且该行为被实时地记录到后台的数据库系统中，他的推荐列表就会发生变化。而从LFM在给用户生成推荐列表时，需要计算用户对所有物品的兴趣 权重，然后排名，返回权重最大的N个物品。因此，LFM不太适合用于物品数非常庞大的系统。另一方面，LFM在生成一个用户推荐列表时速度太慢，因此不能在线实 时计算，而需要离线将所有用户的推荐结果事先计算好存储在数据库中。因此，LFM不 能进行在线实时推荐，也就是说，当用户有了新的行为后，他的推荐列表不会发生变化。</li><li>推荐解释 ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果。 但LFM无法提供这样的解释，它计算出的隐类虽然在语义上确实代表了一类兴趣和物品， 却很难用自然语言描述并生成解释展现给用户。<h3 id="基于图模型"><a href="#基于图模型" class="headerlink" title="基于图模型"></a>基于图模型</h3>用户行为很容易用二分图表示，因此很多图的算法都可以用到推荐系统中。<h4 id="用户行为数据的二分图表示"><a href="#用户行为数据的二分图表示" class="headerlink" title="用户行为数据的二分图表示"></a>用户行为数据的二分图表示</h4>设用户行为 数据是由一系列二元组组成的，其中每个二元组(u, i)表示用户u对物品i产生过行为。这种数据集 很容易用一个二分图表示。<br><img src="images/recommender_practice/erfentu.png" alt="erfentu"></li></ol><p>将用户行为表示为二分图模型后，下面的任务就是在二分图上给用户进行个性化推荐。如果将个性化推荐算法放到二分图模型上，那么给用户u推荐物品的任务就可以转化为 <strong>度量用户顶点 vu和与vu没有边直接相连的物品节点在图上的相关性</strong>，相关性越高的物品在推荐列表中的权重就越高。</p><h4 id="基于随机游走的PersonalRank算法"><a href="#基于随机游走的PersonalRank算法" class="headerlink" title="基于随机游走的PersonalRank算法"></a>基于随机游走的PersonalRank算法</h4><p>假设要给用户u进行个性化推荐，可以从用户u对应的节点vu开始在用户物品二分图上进行随 机游走。游走到任何一个节点时，首先按照概率α决定是继续游走，还是停止这次游走并从vu节点开始重新游走。如果决定继续游走，那么就从当前节点指向的节点中按照均匀分布随机选择一个节点作为游走下次经过的节点。这样，经过很多次随机游走后，每个物品节点被访问到的概率会收敛到一个数。最终的推荐列表中物品的权重就是物品节点的访问概率。<br><img src="images/recommender_practice/PersonalRank.png" alt="PersonalRank"></p><h5 id="PersonalRank缺点"><a href="#PersonalRank缺点" class="headerlink" title="PersonalRank缺点"></a>PersonalRank缺点</h5><p>该算法在时间复杂度上 有明显的缺点。因为在为每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直到 整个图上的每个顶点的PR值收敛。这一过程的时间复杂度非常高，不仅无法在线提供实时推荐， 甚至离线生成推荐结果也很耗时。<br>为了解决PersonalRank每次都需要在全图迭代并因此造成时间复杂度很高的问题，这里给出 两种解决方案。第一种很容易想到，就是减少迭代次数，在收敛之前就停止。这样会影响最终的 精度，但一般来说影响不会特别大。另一种方法就是从矩阵论出发，重新设计算法。我们可以将PersonalRank转化为矩阵的形式。令M为用户物品二 分图的转移概率矩阵</p><h1 id="推荐系统冷启动问题"><a href="#推荐系统冷启动问题" class="headerlink" title="推荐系统冷启动问题"></a>推荐系统冷启动问题</h1><p>冷启动问题主要分3类。</p><ul><li>用户冷启动：对于新用户，我们没有该用户的行为，无法做个性化推荐的问题。</li><li>物品冷启动：对于新物品，我们没有用户对该物品的行为，无法做个性化推荐的问题。</li><li>系统冷启动：系统冷启动主要解决如何在一个新开发的网站上(无用户，只有一些物品的信息)设计个性化推荐系统。</li></ul><p>针对上述冷启动问题，普遍有如下解决方法</p><ul><li>提供非个性化的推荐：直接推荐热门项目，收集User信息</li><li>利用用户注册时的信息做粗粒度个性化</li><li>利用用户社交网络账号登录导入社交网站的好友信息。</li><li>用户注册时要求用户对一些分类或者物品做出反馈</li><li>对于新加入的物品，可以利用内容信息，将它们推荐给喜欢过和它们相似的物品的用户。</li><li>在系统冷启动时，可以引入专家的知识，通过一定的高效方式迅速建立起物品的相关度表。</li></ul><h1 id="利用用户标签数据"><a href="#利用用户标签数据" class="headerlink" title="利用用户标签数据"></a>利用用户标签数据</h1><p>让普通用户给物品打标签（UGC），这种方法是一种表示用户兴趣和物品语义的重要方式。当一个用户对一个物品打上标签，这个标签一方面描述了用户的兴趣，另一方面则表示了物品的语义，从而将用户和物品联系起来。</p><h4 id="To-Be-Continue"><a href="#To-Be-Continue" class="headerlink" title="To Be Continue"></a>To Be Continue</h4>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Atom运行Python3</title>
      <link href="/2018/02/11/Knowledge/atom%E8%BF%90%E8%A1%8Cpython%E5%B9%B6%E4%BF%AE%E6%94%B9%E4%B8%BApython3/"/>
      <url>/2018/02/11/Knowledge/atom%E8%BF%90%E8%A1%8Cpython%E5%B9%B6%E4%BF%AE%E6%94%B9%E4%B8%BApython3/</url>
      <content type="html"><![CDATA[<p>atom是一个由GitHub开发的开源编辑器，以往我是sublime的忠实粉丝，但sublime在Linux下不支持中文输入，没办法愉快的写注释了，只好投奔atom。该文介绍的是如何在atom下运行python。同时应为Mac的默认python版本为2.7，该文也会介绍如何修改atom的运行版本为python3</p><a id="more"></a><h2 id="下载script插件"><a href="#下载script插件" class="headerlink" title="下载script插件"></a>下载script插件</h2><p>点击左上角“Atom” -&gt; “preference” -&gt; “install”，在搜索框输入“script”下载安装script运行插件。下载完成后，我们可以新建一个py文件进行测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">print(sys.version)</span><br></pre></td></tr></table></figure></p><p>运行快捷键：Mac：”command + i”，我们可以看到，控制台输出的python版本为2.7，如果python2.7刚好满足你的日常编程需要，到此atom运行python就搞定了。</p><h2 id="修改python3"><a href="#修改python3" class="headerlink" title="修改python3"></a>修改python3</h2><p>如果你使用的是python3，那么需要修改一下atom的默认python运行版本。点击左上角atom：<br>preference –&gt; Open Config Folder –&gt; packages –&gt; lib –&gt; grammars –&gt; python.coffee<br>可以看到：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">exports.Python =</span><br><span class="line">  &apos;Selection Based&apos;:</span><br><span class="line">    command: &apos;python3&apos;</span><br><span class="line">    args: (context) -&gt; [&apos;-u&apos;, &apos;-c&apos;, context.getCode()]</span><br><span class="line"></span><br><span class="line">  &apos;File Based&apos;:</span><br><span class="line">    command: &apos;python3&apos;</span><br><span class="line">    args: (&#123;filepath&#125;) -&gt; [&apos;-u&apos;, filepath]</span><br></pre></td></tr></table></figure></p><p>修改command: ‘python’ 为 “python3” 即可（本文已经进行修改）</p><h2 id="python2和python3并存"><a href="#python2和python3并存" class="headerlink" title="python2和python3并存"></a>python2和python3并存</h2><p>如果你是大部分时间需要python2，偶尔运行一下python3，那么往复修改配置文件就会显得麻烦，script插件可以配置多个运行环境满足该需求：</p><ul><li>点击上方“packages” -&gt; “Script” -&gt; “configure script”</li><li>到此会出现一个对话框，在“commend”输入python3即可，并‘run’一下，就会发现当前文件已经使用python3进行运行。</li><li>完成上述配置后，以后如果想要运行python3，可以使用快捷键：’command + shift + k’，就会出现你已经配置好的运行环境，再按一下回车就可以运行了。</li><li>默认的python2运行快捷键仍然是 “command + i”</li></ul>]]></content>
      
      <categories>
          
          <category> Skills </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Skills </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kaggle之Titanic</title>
      <link href="/2018/01/23/Project/Kaggle%E9%A1%B9%E7%9B%AE%E4%B9%8BTitanic/"/>
      <url>/2018/01/23/Project/Kaggle%E9%A1%B9%E7%9B%AE%E4%B9%8BTitanic/</url>
      <content type="html"><![CDATA[<p>Titanic是Taggle上的一篇教程性质的项目，具体内容为根据给定的乘客信息，分析乘客能否获救。</p><a id="more"></a><ul><li>通过该项目，对数据处理，特征工程，以及机器学习应用等方面产生一个较为清晰的认识。  </li><li>同时，由于是教学性质的文章，我会对一些代码写尽可能详细的注释，便于理解和学习。</li></ul><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>数据分析</li><li>特征工程</li><li>baseline</li></ol><h2 id="1-数据分析"><a href="#1-数据分析" class="headerlink" title="1. 数据分析"></a>1. 数据分析</h2><p>首先，将给定数据导入到程序中，然后对给定的每个特征，进行简要的分析和可视化，从而探索出数据，特征之间的关系，为之后的特征工程做准备。</p><h3 id="1-1-包导入"><a href="#1-1-包导入" class="headerlink" title="1.1 包导入"></a>1.1 包导入</h3><p>我们会使用总共三大类的python lib，1.数据处理； 2.可视化； 3.机器学习</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据处理</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ML</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> (GradientBoostingClassifier, GradientBoostingRegressor,</span><br><span class="line">                              RandomForestClassifier, RandomForestRegressor)</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score, StratifiedKFold, learning_curve</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure><h3 id="1-2-数据探索"><a href="#1-2-数据探索" class="headerlink" title="1.2 数据探索"></a>1.2 数据探索</h3><p>首先看一下数据的大概状况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>    </tr><br>  </tbody><br></table><br></div><p>对于每个乘客，共有11个特征，以及对应的每个乘客是否获救的label.<br>乘客信息如下</p><ul><li>PassengerId = 乘客ID</li><li>Pclass = 乘客等级（1/2/3等仓）</li><li>Name,Sex, Age</li><li>SibSp = 兄弟姐妹个数</li><li>Parch = 父母和小孩个数</li><li>Ticket = 船票信息</li><li>Fare = 船票价格</li><li>Cabin = 客仓</li><li>Embarked = 登船港口  </li></ul><p>接下来，对每个特征看一下类型信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId    891 non-null int64Survived       891 non-null int64Pclass         891 non-null int64Name           891 non-null objectSex            891 non-null objectAge            714 non-null float64SibSp          891 non-null int64Parch          891 non-null int64Ticket         891 non-null objectFare           891 non-null float64Cabin          204 non-null objectEmbarked       889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB</code></pre><p>通过以上信息，我们发现在给定数据中，Age，Cabin， Embarked存在<strong>数据缺失</strong>，尤其Cabin缺失很严重<br>我们继续对所有的数值数据进行探索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.describe()</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>count</th><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>714.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>    </tr><br>    <tr><br>      <th>mean</th><br>      <td>446.000000</td><br>      <td>0.383838</td><br>      <td>2.308642</td><br>      <td>29.699118</td><br>      <td>0.523008</td><br>      <td>0.381594</td><br>      <td>32.204208</td><br>    </tr><br>    <tr><br>      <th>std</th><br>      <td>257.353842</td><br>      <td>0.486592</td><br>      <td>0.836071</td><br>      <td>14.526497</td><br>      <td>1.102743</td><br>      <td>0.806057</td><br>      <td>49.693429</td><br>    </tr><br>    <tr><br>      <th>min</th><br>      <td>1.000000</td><br>      <td>0.000000</td><br>      <td>1.000000</td><br>      <td>0.420000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>    </tr><br>    <tr><br>      <th>25%</th><br>      <td>223.500000</td><br>      <td>0.000000</td><br>      <td>2.000000</td><br>      <td>20.125000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>7.910400</td><br>    </tr><br>    <tr><br>      <th>50%</th><br>      <td>446.000000</td><br>      <td>0.000000</td><br>      <td>3.000000</td><br>      <td>28.000000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>14.454200</td><br>    </tr><br>    <tr><br>      <th>75%</th><br>      <td>668.500000</td><br>      <td>1.000000</td><br>      <td>3.000000</td><br>      <td>38.000000</td><br>      <td>1.000000</td><br>      <td>0.000000</td><br>      <td>31.000000</td><br>    </tr><br>    <tr><br>      <th>max</th><br>      <td>891.000000</td><br>      <td>1.000000</td><br>      <td>3.000000</td><br>      <td>80.000000</td><br>      <td>8.000000</td><br>      <td>6.000000</td><br>      <td>512.329200</td><br>    </tr><br>  </tbody><br></table><br></div><p>观察下各个特征之间的协方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sns.set(context=<span class="string">"paper"</span>, font=<span class="string">"monospace"</span>)</span><br><span class="line">sns.set(style=<span class="string">"white"</span>)</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">train_corr = train.drop(<span class="string">'PassengerId'</span>,axis=<span class="number">1</span>).corr()</span><br><span class="line">sns.heatmap(train_corr, ax=ax, vmax=<span class="number">.9</span>, square=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># ax.set_xticklabels(train_corr.index, size=15)</span></span><br><span class="line"><span class="comment"># ax.set_yticklabels(train_corr.columns[::-1], size=15)</span></span><br><span class="line">ax.set_title(<span class="string">'train feature corr'</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;train feature corr&apos;)</code></pre><p><img src="/img/titanic/output_12_1.png" alt="png"></p><p>通过协方差，我们可以得出以下结论：</p><ul><li>Pclass乘客等级和获救<strong>负相关</strong></li><li>Fare价格和获救<strong>正相关</strong>，给的钱多的优先</li><li>Pclass和Fare负相关，正常，一等仓最贵</li></ul><h4 id="1-2-1-年龄特征分析："><a href="#1-2-1-年龄特征分析：" class="headerlink" title="1.2.1 年龄特征分析："></a>1.2.1 年龄特征分析：</h4><p>对乘客的年龄特征进行分析，首先画出年龄分布曲线，对于缺失值，使用age=-20进行填补</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>,<span class="number">1</span>,figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">sns.distplot(train.Age.fillna(<span class="number">-20</span>), rug=<span class="keyword">True</span>, color=<span class="string">'b'</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">ax0 = axes[<span class="number">0</span>]</span><br><span class="line">ax0.set_title(<span class="string">'age distribution'</span>)</span><br><span class="line">ax0.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax1 = axes[<span class="number">1</span>]</span><br><span class="line">ax1.set_title(<span class="string">'age survived distribution'</span>)</span><br><span class="line">k1 = sns.distplot(train[train.Survived==<span class="number">0</span>].Age.fillna(<span class="number">-20</span>), hist=<span class="keyword">False</span>, color=<span class="string">'r'</span>, ax=ax1, label=<span class="string">'dead'</span>)</span><br><span class="line">k2 = sns.distplot(train[train.Survived==<span class="number">1</span>].Age.fillna(<span class="number">-20</span>), hist=<span class="keyword">False</span>, color=<span class="string">'g'</span>, ax=ax1, label=<span class="string">'alive'</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">''</span>)</span><br><span class="line">ax1.legend(fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282bf1c0f0&gt;</code></pre><p><img src="/img/titanic/output_15_1.png" alt="png"></p><p>通过对年龄分布进行分析，可以得到以下信息：</p><ol><li>可以看出，小孩和20多岁的青年以及中年获救可能性较大</li><li>获救情况和年龄<strong>不成现线性关系，所以如果使用线性模型需要对年龄进行离散处理，作为类型变量带入模型</strong></li><li>获救的人中，年龄缺少的人比例要小于有年龄信息的人（获救之后对乘客信息重新登记导致？）</li></ol><h5 id="1-2-1-年龄和性别分布"><a href="#1-2-1-年龄和性别分布" class="headerlink" title="1.2.1 年龄和性别分布"></a>1.2.1 年龄和性别分布</h5><p>对年龄和性别两个特征进行探索</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">ax.set_title(<span class="string">'Sex-Age_distribution'</span>)</span><br><span class="line">sns.distplot(train[train[<span class="string">'Sex'</span>] == <span class="string">'female'</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'pink'</span>, label=<span class="string">'female'</span>)</span><br><span class="line">sns.distplot(train[train[<span class="string">'Sex'</span>] == <span class="string">'male'</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'blue'</span>, label=<span class="string">'male'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282bf1c198&gt;</code></pre><p><img src="/img/titanic/output_18_1.png" alt="png"></p><ul><li>男性年龄普遍集中在40岁左右，女性年龄普遍集中在21，22岁左右。</li><li>老年人中，男性比例较多，年轻人中，女性比例较多。</li></ul><h5 id="1-2-1-年龄和仓位分布"><a href="#1-2-1-年龄和仓位分布" class="headerlink" title="1.2.1 年龄和仓位分布"></a>1.2.1 年龄和仓位分布</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">ax.set_title(<span class="string">'Pclass Age dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">1</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'pink'</span>, label=<span class="string">'P1'</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">2</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'blue'</span>, label=<span class="string">'p2'</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">3</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'g'</span>, label=<span class="string">'p3'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b78c278&gt;</code></pre><p><img src="/img/titanic/output_21_1.png" alt="png"></p><ul><li>越老，仓位越好（好像是废话）</li></ul><h4 id="1-2-1-Pclass-仓位分析"><a href="#1-2-1-Pclass-仓位分析" class="headerlink" title="1.2.1 Pclass 仓位分析"></a>1.2.1 Pclass 仓位分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_dead = train[train.Survived==<span class="number">0</span>].groupby(<span class="string">'Pclass'</span>)[<span class="string">'Survived'</span>].count()</span><br><span class="line">y_alive = train[train.Survived==<span class="number">1</span>].groupby(<span class="string">'Pclass'</span>)[<span class="string">'Survived'</span>].count()</span><br><span class="line">pos = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.bar(pos, y_dead, color=<span class="string">'r'</span>, alpha=<span class="number">0.6</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y_alive, color=<span class="string">'g'</span>, bottom=y_dead, alpha=<span class="number">0.6</span>, label=<span class="string">'alive'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">16</span>, loc=<span class="string">'best'</span>)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xticklabels([<span class="string">'Pclass%d'</span>%(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>)], size=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Pclass Surveved count'</span>, size=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Pclass Surveved count&apos;)</code></pre><p><img src="/img/titanic/output_24_1.png" alt="png"></p><ul><li>头等舱（Pclass=1），商务舱（Pclass=2），经济舱（Pclass=3）</li><li>经济舱人数遥遥领先</li><li>比较来说，头等舱获救比例最大，经济舱死亡概率最大</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pos = range(0,6)</span></span><br><span class="line"><span class="comment"># age_list = []</span></span><br><span class="line"><span class="comment"># for Pclass_ in range(1,4):</span></span><br><span class="line"><span class="comment">#     for Survived_ in range(0,2):</span></span><br><span class="line"><span class="comment">#         age_list.append(train[(train.Pclass == Pclass_)&amp;(train.Survived == Survived_)].Age.values)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig, axes = plt.subplots(3,1,figsize=(10,6))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># i_Pclass = 1</span></span><br><span class="line"><span class="comment"># for ax in axes:</span></span><br><span class="line"><span class="comment">#     sns.distplot(age_list[i_Pclass*2-2], hist=False, ax=ax, label='Pclass:%d ,survived:0'%(i_Pclass), color='r')</span></span><br><span class="line"><span class="comment">#     sns.distplot(age_list[i_Pclass*2-1], hist=False, ax=ax, label='Pclass:%d ,survived:1'%(i_Pclass), color='g')</span></span><br><span class="line"><span class="comment">#     i_Pclass += 1</span></span><br><span class="line"><span class="comment">#     ax.set_xlabel('age', size=15)</span></span><br><span class="line"><span class="comment">#     ax.legend(fontsize=15)</span></span><br></pre></td></tr></table></figure><h4 id="1-2-3-性别特征"><a href="#1-2-3-性别特征" class="headerlink" title="1.2.3 性别特征"></a>1.2.3 性别特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train.Sex.value_counts())</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(train.groupby(<span class="string">'Sex'</span>)[<span class="string">'Survived'</span>].mean())</span><br></pre></td></tr></table></figure><pre><code>male      577female    314Name: Sex, dtype: int64===================================Sexfemale    0.742038male      0.188908Name: Survived, dtype: float64</code></pre><p>可以看出：</p><ul><li>全部乘客中，男性人数大于女性，接近1.5倍</li><li>获救乘客中，大部分为女性，是男性的4倍，女性生存率远大于男性</li></ul><h5 id="性别和年龄和生存情况"><a href="#性别和年龄和生存情况" class="headerlink" title="性别和年龄和生存情况"></a>性别和年龄和生存情况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ax = plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.violinplot(x=<span class="string">'Sex'</span>, y=<span class="string">'Age'</span>, hue=<span class="string">'Survived'</span>, data=train.dropna(),split=<span class="keyword">True</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Sex'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.set_xticklabels([<span class="string">'Female'</span>, <span class="string">'male'</span>], size=<span class="number">18</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Age'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">25</span>, loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b7242b0&gt;</code></pre><p><img src="/img/titanic/output_31_1.png" alt="png"></p><p>从分布可以看出：</p><ul><li>女性获救年龄主要分布在20岁左右的青年</li><li>青年男性获救概率要大于中年男性，年轻人和小孩更容易获救</li></ul><h5 id="性别仓位和获救情况分布"><a href="#性别仓位和获救情况分布" class="headerlink" title="性别仓位和获救情况分布"></a>性别仓位和获救情况分布</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> sex_i <span class="keyword">in</span> [<span class="string">'female'</span>,<span class="string">'male'</span>]:</span><br><span class="line">    <span class="keyword">for</span> pclass_i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">        label.append(<span class="string">'sex:%s,Pclass:%d'</span>%(sex_i, pclass_i))</span><br><span class="line"></span><br><span class="line">pos = range(<span class="number">6</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>,<span class="number">4</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.bar(pos,</span><br><span class="line">        train[train[<span class="string">'Survived'</span>]==<span class="number">0</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        color=<span class="string">'r'</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        align=<span class="string">'center'</span>,</span><br><span class="line">        tick_label=label,</span><br><span class="line">        label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos,</span><br><span class="line">        train[train[<span class="string">'Survived'</span>]==<span class="number">1</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        bottom=train[train[<span class="string">'Survived'</span>]==<span class="number">0</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        color=<span class="string">'g'</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        align=<span class="string">'center'</span>,</span><br><span class="line">        tick_label=label,</span><br><span class="line">        label=<span class="string">'alive'</span>)</span><br><span class="line">ax.tick_params(labelsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'sex_pclass_survived'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">10</span>,loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b6ebf60&gt;</code></pre><p><img src="/img/titanic/output_34_1.png" alt="png"></p><p>可以发现，头等舱的女性几乎都获救了</p><h4 id="1-2-4-票价-Fare"><a href="#1-2-4-票价-Fare" class="headerlink" title="1.2.4 票价(Fare)"></a>1.2.4 票价(Fare)</h4><p>分别看一下票价的分布以及票价随仓位的分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax = plt.subplot2grid((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">0</span>,<span class="number">0</span>), colspan=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">ax.tick_params(labelsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Fare dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">sns.kdeplot(train.Fare, ax=ax)</span><br><span class="line">sns.distplot(train.Fare, ax=ax)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br><span class="line">pos = range(<span class="number">0</span>,<span class="number">400</span>,<span class="number">50</span>)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xlim([<span class="number">0</span>, <span class="number">200</span>])</span><br><span class="line">ax.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot2grid((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">0</span>), colspan=<span class="number">2</span>)</span><br><span class="line">ax.set_title(<span class="string">'Fare Pclass dist'</span>, size=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">    sns.kdeplot(train[train.Pclass==i].Fare, ax=ax1, label=<span class="string">'Pclass %d'</span>%(i))</span><br><span class="line">ax1.set_xlim([<span class="number">0</span>,<span class="number">200</span>])</span><br><span class="line">ax1.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b632f60&gt;</code></pre><p><img src="/img/titanic/output_37_1.png" alt="png"></p><h5 id="票价和获救情况的关系"><a href="#票价和获救情况的关系" class="headerlink" title="票价和获救情况的关系"></a>票价和获救情况的关系</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.kdeplot(train[train.Survived==<span class="number">0</span>].Fare, ax=ax1, label=<span class="string">'dead'</span>, color=<span class="string">'r'</span>)</span><br><span class="line">sns.kdeplot(train[train.Survived==<span class="number">1</span>].Fare, ax=ax1, label=<span class="string">'alive'</span>, color=<span class="string">'g'</span>)</span><br><span class="line"><span class="comment">#sns.distplot(train[train.Survived==0].Fare, ax=ax1, color='r')</span></span><br><span class="line"><span class="comment">#sns.distplot(train[train.Survived==1].Fare, ax=ax1, color='g')</span></span><br><span class="line">ax1.set_xlim([<span class="number">0</span>,<span class="number">300</span>])</span><br><span class="line">ax1.legend(fontsize=<span class="number">15</span>)</span><br><span class="line">ax1.set_title(<span class="string">'Fare survived'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'Fare'</span>, size=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,0,&apos;Fare&apos;)</code></pre><p><img src="/img/titanic/output_39_1.png" alt="png"></p><h4 id="1-2-5-sibsp-amp-parch-表亲和直亲"><a href="#1-2-5-sibsp-amp-parch-表亲和直亲" class="headerlink" title="1.2.5 sibsp&amp;parch 表亲和直亲"></a>1.2.5 sibsp&amp;parch 表亲和直亲</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">sns.countplot(train.SibSp)</span><br><span class="line">ax1.set_title(<span class="string">'SibSp'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>, sharex=ax1)</span><br><span class="line">sns.countplot(train.Parch)</span><br><span class="line">ax2.set_title(<span class="string">'Parch'</span>, size=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Parch&apos;)</code></pre><p><img src="/img/titanic/output_41_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">311</span>)</span><br><span class="line">train.groupby(<span class="string">'SibSp'</span>)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'Sibsp Survived Rate'</span>, size=<span class="number">16</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">312</span>)</span><br><span class="line">train.groupby(<span class="string">'Parch'</span>)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">'Parch Survived Rate'</span>, size=<span class="number">16</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax3 = fig.add_subplot(<span class="number">313</span>)</span><br><span class="line">train.groupby(train.SibSp+train.Parch)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax3)</span><br><span class="line">ax3.set_title(<span class="string">'Parch+Sibsp Survived Rate'</span>, size=<span class="number">16</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Parch+Sibsp Survived Rate&apos;)</code></pre><p><img src="/img/titanic/output_42_1.png" alt="png"></p><p>分组统计不同人数亲戚的获救率来看，都近似呈现先高后低, 亲人数目多少和是否获救不是简单的线性关系</p><h4 id="1-2-6-Embarked-上船地点"><a href="#1-2-6-Embarked-上船地点" class="headerlink" title="1.2.6 Embarked 上船地点"></a>1.2.6 Embarked 上船地点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">pos = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y1 = train[train.Survived==<span class="number">0</span>].groupby(<span class="string">'Embarked'</span>)[<span class="string">'Survived'</span>].count().sort_index().values</span><br><span class="line">y2 = train[train.Survived==<span class="number">1</span>].groupby(<span class="string">'Embarked'</span>)[<span class="string">'Survived'</span>].count().sort_index().values</span><br><span class="line">ax.bar(pos, y1, color=<span class="string">'r'</span>, alpha=<span class="number">0.4</span>, align=<span class="string">'center'</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y2, color=<span class="string">'g'</span>, alpha=<span class="number">0.4</span>, align=<span class="string">'center'</span>, label=<span class="string">'alive'</span>, bottom=y1)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xticklabels([<span class="string">'C'</span>,<span class="string">'Q'</span>,<span class="string">'S'</span>])</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>, loc=<span class="string">'best'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Embarked survived count'</span>, size=<span class="number">18</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Embarked survived count&apos;)</code></pre><p><img src="/img/titanic/output_44_1.png" alt="png"></p><p>可以看出，从c港上船的乘客更容易获救</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.set_xlim([<span class="number">-20</span>, <span class="number">80</span>])</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'C'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'C'</span>)</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'Q'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'Q'</span>)</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'S'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'S'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">18</span>)</span><br><span class="line">ax.set_title(<span class="string">'Embarked Age Dist '</span>, size=<span class="number">18</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Embarked Age Dist &apos;)</code></pre><p><img src="/img/titanic/output_46_1.png" alt="png"></p><ul><li>Q上岸的很多没有年龄</li><li>C上岸和S上岸的年龄分布较为相似，区别在于C上岸的年龄分布更加扁平，小孩和老人比例更高</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">y1 = train[train.Survived==<span class="number">0</span>].groupby([<span class="string">'Embarked'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().reset_index()[<span class="string">'Survived'</span>].values</span><br><span class="line">y2 = train[train.Survived==<span class="number">1</span>].groupby([<span class="string">'Embarked'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().reset_index()[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">pos = range(<span class="number">9</span>)</span><br><span class="line">ax.bar(pos, y1, align=<span class="string">'center'</span>, alpha=<span class="number">0.5</span>, color=<span class="string">'r'</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y2, align=<span class="string">'center'</span>, bottom=y1, alpha=<span class="number">0.5</span>, color=<span class="string">'g'</span>, label=<span class="string">'alive'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">xticklabels = []</span><br><span class="line"><span class="keyword">for</span> embarked_val <span class="keyword">in</span> [<span class="string">'C'</span>,<span class="string">'Q'</span>,<span class="string">'S'</span>]:</span><br><span class="line">    <span class="keyword">for</span> pclass_val <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">        xticklabels.append(<span class="string">'%s/%d'</span>%(embarked_val,pclass_val))</span><br><span class="line"></span><br><span class="line">ax.set_xticklabels(xticklabels,size=<span class="number">15</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>, loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b3e07b8&gt;</code></pre><p><img src="/img/titanic/output_48_1.png" alt="png"></p><p>从不同仓位的比例来看，似乎C上岸更容易获救是因为头等舱的人较多？</p><p>但进一步对比C/S 发现，同样的仓位，C获救概率依然更高</p><p>脑洞下：</p><p>C地的人更加抱团，互帮互助- -<br>人数上来看S地的人更多，不同等级分布也更合常理，而C地的人头等舱很多，商务舱几乎没有，屌丝仓的也不少；</p><p>猜想：C地的人更多是权贵，S地的人来自商贸发达的商人？，所有C地的人地位更高- -</p><h4 id="1-2-7-Cabin-船舱号"><a href="#1-2-7-Cabin-船舱号" class="headerlink" title="1.2.7 Cabin 船舱号"></a>1.2.7 Cabin 船舱号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.Cabin.isnull().value_counts()</span><br></pre></td></tr></table></figure><pre><code>True     687False    204Name: Cabin, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(by=train.Cabin.isnull())[<span class="string">'Survived'</span>].mean()</span><br></pre></td></tr></table></figure><pre><code>CabinFalse    0.666667True     0.299854Name: Survived, dtype: float64</code></pre><ul><li>Cabin大部分为空</li><li>为空的获救概率较低，不为空的获救概率较高。<strong>说明该数据可以作为特征</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train.Cabin.apply(<span class="keyword">lambda</span> x:len(x) <span class="keyword">if</span>(x <span class="keyword">is</span> <span class="keyword">not</span> np.nan) <span class="keyword">else</span> <span class="number">0</span>)&gt;<span class="number">4</span>].head()</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>27</th><br>      <td>28</td><br>      <td>0</td><br>      <td>1</td><br>      <td>Fortune, Mr. Charles Alexander</td><br>      <td>male</td><br>      <td>19.0</td><br>      <td>3</td><br>      <td>2</td><br>      <td>19950</td><br>      <td>263.0000</td><br>      <td>C23 C25 C27</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>75</th><br>      <td>76</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Moen, Mr. Sigurd Hansen</td><br>      <td>male</td><br>      <td>25.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>348123</td><br>      <td>7.6500</td><br>      <td>F G73</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>88</th><br>      <td>89</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Fortune, Miss. Mabel Helen</td><br>      <td>female</td><br>      <td>23.0</td><br>      <td>3</td><br>      <td>2</td><br>      <td>19950</td><br>      <td>263.0000</td><br>      <td>C23 C25 C27</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>97</th><br>      <td>98</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Greenfield, Mr. William Bertram</td><br>      <td>male</td><br>      <td>23.0</td><br>      <td>0</td><br>      <td>1</td><br>      <td>PC 17759</td><br>      <td>63.3583</td><br>      <td>D10 D12</td><br>      <td>C</td><br>    </tr><br>    <tr><br>      <th>118</th><br>      <td>119</td><br>      <td>0</td><br>      <td>1</td><br>      <td>Baxter, Mr. Quigg Edmond</td><br>      <td>male</td><br>      <td>24.0</td><br>      <td>0</td><br>      <td>1</td><br>      <td>PC 17558</td><br>      <td>247.5208</td><br>      <td>B58 B60</td><br>      <td>C</td><br>    </tr><br>  </tbody><br></table><br></div><p>不少船舱编号有多个，其个数基本上和直系亲属个数保持一致，说明也许可以通过寻找亲属信息来填充cabin的一部分信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Cabin_Zone'</span>] = train.Cabin.fillna(<span class="string">'0'</span>).str.split(<span class="string">' '</span>).apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">train.groupby(by=<span class="string">'Cabin_Zone'</span>)[<span class="string">'Survived'</span>].agg([<span class="string">'mean'</span>, <span class="string">'count'</span>])</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>mean</th><br>      <th>count</th><br>    </tr><br>    <tr><br>      <th>Cabin_Zone</th><br>      <th></th><br>      <th></th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.299854</td><br>      <td>687</td><br>    </tr><br>    <tr><br>      <th>A</th><br>      <td>0.466667</td><br>      <td>15</td><br>    </tr><br>    <tr><br>      <th>B</th><br>      <td>0.744681</td><br>      <td>47</td><br>    </tr><br>    <tr><br>      <th>C</th><br>      <td>0.593220</td><br>      <td>59</td><br>    </tr><br>    <tr><br>      <th>D</th><br>      <td>0.757576</td><br>      <td>33</td><br>    </tr><br>    <tr><br>      <th>E</th><br>      <td>0.750000</td><br>      <td>32</td><br>    </tr><br>    <tr><br>      <th>F</th><br>      <td>0.615385</td><br>      <td>13</td><br>    </tr><br>    <tr><br>      <th>G</th><br>      <td>0.500000</td><br>      <td>4</td><br>    </tr><br>    <tr><br>      <th>T</th><br>      <td>0.000000</td><br>      <td>1</td><br>    </tr><br>  </tbody><br></table><br></div><p>把每个Cabin中的区域提取出来之后，统计发现不同区域获救概率差别很大，<strong>或许可以作为一个特征</strong></p><h4 id="1-2-8-姓名特征"><a href="#1-2-8-姓名特征" class="headerlink" title="1.2.8 姓名特征"></a>1.2.8 姓名特征</h4><p>最开始认为Name这个特征没有任何意义，但我们发现，所给数据中的姓名信息不仅仅是姓名，而且包括称谓，甚至包括性别，地位，财富，婚姻状况等都可能包含在姓名中。所以，对于Name不能直接将其当作垃圾特征处理。<br>首先，对姓名的长度和获救状况进行探索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(train.Name.apply(<span class="keyword">lambda</span> x:len(x)))[<span class="string">'Survived'</span>].mean().plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f282b56a198&gt;</code></pre><p><img src="/img/titanic/output_59_1.png" alt="png"></p><ul><li>可以发现，似乎名字越长，获救可能性越高，所以我们把名字的长度加入特征：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Name_Len'</span>] = train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line">test[<span class="string">'Name_Len'</span>] = test[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>    </tr><br>  </tbody><br></table><br></div><p>同时我们猜测，不同的称谓应该也和获救情况有关联：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>]))[<span class="string">'Survived'</span>].mean().plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f282b32f6d8&gt;</code></pre><p><img src="/img/titanic/output_63_1.png" alt="png"></p><p>于是我们把称谓信息提取出来，由于有些称谓的人数量过少，我们还需要做一个映射:</p><p>Mme：称呼非英语民族的”上层社会”已婚妇女,及有职业的妇女，相当于Mrs</p><p>Jonkheer:乡绅</p><p>Capt：船长</p><p>Lady：贵族夫人的称呼</p><p>Don唐：是西班牙语中贵族和有地位者的尊称</p><p>sir：先生</p><p>the Countess：女伯爵</p><p>Ms：Ms.或Mz 美国近来用来称呼婚姻状态不明的妇女</p><p>Col：中校:Lieutenant Colonel(Lt. Col.)上校:Colonel(Col.)</p><p>Major：少校</p><p>Mlle:小姐</p><p>Rev：牧师</p><p>测试集合中特殊的Dona：女士尊称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'Don'</span>,<span class="string">'Dona'</span>, <span class="string">'Major'</span>, <span class="string">'Capt'</span>, <span class="string">'Jonkheer'</span>, <span class="string">'Rev'</span>, <span class="string">'Col'</span>,<span class="string">'Sir'</span>,<span class="string">'Dr'</span>],<span class="string">'Mr'</span>)</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'Mlle'</span>,<span class="string">'Ms'</span>], <span class="string">'Miss'</span>)</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'the Countess'</span>,<span class="string">'Mme'</span>,<span class="string">'Lady'</span>,<span class="string">'Dr'</span>], <span class="string">'Mrs'</span>)</span><br><span class="line"></span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'Don'</span>,<span class="string">'Dona'</span>, <span class="string">'Major'</span>, <span class="string">'Capt'</span>, <span class="string">'Jonkheer'</span>, <span class="string">'Rev'</span>, <span class="string">'Col'</span>,<span class="string">'Sir'</span>,<span class="string">'Dr'</span>],<span class="string">'Mr'</span>)</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'Mlle'</span>,<span class="string">'Ms'</span>], <span class="string">'Miss'</span>)</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'the Countess'</span>,<span class="string">'Mme'</span>,<span class="string">'Lady'</span>,<span class="string">'Dr'</span>], <span class="string">'Mrs'</span>)</span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>Title</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>      <td>Mr</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>      <td>Mrs</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>      <td>Miss</td><br>    </tr><br>  </tbody><br></table><br></div><h1 id="2-特征工程"><a href="#2-特征工程" class="headerlink" title="2. 特征工程"></a>2. 特征工程</h1><p>首先查看所有数据中为null的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'============train==========='</span>)</span><br><span class="line">print(train.isnull().sum())</span><br><span class="line">print(<span class="string">'============test============'</span>)</span><br><span class="line">print(test.isnull().sum())</span><br></pre></td></tr></table></figure><pre><code>============train===========PassengerId      0Survived         0Pclass           0Name             0Sex              0Age            177SibSp            0Parch            0Ticket           0Fare             0Cabin          687Embarked         2Cabin_Zone       0Name_Len         0Title            0dtype: int64============test============PassengerId      0Pclass           0Name             0Sex              0Age             86SibSp            0Parch            0Ticket           0Fare             1Cabin          327Embarked         0Name_Len         0Title            0dtype: int64</code></pre><ol><li>age 和 cabin在训练集和测试集都有缺失，cabin缺失数量巨大</li><li>embarked 在训练集存在2个缺失</li><li>fare在测试集有一个缺失</li></ol><h3 id="2-1-embarked-处理"><a href="#2-1-embarked-处理" class="headerlink" title="2.1 embarked 处理"></a>2.1 embarked 处理</h3><p>首先，对embarked进行处理，查看embarked的缺失情况。<br>因为只有2个数据存在缺失，所以不适合将“缺失”重新定义一个分类，我们想办法对其进行填补：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train.Embarked.isnull()]</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>Title</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>61</th><br>      <td>62</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Icard, Miss. Amelie</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>113572</td><br>      <td>80.0</td><br>      <td>B28</td><br>      <td>NaN</td><br>      <td>B</td><br>      <td>19</td><br>      <td>Miss</td><br>    </tr><br>    <tr><br>      <th>829</th><br>      <td>830</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td><br>      <td>female</td><br>      <td>62.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>113572</td><br>      <td>80.0</td><br>      <td>B28</td><br>      <td>NaN</td><br>      <td>B</td><br>      <td>41</td><br>      <td>Mrs</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train.Embarked.value_counts())</span><br><span class="line">print(<span class="string">'========================='</span>)</span><br><span class="line">print(train[train.Pclass==<span class="number">1</span>].Embarked.value_counts())</span><br></pre></td></tr></table></figure><pre><code>S    644C    168Q     77Name: Embarked, dtype: int64=========================S    127C     85Q      2Name: Embarked, dtype: int64</code></pre><ul><li>不分Pclass仓位来看的话，S embarked人数远大于C，Q</li><li>而且两个缺失的数据Pclass都为1，而Pclass==1的乘客中，S embarked的人数也是最多。  </li></ul><p><strong>所以，因为缺失数据很少，可以考虑随最大可能数据填补缺失值，因此设定缺失值为S</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.Embarked.fillna(<span class="string">'S'</span>, inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-cabin处理"><a href="#2-2-cabin处理" class="headerlink" title="2.2 cabin处理"></a>2.2 cabin处理</h3><p>在之前的数据探索环节，我们知道cabin为空的获救概率较低，不为空的获救概率较高。说明该数据可以作为特征。所以我们对cabin作填充处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#若为NaN,则用字符串‘Null’填充，否则用'Not Null'填充</span></span><br><span class="line">train[<span class="string">'Cabin'</span>] = train[<span class="string">'Cabin'</span>].isnull().apply(<span class="keyword">lambda</span> x: <span class="string">'Null'</span> <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">True</span> <span class="keyword">else</span> <span class="string">'Not Null'</span>)</span><br><span class="line">test[<span class="string">'Cabin'</span>] = test[<span class="string">'Cabin'</span>].isnull().apply(<span class="keyword">lambda</span> x: <span class="string">'Null'</span> <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">True</span> <span class="keyword">else</span> <span class="string">'Not Null'</span>)</span><br></pre></td></tr></table></figure><h3 id="2-3-Name和Ticket处理"><a href="#2-3-Name和Ticket处理" class="headerlink" title="2.3 Name和Ticket处理"></a>2.3 Name和Ticket处理</h3><p>已经从Name属性中提取了有价值的特征，所以删除原Name和Ticket属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> train[<span class="string">'Name'</span>], test[<span class="string">'Name'</span>]</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">'Ticket'</span>], test[<span class="string">'Ticket'</span>]</span><br></pre></td></tr></table></figure><h3 id="2-4-年龄离散化处理"><a href="#2-4-年龄离散化处理" class="headerlink" title="2.4 年龄离散化处理"></a>2.4 年龄离散化处理</h3><p>在我们对数据进行观察时，发现获救情况和年龄不是存在简单的线性关系，所以需要对年龄进行离散化处理<br>同时，对于缺失值，需要进行填补。考虑到年龄缺失的数量较多，所以将缺失设为一类<br>年龄以5岁为一个周期进行离散，同时10岁以下，60岁以上分别归为一类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">group = train.groupby([<span class="string">'Title'</span>, <span class="string">'Pclass'</span>])[<span class="string">'Age'</span>]</span><br><span class="line">train[<span class="string">'Age'</span>] = group.transform(<span class="keyword">lambda</span> x: x.fillna(x.median()))</span><br><span class="line">train = train.drop(<span class="string">'Title'</span>,axis=<span class="number">1</span>)</span><br><span class="line">train[<span class="string">'IsChild'</span>] = np.where(train[<span class="string">'Age'</span>]&lt;=<span class="number">12</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">train[<span class="string">'Age'</span>] = pd.cut(train[<span class="string">'Age'</span>],<span class="number">5</span>)</span><br><span class="line"><span class="comment">#train = train.drop('Age',axis=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group = test.groupby([<span class="string">'Title'</span>, <span class="string">'Pclass'</span>])[<span class="string">'Age'</span>]</span><br><span class="line">test[<span class="string">'Age'</span>] = group.transform(<span class="keyword">lambda</span> x: x.fillna(x.median()))</span><br><span class="line">test = test.drop(<span class="string">'Title'</span>,axis=<span class="number">1</span>)</span><br><span class="line">test[<span class="string">'IsChild'</span>] = np.where(test[<span class="string">'Age'</span>]&lt;=<span class="number">12</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">test[<span class="string">'Age'</span>] = pd.cut(test[<span class="string">'Age'</span>],<span class="number">5</span>)</span><br><span class="line"><span class="comment">#test = test.drop('Age',axis=1)</span></span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>male</td><br>      <td>(16.336, 32.252]</td><br>      <td>1</td><br>      <td>0</td><br>      <td>7.2500</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>female</td><br>      <td>(32.252, 48.168]</td><br>      <td>1</td><br>      <td>0</td><br>      <td>71.2833</td><br>      <td>Not Null</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>female</td><br>      <td>(16.336, 32.252]</td><br>      <td>0</td><br>      <td>0</td><br>      <td>7.9250</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="2-5-fare属性"><a href="#2-5-fare属性" class="headerlink" title="2.5 fare属性"></a>2.5 fare属性</h3><p>在test集中存在一个数据的fare为缺失值，对于这个变量，可以用平均值进行填补</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test[test.Fare.isnull()]</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>152</th><br>      <td>1044</td><br>      <td>3</td><br>      <td>male</td><br>      <td>(45.668, 60.834]</td><br>      <td>0</td><br>      <td>0</td><br>      <td>NaN</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>18</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test.loc[test.Fare.isnull(), <span class="string">'Fare'</span>] = \</span><br><span class="line">test[(test.Pclass==<span class="number">1</span>) &amp; (test.Embarked==<span class="string">'S'</span>) &amp; (test.Sex==<span class="string">'male'</span>)].dropna().Fare.mean()</span><br></pre></td></tr></table></figure><h3 id="2-6-归一化处理"><a href="#2-6-归一化处理" class="headerlink" title="2.6 归一化处理"></a>2.6 归一化处理</h3><p>数据中fare分布波动较大，对其进行归一化处理，加速模型收敛</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">fare_scale_param = scaler.fit(train[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">train.Fare = fare_scale_param.transform(train[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">test.Fare = fare_scale_param.transform(test[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="2-7-所有特征转化成数值型编码"><a href="#2-7-所有特征转化成数值型编码" class="headerlink" title="2.7 所有特征转化成数值型编码"></a>2.7 所有特征转化成数值型编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">features = train.drop([<span class="string">"PassengerId"</span>,<span class="string">"Survived"</span>], axis=<span class="number">1</span>).columns</span><br><span class="line">le = LabelEncoder()</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">    le = le.fit(train[feature])</span><br><span class="line">    train[feature] = le.transform(train[feature])</span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>1</td><br>      <td>0</td><br>      <td>18</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>11</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>2</td><br>      <td>1</td><br>      <td>0</td><br>      <td>207</td><br>      <td>0</td><br>      <td>0</td><br>      <td>3</td><br>      <td>39</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>41</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>10</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="2-8-得到训练-测试数据集"><a href="#2-8-得到训练-测试数据集" class="headerlink" title="2.8 得到训练/测试数据集"></a>2.8 得到训练/测试数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_all = train.iloc[:<span class="number">891</span>,:].drop([<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>], axis=<span class="number">1</span>)</span><br><span class="line">Y_all = train.iloc[:<span class="number">891</span>,:][<span class="string">'Survived'</span>]</span><br><span class="line">X_test = train.iloc[<span class="number">891</span>:,:].drop([<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="3-模型训练和调优"><a href="#3-模型训练和调优" class="headerlink" title="3 模型训练和调优"></a>3 模型训练和调优</h1><p>分别考察逻辑回归、支持向量机、最近邻、决策树、随机森林、gbdt、xgbGBDT几类算法的性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression()</span><br><span class="line">svc = SVC()</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors = <span class="number">3</span>)</span><br><span class="line">decision_tree = DecisionTreeClassifier()</span><br><span class="line">random_forest = RandomForestClassifier(n_estimators=<span class="number">300</span>,min_samples_leaf=<span class="number">4</span>,class_weight=&#123;<span class="number">0</span>:<span class="number">0.745</span>,<span class="number">1</span>:<span class="number">0.255</span>&#125;)</span><br><span class="line">gbdt = GradientBoostingClassifier(n_estimators=<span class="number">500</span>,learning_rate=<span class="number">0.03</span>,max_depth=<span class="number">3</span>)</span><br><span class="line">xgbGBDT = XGBClassifier(max_depth=<span class="number">3</span>, n_estimators=<span class="number">300</span>, learning_rate=<span class="number">0.05</span>)</span><br><span class="line">clfs = [lr, svc, knn, decision_tree, random_forest, gbdt, xgbGBDT]</span><br><span class="line"></span><br><span class="line">kfold = <span class="number">10</span></span><br><span class="line">cv_results = []</span><br><span class="line"><span class="keyword">for</span> classifier <span class="keyword">in</span> clfs :</span><br><span class="line">    cv_results.append(cross_val_score(classifier, X_all, y = Y_all, scoring = <span class="string">"accuracy"</span>, cv = kfold, n_jobs=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">cv_means = []</span><br><span class="line">cv_std = []</span><br><span class="line"><span class="keyword">for</span> cv_result <span class="keyword">in</span> cv_results:</span><br><span class="line">    cv_means.append(cv_result.mean())</span><br><span class="line">    cv_std.append(cv_result.std())</span><br><span class="line"></span><br><span class="line">cv_res = pd.DataFrame(&#123;<span class="string">"CrossValMeans"</span>:cv_means,<span class="string">"CrossValerrors"</span>: cv_std,</span><br><span class="line">                       <span class="string">"Algorithm"</span>:[<span class="string">"LR"</span>,<span class="string">"SVC"</span>,<span class="string">'KNN'</span>,<span class="string">'decision_tree'</span>,<span class="string">"random_forest"</span>,<span class="string">"GBDT"</span>,<span class="string">"xgbGBDT"</span>]&#125;)</span><br><span class="line"></span><br><span class="line">g = sns.barplot(<span class="string">"CrossValMeans"</span>,<span class="string">"Algorithm"</span>,data = cv_res, palette=<span class="string">"Set3"</span>,orient = <span class="string">"h"</span>,**&#123;<span class="string">'xerr'</span>:cv_std&#125;)</span><br><span class="line">g.set_xlabel(<span class="string">"Mean Accuracy"</span>)</span><br><span class="line">g = g.set_title(<span class="string">"Cross validation scores"</span>)</span><br></pre></td></tr></table></figure><p><img src="/img/titanic/output_90_0.png" alt="png"></p><p>通过观察，发现不同的模型的feature importance有比较大的差别，所以考虑到把他们组合在一起可能有更好的效果<br>首先定义集成框架：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ensemble</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,estimators)</span>:</span></span><br><span class="line">        self.estimator_names = []</span><br><span class="line">        self.estimators = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> estimators:</span><br><span class="line">            self.estimator_names.append(i[<span class="number">0</span>])</span><br><span class="line">            self.estimators.append(i[<span class="number">1</span>])</span><br><span class="line">        self.clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_x, train_y)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators:</span><br><span class="line">            i.fit(train_x,train_y)</span><br><span class="line">        x = np.array([i.predict(train_x) <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators]).T</span><br><span class="line">        y = train_y</span><br><span class="line">        self.clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = np.array([i.predict(x) <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators]).T</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> self.clf.predict(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        s = precision_score(y,self.predict(x))</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p>将基分类器放入集成框架中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bag = Ensemble(</span><br><span class="line">    [(<span class="string">'xgb'</span>,xgbGBDT),(<span class="string">'lr'</span>,lr),(<span class="string">'rf'</span>,random_forest),(<span class="string">'svc'</span>,svc),(<span class="string">'gbdt'</span>,gbdt),(<span class="string">'dt'</span>,decision_tree),(<span class="string">'knn'</span>,knn)])</span><br><span class="line">score = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">    num_test = <span class="number">0.20</span></span><br><span class="line">    X_train, X_cv, Y_train, Y_cv = train_test_split(X_all, Y_all, test_size=num_test)</span><br><span class="line">    bag.fit(X_train, Y_train)</span><br><span class="line">    <span class="comment">#Y_test = bag.predict(X_test)</span></span><br><span class="line">    acc_xgb = round(bag.score(X_cv, Y_cv) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    score+=acc_xgb</span><br><span class="line">print(score/<span class="number">10</span>)  <span class="comment">#0.8786</span></span><br></pre></td></tr></table></figure><pre><code>68.73299999999999</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def plot_learning_curve(clf, title, X, y, ylim=None, cv=None, n_jobs=3, train_sizes=np.linspace(.05, 1., 5)):</span></span><br><span class="line"><span class="comment">#     train_sizes, train_scores, test_scores = learning_curve(</span></span><br><span class="line"><span class="comment">#         clf, x, y, train_sizes=train_sizes)</span></span><br><span class="line"><span class="comment">#     train_scores_mean = np.mean(train_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     train_scores_std = np.std(train_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     test_scores_mean = np.mean(test_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     test_scores_std = np.std(test_scores, axis=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax = plt.figure().add_subplot(111)</span></span><br><span class="line"><span class="comment">#     ax.set_title(title)</span></span><br><span class="line"><span class="comment">#     if ylim is not None:</span></span><br><span class="line"><span class="comment">#         ax.ylim(*ylim)</span></span><br><span class="line"><span class="comment">#     ax.set_xlabel(u"train_num_of_samples")</span></span><br><span class="line"><span class="comment">#     ax.set_ylabel(u"score")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,</span></span><br><span class="line"><span class="comment">#                      alpha=0.1, color="b")</span></span><br><span class="line"><span class="comment">#     ax.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,</span></span><br><span class="line"><span class="comment">#                      alpha=0.1, color="r")</span></span><br><span class="line"><span class="comment">#     ax.plot(train_sizes, train_scores_mean, 'o-', color="b", label=u"train score")</span></span><br><span class="line"><span class="comment">#     ax.plot(train_sizes, test_scores_mean, 'o-', color="r", label=u"testCV score")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax.legend(loc="best")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2</span></span><br><span class="line"><span class="comment">#     diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])</span></span><br><span class="line"><span class="comment">#     return midpoint, diff</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot_learning_curve(grd, u"learning_rate", train_x, train_y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gender_submission = pd.DataFrame(&#123;'PassengerId':test.iloc[:,0],'Survived':grd.predict(test_x)&#125;)</span></span><br><span class="line"><span class="comment"># gender_submission.to_csv('C:/Users/evilpsycho/Desktop/gender_submission.csv', index=None)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Project </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kaggle解题步骤</title>
      <link href="/2017/12/04/Project/Kaggle%E8%A7%A3%E9%A2%98%E6%AD%A5%E9%AA%A4/"/>
      <url>/2017/12/04/Project/Kaggle%E8%A7%A3%E9%A2%98%E6%AD%A5%E9%AA%A4/</url>
      <content type="html"><![CDATA[<p>最近为了提高ML项目能力，准备刷一下Kaggle的竞赛题，而面对纷繁复杂的题目，最开始的我总是不知道如何下手。在阅读了别人的kernel并完成了几个基础tutorial后，大概对整个流程有了浅显的理解，特此总结了整个Kaggle竞赛的解题步骤。</p><a id="more"></a><h1 id="解题流程"><a href="#解题流程" class="headerlink" title="解题流程"></a>解题流程</h1><h2 id="竞赛之前"><a href="#竞赛之前" class="headerlink" title="竞赛之前"></a>竞赛之前</h2><h4 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h4><p>在选择参加的赛题时候，需要先对赛题进行简单的分析，将其转化为机器学习问题，Kaggle中常见问题类型有：</p><ol><li>回归问题</li><li>分类问题（多分类，多标签）</li></ol><h4 id="数据理解"><a href="#数据理解" class="headerlink" title="数据理解"></a>数据理解</h4><p>同时也需要大概看一眼数据，确定对于自己来说，该问题是否可解（对于显卡渣来说，图像类竞赛还是早早放弃吧= =），而且有些金融类问题，数据集存在着大量的缺失，预测标签也存在较大偏差值，这类问题需要慎重，可能需要专业的金融知识进行特征工程。</p><h4 id="开始竞赛"><a href="#开始竞赛" class="headerlink" title="开始竞赛"></a>开始竞赛</h4><p>在确定对题目有较大信心，数据集合适后，那就开始Kaggle竞赛吧。</p><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h4 id="数据概览"><a href="#数据概览" class="headerlink" title="数据概览"></a>数据概览</h4><p>好了，现在开心的打开你的jupyter notebook，开始Kaggle之旅吧。在载入数据后，我们先要做的是对数据整体有个大概的认识：训练集和测试集都多大？有多少个属性？每个属性是什么类型的？属性是否有缺失值？数值型属性的分布情况（min，max，mean，meduim，std等统计值）？等等，有了这些信息，会让你在脑海里对数据有个大体的认识，这是几个在这个环节常用的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data.shape</span><br><span class="line">data.head()</span><br><span class="line">data.info()</span><br><span class="line">data.describe()</span><br><span class="line">data.isnull().sum()[data.isnull().sum()&gt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><h4 id="具体分析和可视化"><a href="#具体分析和可视化" class="headerlink" title="具体分析和可视化"></a>具体分析和可视化</h4><p>在我们对数据整体有个大体认识的情况下，针对每个特征，进行进一步的探索分析。在这个过程中，使用常见可视化工具，可以较为直观的展现数据。常见的可视化工具有：matplotlib和seaborn，其中，seaborn是对matplotlib的进一步封装，让工程师可以更加便捷的进行可视化，常见的可视化准则有：</p><ul><li>sns.jointplot(): 绘制属性随标签的变化趋势</li><li>sns.heatmap(data.corr()): 绘制热力图，可以对属性的相关性进行简单探索</li><li>sns.distplot(): 绘制分布图，可以展示连续自变量和标签的关系</li><li>sns.boxplot(): 绘制箱型图，适合用于探索取值较少的属性和标签的关系</li></ul><p>在对各个属性进行细致可视化观察过程中，需要记录下这个属性和标签的相关性，进而分析该属性是否是一个有价值的属性，并<br>在观察数据的过程中，需要思考一下几个问题：</p><ol><li>数据应该怎样清洗和处理才是合理的？离散化？连续化？</li><li>根据数据类型可以挖掘出什么特征？</li><li>数据中的哪些特征对预测有帮助？</li><li>（进阶）是否可以构造出新的特征？</li></ol><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>特征是决定效果最关键的一环，利用人为先验知识，从数据中总结出特征。特征工程是整个项目中最重要的一环之一，好的特征工程决定了预测结果的上限，机器学习模型只是帮助你不断逼近这个上限而已。</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>首先，对于刚拿到的数据，会出现噪声，离群点，缺失等问题，我们需要对数据进行清理和加工，对于不同类型的变量，有不同的处理方法：</p><ol><li>对于数值型变量，需要处理离散群点，缺失值，异常值等情况。</li><li>对于类别型变量，可以转化为one-hot编码。</li><li>文本数据较为复杂，文本中会有垃圾字符，错别字，数学公式，不统一度量衡日期格式等，包括标点符号，分词。对于英文文本可能还需要词性还原。</li></ol><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><ul><li>我们应该尽可能多的抽取特征，只要是认为某个特征对解决问题有帮助，它就可以成为一个特征。</li><li>特征抽取需要不断迭代，最为耗费时间。</li><li>常见特征抽取方法：<ol><li>对于数值型特征，可以通过线性组合，多项式组合来发现新的特征。</li><li>对于文本数据，有一些常规的特征：文本长度，embeddings，TF-IDF，LDA等</li><li>如果对数据有更深刻的理解，可以试着构造magic feature。</li><li>通过错误分析也可以发现新的特征。</li></ol></li></ul><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><ul><li>过多的特征会造成冗余，噪声，过拟合等问题。因此需要特征筛选，可以加快模型训练速度，提升效果。</li><li>特征选择方法多种多样，最简单的时相关度系数，用以衡量两个变量之间的线性关系。</li><li>可以通过分析构建相关系数矩阵。特征和标签之间的相关度可以看做该特征的重要度，特征与特征之间的相关度高，则说明这两个特征存在冗余。</li><li>也可以通过训练模型来筛选特征。</li></ul><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><h4 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h4><p>机器学习模型很多，可以都做尝试，不仅可以测试效果，还可以学习各种模型技巧。常见模型：</p><pre><code>1. KNN2. SVM3. Linear Model4. Extra Tree5. RandomForest6. Gradient Boost Tree7. Neural Network</code></pre><h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><p>在训练模型前，我们已经预设了一些模型参数（比如树的深度）和优化过程（比如学习率）。这种参数被称为超参。</p><p>调参虽然被称为一门玄学，但还是有些章法可寻：</p><pre><code>1. 根据经验，选出对模型效果影响最大的超参2. 按照经验设置超参的搜索空间，比如学习率：[0.0001, 0.1]3. 选择搜索算法4. 验证模型的泛化能力</code></pre><p>同时，也可以用可视化工具将不同参数模型在测试集的效果可视化，进而选择最优的参数。</p><h4 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h4><ul><li>简单分割</li><li>交叉验证<ol><li>将整个训练数据随机分成k份，训练k个moxing，取k-1份train，1份valid。</li><li>也叫k-fold</li><li>k一般选值在3到10之间</li></ol></li></ul><h4 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h4><ul><li>每个模型都会犯一些错误，为了了解模型在犯什么错误，我们可以观察被误判的样本，总结他们的共同特征，就可以再训练一个效果更好的模型。</li><li>错误分析-&gt;发现新特征-&gt;训练新模型-&gt;错误分析。 可以不断迭代出更好的效果。</li></ul><h4 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h4><p>我们可以使用模型集成方法，将多个ML模型集成到一起，最后我们可以综合考虑所有ML模型的预测结果，进而可以给出一个更加准确的预测。</p><ul><li>常见方法有：bagging, boosting, stacking, blending.</li><li>bagging<ol><li>bagging是将多个模型的预测结果简单地加权平均或者投票，bagging的好处在于可以并行的训练基学习器。</li><li>bagging通常没有一个明确的优化目标。</li></ol></li><li>Boosting<ol><li>Boosting思想接近于知错能改，每训练一个基学习器，是为了弥补上一个基学习器所犯的错误。</li><li>著名算法有：AdaBoost，GradientBoost</li></ol></li><li>Stacking<ol><li>是用新的模型（次学习器）去学习怎么组合基学习器。</li><li>Stacking的思想是多个基学习器的加权非线性组合</li></ol></li><li>Blending</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Kaggle项目是一个不断往复的过程，正如吴恩达在《Machine Learning》课上所说，对于机器学习项目，我们首先做出一个非常简单粗暴的模型，这个模型的准确率往往是欠佳的，而我们接下来要做的就是不断优化我们的模型，不断修改特征工程，提炼出更好的模型，更好的特征，进而不断提高预测准确率。</p>]]></content>
      
      <categories>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Project </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow常用函数整理</title>
      <link href="/2017/11/28/Deep%20Learning/TensorFlow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86/"/>
      <url>/2017/11/28/Deep%20Learning/TensorFlow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>TensorFlow常用方法整理</p><a id="more"></a><h2 id="tensor形状相关操作"><a href="#tensor形状相关操作" class="headerlink" title="tensor形状相关操作"></a>tensor形状相关操作</h2><h3 id="tf-shape-input"><a href="#tf-shape-input" class="headerlink" title="tf.shape(input)"></a>tf.shape(input)</h3><p>返回输入tensor的形状</p><h3 id="tf-size-input"><a href="#tf-size-input" class="headerlink" title="tf.size(input)"></a>tf.size(input)</h3><p>返回输入tensor中所有元素的个数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]</span><br><span class="line">tf.size(t) ==&gt; 12</span><br></pre></td></tr></table></figure></p><h3 id="tf-rank-input"><a href="#tf-rank-input" class="headerlink" title="tf.rank(input)"></a>tf.rank(input)</h3><p>返回输入tensor的维度数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]</span><br><span class="line"># shape of tensor &apos;t&apos; is [2, 2, 3]</span><br><span class="line">tf.rank(t) ==&gt; 3</span><br></pre></td></tr></table></figure></p><h3 id="tf-reshape-input-shape"><a href="#tf-reshape-input-shape" class="headerlink" title="tf.reshape(input, shape)"></a>tf.reshape(input, shape)</h3><p>改变输入tensor的形状为shape，shape是一个list，表示每个维度的大小。可以让某个维度的值为-1，tf会自动计算该维度大小。</p><h3 id="tf-expand-dims-input-axis-None"><a href="#tf-expand-dims-input-axis-None" class="headerlink" title="tf.expand_dims(input, axis=None)"></a>tf.expand_dims(input, axis=None)</h3><p>在第axis位置上增加一个维度。<br>如果要将批量维度添加到单个元素，则此操作非常有用。 例如，如果有一个单一形状的图片：image=[height，width，channels]，可以使用expand_dims(image, 0)使其成为一批图像（目前仅有一个），这将使形状 images=[1，height，width，channels]。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line"># &apos;t&apos; is a tensor of shape [2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 0)) ===&gt; [1,2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 1)) ===&gt; [2,1,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 2)) ===&gt; [2,3,1,5]</span><br></pre></td></tr></table></figure></p><h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze()"></a>tf.squeeze()</h3><p>从tensor中删除所有大小是1的维度<br>给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸为1的维度，可以通过指定squeeze_dims来删除特定尺寸为1的维度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is a tensor of shape [1, 3, 1, 4]</span><br><span class="line">tf.shape(tf.squeeze(t)) ===&gt; [3, 4]</span><br></pre></td></tr></table></figure></p><h3 id="tf-split-value-name-or-size-split-axis-0"><a href="#tf-split-value-name-or-size-split-axis-0" class="headerlink" title="tf.split(value, name_or_size_split, axis=0)"></a>tf.split(value, name_or_size_split, axis=0)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度不变</p><ul><li>value：输入的tensor</li><li>num_or_size_split：如果是整数你， 就将tensor分割成n个子tensor。如果是个tensor T，就将输入的tensor分割为len(T)个子tensor</li><li>axis：按照指定维度分割，指向的维度的大小一定能被num_or_size_split整除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.split(A, num_or_size_split=3, axis=1)</span><br><span class="line">a1 = [ [[1],[4]],   [[2],[5]],   [[3],[6]] ]</span><br><span class="line">a2 = tf.split(A, num_or_size_split=2, axis=0)</span><br><span class="line">a2 = [ [[1,2,3]],  [[4,5,6]] ]</span><br></pre></td></tr></table></figure></li></ul><h3 id="tf-unstack-value-num-axis"><a href="#tf-unstack-value-num-axis" class="headerlink" title="tf.unstack(value, num, axis)"></a>tf.unstack(value, num, axis)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度-1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.unstack(A, num=3, axis=1)</span><br><span class="line">a1 = [ [1, 4],   [2, 5],   [3, 6] ]</span><br><span class="line">a2 = tf.unstack(A, num=2, axis=0)</span><br><span class="line">a2 = [ [1,2,3],  [4,5,6] ]</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA学习笔记(一)</title>
      <link href="/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
      <url>/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。</p><a id="more"></a><h2 id="第一个CUDA程序"><a href="#第一个CUDA程序" class="headerlink" title="第一个CUDA程序"></a>第一个CUDA程序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">__global__ void kernel(void) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</span><br><span class="line">    printf(&quot;Hello, Word\n&quot;);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与一般c的hello word不同的是，该代码有两个不同的地方：</p><ol><li>一个空函数kernel(),并且带有修饰符<strong>global</strong></li><li>对这个空函数的调用，并带有修饰字符&lt;&lt;&lt;1, 1&gt;&gt;&gt;</li></ol><p>__global__修饰符告诉编译器，该函数应该编译为在设备而不是主机上运行。</p><p>&lt;&lt;&lt;1, 1&gt;&gt;&gt;表示调用设备代码，尖括号表示将一些参数传递给运行时系统，这些参数并不是传递给设备代码的参数，而是告诉运行时如何启动设备代码。</p><h2 id="CUDA加法"><a href="#CUDA加法" class="headerlink" title="CUDA加法"></a>CUDA加法</h2><p>下面我们来看一个更复杂的例子，用CUDA实现GPU加法计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__global__ void add(int a, int b, int *c)&#123;</span><br><span class="line">    *c = a + b;</span><br><span class="line">&#125;</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    int c;</span><br><span class="line">    int *dev_c;</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((void**)&amp;dev_c, sizeof(int)));</span><br><span class="line">    add&lt;&lt;&lt;1, 1&gt;&gt;&gt;(2, 7, dev_c);</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost));</span><br><span class="line">    printf(&quot;2 + 7 = %d\n&quot;, c);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这段代码有两个核心概念：</p><ol><li>可以像调用c函数一样将参数传递给核函数</li><li>当设备执行任何有用的操作时，都需要分配内存，例如将计算值返回给主机。</li></ol><h3 id="CUDA常用函数"><a href="#CUDA常用函数" class="headerlink" title="CUDA常用函数"></a>CUDA常用函数</h3><h4 id="cudaMalloc-函数"><a href="#cudaMalloc-函数" class="headerlink" title="cudaMalloc()函数"></a>cudaMalloc()函数</h4><p>设备函数调用的传参过程和普通c函数类似，需要注意的是通过’cudaMalloc()’来分配内存，<br>‘cudaMalloc()’函数告诉CUDA运行时在设备上分配内存，</p><pre><code>1. 第一个参数是一个指针，指向用于保存新分配内存地址的变量，2. 第二个参数是分配内存的大小。3. 该函数的返回类型和malloc()相同，都是&apos;void*&apos;</code></pre><p>CUDA大大淡化了主机代码和设备代码之间的差异，但程序员一定不能在主机代码中对’cudaMalloc()’返回的指针进行读取或者写入内存。总的来说，主机指针只能访问主机代码中的内存，而设备指针只能访问设备代码中的内存。</p><h4 id="cudaMemcpy-函数"><a href="#cudaMemcpy-函数" class="headerlink" title="cudaMemcpy()函数"></a>cudaMemcpy()函数</h4><p>‘cudaMemcpy()’表示对设备指针的内容和主机指针的内容复制</p><pre><code>1. 第一个参数是主机指针2. 第二个参数是设备指针3. 第三个参数是内存大小4. 第四个参数&apos;cudaMemcpyDeviceToHost&apos;表示从设备复制到主机，同样的&apos;cudaMemcpyHostToDevice&apos;表示主机复制到设备。&apos;cudaMemcpyDeviceToDevice&apos;表示两个指针都位于设备上。</code></pre><h2 id="查询设备"><a href="#查询设备" class="headerlink" title="查询设备"></a>查询设备</h2><p>在编写设备代码并在GPU上进行计算时，如果程序能知道设备中拥有多少内存，都有哪些设备等信息无疑是有用的。</p><ol><li>获得支持CUDA的GPU数量：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int count;</span><br><span class="line">cudaGetDeviceCount(&amp;count);</span><br></pre></td></tr></table></figure></li></ol><p>在获取CUDA设备数量后，可以使用’cudaGetDeviceProperties()’查询每个设备的相关信息，CUDA运行时将返回一个’cudaDeviceProp’类型的结构，包含了设备相关属性。结构内的详细信息请Google。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">intmain(void) &#123;</span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    int count;</span><br><span class="line">    cudaGetDeviceCount(count);</span><br><span class="line">    for(int i = 0;i&lt;count; i++)&#123;</span><br><span class="line">        cudaGetDeviceProperties(&amp;prop, i);</span><br><span class="line">        printf(&quot;Name: %s\n&quot;, prop.name);</span><br><span class="line">        printf(&quot;Total global memory: %ld\n&quot;, prop.totalGlobalMem);</span><br><span class="line">        printf(&quot;Total constant memory: %ld\n&quot;, prop.totalConstMem);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在编写代码中，偶尔会需要指定CUDA设备，比如说主机和设备之间通信频繁的话，最好选用集成GPU。我们可以通过’cudaChooseDevice()’选择设备。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">int main(void) &#123;</span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    int dev;</span><br><span class="line">    cudaGetDevice(&amp;dev);</span><br><span class="line">    printf(&quot;ID of current CUDA device: %d\n&quot;, dev);</span><br><span class="line">    memset(&amp;prop, 0, sizeof(cudaDeviceProp));</span><br><span class="line">    prop.major = 1;</span><br><span class="line">    prop.minor = 3;</span><br><span class="line">    cudaChooseDevice(&amp;dev, &amp;prop);</span><br><span class="line">    printf(&quot;ID of CUDA device choset to revision 1.3: %d\n&quot;, dev);</span><br><span class="line">    cudaSetDevice(dev);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="向量加法"><a href="#向量加法" class="headerlink" title="向量加法"></a>向量加法</h2><p>下面我们尝试使用并行的思想，用CUDA实现向量加法：输入两个vector，将其对应位相加的结果保存在第三个vector中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#define N 10</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    int a[N], b[N], c[N];</span><br><span class="line">    int *dev_a, *dev_b, *dev_c;</span><br><span class="line"></span><br><span class="line">    // 在GPU上分配内存</span><br><span class="line">    cudaMelloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">    cudaMelloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">    cudaMelloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i&lt;N; i++)&#123;//在CPU上初始化三个数组</span><br><span class="line">        a[i] = -i;</span><br><span class="line">        b[i] = i * i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //将a，b两个数组复制到GPU内存上</span><br><span class="line">    cudaMemcpy(dev_a, a, N*sizeof(int), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    add&lt;&lt;&lt;N, 1&gt;&gt;&gt;(dev_a, dev_b, dev_c);</span><br><span class="line"></span><br><span class="line">    //将GPU上的计算结果返回到主机中</span><br><span class="line">    cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost)</span><br><span class="line"></span><br><span class="line">    for(int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        printf(&quot;%d + %d = %d\n&quot;, a[i], b[i], c[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void add(int *a, int *b, int *c)&#123;</span><br><span class="line">    int tid = blockIdx.x;</span><br><span class="line">    if (tid &lt; N)</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里我们要说明的是，在调用设备函数add&lt;&lt;&lt;N, M&gt;&gt;&gt;时，两个参数的意义：</p><pre><code>1. 第一个参数表示运行时创建N个设备函数的副本并以并行的方式运行他们，我们将每个并行执行环境都称为一个线程块(block)。如果指定kernel&lt;&lt;&lt;256,1&gt;&gt;&gt;,那么将有256个线程块在GPU上运行。2. 第二个参数表示在每个线程块中创建的线程数量M。3. 在GPU中，一共有N*M个并行线程运行</code></pre><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><h4 id="线程索引"><a href="#线程索引" class="headerlink" title="线程索引"></a>线程索引</h4><p>但在运行核函数的时候，我们往往需要知道当前的核函数是哪个block，所以在核函数中，我们看到’int tid = blockIdx.x’，blockIdx是一个内置变量，在CUDA运行时已经预先定义了这个变量。而后面的’.x’是因为CUDA支持二维线性块数组，所以有了’.x’, ‘.y’。<br>在启动核函数时，我们将并行线程块的数量指定为N，这个并行线程块集合也称为一个线程格(Grid)，这个告诉运行时，我们想要一个一维的线程格，其中包括N个线程块。每个线程块的blockIdx.x值都是不同的，第一个线程块的blockIdx.x为0，最后一个为N-1。</p><h4 id="线程索引优化"><a href="#线程索引优化" class="headerlink" title="线程索引优化"></a>线程索引优化</h4><p>在上一个矢量加法中，我们调用核函数’add&lt;&lt;&lt;N, 1&gt;&gt;&gt;’，表示启动N个核函数，每个核函数中有1个线程。在本章我们稍作修改，改为启动一个线程块，该线程块包含N个线程：’add&lt;&lt;&lt;1, N&gt;&gt;&gt;’，对应的，在核函数中，取线程块索引改为取线程索引：’int tid = threadIdx.x + blockIdx.x * blockDim.x’。新出现的变量blockDim保存的是线程块中每一维的线程数量，由于使用的是一维线程块，所以只用到blockDim.x。</p><p>两种方法都可以在GPU上启动多个线程，对比这两种方法，GPU可以一次启动65535个线程块，而一般而言，每个线程块内的最多启动512个线程，这是第一个不同。</p><p>而如果我们需要实现任意长度vector加法，以上的数量限制显然是不能实现的，这里我们只需要修改一下核函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__global__ add(int *a, int *b, int *c) &#123;</span><br><span class="line">    int tid = threadIdx.x + blockIdx.x blockDim.x;</span><br><span class="line">    while (tid &lt; N) &#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += gridDim.x * blockDim.x; // 每次位移一个线程grid内的所有线程数量</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>通过在循环内’tid += gridDim.x * blockDim.x’，每次可以让tid移动一个线程grid内线程数量的位移，然后再判断tid是否小于N。通过这种方法实现了计算任意长度的向量加法。</p><h2 id="核函数GPU参数"><a href="#核函数GPU参数" class="headerlink" title="核函数GPU参数"></a>核函数GPU参数</h2><p>核函数只能在主机端调用，调用时必须申明执行参数。调用形式如下：</p><pre><code>Kernel&lt;&lt;&lt;    &gt;&gt;&gt;(param list);</code></pre><p>&lt;&lt;&lt;&gt;&gt;&gt;运算符内是核函数的执行参数，告诉编译器运行时如何启动核函数，用于说明内核函数中的线程数量，以及线程是如何组织的。</p><p>&lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt;运算符对kernel函数完整的执行配置参数形式是</p><ul><li>参数Dg用于定义整个grid的维度和尺寸，即一个grid有多少个block。为dim3类型。Dim3 Dg(Dg.x, Dg.y, 1)表示grid中每行有Dg.x个block，每列有Dg.y个block，第三维恒为1(目前一个核函数只有一个grid)。整个grid中共有Dg.x*Dg.y个block，其中Dg.x和Dg.y最大值为65535。</li><li>参数Db用于定义一个block的维度和尺寸，即一个block有多少个thread。为dim3类型。Dim3 Db(Db.x, Db.y, Db.z)表示整个block中每行有Db.x个thread，每列有Db.y个thread，高度为Db.z。Db.x和Db.y最大值为512，Db.z最大值为62。 一个block中共有Db.x<em>Db.y</em>Db.z个thread。计算能力为1.0,1.1的硬件该乘积的最大值为768，计算能力为1.2,1.3的硬件支持的最大值为1024。</li><li>参数Ns是一个可选参数，用于设置每个block除了静态分配的shared Memory以外，最多能动态分配的shared memory大小，单位为byte。不需要动态分配时该值为0或省略不写。</li><li>参数S是一个cudaStream_t类型的可选参数，初始值为零，表示该核函数处在哪个流之中。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>到此，我们已经学习了如何声明GPU函数，开辟GPU内存以及GPU和CPU内存之间的复制。通过两个例子也大概理解了CUDA的运行过程和工作机制，最后，我们通过优化tid的更新方式实现了任意长度向量的加法。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（五）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍一些补充内容。<br><a id="more"></a></p><h1 id="图形互操作性"><a href="#图形互操作性" class="headerlink" title="图形互操作性"></a>图形互操作性</h1><p>如今的GPU已经不仅仅局限于图形处理，更是广泛的应用于通用处理。本章介绍GPU的图形计算和通用计算之间的互操作。</p><h1 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h1><p>本章内容：</p><ol><li>了解NIVIDIA GPU的计算功能集</li><li>了解原子操作以及为什么需要使用它们</li><li>了解如何在CUDA C核函数中执行带有原子操作的运算</li></ol><p>NVIDIA将对于支持CUDA的不同GPU的各种功能统称为计算功能集，高版本计算功能集是低版本计算功能集的超集。</p><p>在NVIDIA的计算功能集中，只有版本高于1.1的功能集才支持全局内存原子操作，高于1.2的功能集才支持共享内存原子操作。所以在编译时我们需要告诉编译器该代码只有在指定版本的更高版本才可以编译优化，指令如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -arch=sm_11</span><br></pre></td></tr></table></figure></p><h2 id="GPU的工作调度机制"><a href="#GPU的工作调度机制" class="headerlink" title="GPU的工作调度机制"></a>GPU的工作调度机制</h2><h1 id="多个GPU系统上的CUDA"><a href="#多个GPU系统上的CUDA" class="headerlink" title="多个GPU系统上的CUDA"></a>多个GPU系统上的CUDA</h1><p>见书164页</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>## CUBLAS<br>著名的线性代数库</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（四）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍CUDA流。<br><a id="more"></a></p><h1 id="流操作"><a href="#流操作" class="headerlink" title="流操作"></a>流操作</h1><h2 id="页锁定主机内存"><a href="#页锁定主机内存" class="headerlink" title="页锁定主机内存"></a>页锁定主机内存</h2><p>CUDA提供自己独特的方式来分配主机内存’cudaHostAlloc()’。和标准的c内存分配’malloc()’不同的是，后者将分配标准的，可分页的主机内存，而前者将分配页锁定的主机内存。页锁定内存也称为固定内存或者不可分页内存。它有一个重要属性：操作系统将不会对这块内存分页并交换到磁盘上，从而确保该内存始终驻留在物理内存中。由于GPU知道内存的物理地址，因此可以通过“直接内存访问DMA”技术来在GPU和主机之间复制数据，而使用锁页内存时，DMA不需要重新查找内存地址，所以速度会比正常的可分页内存快2倍左右。</p><p>当然使用页锁定内存的缺点也是显而易见的：主机的虚拟内存机制完全作废，每个页锁定内存都需要分配物理内存，这意味着系统将更快的耗尽内存。因此应用程序在物理内存较小的机器上会运行失败，也意味着应用程序将影响系统上其他运行的应用程序性能。</p><h2 id="CUDA流"><a href="#CUDA流" class="headerlink" title="CUDA流"></a>CUDA流</h2><p>CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。</p><p>一个简单的cuda流使用代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#define N (1024*1024)</span><br><span class="line">#define FULL_DATA_SIZE (N*20)</span><br><span class="line"></span><br><span class="line">__global__ void kernel(int *a, int *b, int *c)&#123;</span><br><span class="line">    int tid = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    if (tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    cudaStreamCreate(&amp;stream); //定义并初始化CUDA流</span><br><span class="line"></span><br><span class="line">    int *host_a, *host_b, *host_c;</span><br><span class="line">    int *dev_a, *dev_b, *dev_c;</span><br><span class="line">    //分配设备内存</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line">    //分配由流使用的页锁定内存</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_a, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_b, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_c, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">    for(int i = 0; i&lt;FULL_DATA_SIZE; i++)&#123;</span><br><span class="line">        host_a[i] = i;</span><br><span class="line">        host_b[i] = 2 * i + 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //在整个数据上循环，每个数据块的大小为N</span><br><span class="line">    for(int i = 0;i&lt;FULL_DATA_SIZE; i+=N)&#123;</span><br><span class="line">        //将锁定内存以异步方式复制到设备上</span><br><span class="line">        cudaMemcpyAsync(dev_a, host_a+i, N*sizeof(int), cudaMemcpyHostToDevice, stream);</span><br><span class="line">        cudaMemcpyAsync(dev_b, host_b+i, N*sizeof(int), cudaMemcpyHostToDevice, stream);</span><br><span class="line">        kernel&lt;&lt;&lt;N/256, 256, 0, stream&gt;&gt;&gt;(dev_a, dev_b, dev_c);//声明核函数异步执行stram中的操作</span><br><span class="line">        //将数据从设备复制回锁定内存</span><br><span class="line">        cudaMemcpyAsync(host_c+i, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost, stream);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在使用流操作时，不需要将输入缓冲区整体都复制到GPU，而是将输入缓冲区划分为更小的块，并在每个块上执行一个包含三个步骤的过程：<strong>1.我们将部分输入缓冲区复制到GPU；2.在这部分缓冲区上运行核函数；3.然后将输出缓冲区中的这部分结果复制回主机</strong>。这么操作的应用场景通常是：GPU的内存远小于主机内存，无法一次将整个缓冲区都填充到GPU中。</p><h4 id="cudaHostAlloc"><a href="#cudaHostAlloc" class="headerlink" title="cudaHostAlloc()"></a>cudaHostAlloc()</h4><p>CUDA运行在主机上分配内存，这个内存是不可分页内存（malloc分配的是可分页内存）</p><h4 id="cudaMemcpyAsync"><a href="#cudaMemcpyAsync" class="headerlink" title="cudaMemcpyAsync()"></a>cudaMemcpyAsync()</h4><p>没有使用常见的’cudaMemcpy()’，而是使用一个新函数’cudaMemcpyAsync()’。前者是以同步方式执行，意味着 <strong>当函数返回时，复制操作就已经完成，并且在输出缓冲区中包含了复制进去的内容</strong>。异步函数相反，<strong>调用时只是放置一个请求，表示流中执行一次内存复制操作，这个流是通过参数stream来指定的，当函数返回时，我们无法确保复制操作是否已经启动，只能保证的是复制操作肯定会当下一个被放入流中的操作之前执行。流就像一个有序的GPU工作队列，GPU每次从该队列中取出工作并执行</strong></p><h2 id="使用多个CUDA流"><a href="#使用多个CUDA流" class="headerlink" title="使用多个CUDA流"></a>使用多个CUDA流</h2><p>在上个例子中，由于仅仅使用了一个流，并不会带来多大的性能提升，CUDA流只有在存在多个流操作时才会显示它的强大威力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;cuda_runtime.h&quot;    </span><br><span class="line">#include &lt;iostream&gt;  </span><br><span class="line">#include &lt;stdio.h&gt;    </span><br><span class="line">#include &lt;math.h&gt;    </span><br><span class="line"></span><br><span class="line">#define N (1024*1024)    </span><br><span class="line">#define FULL_DATA_SIZE N*20    </span><br><span class="line"></span><br><span class="line">__global__ void kernel(int* a, int *b, int*c)</span><br><span class="line">&#123;</span><br><span class="line">int threadID = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">if (threadID &lt; N)</span><br><span class="line">&#123;</span><br><span class="line">c[threadID] = (a[threadID] + b[threadID]) / 2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    //创建两个CUDA流  </span><br><span class="line">cudaStream_t stream[2];</span><br><span class="line">cudaStreamCreate(&amp;stream[0]);</span><br><span class="line">cudaStreamCreate(&amp;stream[1]);</span><br><span class="line"></span><br><span class="line">int *host_a, *host_b, *host_c;</span><br><span class="line">int *dev_a, *dev_b, *dev_c;</span><br><span class="line">int *dev_a1, *dev_b1, *dev_c1;</span><br><span class="line"></span><br><span class="line">//在GPU上分配内存  </span><br><span class="line">cudaMalloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">cudaMalloc((void**)&amp;dev_a1, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_b1, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_c1, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">//在CPU上分配页锁定内存  </span><br><span class="line">cudaHostAlloc((void**)&amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line">cudaHostAlloc((void**)&amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line">cudaHostAlloc((void**)&amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">//主机上的内存赋值  </span><br><span class="line">for (int i = 0; i &lt; FULL_DATA_SIZE; i++)</span><br><span class="line">&#123;</span><br><span class="line">host_a[i] = i;</span><br><span class="line">host_b[i] = 2 * i + 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; FULL_DATA_SIZE; i += 2 * N)</span><br><span class="line">&#123;</span><br><span class="line">cudaMemcpyAsync(dev_a, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream[0]);</span><br><span class="line">cudaMemcpyAsync(dev_b, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream[0]);</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream[1]);</span><br><span class="line">cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream[1]);</span><br><span class="line"></span><br><span class="line">kernel &lt;&lt;&lt;N / 1024, 1024, 0, stream[0] &gt;&gt;&gt; (dev_a, dev_b, dev_c);</span><br><span class="line">kernel &lt;&lt;&lt;N / 1024, 1024, 0, stream[1] &gt;&gt;&gt; (dev_a, dev_b, dev_c1);</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(host_c + i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost, stream[0]);</span><br><span class="line">cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream[1]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 等待Stream流执行完成</span><br><span class="line">cudaStreamSynchronize(stream[0]);</span><br><span class="line">cudaStreamSynchronize(stream[1]);</span><br><span class="line"></span><br><span class="line">// free stream and mem    </span><br><span class="line">cudaFreeHost(host_a);</span><br><span class="line">cudaFreeHost(host_b);</span><br><span class="line">cudaFreeHost(host_c);</span><br><span class="line"></span><br><span class="line">cudaFree(dev_a);</span><br><span class="line">cudaFree(dev_b);</span><br><span class="line">cudaFree(dev_c);</span><br><span class="line"></span><br><span class="line">cudaFree(dev_a1);</span><br><span class="line">cudaFree(dev_b1);</span><br><span class="line">cudaFree(dev_c1);</span><br><span class="line"></span><br><span class="line">cudaStreamDestroy(stream);</span><br><span class="line">cudaStreamDestroy(stream1);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用两个流的执行时间一本上是一个流消耗时间的二分之一。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（二）</title>
      <link href="/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，这是第二篇，主要讲的是线程块内共享内存你的概念，然后通过向量点乘的例子理解共享内存的使用方法。</p><a id="more"></a><h1 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h1><h2 id="共享内存和同步"><a href="#共享内存和同步" class="headerlink" title="共享内存和同步"></a>共享内存和同步</h2><p>在CUDA C用 <strong>shared</strong> 关键字添加变量声明，这将使这个变量驻留在共享内存中。主要作用是存放一个 <strong>线程块</strong> 中的线程会频繁访问的数据。对比全局内存来说，共享内存大小要小几个数量级，但访问速度会比全局内存快。<br>共享内存：对于GPU上启动的每个线程块，CUDA编译器都会创建该变量的一个副本，线程块中每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本，这使得线程块中的多个线程能够在计算上进行通信和协作，同时，共享内存缓冲区驻留在GPU上，所以访问共享内存的延迟要远远低于访问普通缓冲区的延迟。但与此同时，需要一种机制来实现线程之间的同步。</p><h4 id="向量内积"><a href="#向量内积" class="headerlink" title="向量内积"></a>向量内积</h4><p>通过共享内存来实现向量内积：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">const int N = 12 * 256;</span><br><span class="line">const int threadsPerBlock = 256;</span><br><span class="line">const int blocksPerGrid = (N+threadsPerBlock-1) / threadsPerBlock;</span><br><span class="line"></span><br><span class="line">__global__ void dot(float *a, float *b, float *c)&#123;</span><br><span class="line">    __shared__ float cache[threadsPerBlock]; //GPU共享内存</span><br><span class="line">    int tid = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    int cacheIndex = threadIdx.x;</span><br><span class="line">    float temp = 0;</span><br><span class="line">    while (tid &lt; N) &#123;</span><br><span class="line">        temp += a[tid] * b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">    cache[cacheIndex] = temp;</span><br><span class="line"></span><br><span class="line">    // 对所有线程块中的线程进行同步，</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    //归约运算</span><br><span class="line">    int i = blockDim.x/2;</span><br><span class="line">    while (i != 0)&#123;</span><br><span class="line">        if (cacheIndex &lt; i)</span><br><span class="line">            cache[cacheIndex] += cache[cacheIndex + i];</span><br><span class="line">        __syncthreads();</span><br><span class="line">        i /= 2;</span><br><span class="line">    &#125;</span><br><span class="line">    if (cacheIndex == 0)&#123;//为了防止带来不必要的冗余计算，只让cacheIndex==0线程进行最后的保存操作</span><br><span class="line">        c[blockIdx.x] = cache[0];</span><br><span class="line">        printf(&quot;%f\n&quot;, cache[0]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(void) &#123;</span><br><span class="line">    float a[N], b[N], c, partial_c[N];</span><br><span class="line">    float *dev_a, *dev_b, *dev_partial_c;</span><br><span class="line"></span><br><span class="line">    //在GPU上分配内存</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, N*sizeof(float));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_b, N*sizeof(float));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_partial_c, blocksPerGrid*sizeof(float));</span><br><span class="line"></span><br><span class="line">    //填充主机内存</span><br><span class="line">    for (int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        a[i] = (float)i;</span><br><span class="line">        b[i] = (float)i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //将主机内存复制到GPU上</span><br><span class="line">    cudaMemcpy(dev_a, a, N*sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(dev_b, b, N*sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line">    dot&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(dev_a, dev_b, dev_partial_c);</span><br><span class="line">    cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid*sizeof(float), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    //在CPU上完成最终求和运算</span><br><span class="line">    c = 0;</span><br><span class="line">    for (int i = 0; i&lt;blocksPerGrid; i++)&#123;</span><br><span class="line">        c += partial_c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\n%f\n&quot;, c);</span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_partial_c);</span><br><span class="line"></span><br><span class="line">    delete [] a;</span><br><span class="line">    delete [] b;</span><br><span class="line">    delete [] partial_c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>整体思路：首先所有线程计算自己对应的乘法，然后将结果保存到block的共享内存中。然后每个block中指定一个thread将<br>该block内所有thread保存在共享内存的乘积结果归约加和到一起，得到该block中所有thread乘积的和。最后再将每个block的共享内存的求和结果最后求和，得到结果。</p><h4 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h4><p>‘ __syncthreads(); ‘<br>对所有线程块中的线程进行同步，<br>该函数确保线程块中的每个线程都执行完syncthreads前面的语句后，才会执行下一条语句。</p><h4 id="归约运算"><a href="#归约运算" class="headerlink" title="归约运算"></a>归约运算</h4><p>基本思想：每个线程将cache[]中的两个值加起来，然后将结果保存回cache[]，因为是将两个值归约成一个值，所以每次执行下来，得到的结果数量为开始时的一半。在log2(threadPerBlock)个步骤后，结果就是cache[]的总和。</p><h4 id="syncthreads-注意事项"><a href="#syncthreads-注意事项" class="headerlink" title="syncthreads()注意事项"></a>syncthreads()注意事项</h4><p>在核函数中，经常出现基于threadIdx的判断语句，说明有些thread是不能执行一部分if条件内的代码的， <strong>当某些线程需要执行一条指令，而其他线程不需要执行时，这种情况就被称为线程发散（Thread Divergence），正常情况下，发散的分支只会使得某些线程处于空闲状态，而其他线程将执行分支中的代码</strong>。 但在syncthreads()的情况中，线程发散造成的后果很糟糕，CUDA将确保： <strong>除非线程块中的每个线程都执行了syncthreads()，否则没有任何线程能执行syncthreads()后面的代码。也就是说，如果syncthreads()位于发散分支中，那么一些线程将永远无法执行syncthreads()，会导致程序出错。</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过向量点乘的例子，理解了共享内存的概念和简单应用方法</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Sklearn之TF-IDF</title>
      <link href="/2017/11/12/Machine%20Learning/TF-IDF%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/"/>
      <url>/2017/11/12/Machine%20Learning/TF-IDF%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>TF-IDF是一种统计方法， 广泛应用于信息检索领域。用于评估一个词对一个文件集或者一个文件的重要程度。<br><a id="more"></a></p><h2 id="TF-IDF概念"><a href="#TF-IDF概念" class="headerlink" title="TF-IDF概念"></a>TF-IDF概念</h2><p><strong>一个词在文件中的重要性随着它在文件中出现的次数成正比，但同时随着它在整个文件集中的出现频率成反比</strong>。 也就是说，一个词在文件中出现的越多，并且在整个其它文件中出现的次数越少，越能代表该文件。</p><h3 id="词频（TF）"><a href="#词频（TF）" class="headerlink" title="词频（TF）"></a>词频（TF）</h3><p>指的是一个词语在该文件中的出现次数，这个数字通常会被归一化（该词词频除以文章总词数），以防其偏向于长的文件。</p><h3 id="逆向文件频率（IDF）"><a href="#逆向文件频率（IDF）" class="headerlink" title="逆向文件频率（IDF）"></a>逆向文件频率（IDF）</h3><p>IDF指的是：对于一个词语，总文件数 除以 包含该词语的所有文件数，再将得到的商取对数，即为IDF。IDF的思想是：如果包含词语t的文档越少，IDF越大，说明该词条有很强的文件区分能力。<br>IDF = log(文件总数 / 包含词条的文件个数+1)<br>分母+1是为了防止除零</p><h3 id="TF-IDF计算公式"><a href="#TF-IDF计算公式" class="headerlink" title="TF-IDF计算公式"></a>TF-IDF计算公式</h3><p>TF-IDF = TF<em>IDF<br>表示一个词语在该文件中的出现次数 </em> IDF</p><h2 id="TF-IDF在Sklearn中"><a href="#TF-IDF在Sklearn中" class="headerlink" title="TF-IDF在Sklearn中"></a>TF-IDF在Sklearn中</h2><p>包含在’sklearn.feature_extraction.txt’中，共包含两个类：’CountVectorizer’和’TfidfTransformer’.</p><h3 id="CountVectorizer"><a href="#CountVectorizer" class="headerlink" title="CountVectorizer"></a>CountVectorizer</h3><p>可以将一个文件集合转换成 <strong>词频矩阵</strong>。例如矩阵中包含一个元素a[i][j]，它表示j词在i类文本下的词频。它通过’fit_transform()’函数计算各个词语出现的次数，通过’get_feature_names()’可获取词袋中所有文本的关键字，通过’toarray()’可看到词频矩阵的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">#语料</span><br><span class="line">corpus = [</span><br><span class="line">    &apos;This is the first document.&apos;,</span><br><span class="line">    &apos;This is the second second document.&apos;,</span><br><span class="line">    &apos;And the third one.&apos;,</span><br><span class="line">    &apos;Is this the first document?&apos;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line">#获取词袋中所有文本关键词</span><br><span class="line">word = vectorizer.get_feature_names()</span><br><span class="line">print(word)</span><br><span class="line">#查看词频结果</span><br><span class="line">print(X.toarray())</span><br></pre></td></tr></table></figure></p><p>输出结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[u&apos;and&apos;, u&apos;document&apos;, u&apos;first&apos;, u&apos;is&apos;, u&apos;one&apos;, u&apos;second&apos;, u&apos;the&apos;, u&apos;third&apos;, u&apos;this&apos;]</span><br><span class="line">[[0 1 1 1 0 0 1 0 1]</span><br><span class="line"> [0 1 0 1 0 2 1 0 1]</span><br><span class="line"> [1 0 0 0 1 0 1 1 0]</span><br><span class="line"> [0 1 1 1 0 0 1 0 1]]</span><br></pre></td></tr></table></figure></p><h3 id="TfidfTransformaer"><a href="#TfidfTransformaer" class="headerlink" title="TfidfTransformaer"></a>TfidfTransformaer</h3><p>用于统计每个词语的TF-IDF值，tfidf[i][j]表示i类文本中第j个词的tf-idf权重。仍然使用上面这个例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">#将词频矩阵X统计成TF-IDF值</span><br><span class="line">tfidf = transformer.fit_transform(X)</span><br><span class="line">#查看数据结构</span><br><span class="line">print(tfidf.toarray())</span><br></pre></td></tr></table></figure></p><h3 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h3><p>通常情况下，对于给定的corpus，可以同时使用CountVectorizer和TfidfTransformer()。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector = CountVectorizer()</span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">tf_idf = transformer.fit_transform(vector.fit_transform(corpus));</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（三）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍了常量内存和纹理内存。<br><a id="more"></a></p><h1 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h1><p>本章介绍通过GPU上特殊的内存区域来加速应用程序的执行：常量内存。以及如果通过事件来测量CUDA应用程序的性能。</p><h2 id="使用常量内存"><a href="#使用常量内存" class="headerlink" title="使用常量内存"></a>使用常量内存</h2><p>从名字可以看出，常量内存用于保存在核函数执行期间不会发生变化的数据，在某些情况中，用常量内存替换全局内存能有效的减少内存带宽。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#define N 40</span><br><span class="line">using namespace std;</span><br><span class="line">const int nMax = 50;</span><br><span class="line"></span><br><span class="line">__constant__ float const_num[N];</span><br><span class="line">__global__ void exchangeKernel(float *nums)&#123;</span><br><span class="line">    int offset = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    nums[offset] = const_num[offset];</span><br><span class="line">&#125;</span><br><span class="line">int main()&#123;</span><br><span class="line">    float *dev_a, temp[N], res[N];</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, 40*sizeof(float));</span><br><span class="line">    for(int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">        temp[i] = 1.5 * i;</span><br><span class="line">    &#125;</span><br><span class="line">    cudaMemcpyToSymbol(const_num, temp, N*sizeof(float));</span><br><span class="line">    exchangeKernel&lt;&lt;&lt;4, N/4&gt;&gt;&gt;(dev_a);</span><br><span class="line">    cudaMemcpy(res, dev_a, N*sizeof(float), cudaMemcpyDeviceToHost);</span><br><span class="line">    for(int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        cout &lt;&lt; res[i] &lt;&lt; &quot; &quot;&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在这个例子中，我们先声明了N个float大小的’<strong>constant</strong>‘空间。在主机上将主机内存中的数值通过’cudaMemcpyToSymbol’复制到device上的常量内存上。然后调用核函数每次获取常量内存上的一个值。最后将核函数的运行结果复制回主机内存。</p><h4 id="cudaMemcpyToSymbol"><a href="#cudaMemcpyToSymbol" class="headerlink" title="cudaMemcpyToSymbol()"></a>cudaMemcpyToSymbol()</h4><p>使用’cudaMemcpyToSymbol(dev_const, host_const, size)’从主机内存复制到GPU的常量内存中。</p><h4 id="常量内存提高带宽"><a href="#常量内存提高带宽" class="headerlink" title="常量内存提高带宽"></a>常量内存提高带宽</h4><p>使用常量内存可以节省内存带宽，主要原因有两个：</p><pre><code>1. 对于常量内存的单次读操作可以广播到其他的“邻近”线程，这将节约15次读取操作。2. 常量内存的数据将被缓存起来，因此对相同地址的连续操作将不会产生额外的内存通信量。</code></pre><p>为了回答“临近线程”的概念，我们需要先知道“线程束(Warp)”是什么。<strong>线程束可以看成是一组线程通过交织而形成的一个整体，在CUDA架构中，线程束是指一个包含32个线程的集合，这个线程集合被“编织在一起”并且以“步调一致”的形式执行。</strong> 在程序中的每一行，线程束中的每个线程都将在不同的数据上执行相同的指令。</p><p>当处理常量内存时， NVIDIA将把单词内存读取操作广播到每个半线程束（half-warp）包含了16个线程，如果半个线程束中的每个线程都从常量内存上读取数据，那么GPU只会产生一次读取请求然后将数据广播到每个线程。如果从常量内存中读取大量的数据，那么这种方式产生的内存流量只是使用全局内存的1/16。同时在实际应用中，不仅仅减少15/16，由于这块内存时不会发生变化的，所以硬件会主动把常量数据缓存在GPU缓存中，在第一次从常量内存读取后，当其他半线程束请求同一个地址时，将直接命中缓存，更加减少了内存流量。</p><p>需要注意的是： 半线程束广播实际上是一把双刃剑，</p><pre><code>- 当16个线程都读取相同地址时，性能确实可以大大提高，但当所有16个线程分别读取不同地址时，它的效率实际上会降低- 只有当16个线程是相同读取请求时，才值得将这个读取操作广播到16个线程。然而，如果16个线程需要访问的常量内存中的不同数据，那么这16次不同的读取操作会被 **串行化**， 从而需要16倍的时间来发出请求，但如果从全局内存中读取，这些请求会同时发出，这种情况下，从常量内存读取就会慢于从全局内存读取。</code></pre><h1 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h1><p>和常量内存一样，纹理内存是另一种 <strong>只读内存</strong>，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。<br>纹理内存是专门为那些在内存访问模式中存在大量空间局部性的图形应用程序设计的，在某个计算程序中，这意味着 <strong>一个线程读取的位置可能与邻近线程读取的位置“非常接近”</strong>。</p><h2 id="使用纹理内存"><a href="#使用纹理内存" class="headerlink" title="使用纹理内存"></a>使用纹理内存</h2><p>定义纹理变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;float&gt; textureA;</span><br><span class="line">texture&lt;float&gt; textureB;</span><br><span class="line"></span><br><span class="line">cudaMalloc((void**)&amp;deviceA, sizeof(float));</span><br><span class="line">cudaBindTexture(NULL, textureA, deviceA, sizeof(float));</span><br><span class="line"></span><br><span class="line">cudaUnbindTexture(textureA); //释放纹理绑定</span><br><span class="line">cudaUnbindTexture(textureB);</span><br></pre></td></tr></table></figure></p><h4 id="cudaBindTexture"><a href="#cudaBindTexture" class="headerlink" title="cudaBindTexture()"></a>cudaBindTexture()</h4><p>定义之后，我们需要为纹理变量分配内存，使用’cudaBindTexture()’将这些变量绑定到内存缓冲区，相当于告诉CUDA两件事：</p><ol><li>我们希望将指定的缓冲区作为纹理来使用。</li><li>我们希望将纹理的引用作为纹理的“名字”。</li></ol><h2 id="使用二维纹理内存"><a href="#使用二维纹理内存" class="headerlink" title="使用二维纹理内存"></a>使用二维纹理内存</h2><p>声明二维纹理内存方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;float, 2&gt; textureA;</span><br><span class="line">texture&lt;float, 2&gt; textureB;</span><br></pre></td></tr></table></figure></p><p>纹理内存的使用是常见的GPU优化手段之一，多用于图形计算领域。但由于纹理内存的特殊性，在使用纹理内存时要慎重，使用不当往往会适得其反。</p><h1 id="用事件来测试性能"><a href="#用事件来测试性能" class="headerlink" title="用事件来测试性能"></a>用事件来测试性能</h1><p>CUDA事件本质上是一个GPU时间戳，使用起来很容易，只需要两步：首先创建一个事件，然后记录一个事件。当然，最后也需要另一个事件记录结束时间。<br>下面是使用事件来计算GPU计算时间的常用流程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, end;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventCreate(&amp;end);</span><br><span class="line">cudaEventRecord(start, 0);</span><br><span class="line"></span><br><span class="line">// do something in GPU</span><br><span class="line"></span><br><span class="line">cudaEventRecord(end, 0);</span><br><span class="line">cudaEventSynchronize(stop); //同步事件，告诉运行时阻塞后面的语句，直到GPU执行到达stop事件。</span><br><span class="line"></span><br><span class="line">float elapsedTime;</span><br><span class="line">cudaEventElapsedTime(&amp;elapsedTime, start, end);//计算用时</span><br><span class="line">cudaEventDestroy(start);</span><br><span class="line">cudaEventDestroy(end);</span><br></pre></td></tr></table></figure></p><p>但在CUDA程序中，实际上当GPU开始执行代码，在GPU执行完之前，CPU会继续执行程序的下一行代码。这样做可以提高性能，让GPU和CPU并行，但从逻辑上来说，计时工作就会变得复杂。CUDA有’cudaEventSynchronize()’函数来告诉CPU在某个事件上同步。</p><p>‘cudaEventElapsedTime(float*, cudaEvent_t, cudaEvent_t)’用于计算时间时间，返回单位为毫秒。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节主要介绍了CUDA的两种 <strong>只读内存：常量内存和纹理内存</strong>，使用GPU只读内存可以对CUDA程序的运行速度进一步优化，但使用时需要精心设计避免适得其反。同时也介绍了CUDA事件，事件的本质是事件戳，用来记录GPU运行事件，计算性能。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>生成模型和判别模型</title>
      <link href="/2017/10/12/Machine%20Learning/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2017/10/12/Machine%20Learning/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>监督学习的任务就是从数据中学习一个模型，应用这一模型，对给定的输入X预测相应的输出Y。这个模型的一般形式为决策函数Y=f(X)或者条件概率分布P(Y|X)。而生成模型和判别模型的区别就是如何求得P(Y|X).<br><a id="more"></a></p><p>决策函数Y=f(X)：你输入一个X，它就输出一个Y，这个Y与一个阈值比较，根据比较结果判定X属于哪个类别。例如两类（w1和w2）分类问题，如果Y大于阈值，X就属于类w1，如果小于阈值就属于类w2。这样就得到了该X对应的类别了。</p><p>条件概率分布P(Y|X)：你输入一个X，它通过比较它属于所有类的概率，然后输出概率最大的那个作为该X对应的类别。例如：如果P(w1|X)大于P(w2|X)，那么我们就认为X是属于w1类的。</p><h2 id="生成方法和判别方法"><a href="#生成方法和判别方法" class="headerlink" title="生成方法和判别方法"></a>生成方法和判别方法</h2><p>监督学习方法分生成方法（Generative approach）和判别方法（Discriminative approach），所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。</p><h4 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h4><p>由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>基本思想是有限样本条件下建立判别函数，不关心样本是以何种概率分布产生的，直接研究预测模型。典型的判别模型包括： <strong>k近邻，感知机，决策树，支持向量机等。</strong></p><h6 id="判别方法的特点"><a href="#判别方法的特点" class="headerlink" title="判别方法的特点"></a>判别方法的特点</h6><ul><li>判别方法寻找不同类别之间的最优分类面，反映的是异类数据之间的差异;</li><li>判别方法利用了训练数据的类别标识信息，直接学习的是条件概率P(Y|X)或者决策函数f(X)，直接面对预测，往往学习的准确率更高；</li><li>由于直接学习条件概率P(Y|X)或者决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li><li>缺点是不能反映训练数据本身的特性</li></ul><h4 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h4><p>由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。<br>这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。典型的生成模型有：<strong>朴素贝叶斯和隐马尔科夫模型等。</strong> 这种方法一般建立在统计学和Bayes理论的基础之上。</p><h6 id="生成方法的特点"><a href="#生成方法的特点" class="headerlink" title="生成方法的特点"></a>生成方法的特点</h6><ul><li>从统计的角度表示数据的分布情况，能够反映同类数据本身的分布情况;</li><li>生成方法还原出联合概率分布P(X, y)，而判别方法不能；</li><li>生成方法的学习收敛速度更快、即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型（由于已经学习到了概率分布）；</li><li>当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用</li></ul><h2 id="判别模型和生成模型对比"><a href="#判别模型和生成模型对比" class="headerlink" title="判别模型和生成模型对比"></a>判别模型和生成模型对比</h2><p>（1）训练时，二者优化准则不同<br>生成模型优化训练数据的联合分布概率；<br>判别模型优化训练数据的条件分布概率，判别模型与序列标记问题有较好的对应性。<br>（2）对于观察序列的处理不同<br>生成模型中，观察序列作为模型的一部分；<br>判别模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。<br>（3）训练复杂度不同<br>判别模型训练复杂度较高。<br>（4）本质区别<br>判别模型估计的是条件概率分布P(Y|X), 生成模型估计的是联合概率分布P(X, Y)<br>（5）由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>例如我们有一个输入数据x，然后我们想将它分类为标签y。（迎面走过来一个人，你告诉我这个是男的还是女的）</p><p>生成模型学习联合概率分布p(x,y)，而判别模型学习条件概率分布p(y|x)。</p><p>下面是个简单的例子：</p><p>例如我们有以下(x,y)形式的数据：(1,0), (1,0), (2,0), (2, 1)</p><p>那么p(x,y)是：</p><pre><code>      y=0   y=1    -----------x=1 | 1/2   0x=2 | 1/4   1/4</code></pre><p>而p(y|x) 是：</p><pre><code>    y=0   y=1    -----------x=1| 1     0x=2| 1/2   1/2</code></pre>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java字符串</title>
      <link href="/2017/09/12/Programming%20Language/Java%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2017/09/12/Programming%20Language/Java%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      <content type="html"><![CDATA[<p>Java String类和StringBuffer类的常见函数<br><a id="more"></a></p><h3 id="Java-String类"><a href="#Java-String类" class="headerlink" title="Java String类"></a>Java String类</h3><h4 id="创建字符串"><a href="#创建字符串" class="headerlink" title="创建字符串"></a>创建字符串</h4><p>Java String 共有11种构造函数，包括传入一个 char 数组进行构造。</p><p>注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了（详看笔记部分解析）。</p><p>如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。</p><h4 id="split"><a href="#split" class="headerlink" title="split()"></a>split()</h4><p>split()函数是根据参数如”,”, “-“, “ “等, 分割String字符串, 返回一个String的数组。</p><p>如果未找到, 则返回整个String字符串, 作为String数组(String[])的第0个元素.</p><h4 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h4><ul><li>string1.concat(string2);</li><li>该函数将返回两个字符串连接在一起的新串</li><li>直接使用 ‘+’ 进行字符串连接</li><li>char和string可以直接相加</li></ul><h4 id="string-contains"><a href="#string-contains" class="headerlink" title="string.contains()"></a>string.contains()</h4><ul><li>string.contains(substring) 返回布尔，字符串是否包含该子字符串</li></ul><h4 id="字符串比较字典序-string1-compareTo-string2"><a href="#字符串比较字典序-string1-compareTo-string2" class="headerlink" title="字符串比较字典序 string1.compareTo(string2)"></a>字符串比较字典序 string1.compareTo(string2)</h4><p>比较两个字符串的字典序，如果string1&lt;string2则返回-1， 如果相等则返回0， 如果string1&gt;string2则返回1.</p><h4 id="数字字符串转int"><a href="#数字字符串转int" class="headerlink" title="数字字符串转int"></a>数字字符串转int</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String s = &quot;123&quot;;</span><br><span class="line">int a = Integer.valueOf(s);</span><br><span class="line">// 或者</span><br><span class="line">int b = Integer.parseInt(s);</span><br></pre></td></tr></table></figure><h4 id="int转字符串："><a href="#int转字符串：" class="headerlink" title="int转字符串："></a>int转字符串：</h4><ul><li>String s = String.valueOf(intv);</li><li>String s = Integer.toString(intv);</li><li>String s = “” + intv;</li></ul><h4 id="字符串反转"><a href="#字符串反转" class="headerlink" title="字符串反转"></a>字符串反转</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String reverse = new StringBuffer(s).reverse().toString();</span><br></pre></td></tr></table></figure><h4 id="charAt"><a href="#charAt" class="headerlink" title="charAt()"></a>charAt()</h4><p>string.charAt(index) 返回index处的char值</p><h4 id="string-equal"><a href="#string-equal" class="headerlink" title="string.equal()"></a>string.equal()</h4><p>string1.equal(string2) 判断两个字符串是否相等</p><h4 id="string-getChars"><a href="#string-getChars" class="headerlink" title="string.getChars()"></a>string.getChars()</h4><p>string.getChars(int srcBegin, int srcEnd, char[] dst,  int dstBegin)</p><ul><li>复制字符串的一个子串到一个char数组中</li><li>参数分别为：起始index，结束index，输出到的char数组，和目标数组中的起始偏移量(置零）。</li></ul><h4 id="string-index"><a href="#string-index" class="headerlink" title="string.index()"></a>string.index()</h4><ul><li>string.index(char c)</li><li>string.index(String substring)</li><li>返回参数（char或者字符串）出现的index</li></ul><h4 id="string-replace"><a href="#string-replace" class="headerlink" title="string.replace()"></a>string.replace()</h4><ul><li>public String replace(char oldChar,<pre><code>char newChar)</code></pre></li><li>在字符串中使用new char 替换 old char</li><li>删除字符串中指定的字符，用空白替换方法：<br>string = string.replace(“delete”, “”)</li></ul><h4 id="string-trim"><a href="#string-trim" class="headerlink" title="string.trim()"></a>string.trim()</h4><ul><li>public String trim()</li><li>用于删除字符串的头尾空白符。</li></ul><h4 id="大小写转换"><a href="#大小写转换" class="headerlink" title="大小写转换"></a>大小写转换</h4><ul><li>string.toUpperCase()</li><li>string.toLowerCase()</li></ul><h4 id="string-toCharArray"><a href="#string-toCharArray" class="headerlink" title="string.toCharArray()"></a>string.toCharArray()</h4><ul><li>将字符串转换为字符数组</li></ul><h4 id="string-substring"><a href="#string-substring" class="headerlink" title="string.substring()"></a>string.substring()</h4><ul><li>返回字符串的子字符串。</li><li>public String substring(int beginIndex, int endIndex)</li><li>public String substring(int beginIndex)</li></ul><h4 id="string-indexOf-substring-m"><a href="#string-indexOf-substring-m" class="headerlink" title="string.indexOf(substring, m)"></a>string.indexOf(substring, m)</h4><p>返回substring 在string中的起始index，如果string不包含则返回1。第二个参数m表示string从第m个元素开始判断，缺失时默认从0开始判断。</p><h4 id="string-lastIndexOf-substring"><a href="#string-lastIndexOf-substring" class="headerlink" title="string.lastIndexOf(substring)"></a>string.lastIndexOf(substring)</h4><p>从后往前判断substring在string中的位置。</p><h3 id="Java-StringBuffer类"><a href="#Java-StringBuffer类" class="headerlink" title="Java StringBuffer类"></a>Java StringBuffer类</h3><p>StringBuffer是线程安全的，所以在多线程程序中也可以很方便的进行使用，但是程序的执行效率相对来说就要稍微慢一些。</p><h4 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StringBuffer sb = new StringBuffer(&quot;abc&quot;)</span><br></pre></td></tr></table></figure><h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><h4 id="sb-append-“string1”"><a href="#sb-append-“string1”" class="headerlink" title="sb.append(“string1”)"></a>sb.append(“string1”)</h4><p>该方法的作用是追加内容到当前StringBuffer对象的末尾，类似于字符串的连接。</p><h4 id="sb-deleteCharAt-index"><a href="#sb-deleteCharAt-index" class="headerlink" title="sb.deleteCharAt(index)"></a>sb.deleteCharAt(index)</h4><p>该方法的作用是删除指定位置的字符，然后将剩余的内容形成新的字符串。</p><h4 id="sb-delete-start-end"><a href="#sb-delete-start-end" class="headerlink" title="sb.delete(start, end)"></a>sb.delete(start, end)</h4><p>该方法的作用是删除指定区间以内的所有字符，包含start，不包含end索引值的区间。</p><h4 id="sb-insert-index-string1"><a href="#sb-insert-index-string1" class="headerlink" title="sb.insert(index, string1)"></a>sb.insert(index, string1)</h4><p>该方法的作用是在StringBuffer对象中插入内容，然后形成新的字符串。</p><h4 id="sb-reverse"><a href="#sb-reverse" class="headerlink" title="sb.reverse()"></a>sb.reverse()</h4><p>该方法的作用是将StringBuffer对象中的内容反转，然后形成新的字符串。</p><h4 id="sb-setCharAt-index-char1"><a href="#sb-setCharAt-index-char1" class="headerlink" title="sb.setCharAt(index, char1)"></a>sb.setCharAt(index, char1)</h4><p>该方法的作用是修改对象中索引值为index位置的字符为新的字符ch。</p>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
