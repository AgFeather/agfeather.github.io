<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>《程序员的自我修养》</title>
      <link href="/2019/06/26/Reading/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/06/26/Reading/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="第一章：计算机基础概念"><a href="#第一章：计算机基础概念" class="headerlink" title="第一章：计算机基础概念"></a>第一章：计算机基础概念</h1><h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><ol><li>北桥：为了协调CPU，内存和GPU的高速，大量数据交换的通道。</li><li>南桥：专门用来处理低速IO设备，如磁盘，USB，键盘，鼠标等。由南桥将其汇总后再连接到北桥上</li><li>多道程序：早年用来提高CPU利用率。当一个程序等待IO导致CPU空闲时监控程序会把另外的正在等待CPU资源的程序启动。</li><li>分时系统：每个程序占用CPU一段时间后，主动让出CPU给其他程序，使得一段时间内每个程序都有机会运行一小段时间。这时候监控程序已经比多到程序复杂很多。</li><li>多任务系统：操作系统接管所有硬件资源，并且本身运行在一个受硬件保护的级别。所有的程序都以进程的方式运行在比操作系统权限更低的级别，每个进程都有自己独立的地址空间。CPU由操作系统统一进行分配，每个进程根据优先级的高低都有机会得到CPU，但是，如果运行时间超过了一定的范围，操作系统会停止该进程，将CPU分配给其他等待进程，这种分配方式即为所谓的抢占式，操作系统可以强制剥夺CPU资源并分配给它认为目前最需要运行的进程。</li><li>硬件驱动：针对一个特定的硬件所开发的，可以完成一系列繁琐硬件细节操作并对外提供调用接口的程序。驱动程序可以看做是操作系统的一部分，往往跟操作系统内核一起运行在特权级。通常又硬件厂商完成。操作系统为硬件厂商提供一系列的接口和框架。</li><li>扇区：硬盘基本存储单位，一般为512字节。一个硬盘有多个盘片，每个盘片分两面，每个面按照同心圆划分若干个磁道，每个磁道划分为若干个扇区。</li><li>IO硬件端口：在x86平台，共有65536个硬件端口寄存器。CPU提供两条专门指令in和out来实现对端口的读写。<h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2></li><li>虚拟地址：在程序地址和物理地址之间增加一个中间层，这样秩序要处理两个地址之间的映射关系，就可以达到地址空间隔离的效果。</li><li>分段：处理虚拟地址和物理地址映射的一种方法。思路是把一段与程序所需要的内存空间大小的虚拟空间映射到某个地址空间。这样做的好处是，程序员不需要关注具体的物理地址，而是连续的，从0开始的虚拟地址。虚拟地址到物理地址的映射交给OS搞定。</li><li>分页：分段方法没有办法解决由内存不足导致的整个程序不断调入调出内存的问题。分页的思想是把物理/虚拟地址空间和磁盘人为的分成固定大小的页，目前几乎所有OS都使用4KB大小的页。分别叫做物理页，虚拟页，磁盘页。分页的好处是，当有限的内存资源不足时，只需要将内存中没目前没被用到的页写回到磁盘中，将磁盘中即将被用到的页读入到内存中。这样不再需要整个程序从内存到磁盘的读写。同时，虚拟内存的页数也可以大于物理内存页数，只需要物理内存不断调页即可（此时程序不可以同时使用所有虚拟内存页）。<h2 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h2></li><li>线程：一个标准线程由线程ID，当前指令指针（pc），寄存器集合和堆栈组成。一个进程由多个线程组成，各个线程之间共享程序的内存空间（代码段，数据段，堆等）以及一些进程级资源（打开文件，信号）。</li><li>线程调度：处理器切换不同线程进行执行。在线程调度中，线程通常拥有至少三个状态：运行（正在执行），就绪（可以立刻执行，但CPU被占用），等待（线程正在等待某一事件IO等，无法执行）</li><li>时间片：处于运行中的线程拥有一段可以执行的时间，称为时间片。当时间片用尽时，该进程进入就绪状态。如果在时间片用尽之前进程就开始等待某一事件，它将进入等待状态。</li><li>调度算法：主流线程调度算法都带有优先级调度和轮转法。前者可以决定每个线程的优先级，改变线程执行顺序，后者表示每个线程在一段时间内都可以被执行。通常情况下，IO密集型线程比CPU密集型线程由更高的优先级。</li><li>线程饿死：一个线程被饿死，说明它的优先级较低，在它执行前，总有高优先级的线程插队。为避免饿死，调度系统通常会逐步提升那些等待很长时间线程的优先级。</li><li>信号量：对于允许多个线程并发访问的资源，定义信号量。一个初始值为N的信号量允许N个线程同时并发访问。</li><li>volatile关键字：用来阻止编译器为了提高速度将一个变量缓存到寄存器内而不写回。阻止编译器调整操作volatile变量的指令顺序</li><li>用户线程到内核线程的映射：一对一模型，多对一模型，多对多模型</li></ol><h1 id="第二章：静态链接"><a href="#第二章：静态链接" class="headerlink" title="第二章：静态链接"></a>第二章：静态链接</h1><h2 id="编译和链接"><a href="#编译和链接" class="headerlink" title="编译和链接"></a>编译和链接</h2><p>代码执行过程可以分解为四个步骤：预处理，编译，汇编，链接</p><h4 id="预编译"><a href="#预编译" class="headerlink" title="预编译"></a>预编译</h4><p>源码中的头文件，如stdio.h等被预编译器预编译成一个.i文件。gcc中可以用如下命令执行预编译：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -E hello.c -o hello.i</span><br></pre></td></tr></table></figure></p><p>预编译过程主要处理源码中以#开头的预编译指令。处理规则如下：</p><ol><li>将所有#define删除并展开所有宏定义</li><li>处理所有条件预编译指令，如#if，#ifdef，#elif，#else</li><li>处理#include指令，将被包含的文件插入到该预编译指令的位置。这个过程是递归进行的，也就是说被包含的文件可能还包含其他文件。</li><li>删除所有注释</li><li>添加行号和文件名标识，以便编译器可以产生调试用的行号信息以及错误位置信息。</li><li>保存所有#pragma编译器指令，因为编译器要用到他们。<br>经过预编译后的.i文件不包含任何宏定义，并且被包含的文件都已经被插入。所以当我们无法判断宏定义是否正确或者头文件包含是否正确时，可以查看预编译后的文件来锁定问题。<h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4>编译过程是将.i文件进行一系列的词法分析，语法分析，语义分析以及优化后生产相应的汇编代码文件。gcc编译命令如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -S hello.i -o hello.s</span><br></pre></td></tr></table></figure></li></ol><h4 id="汇编"><a href="#汇编" class="headerlink" title="汇编"></a>汇编</h4><p>汇编器是将编译器产生的汇编代码转变成机器指令。每一个汇编语句几乎都对应一条机器指令，所以汇编过程相对于编译过程要简单，没有复杂语法语义，也不需要指令优化。只需要根据汇编指令和机器指令的对照表一一翻译即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -c hello.s -o hello.o</span><br></pre></td></tr></table></figure></p><h4 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h4><p>经过汇编器产生的.o文件最后需要进行链接才可以生成最后的a.out可执行文件。</p><h2 id="编译器"><a href="#编译器" class="headerlink" title="编译器"></a>编译器</h2><p>编译过程一般可以分为六步：词法分析，语法分析，语义分析，源代码优化，代码生成和目标代码优化。</p><h4 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h4><p>源码被输入到扫描器进行简单词法分析，用一种有限状态机将源码的字符序列分割成一系列的记号（token）。</p><h4 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h4><p>语法分析器将对token序列进行语法分析，从而生成一个AST。蒸锅分析过程采用上下文无关语法。</p><h4 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h4><p>语法分析仅仅是完成对表达式的语法成眠的分析，但它不懂这个语句是否真的有意义。如C语言中两个指针做乘法是没有意义的，但在语法上是合法的。在这个基础上，编译器能分析的语义是 <strong>静态语义</strong>，即编译期可以确定的语义。与之对应的 <strong>动态语义</strong> 是只有在运行期才能确定的语义。<br>静态语义通常包括声明和类型匹配，类型转换。这个期间可以找到所有类型不匹配的错误。动态语义如将0作为一个除数是非法的只能在运行期报错。<br>经过语义分析后，整个AST的表达式都会被标识上类型。</p><h4 id="中间代码生成"><a href="#中间代码生成" class="headerlink" title="中间代码生成"></a>中间代码生成</h4><p>现代编译器有着很多层次的优化，源代码优化器会在源代码级别进行优化。由于在AST上直接优化比较困难，所以优化器会将AST转换成中间代码，他是语法树的顺序表示，已经非常接近目标代码。<br>中间代码使得编译器可以被分为前端和后端，前端负责生产机器无关的中间代码。后端负责将中间代码转换为目标机器码。这样针对跨平台编译器而言，可以共用同一个前端而设计不同个后端。</p><h4 id="目标代码生成和优化"><a href="#目标代码生成和优化" class="headerlink" title="目标代码生成和优化"></a>目标代码生成和优化</h4><p>编译器后端主要包括代码生成器和目标代码优化器。代码生成器将中间代码转化成目标机器代码，这个过程依赖于目标机器。 然后目标代码优化器将目标代码进行优化，比如选择合适的寻址方式，用位移代替乘法运算，删除多余指令等。</p><h4 id="编译器的输出"><a href="#编译器的输出" class="headerlink" title="编译器的输出"></a>编译器的输出</h4><p>编译器生成的目标代码有一个问题是：变量的地址没有确定。如果有些变量的地址在其他模块就需要链接器最终将这些目标文件链接起来形成可执行文件。</p><h2 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h2><p>在一个程序被分割成多个模块（文件）后，这些模块如何最后组成一个单一程序是一个问题。常见的组合问题有两种，一个是模块之间的函数调用，而是模块间的变量访问。函数、变量访问须知道目标函数、变量的地址。模块之间的调用和访问过程就是链接。<br>链接器将多个有互相关联的模块拼接到一起，最后产生一个可执行程序。   </p><h4 id="静态链接"><a href="#静态链接" class="headerlink" title="静态链接"></a>静态链接</h4><p>一个复杂系统由多个模块组成，人们把每个源代码模块独立的编译，然后按照需要将他们组装起来，组装的过程就是链接。链接的主要内容就是把模块之间的互相引用处理好。从原理上讲，链接就是把一些指令对其他符号地址的引用加以修正。<br>链接过程主要包括地址和空间分配，符号决议和重定位。</p><h1 id="第三章：目标文件"><a href="#第三章：目标文件" class="headerlink" title="第三章：目标文件"></a>第三章：目标文件</h1><p>目标文件是编译器编译源码后产生的文件，从结构上讲，它是已经编译后的可执行文件，只是还没有经过链接过程。一些符号和地址可能没有被调整。</p><h2 id="目标文件的格式"><a href="#目标文件的格式" class="headerlink" title="目标文件的格式"></a>目标文件的格式</h2><p>从广义上看，目标文件和可执行文件的格式几乎一样。该格式在Windows下主要PE-COFF格式，在Linux下是ELF格式。<br>不光是可执行文件（.exe和ELF），动态链接库DLL（windows下的ddl和linux下的.so）以及静态链接库SLL（Windows下的.lib和linux下的.a）文件都是按照可执行文件格式进行存储。<br>ELF文件标准把系统中采用ELF格式存储的文件归为四类：</p><ol><li>可重定位文件：.o .obj</li><li>可执行文件：.exe</li><li>共享目标文件：.so .dll</li><li><p>核心转储文件：linux下的core dump</p><h2 id="目标文件内容"><a href="#目标文件内容" class="headerlink" title="目标文件内容"></a>目标文件内容</h2><p>目标文件中包含：编译后的机器指令代码，数据。链接需要的一些信息：符号表，调试信息，字符串等。一般目标文件将这些信息按照不同属性，以段或者节的形式存储。</p></li><li><p>程序源码编译后的机器指令经常被放在代码段里，<strong>代码段</strong> 常见的后缀有.core和.text。</p></li><li>全局变量和局部静态变量数据经常放在 <strong>数据段</strong>，数据段一般以.data结尾。</li><li>常量只读数据放在 <strong>只读段.rodata</strong> 中。</li><li>未初始化的全局变量和局部静态变量一般放在一个叫.bss的段里。</li></ol><h2 id="ELF格式细节"><a href="#ELF格式细节" class="headerlink" title="ELF格式细节"></a>ELF格式细节</h2><p>通过一个c语言例子详细讲解ELF格式文件的细节<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">int printf(const char* format, ...);</span><br><span class="line"></span><br><span class="line">int global_init_var = 84;</span><br><span class="line">int global_uninit_var;</span><br><span class="line"></span><br><span class="line">void func1(int i)&#123;</span><br><span class="line">    printf(&quot;%d\n&quot;, i);</span><br><span class="line">&#125;</span><br><span class="line">int main(void)&#123;</span><br><span class="line">    static int static_var = 85;</span><br><span class="line">    static int static_var2;</span><br><span class="line">    int a = 1;</span><br><span class="line">    int b;</span><br><span class="line">    func1(static_var + static_var2 + a + b);</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>通过指定-c，告诉gcc只编译不链接。之后可以得到一个1104字节的.o目标文件。</p><h3 id="objdump分析工具"><a href="#objdump分析工具" class="headerlink" title="objdump分析工具"></a>objdump分析工具</h3><p>使用objdump工具对以ELF格式的目标.o文件的内容进行分析。</p><h3 id="自定义段"><a href="#自定义段" class="headerlink" title="自定义段"></a>自定义段</h3><p>有时候为了满足特殊的需求，如Linux内核初始化或者某些硬件的内存等，需要制定某些部分代码能被放到所期望的段去。GCC提供一个扩展机制，直需要在代码中定义即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">__attribute__((section(&quot;FOO&quot;))) int global = 42;</span><br><span class="line">__attribute__((section(&quot;BAR&quot;))) void foo()&#123;&#125;</span><br></pre></td></tr></table></figure></p><p>在全局变量或函数之前加上 “<strong>attribute</strong>((section(“name”)))”属性就可以把相应的变量或函数放到以“name”作为段名的段中。</p><h3 id="ELF文件结构"><a href="#ELF文件结构" class="headerlink" title="ELF文件结构"></a>ELF文件结构</h3><p>ELF文件中，最常见的格式如下：</p><ol><li>header 文件头</li><li>.text 代码段</li><li>.data 数据段</li><li>.bss 未初始化的常量，全局变量段</li><li>.rel.text 重定位表，链接器在处理目标文件时，需要对每个目标文件中的某些部位进行重定位</li><li>字符串表 ELF文件中很多字符串如段名，变量名等。字符串表将所有字符串连在一起存放在一个表中，然后使用字符串在表中的偏移来引用字符串。<h4 id="ELF-Header"><a href="#ELF-Header" class="headerlink" title="ELF Header"></a>ELF Header</h4>文件头描述了整个文件的基本属性，版本，目标机器型号，程序入口地址等。<br>可以使用readelf命令来详细查看ELF文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">readelf -h SimpleSection.o</span><br></pre></td></tr></table></figure></li></ol><p>ELF文件头中定义了ELF魔数，文件机器字节长度，数据存储方式，版本，运行平台，硬件平台，硬件平台版本，入口地址，段表位置等。<br><strong>魔数</strong> 用来确认文件的类型，操作系统在加载可执行文件的时候回确认魔数是否正确， 如果不正确会拒绝加载。<br><strong>机器类型</strong> ELF文件被设计成可以在多个平台使用，但并不代表一个ELF文件可以在不同平台下运行。e_machine表示该ELF文件平台的属性。<br><strong>段表位置</strong> 保存ELF文件中各种各样段的基本属性以及结构，比如每个段的段名，段的长度，在文件中的偏移，读写权等。段表是ELF中最重要的结构之一。编译器，链接器和装载器都是依靠段表来定位和访问各个段的属性的。段表在ELF文件中的位置由文件头中的 “e_shoff”决定，表示段表的偏移位置。</p><h2 id="链接的接口–符号"><a href="#链接的接口–符号" class="headerlink" title="链接的接口–符号"></a>链接的接口–符号</h2><p>链接的本质就是把多个不同的目标文件之间相互粘到一起。目标文件之间的相互拼合实际上是目标文件之间对地址的引用，即对函数和变量的地址的引用。在连接过程中，将函数和变量统称为 <strong>符号</strong>， 函数名或变量名就是 <strong>符号名</strong>。链接过程中很关键的部分就是符号的管理，每个目标文件都会有一个相应的符号表，用来记录目标文件中所用到的所有符号。每个符号有一个对应的值，叫做 <strong>符号值</strong>，对于变量和函数来说，符号值就是他们的地址。</p><h3 id="符号表"><a href="#符号表" class="headerlink" title="符号表"></a>符号表</h3><p>在ELF文件中，符号表往往是其中的一个段，命名为.symtab。</p><p>符号表中所有可能的符号有：</p><ol><li>定义在本目标文件中的全局符号（变量或函数）。</li><li>在目标文件中引用的全局符号，却没有定义在本目标文件叫外部符号。如函数声明等。</li><li>段名，往往由编译器产生，值为该段的起始地址。如.text 或.data等。</li><li>局部符号，只在编译单元内部可见</li><li>行号信息<br>这里面最重要的就是第一个和第二个，因为链接器正是使用1和2进行文件粘合。</li></ol><p>可以使用readelf工具查看可执行文件的符号表.symtab：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">readelf -s file.o</span><br></pre></td></tr></table></figure></p><h3 id="符号冲突"><a href="#符号冲突" class="headerlink" title="符号冲突"></a>符号冲突</h3><p>对于C++而言，因为有函数重载机制，或者变量名相同但作用域不同等情况，会有两个函数名字相同，但参数列表不相同的情况。针对这种情况，使用符号修饰或符号改编的机制。</p><p><strong>函数签名</strong> 包含了一个函数的信息，包括函数名，参数类型，所在的类和名称空间等信息。函数签名用不来识别不同的函数。在编译器和链接器处理符号时，使用某种 <strong>名称修饰</strong> 的方法，是每个函数签名都对应一个修饰后的名称。也就是说会将函数和变量的名字进行修饰形成符号名。 编译器产生的目标文件中的所用的符号名其实是 <strong>相应函数和变量修饰之后的名称</strong>。</p><h3 id="弱符号和强符号"><a href="#弱符号和强符号" class="headerlink" title="弱符号和强符号"></a>弱符号和强符号</h3><p>在多个目标文件中包含有相同名字全局符号的定义，那么这两个目标文件在链接的时候回出现符号重复定义的错误。比如在两个文件A和B中都定义一个全局整型变量global并都进行了初始化，那么链接器在将A和B进行链接时会报错。</p><p>这种会报错的符号被定义为 <strong>强符号</strong>。 对于C++而言，编译器默认 <strong>函数和初始化了的全局变量</strong> 为强符号。未初始化的全局变量为弱符号。</p><p>针对强弱符号，编译器有如下处理规则：</p><ol><li>不允许强符号被多次定义（多个目标文件不允许出现同名强符号）</li><li>如果一个符号在某个目标文件中是强符号，其他文件中是弱符号，那么选择强符号。</li></ol><p>在CGG中，可以通过“<strong>attribute</strong>((weak))”来定义任何一个强符号为弱符号。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__attribute__((weak)) weak = 2;</span><br></pre></td></tr></table></figure></p><h3 id="强引用弱引用"><a href="#强引用弱引用" class="headerlink" title="强引用弱引用"></a>强引用弱引用</h3><p>如果在链接过程中，链接器没有找到该符号的定义，链接器就会报符号未定义的错误，这种被称为 <strong>强引用</strong> 。 <strong>弱引用</strong> 则是如果该符号未被定义，链接器对该引用不报错。对于一般未定义的弱引用，链接器某人其为0.</p><p>在GCC中，可以使用 “<strong>attribute</strong>((weakref))”来声明一个外部函数的引用为弱引用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__attribute__((weakref)) void foo();</span><br><span class="line">int main()&#123;</span><br><span class="line">    foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们可以将上述代码编译成一个可执行文件，GCC并不会报链接错误。但当我们运行这个可执行文件时，会发生运行错误，因为当main函数试图调用foo函数时，foo的函数地址为0.</p><p>弱符号和弱引用机制对于库来说十分有用。比如说库中定义的弱符号可以被用户定义的强符号所覆盖，从而使程序可以使用自定义的库函数。</p><h1 id="第四章：静态链接"><a href="#第四章：静态链接" class="headerlink" title="第四章：静态链接"></a>第四章：静态链接</h1><p>静态链接的核心目标是：有两个目标文件时，如何将它们链接起来形成一个可执行文件？<br>本章使用如下两个c代码a.c和b.c进行解释说明。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// a.c</span><br><span class="line">extern int shared;</span><br><span class="line">int main()&#123;</span><br><span class="line">    int a = 100;</span><br><span class="line">    swap(&amp;a, &amp;shared);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//b.c</span><br><span class="line">int shared = 1;</span><br><span class="line">void swap(int* a, int* b)&#123;</span><br><span class="line">    *a ^= *b ^= *a ^= *b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>首先使用GCC将两个文件编译成目标文件a.o和b.o</p><h2 id="空间和地址分配"><a href="#空间和地址分配" class="headerlink" title="空间和地址分配"></a>空间和地址分配</h2><p>对于链接器来说，在这个例子中输入的目标文件是a.o和b.o输出是一个可执行文件ab。从ELF文件格式可知，ab的代码段和数据段都是输入文件合并过来的。问题是如何将两者进行合并？</p><h3 id="按序叠加"><a href="#按序叠加" class="headerlink" title="按序叠加"></a>按序叠加</h3><p>最简单的方法就是将输入的目标文件按照次序叠加起来。这么做有一个问题就是对于大型项目有很多输入目标文件的情况下，输出可执行文件会有很多零散的段，这种做法非常浪费空间。</p><h3 id="相似段合并"><a href="#相似段合并" class="headerlink" title="相似段合并"></a>相似段合并</h3><p>将相同性质的段合并到一起，比如说将所有.text代码段合并，接着是.data段，.bas段等。</p><h2 id="两步链接"><a href="#两步链接" class="headerlink" title="两步链接"></a>两步链接</h2><p>整个链接过程分为两步。</p><ol><li>第一步：空间与地址分配。扫描所有输入目标文件，获取各个段的长度，属性和位置并将各个表合并。同时统计符号表信息，符号定义和符号引用统一成一个全局符号表。</li><li>第二步：符号解析与重定位。使用上一步收集到的信息，进行符号解析与重定位，调整代码中的地址。</li></ol><p>链接结束后程序中所有使用的地址已经是在进程中的虚拟地址VMA (Virtual Memory Address)和大小。在链接前所有目标文件中的VMA都为0因为虚拟空间还没有被分配。</p><h3 id="符号地址确定"><a href="#符号地址确定" class="headerlink" title="符号地址确定"></a>符号地址确定</h3><p>在第一步扫描的过程汇总，链接器会将虚拟地址分配给各个符号。由于是多个文件组合在一起，所以需要给各个符号添加一个偏移量使其被调整到正确的虚拟地址。一般情况下偏移量都是之前目标文件代码段的整段大小。（顺序链接）</p><h2 id="符号解析和重定位"><a href="#符号解析和重定位" class="headerlink" title="符号解析和重定位"></a>符号解析和重定位</h2><h3 id="重定位"><a href="#重定位" class="headerlink" title="重定位"></a>重定位</h3><p>分配完空间和地址后，链接器进入符号解析和重定位步骤。</p><p>当在一个文件A中调用另一个文件B中的成员或者函数foo()时，在A的目标文件中并不知道foo的具体地址，所以此时的目标文件中关于foo的调用时所指向的地址是0.而当链接器经过第一步扫描后，其实它已经知道了所有符号的位置， <strong>所以需要对所有符号调用进行重定向，使其指向正确的符号的地址</strong>。</p><p>当链接是如果没有导入需要的库（遗漏了某些需要被链接的库）会报错：链接时符号未定义。</p><h3 id="重定位表"><a href="#重定位表" class="headerlink" title="重定位表"></a>重定位表</h3><p>在ELF文件中，有一个叫重定位表的专门结构来保存于重定位相关的信息。一般情况下重定位表示ELF文件中的一个或多个段。对代码段进行重定位的表名称为：.rel.text。对数据段进行重定位的表名称为：.rel.data</p><h2 id="COMMON块"><a href="#COMMON块" class="headerlink" title="COMMON块"></a>COMMON块</h2><p>对于多符号定义类型冲突的情况，尤其是弱符号来说，使用COMMON机制处理该问题。</p><p>举例来说，编译器将未初始化的全局变量定义作为弱符号处理，如果在不同文件中定义了两个名称相同的全局未初始化变量，按照COMMON链接规则，该符号的大小以所有该弱符号中占据字节最大的为准。</p><h2 id="静态库链接"><a href="#静态库链接" class="headerlink" title="静态库链接"></a>静态库链接</h2><p>一般情况下，一个语言的开发环境往往附带有 <strong>语言库</strong>，这些库是对操作系统的API的封装。包括C中常见的printf函数等。一个静态库可以简单的看做是一组目标文件的组合。</p><p>例如C语言的标准库glibc本身是用C开发的，包含很多C语言源码文件，编译完成后有同样数量的目标文件。人们把这些目标文件压缩到一起并对其进行行编号和索引。进而就形成了libc.a静态库文件。</p><p>当我们写个最简单的C代码，打印hello world时，就会调用libc.a库中的printf.o目标文件。但当我们尝试只用目标文件helloworld.o和printf.o进行链接时，会报错，因为printf.o还依赖于libc.a中其它的目标文件。</p><h2 id="链接控制"><a href="#链接控制" class="headerlink" title="链接控制"></a>链接控制</h2><p>通常情况下直接使用链接器默认的配置就可以完成链接过程，但对于一些特殊情况，可能需要制定链接器的一些配置进行链接以便达到目的。通常情况下有一下三种方式控制链接器：</p><ol><li>命令行参数，如-o，-e等</li><li>链接指令存放在目标文件里，编译器通常使用这种方法向链接器传递指令。</li><li>使用链接控制脚本<br>通常情况下，可以自己写一个脚本，并指定该脚本作为链接控制脚本，对于ld链接器来说，使用-T参数指定脚本：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ld -T link.script</span><br></pre></td></tr></table></figure></li></ol><p>链接控制脚本程序使用一种特殊的链接脚本语言编写而成。一般使用.lds后缀作为扩展名。</p><h2 id="BFD库"><a href="#BFD库" class="headerlink" title="BFD库"></a>BFD库</h2><p>由于不同硬件平台都有自己独特的目标文件格式，对于像GCC这种跨平台编译器来说需要定义一个统一接口来处理不同的目标文件格式。 <strong>BFD库可以把目标文件抽象成一个统一的模型</strong>。 GCC都通过操作BFD库来处理目标文件，通过这种方式可以将编译器和目标文件隔离。一旦需要支持新的目标文件格式，只需要在BFD库添加一种新格式而不需要对编译器做出修改。</p><h1 id="第五章：Window-PE-COFF"><a href="#第五章：Window-PE-COFF" class="headerlink" title="第五章：Window PE/COFF"></a>第五章：Window PE/COFF</h1><p>在Windows平台下，使用一种叫PE的格式作为标准可执行文件。PE文件格式和ELF同根同源，都是COFF格式的发展。在windows下，习惯将目标文件默认为COFF格式，可执行文件为PE格式。统称为PE/COFF文件。</p><p>和ELF一样，PE也允许程序员将变量或者函数放在自定义段中，VC++使用“pragma”编译器指示。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#pragma data_seg(&quot;FOO&quot;)</span><br><span class="line">int global = 1;</span><br><span class="line">#pragma data_seg(&quot;.data&quot;)</span><br></pre></td></tr></table></figure></p><p>上述代码表示将global放到FOO段中，然后指导编译器将段恢复到.data段。</p><p>PE/COFF 文件格式和ELF文件非常相似，也是在window下最广泛应用的可执行文件格式，微软的编译器产生的目标文件都是COFF格式。COFF文件中有个很特别的段叫 <strong>.drectve段</strong>，这个段保存了编译器传递给链接器的命令行参数，可以通过这个段实现指定运行库等功能。</p><p>PE格式是windows下的可执行文件和动态链接库的格式。PE格式是COFF格式的改进版本，增加了PE文件头，数据目录等。</p><h1 id="第六章：可执行文件的装载与进程"><a href="#第六章：可执行文件的装载与进程" class="headerlink" title="第六章：可执行文件的装载与进程"></a>第六章：可执行文件的装载与进程</h1><p>可执行文件只有被装载到内存中，才会被CPU执行。</p><h2 id="进程虚拟地址空间"><a href="#进程虚拟地址空间" class="headerlink" title="进程虚拟地址空间"></a>进程虚拟地址空间</h2><p>程序被运行起来后，它将拥有独立的虚拟地址空间。虚拟地址空间由CPU的位数决定大小，32位的硬件平台决定了虚拟空间的最大值时2的32次方。64位则是2的64次方。一般情况下，可以通过C语言中的指针大小来判断硬件平台的位数：32位平台下指针大小为4字节，64位平台下大小为8字节。</p><h2 id="装载方式"><a href="#装载方式" class="headerlink" title="装载方式"></a>装载方式</h2><p>一次性将程序执行需要的所有数据和指令都装载到内存的方法称为 <strong>静态装载</strong>，而通过将常用的部分驻留在内存中，不常用的部分存放在磁盘上，这种方法称为 <strong>动态装载</strong>。  </p><p><strong>覆盖载入</strong> 和 <strong>页映射</strong> 是两种最典型的动态装载方法。</p><h3 id="覆盖载入"><a href="#覆盖载入" class="headerlink" title="覆盖载入"></a>覆盖载入</h3><p>现在几乎已经被淘汰。覆盖载入将整个管理过程交给程序员，程序员先将程序分割成若干块。然后在编写一个 <strong>覆盖管理器</strong> ，该管理器负责管理哪个模块需要读入内存，哪个模块目前可以被覆盖。覆盖管理器一般都常驻内存。在有多个模块的情况下，程序员需要手工将模块按照他们之间的调用依赖关系组织成树状结构。</p><h3 id="页映射"><a href="#页映射" class="headerlink" title="页映射"></a>页映射</h3><p>页映射是虚拟存储机制的一部分。页映射将内存和所有磁盘中的数据和指令按照 “页” 为单位划分若干页，这也是装载的操作单位。</p><p>页映射算法先将程序入口装载到内存中运行，之后需要用到哪个页，就将哪个页装载到内存中，当发现内存中所有也都被占用时，根据算法决定内存中的哪个页会被覆盖，常见的算法有 <strong>先进先出算法FIFO</strong> 和 <strong>最少使用算法LUR</strong>。</p><p>整个过程负责页调度的管理器就是现代操作系统中的存储管理器。</p><h2 id="从操作系统看文件装载"><a href="#从操作系统看文件装载" class="headerlink" title="从操作系统看文件装载"></a>从操作系统看文件装载</h2><h3 id="进程的建立"><a href="#进程的建立" class="headerlink" title="进程的建立"></a>进程的建立</h3><p>进程建立主要分为三个步骤</p><ol><li>创建一个独立的虚拟地址空间<ol><li>虚拟空间由一组页映射函数将虚拟空间的各个页映射到物理空间</li></ol></li><li>读取可执行文件头，并建立虚拟空间与可执行文件的映射<ol><li>由于可执行文件装载时实际上是被映射到虚拟空间，所以可执行文件很多时候又被叫做映射文件</li><li>Linux将进程虚拟空间中的一个段叫做虚拟内存区域（VMA），用来记录这个映射关系。</li></ol></li><li>将CPU的指令寄存器设置成可执行文件的入口地址，启动运行<h3 id="页错误"><a href="#页错误" class="headerlink" title="页错误"></a>页错误</h3>上述过程执行结束后，可执行文件的指令和数据并没有真正的被装载到物理内存中，仅仅是建立了可执行文件–虚拟内存–物理内存 三者之间的映射关系。  </li></ol><p>当CPU开始执行时，会发现程序入口的物理地址为空，进而引发一个页错误。CPU将权限交给操作系统，操作系统通过之前建立的映射，将可执行文件的一个运行页装载到内存中，然后CPU进行运行。整个页错误处理过程不断循环，直到程序结束。</p><h3 id="段合并"><a href="#段合并" class="headerlink" title="段合并"></a>段合并</h3><p>由于一个可执行文件往往包含十多个小段，而内存空间的空间分配是以页为单位的，如果可执行文件中包含大量的小段，会造成大量的内存浪费。所以ELF格式可执行文件引入segment概念，可以将多个 <strong>较小的，读写运行权限相同的 section合并成几个segment</strong> 进而减少内存浪费。</p><p>所以说，对于ELF文件，可以说是从不同的角度被分割，不同的角度称为 <strong>视图</strong>，从section的角度来看，ELF文件就是链接视图。从segment角度来看，就是执行视图。</p><h3 id="VMA"><a href="#VMA" class="headerlink" title="VMA"></a>VMA</h3><p>一个进程基本上可以分为如下几个VMA区域：</p><ol><li>代码VMA，权限只读，可执行，有映像文件</li><li>数据VMA，权限读写，可执行，有映像文件</li><li>堆VMA，权限读写，可执行，无映像文件，匿名，可向上扩展</li><li>栈VMA，权限读写，不可执行，无映像文件，匿名，可向下扩展。</li></ol><h3 id="堆和栈"><a href="#堆和栈" class="headerlink" title="堆和栈"></a>堆和栈</h3><p>VMA也会被用来对进程的地址空间堆栈进行管理。</p><h3 id="段的地址对齐"><a href="#段的地址对齐" class="headerlink" title="段的地址对齐"></a>段的地址对齐</h3><p>即便是将可执行文件中的多个section按照读写权限合并成几个segment，仍然会出现物理内存页浪费的情况（每个segment大小都不是页的整数倍，会有一部分小数据单独占一整页）。UNIX系统通过 <strong>各个segment接壤部分共享一个物理页，然后将该物理页分别映射两次到两个虚拟页中</strong> 的方法，实现了物理页覆盖，减少页浪费的方法。使用这种方法，一个物理页可能同时包含两个segment的数据，有时甚至多余两个segment。</p><h3 id="进程栈初始化"><a href="#进程栈初始化" class="headerlink" title="进程栈初始化"></a>进程栈初始化</h3><p>操作系统在进程启动前将一些进程运行所需要的环境提前保存到虚拟空间的栈中。</p><h1 id="第七章：动态链接"><a href="#第七章：动态链接" class="headerlink" title="第七章：动态链接"></a>第七章：动态链接</h1><p>静态链接存在浪费内存和磁盘空间，模块更新困难等问题。因为每个程序除了保留着printf(),scanf()等公用库函数，还有相当数量的其他库静态函数。在Linux中，一个普通的程序会使用到C语言静态库至少1M以上。  </p><p>例如，两个程序program1.o 和program2.o都用到了lib.o这个静态链接库。那么当内核运行这两个程序时，lib.o在磁盘和内存中都有两份副本。当系统中存在大量类似于lib.o的被多个程序共享的目标文件时，浪费的空间无法想象。</p><p>另一个问题是静态链接对程序的更新，部署和发布都会造成更多麻烦。如果program.o包含一个lib.o是由第三方提供，当该第三方更新lib.o后，program的发布者必须马上拿到最新版的lib.o并与program.o链接后重新发布给用户。也就是说，任何一个模块的更新，程序都需要重新进行链接，发布给用户。</p><p>解决这个两个问题的方法就是动态链接：<strong>不对那些组成程序的目标文件进行链接，等到程序要运行时才进行链接。</strong> 也就是说，把链接这个过程推迟到运行时再进行，这就是动态链接的基本思想。</p><p>上述program1.o的例子中，如果运用动态链接，就是保留program1.o, program2.o 和 lib.o三个目标文件。当运行program1.o是，系统首先加载program1.o，当系统发现program1.o中用到了lib.o，系统接着加载lib.o，然后系统再进行动态链接工作，这个链接工作和静态连接很像，完成这些步骤后，系统把控制权交给program1.o的程序入口，程序开始运行。 而如果这个时候需要运行program2.o，那么系统只需要加载program2.o而不需要再一次加载lib.o因为内存中已经有一份lib.o存在，系统只需要将program2.o和lib.o链接起来即可。整个链接过程是由专门的 <strong>动态链接器</strong> 完成的。</p><p><strong>程序可扩展性和兼容性</strong> 动态链接还有一个特点就是程序在运行时可以动态选择加载各种程序模块，这个有点后来被制作程序的 <strong>插件</strong>。</p><p>比如公司开发完一个产品，可以规定一个程序的接口，其他开发者只需要按照这个接口来编写符合要求的动态链接文件，这个产品就可以动态的载入各种第三方模块。在程序运行时动态链接，实现程序的扩展功能。</p><h3 id="动态链接基本实现"><a href="#动态链接基本实现" class="headerlink" title="动态链接基本实现"></a>动态链接基本实现</h3><p>动态链接基本思想是将程序按照模块拆分成各个相对独立的部分，在程序运行时才将他们链接在一起形成一个完整的程序。而不像静态链接那样，先把所有程序模块都链接成一个单独的可执行文件然后再执行。在linux中，ELF动态链接文件被称为 <strong>动态共享对象</strong>， 简称 <strong>共享对象，一般是以.so为扩展名的一些文件</strong> 。在window中，动态链接文件被称为 <strong>动态链接库，常常以.dll为扩展名。</strong></p><p>在linux中，常用的c语言运行库为glibc，它的动态链接形式保存在‘/lib’目录下，文件名叫做‘libc.so’。</p><p>可以使用gcc将一个c文件编译成共享文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -fPIC -shared -o lib.so lib.c</span><br></pre></td></tr></table></figure></p><p>-shared表示产生共享文件。</p><p>主函数program1.c和program2.c在编译过程也需要制定动态链接文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc -o program1 program1.c ./lib.so</span><br><span class="line">gcc -o program2 program1.c ./lib.so</span><br></pre></td></tr></table></figure></p><h2 id="延迟绑定"><a href="#延迟绑定" class="headerlink" title="延迟绑定"></a>延迟绑定</h2><p>动态链接往往比静态链接慢5%左右，因为动态链接需要进行复杂的GOT定位，然后间接寻址。</p><p>动态链接下模块之间包含大量的函数引用，所以在开始执行前，动态链接会耗费不少时间用于解决模块之间的函数引用的符号查找以及重定位。但程序中多少会存在一些即是程序执行完也没有被用到的函数（比如一些错误处理函数等），所以提前将所有模块都进行动态链接是一种浪费。所以ELF采用一种延迟绑定的做法。 <strong>基本思想是当函数第一次被用到时才进行绑定（符号查找，重定位等），如果没用到则不用绑定</strong>。 这样做可以大大加快程序的启动速度。</p><h2 id="总结动态链接过程"><a href="#总结动态链接过程" class="headerlink" title="总结动态链接过程"></a>总结动态链接过程</h2><p>动态链接的情况下，可执行文件的装载与静态链接基本一样，首先OS读取可执行文件的文件头，并将各个segment映射到进程的虚拟空间中。在静态链接下，OS接着就将控制权交给可执行文件的入口地址，程序开始执行。  </p><p>但对于动态链接而言，OS还不能在装载完之后就把控制权交给可执行文件，因为由于动态链接的缘故，此时可执行文件中很多外部符号的引用还处于无效地址状态，即还没有跟相应的共享对象中的实际位置链接起来。所以在装载完可执行文件后，OS会先启动一个动态链接器。</p><p>动态链接器得到控制权后，首先初始化自身，然后根据当前环境参数，开始对可执行文件进行动态链接工作。</p><p>链接工作分为3步，先是启动动态链接器本身，然后装载所有需要共享的对象，最后是重定位和初始化。</p><p>动态链接器启动后，会将可执行文件和链接器本身的符号表合并到一个符号表中，称为 <strong>全局符号表</strong>，然后链接器开始寻找可执行文件所依赖的共享对象。ELF文件中的 “.dynamic”中指出了可执行文件所依赖的共享对象。然后一个个的遍历依赖的共享对象，将它对应的代码段和数据段映射到进程空间中。当一个共享对象被装载，它的符号表会被合并到全局符号表中。所有共享对象都被装载后，全局符号表中包含进程运行所需要的所有动态链接符号。</p><p>完成上述步骤后，链接器开始遍历可执行文件和每个共享对象的重定位表，将每个需要重定位的位置进行修正。</p><p>所有链接工作结束后，动态链接器将控制权交还给可执行文件，程序开始正式执行。</p><h2 id="动态符号表"><a href="#动态符号表" class="headerlink" title="动态符号表"></a>动态符号表</h2><p>为了表示动态链接模块之间的符号导入导出关系，ELF用动态符号表段来保存这些信息，段名为 “.dynsym”。动态符号表只保存和动态链接相关的符号信息。</p><h2 id="显式运行时链接"><a href="#显式运行时链接" class="headerlink" title="显式运行时链接"></a>显式运行时链接</h2><p>也叫运行时加载，是让程序自己在运行时加载指定的模块，并可以在不需要改模块时将其卸载。运行时加载使得程序的模块组织更加灵活，可以用来实现一些诸如插件，驱动等功能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>动态链接的两个优点：大大降低了对内存和磁盘空间的浪费；更加方便的维护升级程序。</li><li>动态链接装在地址不缺的的解决方法：装载时重定位和地址无关代码。<ol><li>装载时从定位无法共享代码段，但速度快</li><li>地址无关代码缺点是速度慢，但可以实现代码段在各个进程之间的共享，节省内存空间</li></ol></li></ol><h1 id="第八章：Linux共享库的组织"><a href="#第八章：Linux共享库的组织" class="headerlink" title="第八章：Linux共享库的组织"></a>第八章：Linux共享库的组织</h1><p><strong>共享库</strong> 和共享对象没有区别，Linux下的共享库就是普通的ELF共享对象。  </p><h2 id="共享库版本"><a href="#共享库版本" class="headerlink" title="共享库版本"></a>共享库版本</h2><p>因为程序和共享库可以彼此分离，所以程序对共享库提供的ABI（二进制接口）的兼容就变得非常重要。Linux通过规定明明系统来管理每个共享库，Linux下的每个共享库文件名必须如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libname.so.x.y.z</span><br></pre></td></tr></table></figure></p><ul><li>x为主版本号，表示重大升级，不同主版本号之间不兼容。</li><li>y为次版本号，表示增量升级，即增加一些新的接口符号，但保持原来符号不变。不同次版本号之间可以相互兼容</li><li>z为发布版本号，表示丢一些错误的修正，性能改进等，完全不添加新的接口，也不对旧接口进行修改。</li></ul><h2 id="SO-NAME"><a href="#SO-NAME" class="headerlink" title="SO-NAME"></a>SO-NAME</h2><p>OS普遍采用一种叫SO-NAME的命名机制来记录共享库的依赖关系。防止动态链接器调用了错误的共享库版本导致无法正确链接。</p><h2 id="共享库系统路径"><a href="#共享库系统路径" class="headerlink" title="共享库系统路径"></a>共享库系统路径</h2><p>目前大多数Linux都遵守FHS（FILE Hierarchy Standard）标准，该标准规定了一个系统中的系统文件如何存放，包括各个目录的结构组织和作用。有利于促进各个OS的兼容性。</p><p>FHS标准规定，一个系统主要有两个存放共享库的位置：</p><ol><li>/lib 主要存放系统中最关键和基础的共享库，主要是那些/bin和/sbin下程序所需要的库</li><li>/usr/lib 主要保存一些非系统运行时所需要的关键共享库，主要是一些开发时用的共享库。</li><li>/usr/local/lib 用来防止一些跟操作系统本身并不相关的库，主要是第三方的应用程序的库，比如python解释器相关的共享库（它的可执行文件可能被放到/usr/local/bin下）</li></ol><h3 id="共享库的安装"><a href="#共享库的安装" class="headerlink" title="共享库的安装"></a>共享库的安装</h3><p>我们需要将一个共享库安装到OS中，以便各种程序都可以共享使用它。最简单的办法是将新的共享库复制到某个标准的共享库目录中。如/lib等，然后运行ldconfig即可。</p><h1 id="第九章：Windows下的动态链接"><a href="#第九章：Windows下的动态链接" class="headerlink" title="第九章：Windows下的动态链接"></a>第九章：Windows下的动态链接</h1><p>Windows下的动态链接库缩写为DLL，是和EXE文件一个概念，都是PE格式的二进制文件。相当于Linux下的ELF格式的共享对象。</p><h2 id="DLL"><a href="#DLL" class="headerlink" title="DLL"></a>DLL</h2><h3 id="创建DLL"><a href="#创建DLL" class="headerlink" title="创建DLL"></a>创建DLL</h3><p>使用关键字__declspec指定导入导出符号。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__declspec(dllexport) double Add(double a, double b)&#123;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用编译器将其编译成DLL：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl /LDd math.c</span><br></pre></td></tr></table></figure></p><p>就会生成math.dll文件和math.lib文件</p><h3 id="使用DLL"><a href="#使用DLL" class="headerlink" title="使用DLL"></a>使用DLL</h3><p>当需要使用DLL中的某个符号时，需要使用__declspec(dllimport)关键字显式声明某个符号为导入符号。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#inlcude &lt;stdio.h&gt;</span><br><span class="line">__declspec(dllimport) double Add(double a, double b);</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    double result = Add(3.0, 2.3);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后将程序进行编译并声明需要用到的动态链接库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cl /c testmath.c</span><br><span class="line">link testmath.obj math.lib</span><br></pre></td></tr></table></figure></p><p>math.lib并不是如同静态链接中的那样，包含着math.c的二进制代码和数据，而是包含了用来描述math.dll的导出符号的文件。</p><h3 id="导出表"><a href="#导出表" class="headerlink" title="导出表"></a>导出表</h3><p>当一个PE需要将一些函数或变量提供给其他PE文件使用是，我们将这些符号导出，最典型的情况是一个DLL文件将符号到处给EXE文件使用。所有导出的符号被集中在一个 <strong>导出表</strong> 中。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>动态链接对Window非常重要，整个window系统本身即基于动态链接机制，window的API也是通过DLL形式提供给开发者， 而不是linux那样以系统调用为OS的入口。</p><h1 id="第十章：内存"><a href="#第十章：内存" class="headerlink" title="第十章：内存"></a>第十章：内存</h1><p>程序的运行：程序的环境由以下三部分组成：内存，运行库，系统调用。  </p><p>在32位系统里，内存空间拥有4GB的寻址能力。这4GB会被分割一部分用于OS的运行被称为 <strong>内核空间</strong> 。用户仅可以使用剩下的2GB或3GB的 <strong>用户空间</strong>。</p><h2 id="程序的内存布局"><a href="#程序的内存布局" class="headerlink" title="程序的内存布局"></a>程序的内存布局</h2><p>在用户空间里，内存也被继续分割：应用程序使用的内存空间有如下默认区域：</p><ol><li>栈：栈用于维护函数调用的上下文。栈通常在用户空间的最高地址出分配，有数兆字节大小。通常情况下栈是向下增长的</li><li>堆：堆用来容纳应用程序动态分配的内存区域，当程序使用malloc或者new分配内存时，得到的内存就来自于堆。堆通常在栈的下方，一般情况下比栈大很多，可以有数百兆的容量。</li><li>可执行文件映像：存储着可执行文件在内存里的映像，由装载器在装载时将可执行文件映射到这个空间中。</li><li>保留区：保留区是对内存中收到保护禁止访问的内存区域总称。例如在大多数OS中，极小地址都是不允许访问的，如NULL。</li><li>动态链接区：这个区域用来映射装载的动态链接库，如果可执行文件依赖动态链接库，那么系统将为它分配相应的空间并将共享库载入到该空间。<h2 id="栈与调用惯例"><a href="#栈与调用惯例" class="headerlink" title="栈与调用惯例"></a>栈与调用惯例</h2>栈在程度运行中具有举足轻重的地位，栈保存了一个函数调用所需要的维护信息，通常被称为 <strong>堆栈帧或者活动记录</strong>。 堆栈帧一般包含如下几方面内容：</li><li>函数的返回地址和参数</li><li>临时变量：包括函数的非静态全局变量以及编译器自动生成的其他临时变量</li><li>保存的上下文：包括在函数调用前后需要保持不变的寄存器。</li></ol><p>一个函数的活动记录用ebp和esp两个寄存器划定范围，esp始终指向栈的顶部，也就是当前活动记录的顶部。ebp指向函数活动记录的一个固定位置，ebp又被称为帧指针。ebp不随当前函数的执行而变化，esp始终指向栈顶，因此随着函数的执行会不断变化。固定不变的ebp可以用来定位函数活动记录中的各个数据。<br>在ebp之前首先是这个函数的返回地址，再往前是压入栈中的参数。ebp所直接指向的数据是调用该函数前ebp的值，这样在函数返回的时候，ebp可以通过读取这个值恢复到调用前的值。</p><h5 id="函数调用的过程如下："><a href="#函数调用的过程如下：" class="headerlink" title="函数调用的过程如下："></a>函数调用的过程如下：</h5><ol><li>把传入的参数压入栈中</li><li>把当前指令的下一条指令的地址压入栈中（即调用函数后的下一条指令的地址，可以理解为返回地址）</li><li>跳到函数体执行</li><li>push ebp：把ebp压入栈中（old ebp）</li><li>mov ebp,esp：ebp=esp （这时ebp指向栈顶）<h5 id="函数返回的过程如下："><a href="#函数返回的过程如下：" class="headerlink" title="函数返回的过程如下："></a>函数返回的过程如下：</h5></li><li>mov esp, ebp：回复esp同时回收全局变量空间</li><li>pop ebp：从栈中回复保存的ebp的值</li><li>ret：从栈中去的返回地址，并跳转到该位置。继续执行原来的函数</li></ol><h3 id="调用惯例"><a href="#调用惯例" class="headerlink" title="调用惯例"></a>调用惯例</h3><p>各个函数对栈的工作过程，函数调用过程，参数读取过程等信息所遵守的共同约定。一般会规定如下几个方面：函数参数的传递顺序和方式，栈的维护方式，名字修饰的策略等。  </p><p>在C语言中，有多个调用惯例，默认调用惯例是cdecl。例如，对函数foo的声明，他的完整形式是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int _cdecl foo(int n, float m)</span><br></pre></td></tr></table></figure></p><p>_cdecl并非标准关键字，在不同编译器有不同写法。</p><h3 id="函数返回值传递"><a href="#函数返回值传递" class="headerlink" title="函数返回值传递"></a>函数返回值传递</h3><p>常见的调用惯例使用eax和edx联合，用以处理返回值。<br>假设main函数中调用了return_test()函数，并得到一个较大字节的返回值，常见返回值处理过程如下：</p><ol><li>在main函数在栈上额外开辟一片空间，并将这个空间的一部分作为传递返回值的临时对象，称为temp</li><li>将temp对象的地址作为隐藏参数传递给return_test函数</li><li>return_test函数将数据拷贝给temp对象，并将temp对象的地址用eax传出</li><li>return_test返回后，main函数将eax指向的temp对象的内容拷贝给n，n即为在main函数中被return_test函数返回值赋值的对象。</li></ol><h3 id="声名狼藉的C-返回对象"><a href="#声名狼藉的C-返回对象" class="headerlink" title="声名狼藉的C++返回对象"></a>声名狼藉的C++返回对象</h3><p>C++在返回一个对象时，对象要经过两次拷贝构造函数的调用，甚至更多。所以返回一个较大的对象会有非常多的额外开销。所以 <strong>C++程序中要尽量避免返回对象</strong></p><h2 id="堆与内存管理"><a href="#堆与内存管理" class="headerlink" title="堆与内存管理"></a>堆与内存管理</h2><p>因为栈的数据在函数返回的时候会被释放掉，所以无法将数据传递至函数外。而全局变量没办法动态的产生，只能在编译的时候定义。</p><p>堆是一块巨大的内存空间，尝尝占据整个虚拟空间的绝大部分。程序可以在堆中申请一块连续内存并自由使用。这块内存在程序主动放弃之前会一直保持有效。程序通过new和malloc申请堆空间。  </p><p>程序创建初始时，向OS申请一块适当大小的堆空间，然后由程序自己管理这块空间，具体来说，往往是程序的运行库负责管理堆空间的分配。运行库管理堆空间的算法就是堆的分配算法。</p><h3 id="堆分配算法"><a href="#堆分配算法" class="headerlink" title="堆分配算法"></a>堆分配算法</h3><p>如何管理一大块连续的内存空间，能够按需求分配，释放其中的空间，这就是堆分配算法。</p><ol><li>空闲链表：该方法是把堆中各个空闲的块按照链表的方式连接起来，当用户请求一块空间时，可以遍历整个链表直到找到合适大小的块并将其拆分。当用户释放空间时将它合并到空闲链表中。</li><li>位图(Bitmap)：核心思想是将整个堆划分为大量的块，每个块的大小相同，当用户请求内存的时候，总是分配整数个块的空间给用户，第一个块称为已分配区域的头，其余的称为已分配区域的主体。<strong>可以使用一个整数数组来记录块的使用情况，由于每个块只有头、主体、空闲三个状态，因此仅仅需要两bit即可表示一个块</strong>。使用位图有如下几个优缺点：<ol><li>速度快，整个堆的空闲信息存储在一个数组内</li><li>稳定性好，为避免越界写数据，只需要将位图简单备份一下即可</li><li>块不需要额外信息，易于管理</li><li>缺点：分配时容易产生碎片，导致浪费。</li></ol></li><li>对象池：在一些场合下，被分配对象的大小是较为固定的几个值，如果每次分配的空间大小都一样，那么就可以按照这个每次请求分配的大小作为一个单位，吧整个堆空间划分为大量的小块，每次请求的时候只需要找到一个小块就可以。对象池的管理方法可以使用空闲链表或者位图。<br>实际应用中，对的分配算法往往采用多种算法复合而成。</li></ol><h1 id="第十一章：运行库"><a href="#第十一章：运行库" class="headerlink" title="第十一章：运行库"></a>第十一章：运行库</h1><p>本章介绍从程序创始开始，在程序背后保证程序正常运行的运行库。</p><h2 id="入口函数和程序初始化"><a href="#入口函数和程序初始化" class="headerlink" title="入口函数和程序初始化"></a>入口函数和程序初始化</h2><p>OS装载程序后，首先运行的并不是main的第一行，而是一些负责准备好main函数执行所需要的环境，并负责调用main函数的代码。<br>一个典型的程序运行步骤如下：</p><ol><li>OS创建进程后，把控制权交给程序的入口，这个入口往往是运行库中的某个入口函数</li><li>入口函数对运行库和运行环境进行初始化，包括堆，IO，线程，全局变量构造等等。</li><li>入口函数在完成初始化后，调用main函数，正式开始执行程序的主体部分。</li><li>main函数执行完毕后，返回到入口函数，入口函数进行清理工作，包括全局变量析构，堆销毁，关闭IO等，然后进行系统调用结束进程。</li></ol><p>大部分情况下程序员都不会和入口函数打交道，常见的有 <strong>glibc和MSVC的入口函数</strong>，并且各个入口函数的启动过程在不同情况下差别很大，比如静态的glibc和动态的glibc，glibc用于可执行文件和用于共享库文件的差别。</p><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>环境变量是存在于系统中的一些公用数据，任何程序都可以访问。通常情况下环境变量存储的都是一些系统的公共信息，例如系统搜索路径，当前OS版本。环境变量格式为key=value字符串。在Linux下，直接在命令行里输入export即可。</p><h2 id="C语言运行库"><a href="#C语言运行库" class="headerlink" title="C语言运行库"></a>C语言运行库</h2><p>C语言背后有一套庞大的代码支持，这套代码至少包括入口函数，以及所依赖的函数所构成的函数集合，各种标准库函数的实现等。这个代码集合被称为 <strong>运行时库Runtime Library</strong> C语言的运行库被称为 <strong>C运行库（CRT）</strong></p><p>一个C语言运行库大致包括了如下功能：</p><ul><li>启动和退出：包括入口函数以及入口函数所依赖的其他函数等</li><li>标准函数：由C语言标准规定的C语言标准库所拥有的函数实现</li><li>IO：IO功能的封装和实现</li><li>堆：堆的封装和实现</li><li>语言实现：语言中一些特殊功能的实现</li><li>调试：实现调试功能的代码</li></ul><h3 id="C语言标准库"><a href="#C语言标准库" class="headerlink" title="C语言标准库"></a>C语言标准库</h3><p>C语言标准库占据CRT的大部分，ANSIC标准库由24个C头文件组成，非常轻量，仅包含了数学函数，字符串处理，IO等基本方法。常见头文件如下：</p><ol><li>标准输入输出：stdio.h</li><li>文件操作：stdio.h</li><li>字符操作：ctype.h</li><li>字符串操作：string.h</li><li>数学函数：math.h</li><li>资源管理：stdlib.h</li></ol><p>运行库是平台相关的，因为它和操作系统结合的非常紧密，C语言的运行库从某种程度上来讲是C语言的程序和不同操作系统平台之间的抽象层，将不同的操作系统API抽象成相应的库函数。Linux和Windows两个主要C语言运行库分别是glibc（GNU C Library）和MSVCRT。</p><p>事实上，glibc和MSVCRT是C标准库的超集，例如线程操作并不包含在标准库中，但两者都提供线程操作库。</p><h1 id="第十二章：系统调用与API"><a href="#第十二章：系统调用与API" class="headerlink" title="第十二章：系统调用与API"></a>第十二章：系统调用与API</h1><p>系统调用是应用程序与操作系统内核之间的接口，决定了应用程序如何与啮合打交道，无论程序是直接及逆行系统调用还是通过运行库，最后都会到达系统调用这个层面上。</p><p>其中Windows系统完全基于DLL机制，所以它通过DLL对系统调用进行了包装，形成了所谓的Windows API。应用程序所能看到Windows系统最底层的接口就是Windows API。所以Windows程序相当于在运行库和系统调用之间又多了一层API。</p><p>现代OS通常提供两种特权级别给程序分别为 <strong>用户模式</strong> 和 <strong>内核模式</strong>，系统调用是运行在内核模式的，而应用程序基本上都运行在用户模式。而应用程序通常通过<strong>中断</strong> 来从用户态切换到内核态。</p><p>Windows API是以DLL导出函数的形式暴露给应用程序开发者的，微软把这些DLL导出函数的头文件，导出库，相关文件和工具一起提供给开发者，包装成了Software Development Kit（SDK）。</p>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python内存管理</title>
      <link href="/2019/05/26/Programming%20Language/Python/Python%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2019/05/26/Programming%20Language/Python/Python%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<p>介绍python内存管理模型。</p><a id="more"></a><h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><h3 id="变量和对象"><a href="#变量和对象" class="headerlink" title="变量和对象"></a>变量和对象</h3><ol><li>变量可以理解为没有类型的指针，指向一个对象内存空间，该地址保存对象的类型即为变量的类型。</li><li>对象类型已知，每个对象都包含一个头部信息（头部信息：类型标识符和引用计数器）</li><li>变量名没有类型，类型属于对象（因为变量引用对象，所以类型随对象），变量引用什么类型的对象，变量就是什么类型的。</li><li>使用内置函数id(obj)，返回对象的内存地址</li><li>使用关键字is进行引用判断，即判断两个引用（变量）所指的对象（地址空间）是否相同。</li><li>缓存引用：在Python中，整数和短小的字符，Python都会缓存这些对象，以便重复使用。当我们创建多个等于1的引用时，实际上是让所有这些引用指向同一个对象。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">整数</span><br><span class="line">In [46]: a=1</span><br><span class="line">In [47]: b=1</span><br><span class="line">In [48]: print(a is b)</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">短字符串</span><br><span class="line">In [49]: c=&quot;good&quot;</span><br><span class="line">In [50]: d=&quot;good&quot;</span><br><span class="line">In [51]: print(c is d)</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">长字符串</span><br><span class="line">In [52]: e=&quot;very good&quot;</span><br><span class="line">In [53]: f=&quot;very good&quot;</span><br><span class="line">In [54]: print(e is f)</span><br><span class="line">False #和自己测验的不一样</span><br><span class="line"></span><br><span class="line">列表</span><br><span class="line">In [55]: g=[]</span><br><span class="line">In [56]: h=[]</span><br><span class="line">In [57]: print(g is h)</span><br><span class="line">False</span><br></pre></td></tr></table></figure></li></ol><p>由运行结果可知：</p><p>　　1、Python缓存了整数和短字符串，因此每个对象在内存中只存有一份，引用所指对象就是相同的，即使使用赋值语句，也只是创造新的引用，而不是对象本身；</p><p>　　2、Python没有缓存长字符串、列表及其他对象，可以由多个相同的对象，可以使用赋值语句创建出新的对象。</p><h4 id="引用计数"><a href="#引用计数" class="headerlink" title="引用计数"></a>引用计数</h4><h5 id="普通引用"><a href="#普通引用" class="headerlink" title="普通引用"></a>普通引用</h5><p>在Python中，每个对象都有指向该对象的引用总数—引用计数<br>查看对象的引用计数：sys.getrefcount()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [1,2,3]</span><br><span class="line">print(sys.getrefcount(a))</span><br><span class="line">b = a</span><br><span class="line">print(sys.getrefcount(a))</span><br></pre></td></tr></table></figure></p><p>当使用某个引用作为参数，传递给getrefcount()时，参数实际上创建了一个临时的引用。因此，getrefcount()所得到的结果，会比期望的多1。</p><h5 id="容器对象引用"><a href="#容器对象引用" class="headerlink" title="容器对象引用"></a>容器对象引用</h5><p>Python的一个容器对象(比如：表、词典等)，可以包含多个对象。<br>容器对象时变量指针和元素对象内存空间之间的一层对象。变量指针指向容器对象，容器对象再分别指向各个元素的空间。所以说，容器对象中包含的并不是元素对象本身，是指向各个元素对象的引用。</p><h4 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h4><p>当Python中的对象越来越多，占据越来越大的内存，启动垃圾回收(garbage collection)，将没用的对象清除。</p><ol><li>当Python的某个对象的引用计数降为0时，说明没有任何引用指向该对象，该对象就成为要被回收的垃圾。比如某个新建对象，被分配给某个引用，对象的引用计数变为1。如果引用被删除，对象的引用计数为0，那么该对象就可以被垃圾回收。</li><li>垃圾回收时，Python不能进行其它的任务，频繁的垃圾回收将大大降低Python的工作效率；</li><li>Python只会在特定条件下，自动启动垃圾回收（垃圾对象少就没必要回收）</li><li>当Python运行时，会记录其中分配对象(object allocation)和取消分配对象(object deallocation)的次数。当两者的差值高于某个阈值时，垃圾回收才会启动。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import gc</span><br><span class="line">gc.get_threshold() # gc模块中查看阈值的方法</span><br><span class="line">(700, 10, 10)</span><br></pre></td></tr></table></figure></li></ol><p>阈值分析：<br>　　700即是垃圾回收启动的阈值；<br>　　每10次0代垃圾回收，会配合1次1代的垃圾回收；而每10次1代的垃圾回收，才会有1次的2代垃圾回收；<br>当然也是可以手动启动垃圾回收：　<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gc.collect()</span><br></pre></td></tr></table></figure></p><h5 id="分代回收"><a href="#分代回收" class="headerlink" title="分代回收"></a>分代回收</h5><p>Python同时采用了分代(generation)回收的策略。这一策略的基本假设是，存活时间越久的对象，越不可能在后面的程序中变成垃圾。我们的程序往往会产生大量的对象，许多对象很快产生和消失，但也有一些对象长期被使用。出于信任和效率，对于这样一些“长寿”对象，我们相信它们的用处，所以减少在垃圾回收中扫描它们的频率。</p><p>Python将所有的对象分为0，1，2三代。所有的新建对象都是0代对象。当某一代对象经历过垃圾回收，依然存活，那么它就被归入下一代对象。垃圾回收启动时，一定会扫描所有的0代对象。如果0代经过一定次数垃圾回收，那么就启动对0代和1代的扫描清理。当1代也经历了一定次数的垃圾回收后，那么会启动对0，1，2，即对所有对象进行扫描。</p><h3 id="内存池机制"><a href="#内存池机制" class="headerlink" title="内存池机制"></a>内存池机制</h3><p>Python中有分为大内存和小内存：（256K为界限分大小内存）<br>大内存使用malloc进行分配<br>小内存使用内存池进行分配</p><h4 id="金字塔内存"><a href="#金字塔内存" class="headerlink" title="金字塔内存"></a>金字塔内存</h4><p>第3层：最上层，用户对Python对象的直接操作<br>第1层和第2层：内存池，有Python的接口函数PyMem_Malloc实现—–若请求分配的内存在1~256字节之间就使用内存池管理系统进行分配，调用malloc函数分配内存，但是每次只会分配一块大小为256K的大块内存，不会调用free函数释放内存，将该内存块留在内存池中以便下次使用。<br>第0层：大内存—–若请求分配的内存大于256K，malloc函数分配内存，free函数释放内存。<br>第-1，-2层：操作系统进行操作</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2019/04/10/Linux%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/04/10/Linux%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><h4 id="预编译指令"><a href="#预编译指令" class="headerlink" title="预编译指令"></a>预编译指令</h4><p>在源文件中，以“#”开始的内容就是预编译指令。它的作用是告诉编译器，让它在真正进行编译之前对源文件进行一些插入文件、替换字符串等预处理，以得到最终参与编译的源文件。</p><p>“#include”指令用于将指定的文件插入该指令所在的位置，作为整个源文件的一部分。因为这样的文件总是在一个源文件的头部被插入，所 以我们通常将这样的文件称为头文件（header file）。</p><p>“#include”指令后的文件名有两种表示方式：如果使用双引号””来包围一个文件名，则预 处理器在处理这个指令的时候，将首先在当前目录（也就是这个源文件所在的目录）下搜索这个文件，如果不存在，则继续在项目的包含目录（包括项目的默认头文 件目录，也就是Visual Studio安装目录下的“\VC\include”文件夹,以及在项目属性中设置的项目附加头文件目录）下搜索这个文件；而如果使用尖括 号&lt;&gt;来包围一个文件名，预处理器则会直接在项目的包含目录下搜索这个文件。所以，通常我们使用””来插入当前项目目录下的头文件（比如我们 自己创建的头文件），而使用&lt;&gt;来插入各种项目包含目录下的库头文件（比如这里的iostream）。</p><h4 id="C-执行过程"><a href="#C-执行过程" class="headerlink" title="C++执行过程"></a>C++执行过程</h4><ol><li>由编译器将.cpp文件编译成.obj文件</li><li>由链接器将.obj文件和Visual C++所提供的标准库目标文件整合成最终的可执行文件</li><li>在操作系统执行文件前，会进行准备工作，如创建进程空间，将文件的数据段和代码段映射到进程的虚拟内存空间，初始化全局变量等。</li><li>操作系统执行可执行文件</li></ol><h4 id="size-t"><a href="#size-t" class="headerlink" title="size_t"></a>size_t</h4><p>size_t是一些C/C++标准在stddef.h中定义的。这个类型足以用来表示对象的大小。</p><p>size_t的真实类型与操作系统有关，在32位架构中被普遍定义为：</p><p>typedef   unsigned int size_t;</p><p>而在64位架构中被定义为：</p><p>typedef  unsigned long size_t;<br>size_t在32位架构上是4字节，在64位架构上是8字节，在不同架构上进行编译时需要注意这个问题。<br>而int在不同架构下都是4字节，与size_t不同；且int为带符号数，size_t为无符号数。</p><p>size_t是无符号的，并且是平台无关的，表示0-MAXINT的范围；</p><p>int是有符号的；</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>OpenCV学习笔记</title>
      <link href="/2019/01/20/OpenCV%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/01/20/OpenCV%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>openCV学习笔记。</p><a id="more"></a><h2 id="Mat类"><a href="#Mat类" class="headerlink" title="Mat类"></a>Mat类</h2><p>Mat类是OpenCV中最基本也是最常用的类。Mat不再需要手动分配其大小并且当不需要它的时候你不再需要手动释放它。<br>Mat类还有一个额外的好处是如果传递一个已存在Mat对象，它已经为矩阵分配所需的空间，这段空间将被重用。也就是说我们在任何时候只使用与我们执行任务时所必须多的内存一样多的内存。</p><h3 id="Mat类构成"><a href="#Mat类构成" class="headerlink" title="Mat类构成"></a>Mat类构成</h3><p>Mat本质上是由两个部分组成的类：</p><ol><li>包含信息有矩阵的大小，用于存储的方法，矩阵存储的地址等的矩阵头</li><li>一个指针，指向包含了像素值的矩阵（可根据选择用于存储的方法采用任何维度存储数据）。<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3>Mat的存储是逐行存储的，矩阵中的数据类型包括：Mat_<uchar>对应的是CV_8U，Mat_<uchar>对应的是CV_8U，Mat_<char>对应的是CV_8S，Mat_<int>对应的是CV_32S，Mat_<float>对应的是CV_32F，Mat_<double>对应的是CV_64F<h3 id="创建Mat矩阵"><a href="#创建Mat矩阵" class="headerlink" title="创建Mat矩阵"></a>创建Mat矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//1.使用构造函数，常见的几个如下：</span><br><span class="line">Mat::Mat();//default</span><br><span class="line">Mat::Mat(int rows, int cols, int type);</span><br><span class="line">Mat::Mat(Size size, int type);</span><br><span class="line">Mat::Mat(int rows, int cols, int type, const Scalar&amp; s);</span><br><span class="line">Mat::Mat(Size size, int type, const Scalar&amp; s);</span><br><span class="line">Mat::Mat(const Mat&amp; m);</span><br><span class="line">//参数说明：</span><br><span class="line">//int rows：行</span><br><span class="line">//int cols：列</span><br><span class="line">//int type：参见&quot;Mat数据类型类型定义&quot;</span><br><span class="line">//Size size：矩阵尺寸，注意宽和高的顺序：Size(cols, rows)</span><br><span class="line">//const Scalar&amp; s：用于初始化矩阵元素的数值</span><br><span class="line">//const Mat&amp; m：拷贝m的矩阵头给新的Mat对象，但是不复制数据！相当于创建了m的一个引用对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//2.使用create函数：</span><br><span class="line">Mat a = create(10, 9, CV_32F);//创建10*9的矩阵并指定元素类型</span><br><span class="line">//create的一个特殊用法：如果初始化的时候没有传入size的参数，或者后面需要改变size的参数，可以使用create来调整</span><br></pre></td></tr></table></figure></double></float></int></char></uchar></uchar></li></ol>]]></content>
      
      <categories>
          
          <category> openCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openCV </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Xcode运行OpenCV项目</title>
      <link href="/2019/01/16/Knowledge/Xcode%E8%BF%90%E8%A1%8COpenCV%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/01/16/Knowledge/Xcode%E8%BF%90%E8%A1%8COpenCV%E9%A1%B9%E7%9B%AE/</url>
      <content type="html"><![CDATA[<p>介绍如何在Mac上安装OpenCV并用XCode运行OpenCV项目。</p><a id="more"></a><h3 id="Mac安装OpenCV"><a href="#Mac安装OpenCV" class="headerlink" title="Mac安装OpenCV"></a>Mac安装OpenCV</h3><ol><li>安装homebrew</li><li>安装wget：brew install wget</li><li>安装cmake: brew install cmake</li><li>安装opencv: brew install opencv<br>注：这里OpenCV会被安装到一下路径 /usr/local/Cellar/opencv/&lt;opencv_version&gt;/ 这个路径接下来会用到<h3 id="安装Xcode"><a href="#安装Xcode" class="headerlink" title="安装Xcode"></a>安装Xcode</h3>在MacOS store下载安装XCode<h3 id="Xcode配置OpenCV"><a href="#Xcode配置OpenCV" class="headerlink" title="Xcode配置OpenCV"></a>Xcode配置OpenCV</h3>不同于python有自己的pip包管理工具，C++需要自己指定外部包的位置，所以XCode需要配置才可以运行OpenCV项目。<br>首先创建一个空C++项目：</li></ol><ul><li>Click on File&gt;New&gt;Project</li><li>Under Choose a template for your new project click on macOS</li><li>Under Application click on Command Line Tool</li><li>Fill in the details and set the Language to C++<br>然后需要对项目进行设定，首先点击Build Settings<h4 id="Header-Search-Path"><a href="#Header-Search-Path" class="headerlink" title="Header Search Path"></a>Header Search Path</h4>然后搜索’Header Search Paths’并添加OpenCV的路径到Search Path中，路径应该如下所示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/Cellar/opencv/&lt;version_number&gt;/include</span><br></pre></td></tr></table></figure></li></ul><p>还需要添加默认路径如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/include</span><br></pre></td></tr></table></figure></p><h4 id="Library-Search-Paths"><a href="#Library-Search-Paths" class="headerlink" title="Library Search Paths"></a>Library Search Paths</h4><p>然后搜索’Library Search Paths’并添加OpenCV的路径到Search Paths中，路径应该如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/Cellar/opencv/&lt;version_number&gt;/lib</span><br></pre></td></tr></table></figure></p><p>还需要添加默认路径如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/lib</span><br></pre></td></tr></table></figure></p><h4 id="Other-Linker-Flags"><a href="#Other-Linker-Flags" class="headerlink" title="Other Linker Flags"></a>Other Linker Flags</h4><p>搜索’Other Linker Flags’ 并添加信息如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_imgcodecs</span><br></pre></td></tr></table></figure></p><p>如果不添加这一步，会导致’linker command failed with exit code 1 clang’错误。</p><p>至此，opencv在Xcode上的配置已经完成。</p><h4 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;opencv2/core/core.hpp&gt;</span><br><span class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;</span><br><span class="line">#include &lt;opencv2/opencv.hpp&gt;</span><br><span class="line">using namespace cv;</span><br><span class="line">int  main()</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    Mat img = imread(&quot;1.jpg&quot;);</span><br><span class="line"></span><br><span class="line">    if(img.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        fprintf(stderr, &quot;failed to load input image\n&quot;);</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    imshow(&quot;Display Image&quot;, img);</span><br><span class="line">    waitKey(0);</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>自动驾驶入门</title>
      <link href="/2019/01/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%85%A5%E9%97%A8/"/>
      <url>/2019/01/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<p>自动驾驶入门笔记整理。</p><a id="more"></a><h1 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h1><h2 id="高精度地图"><a href="#高精度地图" class="headerlink" title="高精度地图"></a>高精度地图</h2><p>高精度地图是整个无人驾驶软件模块的核心，因为其他模块都是基于HD地图进行的。高精地图包含大量驾驶辅助信息，帮助汽车更好的理解路面情况，位置信息等。</p><h3 id="高精地图构建"><a href="#高精地图构建" class="headerlink" title="高精地图构建"></a>高精地图构建</h3><h4 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h4><ol><li>道路信息和建筑信息是不断变化的，这就需要数据采集团队不断更新数据集</li><li>通常采用GPS，雷达，照相机，摄像机等传感器采集数据<h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4></li><li>对收集到的数据进行整理、分类、清洗，进而得到没有任何语义信息或者注释的初始地图模板<h4 id="对象检测"><a href="#对象检测" class="headerlink" title="对象检测"></a>对象检测</h4></li><li>根据处理后生成的原始数据，使用AI技术检测静态对象并对其进行分类，如车道线，交通标志等。<h4 id="手动验证"><a href="#手动验证" class="headerlink" title="手动验证"></a>手动验证</h4><ol><li>对于AI标注的数据，需要进行手动验证，保证准确性并及时发现问题。<h4 id="地图发布"><a href="#地图发布" class="headerlink" title="地图发布"></a>地图发布</h4><h3 id="高精地图用处"><a href="#高精地图用处" class="headerlink" title="高精地图用处"></a>高精地图用处</h3><h4 id="高精地图用于导航"><a href="#高精地图用于导航" class="headerlink" title="高精地图用于导航"></a>高精地图用于导航</h4>高精地图可以根据用户的目标，智能规划出几条路线，甚至可以考虑到路线的拥挤程度，交通管制（信号灯，限速等）等种种因素。<br>不同于一般导航地图，高精地图的精确度往往需要厘米级别，而一般导航地图往往只有米级别的精确度。<h4 id="高精地图用于定位"><a href="#高精地图用于定位" class="headerlink" title="高精地图用于定位"></a>高精地图用于定位</h4>高精地图用于定位类似于拼图游戏，找到一块小拼图在整个地图上的位置，通常情况下有一下几步：</li></ol></li><li>车辆会首先根据摄像头和雷达找到当前位置的地标（比如周围建筑等），得到地标的图片数据和三维点云数据等</li><li>车辆将地标数据和高精地图上已知的地标进行比较该步骤往往需要如下处理过程：<ol><li>数据预处理，消除不准确或质量差的数据</li><li>坐标转换，将来自不同视角的数据转换到统一坐标系</li><li>数据融合，将来自各种车辆和传感器的数据合并<h4 id="高精地图用于感知"><a href="#高精地图用于感知" class="headerlink" title="高精地图用于感知"></a>高精地图用于感知</h4>车辆上的传感器受限于物理或环境等因素，无法感知到极远处的信息（如同人不能看到和听到很远处的情况一样），而高精地图可以直接告诉车辆远处的交通信息。<h4 id="高精地图用于行驶规划"><a href="#高精地图用于行驶规划" class="headerlink" title="高精地图用于行驶规划"></a>高精地图用于行驶规划</h4></li></ol></li><li>高精地图可以明确的告诉车辆中心线的位置，进而车辆可以尽可能的靠中间行驶，也可以提前告诉限速标志的路段进而车辆可以更早的做准备。</li></ol><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p>车辆将传感器识别的地标，与高精度地图上存在的地标进行对比。为了进行对比，必须能够在它自身坐标系和地图坐标系之间转换数据，然后系统必须在地图上以十厘米的精度确定车辆的精确位置。通常通过以下几种方式进行定位。</p><h3 id="GNSS-RTK"><a href="#GNSS-RTK" class="headerlink" title="GNSS RTK"></a>GNSS RTK</h3><p>使用GPS进行定位</p><h3 id="惯性导航定位"><a href="#惯性导航定位" class="headerlink" title="惯性导航定位"></a>惯性导航定位</h3><p>通过对汽车初始位置，初始速度，加速度等信息进行计算，得到任意时刻汽车的位置，即为惯性导航。</p><h3 id="激光雷达定位"><a href="#激光雷达定位" class="headerlink" title="激光雷达定位"></a>激光雷达定位</h3><p>将激光雷达收集到的点云数据和预先存在的HD map进行匹配，进而从HD地图得到定位信息。</p><h3 id="视觉定位"><a href="#视觉定位" class="headerlink" title="视觉定位"></a>视觉定位</h3><p>根据通过摄像头手机的信息准确定位汽车。常用的算法包括粒子滤波算法。使用视觉定位的优点是视觉数据是最容易收集的数据，缺点是视觉数据缺失了三维信息特征。</p><h2 id="感知"><a href="#感知" class="headerlink" title="感知"></a>感知</h2><p>汽车想要实现自动驾驶功能的第一个需求就是感知，汽车需要能够知道周围的环境。这里会使用很多计算机视觉的技术帮助汽车感知和理解周围环境</p><h3 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h3><p>在感知环节中，计算机视觉被大量应用，主要有以下几个方面：</p><ol><li>检测：找出物品在环境中的位置</li><li>分类：明确对象是什么</li><li>跟踪：随时间的推移观察移动的物体（如其他汽车，行人等）</li><li>语义分割：将图像中的每个像素与语义类别进行匹配（如道路，汽车，天空等），进而可以尽可能详细的了解环境，并确定车辆可以行驶的区域<h3 id="数据捕捉"><a href="#数据捕捉" class="headerlink" title="数据捕捉"></a>数据捕捉</h3>通常情况下，自动驾驶汽车不仅仅使用摄像机，照相机捕捉周围环境的图片，也会使用雷达，激光雷达等传感器捕捉三维点云图（包含了更多距离信息）</li></ol><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>在自动驾驶车辆行驶过程中，车辆周围的物体也在不断运动（如其它汽车，行人等），无人车需要预测这些可移动物体接下来的行为，这样才能确保无人车做出最佳决策。<br>自动驾驶中的无人技术的一个特殊性是实时性，也就是说，预测算法的延迟要越短越好</p><h3 id="基于模型的预测"><a href="#基于模型的预测" class="headerlink" title="基于模型的预测"></a>基于模型的预测</h3><p>基于模型的预测算法将首先对被观察的物体所有可能的行为进行建模，并假设物体接下来的行为是等概率的。然后继续观察物体的行为，并不断调整各个预测行为模型的准确率，看物体与哪个模型的预测行为更加匹配。</p><p>基于模型的预测方法的优点是可解释性更强，更加直观。结合了现有的物理知识和交通法规，人类行为等多方面知识。</p><h3 id="基于数据驱动的预测"><a href="#基于数据驱动的预测" class="headerlink" title="基于数据驱动的预测"></a>基于数据驱动的预测</h3><p>数据驱动算法使用机器学习算法，通过观察结果来训练模型，数据驱动方法的优点是训练数据越多，模型的准确率越高。</p><h3 id="车道序列模型"><a href="#车道序列模型" class="headerlink" title="车道序列模型"></a>车道序列模型</h3><p>通常情况下，我们不会直接对一个物体每时每刻的行为或移动位置进行预测，这样会使得模型过于复杂。常见的做法是构建车道序列模型：将车道分割成多个片段，然后预测物体在各个车道片段的移动规律，这也就是更粗粒度上的目标行为预测。这样讲就将问题转换成了时间序列预测模型。<br>在实践中，通常使用RNN神经网络对目标的行为序列进行预测。</p><h3 id="轨迹生成"><a href="#轨迹生成" class="headerlink" title="轨迹生成"></a>轨迹生成</h3><p>在得到车辆在每个指定时间点的预测位置后，需要结合物理运动规律，生成两个相邻时间点车辆的运行轨迹。</p><h2 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h2><p>在规划模块，我们会结合HD地图，定位和预测来构建车辆轨迹，规划模块的目标是生成避免碰撞和舒适的可执行轨迹，一般用一系列带关联速度的点序列来表示，每个点也包含一个指示何时应帝大那个点的时间戳。<br>规划模块一共分为如下几步：</p><ol><li>路线导航，侧重于如何从地图的A点到B点</li><li>轨迹规划，得到导航路线后，通过微妙的决策来驾驶车辆以避免障碍物</li></ol><p>路线规划一共有三个输入：</p><ol><li>地图，包括公路网和实时交通信息</li><li>在地图上的当前位置</li><li>目的地</li></ol><p>通常情况下，使用图搜索方法来寻找到达目的地的最优路径。在开始搜索前，通常将地图数据重新格式化成图（只包含点和边）的数据结构，然后可以使用已经存在的大量图搜索算法来搜索最优路径（A star等算法）</p><h3 id="轨迹生成-1"><a href="#轨迹生成-1" class="headerlink" title="轨迹生成"></a>轨迹生成</h3><p>在得到从起始点到终点的路径后，需要生成更细粒度的轨迹，更细粒度意味着在每条街道，每个拐口的具体轨迹。<br>轨迹生成的目的是生成由一系列路径点所定义的轨迹，每个路径点都有一个时间戳，速度和方向。然后拟合各个路径点最后生成轨迹的几何表征。而在无人驾驶车辆运行时，生成的路径点序列可能会被其他的可移动障碍物（其它车辆，行人等）暂时阻挡。</p><h4 id="轨迹约束"><a href="#轨迹约束" class="headerlink" title="轨迹约束"></a>轨迹约束</h4><p>在现实中，生成的轨迹会有很多约束：</p><ol><li>轨迹应能免于相撞，也就是说保证无障碍物。</li><li>让乘客感到舒适，所以需要轨迹和速度的变化尽可能平滑。</li><li>轨迹应该对车辆实际可行，避免如立即180度转弯等问题。</li><li>合法，轨迹应该符合所在道路的交通规则。<h4 id="Frenet坐标系"><a href="#Frenet坐标系" class="headerlink" title="Frenet坐标系"></a>Frenet坐标系</h4>如何表示车辆在路上的位置？最直接的方法是建立笛卡尔坐标系，也就是说给车辆一个(x,y)坐标，但这个方法的缺点是我们不知道路的坐标，所以也无法简单的计算车在公路上的具体位置。<br>笛卡尔坐标系的替代方法是Frenet坐标系，Frenet坐标系描述了汽车相对于道路的位置，在Frenet框架中，s表示沿道路的距离，被称为纵坐标，d表示与道路方向垂直的位移，被称为横坐标。</li></ol><h4 id="路径规划和速度规划"><a href="#路径规划和速度规划" class="headerlink" title="路径规划和速度规划"></a>路径规划和速度规划</h4><p>将轨迹规划分割成两个子问题分别讨论：路径规划和速度规划。<br>路径规划首先生成多个候选路径，然后使用成本函数对每条路径进行评估，该函数包含平滑度，安全性，道路中心偏离等因素。然后选择成本函数最低的路径作为最优路径规划。<br>然后需要做的是确定沿着这条路径行进的速度。我们将这个路径上的速度称为“速度曲线”</p><h5 id="ST图"><a href="#ST图" class="headerlink" title="ST图"></a>ST图</h5><p>我们使用ST图帮助设计和选择速度曲线</p>]]></content>
      
      <categories>
          
          <category> self-driving </category>
          
      </categories>
      
      
        <tags>
            
            <tag> self-driving </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python单元测试</title>
      <link href="/2018/11/23/Programming%20Language/Python/Python%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"/>
      <url>/2018/11/23/Programming%20Language/Python/Python%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<p>总结python单元测试包unittest的基本用法。</p><a id="more"></a><h2 id="python单元测试"><a href="#python单元测试" class="headerlink" title="python单元测试"></a>python单元测试</h2><p>Python标准库中的模块unittest 提供了代码测试工具。单元测试 用于核实函数的某个方面没有问题;测试用例 是一组单元测试，这些单元测试一起核实函数在各种情形下的 行为都符合要求。</p><p>创建测试用例的语法需要一段时间才能习惯，但测试用例创建后，再添加针对函数的单元测试就很简单了。要为函数编写测试用例，可先导入模块unittest 以及要测试的函<br>数，再创建一个继承unittest.TestCase 的类</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import unittest</span><br><span class="line">from test_func import add_func, subtract_func</span><br><span class="line"></span><br><span class="line">class AddFuncTestCase(unittest.TestCase):</span><br><span class="line">    def test_add_func(self):</span><br><span class="line">        result = add_func(1,2)</span><br><span class="line">        self.assertEqual(result, 3)</span><br><span class="line">    def test_subtract_func(self):</span><br><span class="line">        result = subtract_func(1, 2)</span><br><span class="line">        self.assertEqual(result, -1)</span><br><span class="line"></span><br><span class="line">unittest.main()  #让Python运行这个文件中的测试。会在命令行打印测试个数，时间，是否通过等信息</span><br></pre></td></tr></table></figure><p>unittest.TestCase中提供如下常用断言方法：</p><ul><li>assertEqual(a, b) 核实a == b</li><li>assertNotEqual(a, b) 核实a != b</li><li>assertTrue(x) 核实x 为True</li><li>assertFalse(x) 核实x 为False</li><li>assertIn(item , list ) 核实 item 在 list 中</li><li>assertNotIn(item , list ) 核实 item 不在 list 中</li></ul>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++ STL映射字典</title>
      <link href="/2018/11/21/Programming%20Language/CPP/C++%20STL%E6%98%A0%E5%B0%84%E5%AD%97%E5%85%B8/"/>
      <url>/2018/11/21/Programming%20Language/CPP/C++%20STL%E6%98%A0%E5%B0%84%E5%AD%97%E5%85%B8/</url>
      <content type="html"><![CDATA[<p>C++ STL中的字典，类似于Java中的TreeMap或者Python中的dict。这里介绍C++ map的常见用法。<br><a id="more"></a><br>所有元素会根据key值排序（默认升序，如果键值为字符串就是字典序），map中的所有元素都是pair，同时拥有实值和键值，pair的first为键值，second为实值，底层将它的first作为红黑树的排序key。<br>map不允许有两个相同键值的元素。map的迭代器不能修改键值，但可以修改实值。主要用于处理带有键值的记录性元素数据的快速插入、删除和检索。</p><h2 id="增删改查基本操作"><a href="#增删改查基本操作" class="headerlink" title="增删改查基本操作"></a>增删改查基本操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;map&gt;</span><br><span class="line">map&lt;string, double&gt; m;</span><br><span class="line"></span><br><span class="line">m[&quot;abc&quot;] = 1.0; // 插入一个键值对</span><br><span class="line">m.erase(&quot;abc&quot;)  // 删除一个键值对</span><br><span class="line">m.clear()     // 清空map</span><br></pre></td></tr></table></figure><h2 id="迭代遍历"><a href="#迭代遍历" class="headerlink" title="迭代遍历"></a>迭代遍历</h2><ul><li><p>使用map迭代器进行遍历</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">map&lt;string, double&gt;::iterator it;</span><br><span class="line">for(it=m.begin(); it!=m.end(); it++)</span><br><span class="line">    cout&lt;&lt;(*it).first&lt;&lt;&quot;:&quot;&lt;&lt;(*it).second&lt;&lt;endl;</span><br></pre></td></tr></table></figure></li><li><p>使用for each遍历容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for each(std::pair&lt;string, double&gt; pa in m)&#123;</span><br><span class="line">    cout&lt;&lt;pa.first&lt;&lt;&quot;:&quot;&lt;&lt;pa.second&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="判断是否包含在map中"><a href="#判断是否包含在map中" class="headerlink" title="判断是否包含在map中"></a>判断是否包含在map中</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if(m.count(&quot;abc&quot;))&#123;</span><br><span class="line">    //用count函数来判定键值是否出现，count函数的返回值只有两个，键值出现返回1，否则返回0</span><br><span class="line">    cout&lt;&lt;&quot;NOT in map&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line">if(m[&quot;abc&quot;])&#123;</span><br><span class="line">    cout&lt;&lt;&quot;NOT in map&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上下两个判断是否包含键值的方法是相同的。使用count(key)和m[key]两种方法会返回是否包含键值。</p><h2 id="map大小"><a href="#map大小" class="headerlink" title="map大小"></a>map大小</h2><ul><li>m.empty() 判断是否为空</li><li>m.size() 返回元素个数</li><li>m.max_size() 返回map最大空间大小</li></ul><h2 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h2><p>可以直接用m[key]来访问元素，也可以用内置函数m.at(key)访问元素。两者的区别是通过后者的方法无法修改key对应的值。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++ vector容器</title>
      <link href="/2018/11/12/Programming%20Language/CPP/C++%20vector%E5%AE%B9%E5%99%A8/"/>
      <url>/2018/11/12/Programming%20Language/CPP/C++%20vector%E5%AE%B9%E5%99%A8/</url>
      <content type="html"><![CDATA[<p>向量（Vector）是一个封装了动态大小数组的顺序容器（Sequence Container）。跟任意其它类型容器一样，它能够存放各种类型的对象。可以简单的认为，向量是一个能够存放任意类型的动态数组。</p><a id="more"></a><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ol><li>顺序序列</li><li>动态数组</li><li>动态感知内存分配器</li></ol><h2 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h2><ol><li>vector<templete t=""> v1; 声明一个空的vector</templete></li><li>vector<templete t=""> v2(int nSize); 声明一个指定大小的vector</templete></li><li>vector<templete t=""> v3(int nSize, const t&amp; t); 创建一个vector，元素个数为nSize, 且值均为t</templete></li><li>vector<templete t=""> v4(v3); 用已经存在的vector创建新的vector。</templete></li><li>vector<templete t=""> v5(v3.begin()+1, v3.begin()+4); 用已存在的vector中的一部分创建。</templete></li><li>vector<templete t=""> v6(arr, arr+4);用已经存在的数组创建vector。<h3 id="C-11新特性"><a href="#C-11新特性" class="headerlink" title="C++11新特性"></a>C++11新特性</h3>C++11提供了直接根据列表进行初始化的方法。</templete></li><li>vector<int> v{1,2,3};</int></li><li>vector<int> v = {1,2,3};<h3 id="二维数组创建"><a href="#二维数组创建" class="headerlink" title="二维数组创建"></a>二维数组创建</h3>创建二维数组的两种方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int N=5, M=6;</span><br><span class="line">vector&lt;vector&lt;int&gt;&gt; matrix(N);</span><br><span class="line">for(int i=0; i&lt;matrix.size(); i++)&#123;</span><br><span class="line">    matrix[i].resize(M);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></int></li></ol><p>或者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int N=5, M=6;</span><br><span class="line">vector&lt;vector&lt;int&gt;&gt; matrix(N, vector&lt;int&gt;(M));</span><br></pre></td></tr></table></figure></p><h2 id="增加函数"><a href="#增加函数" class="headerlink" title="增加函数"></a>增加函数</h2><ol><li>v.push_back(const T&amp; x) 尾部添加一个元素</li><li>v.insert(v.beign()+4, const T&amp; t) 向指向位置插入一个元素t</li><li>v.insert(b.end()-1, int n, const T&amp; t) 向指向位置插入n个元素t</li></ol><h2 id="删除函数"><a href="#删除函数" class="headerlink" title="删除函数"></a>删除函数</h2><ol><li>v.erase(v.begin()+2):删除vector指定位置的元素</li><li>v.erase(v.begin()+2, v.begin()+4):删除vector中[first,last)中元素</li><li>v.pop_back():删除向量中最后一个元素</li><li>v.clear():清空向量中所有元素</li></ol><h2 id="遍历函数"><a href="#遍历函数" class="headerlink" title="遍历函数"></a>遍历函数</h2><ol><li><p>使用下标</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; v(10,1);</span><br><span class="line">for(int i=0; i&lt;v.size(); i++)&#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用迭代器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; v(10,1);</span><br><span class="line">for(vector&lt;int&gt;::iterator iter=v.begin(); iter!=v.end(); iter++)&#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用auto关键字</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; v(10, 1);</span><br><span class="line">for(auto iter=v.begin(); iter!=v.end(); iter++)&#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用auto关键字2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; v(10, 1);</span><br><span class="line">for(auto i : v)&#123;&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="大小函数"><a href="#大小函数" class="headerlink" title="大小函数"></a>大小函数</h2><ul><li>v.empty() 判断是否为空</li><li>int size() const:返回向量中元素的个数</li><li>int capacity() const:返回当前向量中所能容纳的最大元素值</li></ul><h2 id="algorithm中的辅助函数"><a href="#algorithm中的辅助函数" class="headerlink" title="algorithm中的辅助函数"></a>algorithm中的辅助函数</h2><p>通过 ‘#include<algorithm>‘可以使用一些算法对vector中的元素进行操作</algorithm></p><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><ul><li>sort(vec.begin(), vec.end())  对vector中元素排序。在用sort函数进行排序时，可以通过指定比较方式，实现复杂数据的排序或满足特定的排序内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">int compare(vector&lt;int&gt; a, vector&lt;int&gt; b)&#123;</span><br><span class="line">    if(a[0] &gt; b[0])</span><br><span class="line">        return true;</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line">vector&lt;vector&lt;int&gt;&gt; vec(10, vector&lt;int&gt;(2, 1))</span><br><span class="line">//vec中的元素为int型的vector，指定用vector的第一个元素从大到小排序</span><br><span class="line">sort(vec.begin(), vec.end(), compare);</span><br></pre></td></tr></table></figure></li></ul><h4 id="翻转数组"><a href="#翻转数组" class="headerlink" title="翻转数组"></a>翻转数组</h4><ul><li>reverse(vec.begin(), vec.end()) 对vector元素进行翻转<h4 id="查找指定元素"><a href="#查找指定元素" class="headerlink" title="查找指定元素"></a>查找指定元素</h4></li><li>iterator find(v.begin(), v.end(), target_value);  查找成功会返回一个指向该元素的迭代器，查找失败返回end迭代器。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto index = find(vec.beign(), vec.end(), target_value);</span><br><span class="line">if(index != vec.end())&#123;</span><br><span class="line">    cout&lt;&lt;*index&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="查找最大最小元素"><a href="#查找最大最小元素" class="headerlink" title="查找最大最小元素"></a>查找最大最小元素</h4><ul><li><p>iterator max_element(v.begin(), v.end()) 返回vector中最大元素的位置的指针</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; a = &#123; 2,4,6,7,1,0,8,9,6,3,2 &#125;;</span><br><span class="line">    auto maxPosition = max_element(a.begin(), a.end());</span><br><span class="line">    cout &lt;&lt; *maxPosition &lt;&lt; &quot; at the postion of &quot; &lt;&lt; maxPosition - a.begin() &lt;&lt;endl;</span><br></pre></td></tr></table></figure></li><li><p>iterator min_element(v.begin(), v.end())</p></li></ul>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Cmake和makefile</title>
      <link href="/2018/11/02/Programming%20Language/CPP/Cmake%E5%92%8Cmakefile/"/>
      <url>/2018/11/02/Programming%20Language/CPP/Cmake%E5%92%8Cmakefile/</url>
      <content type="html"><![CDATA[<p>什么是Cmake以及makefile。</p><a id="more"></a><h3 id="makefile文件"><a href="#makefile文件" class="headerlink" title="makefile文件"></a>makefile文件</h3><p>makefile关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为makefile就像一个Shell 脚本一样，其中也可以执行操作系统的命令。<br>makefile带来的好处就是——“自动化编译”，一旦写好，只需要一个make命令，整个工程完全自动编译，极大的提高了软件开发的效率。make是一个命令工具，是一个解释makefile中指令的命令工具，一般来说，大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。可见，makefile都成为了一种在工程方面的编译方法。</p><h3 id="Cmake"><a href="#Cmake" class="headerlink" title="Cmake"></a>Cmake</h3><p>开发过程中当修改了少量几个文件后，往往只需要重新编译、生成少数几个文件。有效地描述这些文件之间的依赖关系以及处理命令，当个别文件改动后仅执行必要的处理，而不必重复整个编译过程，可以大大提高软件开发的效率。<br><strong>Cmake是用来makefile的一个工具：读入所有源文件之后，自动生成makefile。</strong></p><h3 id="cmake与gcc"><a href="#cmake与gcc" class="headerlink" title="cmake与gcc"></a>cmake与gcc</h3><p>.gcc是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。<br>当你的程序只有一个源文件时，直接就可以用gcc命令编译它。但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大，所以出现了make工具。<br>make工具可以看成是一个智能的批处理工具，<strong>它本身并没有编译和链接的功能</strong>，而是用类似于批处理的方式—通过调用makefile文件中用户指定的命令来进行编译和链接的。<br>makefile文件中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。<br>makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。<br>这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。<br>可是cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。到最后CMakeLists.txt文件由程序员手动编写。<br>当然如果使用IDE，类似VS这些一般它都能帮你弄好了，你只需要按一下那个三角形</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Unix编程实践教程</title>
      <link href="/2018/11/01/Unix%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%95%99%E7%A8%8B/"/>
      <url>/2018/11/01/Unix%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%95%99%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>《Unix/Linux编程实践教程》的读书笔记。</p><a id="more"></a><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><h3 id="为什么要有操作系统"><a href="#为什么要有操作系统" class="headerlink" title="为什么要有操作系统"></a>为什么要有操作系统</h3><p>在现代计算机中，会有多个程序同时在内存中运行，而有些程序需要访问资源，有些需要占用设备，如果任由各个程序随便访问计算机中的各种资源很显然会引起计算机的崩溃。而操作系统就是用户程序和计算机内部设备之间的桥梁。</p><p>操作系统也是一种程序，所有的程序都必须在内存中才可以运行，用来容纳操作系统的内存空间叫做系统空间，容纳用户程序的内存叫做用户空间。操作系统也被称为内核。</p><p>由于内核是用户程序和计算机设备之间的桥梁，任何程序想要访问设备都需要通过内核进行。</p><h3 id="系统编程"><a href="#系统编程" class="headerlink" title="系统编程"></a>系统编程</h3><p>在编写普通程序时可以认为，程序时直接连接到各个设备的（因为操作系统自动完成了这个过程），但在进行系统编程时，必须对系统的结构和工作方式有更深的了解，知道内核提供哪些服务（系统调用），如何使用这些API，系统都有哪些资源和设备，以及不同设备之间如何操作。</p><h4 id="系统资源"><a href="#系统资源" class="headerlink" title="系统资源"></a>系统资源</h4><ol><li>处理器</li><li>输入输出(I/O)</li><li>进程管理</li><li>内存</li><li>设备</li><li>计时器</li><li>进程间通信</li><li>网络</li></ol><h3 id="从用户角度理解Unix"><a href="#从用户角度理解Unix" class="headerlink" title="从用户角度理解Unix"></a>从用户角度理解Unix</h3><p>整个Unix的使用过程一般如下：用户登录–运行程序–用户退出。下面分别看这几个步骤Unix系统做了什么。</p><p>Unix允许多个用户同时登录到系统，当用户名和密码验证通过后，Unix会启动一个叫shell的进程并将用户交给这个进程，用这个进程处理用户的请求，每个用户都有属于自己的shell进程。<br>shell会在屏幕上显示提示符，一般为$，用户可以在提示符后输入要运行的程序，内核负责把用户的输入传给shell。当用户注销时，内核会结束所有分配给这个用户的进程。</p><h3 id="more指令"><a href="#more指令" class="headerlink" title="more指令"></a>more指令</h3><p>这里，编写了一个简单版本的 ‘more’指令，来看整个Unix的具体运行过程，more.c的代码在如下URL。<br><a href="https://github.com/YHfeather" target="_blank" rel="noopener">simple_more.c</a></p><h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><p>在使用Unix时，需要经常确认那些用户正在使用系统，通过who命令可以显示系统中活动用户的情况。<br>本章通过分析Unix中的who命令学习Unix的文件系统。</p><h3 id="命令即程序"><a href="#命令即程序" class="headerlink" title="命令即程序"></a>命令即程序</h3><p>一个补充知识是：在Unix中，几乎所有命令都是人为用C语言编写的程序，当在命令行输入命令时，shell就知道你想要运行的命令对应的程序，如果你认为对某个命令提供的功能不满意（比如ls， cd等），完全可以自己编写自己的ls，cd命令。<br>在 Unix 系统中增 加新的命令是一件很容易的事 。 把 程序的可执行文件放到以下任意 一个目录就可以了: /bin、/usr/bin、/usr/local/bin，这些目录里面存放着很多系统命令 。 Unix 系统中一开始并没有这么多的命令， 一 些人编写程序用来解决某个特定的问题 ，而其 他 人也觉得这个程序很有用，随着越来越多的使用，这个程序就逐渐成了 Unix 的标准命令 。</p><h3 id="who命令如何工作"><a href="#who命令如何工作" class="headerlink" title="who命令如何工作"></a>who命令如何工作</h3><p>你可能会认为，像 who 这样的系统程序一定会用到一些特殊的系统调用，需要高级管理 员的权限，要编写这样的程序得要花很多钱来购买系统开发工具，包括光盘、参考书等。<br>实际上，所需的资料都在系统中，你要知道的仅仅是如何找到这些资料。<br>通过’man who’阅读联机帮助，就可以得到很多有用的信息。</p><blockquote><p>已登录用户的信息是放在文件/var/ adm/utmp 中的，who 通过读该文件 获得信息。</p></blockquote><p>继续查找utmp文件的帮助：’man -k utmp’可以得到如下信息：</p><blockquote><p> utmp 这个文件里面保存的是结构数组，数组元素是 utmp 类型的结构，可 以在 utmp. h 中找到 utmp 类型的定义</p></blockquote><p>接下来的问题是: utmp. h 这个文件在哪里?</p><blockquote><p>可以在 FILES 部分找到 utmp. h 这个文件的位置，在/usr/include/utmp.h 目 录里。</p></blockquote><h4 id="阅读-h文件"><a href="#阅读-h文件" class="headerlink" title="阅读.h文件"></a>阅读.h文件</h4><p>从 utmp 的联机帮助中可以知道， utmp 中的数据结构定义在 /usr/include/utmp. h 中。 在 Unix 系统中，大多数的头文件都存放在 /usr/include 这个目录里，当 C 语言编译器在源程 序中发现如下的定义:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># include &lt;stdio. h&gt;</span><br></pre></td></tr></table></figure></p><p>它会到/usr/include 中寻找相应的头文件。</p><h4 id="使用open，read和close"><a href="#使用open，read和close" class="headerlink" title="使用open，read和close"></a>使用open，read和close</h4><p>使用上述 3 个系统调用可以从 utmp 文件中取得用户登录信息。</p><h5 id="使用open打开一个文件。"><a href="#使用open打开一个文件。" class="headerlink" title="使用open打开一个文件。"></a>使用open打开一个文件。</h5><p>这个系统调用在进程和文件之间建立 一 条连接，这个连接被称为文件描述符，它就像 一 条由进程通向内核的管道。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;fcntl.h&gt;</span><br><span class="line">int fd = open(char *name, int how)</span><br><span class="line">// name 表示文件名，how表示打开方式，返回值叫做文件描述符，为-1时表示遇到错误。</span><br><span class="line">// how=O_RDONLY; O_WRONLY; O_RDWR. 分别表示只读，只写，读写</span><br></pre></td></tr></table></figure></p><p>打开文件是内核提供的服务，如果在打开过程中内核检测到任何错误，这个系统调用就 会返回 -1。<br>当→个文件已经被打开，是否允许再次打开呢?这种情况发生在有多个进程要同时访 问一个文件的时候。 Unix 并不禁止一个文件同时被多个进程访问，如果禁止的话，那两个用 户就无法同时使用 who命令了。</p><h5 id="从文件读取数据：read"><a href="#从文件读取数据：read" class="headerlink" title="从文件读取数据：read"></a>从文件读取数据：read</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;unistd.h&gt;</span><br><span class="line">ssize_t numread = read(int fd, void *buf, size_t qty)</span><br><span class="line">fd 为open返回的文件描述符</span><br><span class="line">buf表示用来存放数据的缓冲区</span><br><span class="line">qty表示要读取的字节数</span><br><span class="line">返回-1时表示读取错误</span><br></pre></td></tr></table></figure><p>read 这个系统调用请求内核从 fd 所指定的文件中读取 qty 字节的数据，存放到 buf 所指定的内存空间中，内核如果成功地读取了数据，就返回所读取的字节数目，否则返回-1.</p><h5 id="关闭一个文件：close"><a href="#关闭一个文件：close" class="headerlink" title="关闭一个文件：close"></a>关闭一个文件：close</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#inlcude&lt;unistd.h&gt;</span><br><span class="line">int result = close(int fd)</span><br><span class="line">fd 为文件描述符</span><br><span class="line">返回-1表示遇到错误，返回0表示成功关闭</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Unix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unix </tag>
            
            <tag> C </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>从RCNN到Faster-RCNN</title>
      <link href="/2018/10/14/Deep%20Learning/CV/%E4%BB%8ERCNN%E5%88%B0Faster-RCNN/"/>
      <url>/2018/10/14/Deep%20Learning/CV/%E4%BB%8ERCNN%E5%88%B0Faster-RCNN/</url>
      <content type="html"><![CDATA[<p>总结图像分割识别算法从RCNN到 Faster RCNN的原理和演化过程。</p><a id="more"></a><p>图像分割识别的主要问题有两个：1.找到候选框；2.识别候选框中的物体。可以说所有图像分割识别的算法都是为了解决这两个问题。经典的图像分割识别算法即定义大小不同的窗口，在图片上进行滑动并对窗口内的物体进行判断，但这种遍历所有可能窗口的方法由于窗口冗余，识别起来非常慢。而目前的研究改为仅对可能是物体的候选区域进行识别，减少了候选框的数量，加快了模型的运行速度。以YOLO，SSD算法为代表的one-stage算法是将两个问题捏合成一个问题，用回归思想一步到位。而RCNN系列算法则是two-stage算法，即使用不同模型对两个问题分开处理。</p><p>整个RCNN系列算法的框架是一样的，主要都是一下四个步骤：</p><ol><li>找到候选框</li><li>提取图片特征</li><li>对候选框中的物体进行分类</li><li>对候选框位置进行精修<br>可以说三个RCNN算法都是以上的步骤流程，不同的是各个步骤使用的模型不相同，本文从RCNN出发，介绍RCNN，Fast-RCNN，Faster-RCNN的算法原理。</li></ol><h2 id="R-CNN算法"><a href="#R-CNN算法" class="headerlink" title="R-CNN算法"></a>R-CNN算法</h2><p>RCNN算法使用Selective Search算法对原始图片提取可能的候选框，然后使用CNN提取每个候选框的特征，最后使用SVM对候选框特征进行分类，同时使用一个回归器对候选框的位置进行精修。</p><h3 id="两个数据集"><a href="#两个数据集" class="headerlink" title="两个数据集"></a>两个数据集</h3><ol><li>一个较大的图片识别分类库，标定每张图片中物体的类别</li><li>一个较小的图片分割检测库，标定每张图片中物体的类别和位置<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3>RCNN算法有四个步骤</li><li>使用Selective Search算法对一张图片生成1k~2k个候选区域框</li><li>对于每个候选区域，使用CNN提取特征</li><li>特征送入每一类的SVM分类器，判断是否属于该类</li><li>使用回归器精细修正候选框的位置<h3 id="候选框生成"><a href="#候选框生成" class="headerlink" title="候选框生成"></a>候选框生成</h3>使用了Selective Search方法从一张图像生成约2000-3000个候选区域。基本思路如下：</li></ol><ul><li>使用一种过分割手段，将图像分割成小区域</li><li>查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置</li><li>输出所有曾经存在过的区域，所谓候选区域<br>候选区域生成和后续步骤相对独立，实际可以使用任意算法进行。<br>上述合并规则只涉及区域的颜色直方图、纹理直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><h4 id="模型预训练"><a href="#模型预训练" class="headerlink" title="模型预训练"></a>模型预训练</h4>首先使用大的图片分类数据集，对一个CNN分类模型进行训练：输入一张照片，输出1000维的类别标号。也可以下载训练好的CNN模型。<h4 id="调优训练"><a href="#调优训练" class="headerlink" title="调优训练"></a>调优训练</h4>首先把候选框归一化成统一尺寸227×227。<br>然后对预训练的CNN模型进行修改，将全连接的输出层由1000维改为21维（表示20类+背景）。<br>然后使用候选框对预训练的CNN模型进行调优训练。<br>在模型调优训练结束后，输入一个候选框，就可以得到该候选框的特征向量表示。<h3 id="类别判断"><a href="#类别判断" class="headerlink" title="类别判断"></a>类别判断</h3><h4 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h4>然后对每种target，使用一个线性SVM二类分类器进行判别。输入为CNN输出的特征表示，输出是否属于此类。<h4 id="位置精修"><a href="#位置精修" class="headerlink" title="位置精修"></a>位置精修</h4>通过Selective Search方法得到的候选框往往不是最完美的候选框。 解决方法是构建一个回归器，找到最优候选框。<br>对每一类目标，使用一个线性脊回归器进行精修。<br>输入为CNN输出的候选框特征表示，输出为（dx, dy, dw, dh），分别表示坐标(x,y)和宽高。<h3 id="RCNN的三个问题"><a href="#RCNN的三个问题" class="headerlink" title="RCNN的三个问题"></a>RCNN的三个问题</h3></li></ul><ol><li>测试速度慢，RCNN中一张图像内候选框大量重叠，提取特征操作冗余</li><li>训练速度慢，理由同上</li><li>训练所需内存大，RCNN中独立的分类器和回归器需要大量特征作为训练样本<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3>成功将深度学习引入检测领域，RCNN的前两个步骤（候选区域提取+特征提取）与待检测类别无关，可以在不同类之间共用。这两步在GPU上约需13秒。<br>同时检测多类时，需要倍增的只有后两步骤（判别+精修），都是简单的线性运算，速度很快。这两步对于100K类别只需10秒。</li></ol><h2 id="Fast-R-CNN算法"><a href="#Fast-R-CNN算法" class="headerlink" title="Fast R-CNN算法"></a>Fast R-CNN算法</h2><p>在RCNN的基础上，训练时间从84小时减少为9.5小时，测试时间从47秒减少为0.32秒。但准确率相差无几。</p><h3 id="Fast-RCNN的改进"><a href="#Fast-RCNN的改进" class="headerlink" title="Fast RCNN的改进"></a>Fast RCNN的改进</h3><p>原始RCNN对于每个图片会先用Selective Search方法生成2k个候选框，然后对每个候选框都进行特征提取，这是整个RCNN最耗时的地方。<br>我们完全可以直接对图片提取一次卷积特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后对卷积层的输出feature map上的region proposal进行回归和分类。<br>与R-CNN框架图对比，可以发现主要有两处不同：</p><ol><li>卷积操作后引入ROI池化层，使得网络的输入图像可以是任意尺寸，输出则不变，是一个固定的维度。</li><li>损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练。<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3></li><li>直接将原始图像归一化，然后输入到CNN中，提取整个图片的特征。</li><li>将创建候选框的方法直接应用到卷积操作输出的特征图上。</li><li>将特征图上的每个region proposal进行ROI池化，得到统一大小的输出特征。</li><li>将固定大小的region proposal特征输入到全连接层，进行分类和回归定位<br>整个流程可以用如下伪代码表示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feature_map = cnn(image)</span><br><span class="line">region_feature_maps = region_proposal(feature_map)</span><br><span class="line">for rf_map in region_feature_maps:</span><br><span class="line">    patch = roi_pooling(rf_map)</span><br><span class="line">    class_pred, local_pred = full_connect(patch)</span><br></pre></td></tr></table></figure></li></ol><h3 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h3><p>通过ROI池化，可以得到每个region proposa固定大小的特征表示，然后将其输入到全连接层，最后连接两个并列的的输出层，大小分别为21和84，前者是分类的输出，代表每个region proposal属于每个类别（20 + 1类）的得分，后者是回归的输出，代表每个region proposal的四个坐标。<br>整个算法流程如图所示：<br><img src="/images/deep_learning/fast_rcnn.jpg" alt="fast rcnn"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>Fast RCNN通过减少卷积特征提取的冗余操作，大大减少了模型的训练和测试时间，尤其是将测试一张照片的时间控制在3秒内，使得Fast RCNN有了被实时应用的可能。</p><h2 id="Faster-R-CNN算法"><a href="#Faster-R-CNN算法" class="headerlink" title="Faster R-CNN算法"></a>Faster R-CNN算法</h2><p>RCNN算法和Fast R-CNN都依赖于外部候选区域方法，如selective search。但这些算法在 CPU 上运行且速度很慢。在测试中，Fast R-CNN 需要 2.3 秒来进行预测，其中 2 秒用于生成 2000 个 ROI。<br>Faster RCNN可以简单地看做“区域生成网络+fast RCNN“的系统，用区域生成网络RPN代替fast RCNN中的Selective Search方法。所有计算没有重复，完全在GPU中完成，大大提高了运行速度。<br>整个网络结构如下图所示：<br><img src="/images/deep_learning/faster_rcnn.jpg" alt="faster_rcnn"><br>可以发现，Fast-RCNN和Faster-RCNN的主要差别就是region proposal的生成方法。</p><h3 id="候选区域生成网络"><a href="#候选区域生成网络" class="headerlink" title="候选区域生成网络"></a>候选区域生成网络</h3><p>基本设想是：RPN的核心思想是使用CNN卷积神经网络直接产生Region Proposal。<br>候选区域网络（RPN）将上一步卷积层提取的原始图片的feature map作为输入。在输入特征图上滑动一个 3×3 的卷积核，对于特征图中的每一个位置，RPN 会做 k 次预测。下图展示了一个 8×8 的特征图，且有一个 3×3 的卷积核执行运算，它最后输出 8×8×3 个 region proposal（其中 k=3）。<br><img src="/images/deep_learning/RPN1.jpg" alt="PRN1"><br>对于RPN提取出的anchor box，经过ROI层得到固定大小的特征表示，然后输出到全连接层中，在全连接之后构建两个输出层：物体分类和框位置回归。滑动窗口的位置提供了物体的大体位置信息，框的回归提供了框更精准的位置。</p><h3 id="分类和回归-1"><a href="#分类和回归-1" class="headerlink" title="分类和回归"></a>分类和回归</h3><p>通过RPN网络，可以得到多个region proposal，然后在将各个region proposal对应的feature map输入到POI池化层，得到大小相同的feature map，最后将其输入到全连接神经网络，和Fast-RCNN相同，全连接包含两个输出层，一个用于物品分类，一个用于region proposal回归。</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>Faster-RCNN是整个RCNN系列中的集大成者，它将图片分割识别的各个步骤统一到了一起，使得模型的运行速度和准确率都得到了提高。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Version </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>图片分割识别之YOLO</title>
      <link href="/2018/10/14/Deep%20Learning/CV/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E4%B9%8BYOLO/"/>
      <url>/2018/10/14/Deep%20Learning/CV/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E4%B9%8BYOLO/</url>
      <content type="html"><![CDATA[<p>介绍one-stage图像分割识别算法：YOLO算法。<br><a id="more"></a></p><h2 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h2><p>YOLO算法使用了回归的思想，直接将整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。对于Faster-RCNN，YOLO更加快速，便于实时检测。<br>YOLO的特点是直接学习图像的全局信息。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li>给个一个输入图像，首先将图像划分成S*S个网格</li><li>对于每个网格，我们都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h以及confidence。同时，<strong>对于每个网格，预测出C个类别的概率值</strong></li><li>然后将每个bounding box的confidence值和每个网格C个类别的score相乘，得到该bounding box属于每一类的confidence score。所以可以得到 C<em>(S</em>S*B)个score矩阵，表示的是每个bounding box属于每个种类的score得分。</li><li>训练的最后一步，即针对定位误差和分类误差，定义损失函数，训练整个神经网络。</li><li>预测时，首先对20个类别遍历进行：在某个类别中，将得分少于阈值的边界框设置为0。然后用NMS算法去掉重复率较大的bounding box。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别，如果小于0，说明这个bounding box里面没有物体，跳过即可。</li></ol><p>算法结构图如下所示：<br><img src="images/Deep_learning/yolo_algorithm.jpg" alt="YOLO"></p><h3 id="对bounding-boxes的预测"><a href="#对bounding-boxes的预测" class="headerlink" title="对bounding boxes的预测"></a>对bounding boxes的预测</h3><p>对于每个bounding boxes都包含5个预测值：x,y,w,h以及confidence。其中：</p><pre><code>1. x,y是box的中心坐标并与网格对齐（相当于对当前网格偏移值，所以取值为0到1）。2. w,h表示图像的宽高，并对w,h进行归一化（0到1取值范围）。3. confidence表示box的置信度，如果该box中没有object，confidence=0，否则confidence等于IOU值。</code></pre><p>对于每个网格（不是bounding boxes），预测出C个类别的概率值。<br>然后用bounding boxes的confidence乘各个类的概率值，得到每个bounding boxes属于各个类的confidence score。<br>论文中作者取：S=7，B=2，C=20（一共有20个类），所以最后一张图片可以输出：S<em>S</em>(B*5+C)个值。</p><h3 id="网络结构设计"><a href="#网络结构设计" class="headerlink" title="网络结构设计"></a>网络结构设计</h3><p>Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层。<br>在论文中，对于一个输入图片，经过神经网络会得到一个7 <em> 7 </em> （2 <em> 5 + 20）大小的tensor。表示将原图片分割成7</em>7个网格，每个网格生成2个bounding boxes，一共有20个类别。<br>具体tensor表示如下：<br><img src="images/Deep_learning/yolo2.jpg" alt="YOLO2"><br>可以看到，对于每个网格，前20位是对类的预测，然后是该网格的两个bounding boxes的confidence score，最后8位是两个bounding boxes的位置信息(x, y, w, h).</p><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>YOLO首先预训练一个googleNet，然后在训练好的GooglenNet的卷积层后加上四个随机初始化的卷积层进一步提取特征，最后是2个全连接层，对各个分数进行预测。<br><img src="images/Deep_learning/yolo3.jpg" alt="YOLO2"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>YOLO将目标检测看做回归问题，所以采用平方损失。但对于不同部分采用了不同的权重。<br>损失函数一共包含两个部分：<strong>定位误差和分类误差</strong>。<br>定位误差即边框的坐标(x,y,w,h)的预测误差，论文采用了较大的权重。然后其区分不包含目标的边界框和含有目标的边界框的置信度。<br><img src="images/Deep_learning/yolo4.jpg" alt="YOLO2"></p><h3 id="预测（NMS算法）"><a href="#预测（NMS算法）" class="headerlink" title="预测（NMS算法）"></a>预测（NMS算法）</h3><p>在检测算法中，N非极大值抑制算法（NMS）被广泛应用解决 <strong>一个目标被多次检测的问题</strong>。比如在人脸定位问题中，一个人脸被多次检测到，而我们其实只想让算法输出一个最好的预测框。NMS算法即被用来解决这个问题。</p><h4 id="NMS算法原理"><a href="#NMS算法原理" class="headerlink" title="NMS算法原理"></a>NMS算法原理</h4><p>首先从所有的检测框中找到置信度最大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。<br>但是YOLO也存在问题：没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Version </tag>
            
            <tag> Deep learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>图片分割识别之SSD</title>
      <link href="/2018/10/14/Deep%20Learning/CV/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E4%B9%8BSSD/"/>
      <url>/2018/10/14/Deep%20Learning/CV/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E4%B9%8BSSD/</url>
      <content type="html"><![CDATA[<p>介绍one-stage图像分割识别算法：SSD算法。<br><a id="more"></a></p><h2 id="SSD算法"><a href="#SSD算法" class="headerlink" title="SSD算法"></a>SSD算法</h2><p>SSD算法是对YOLO算法存在问题（使用整图特征在7*7的粗糙网格内回归对目标的定位并不是很精准）的改进，SSD结合了YOLO的回归思想以及Faster-RCNN的anchor机制，实现了目标的精准定位。<br>SSD使用全图各个位置的多尺度区域特征进行回归，即保持了YOLO速度快的特性，也保证了窗口定位的准确度。</p><h3 id="算法思路"><a href="#算法思路" class="headerlink" title="算法思路"></a>算法思路</h3><p>SSD和Yolo一样都是采用一个CNN网络来进行检测，但是却采用了多尺度的特征图，其基本架构如下图所示：<br><img src="/images/deep_learning/ssd1.png" alt="ssd1"><br>从上图可以看出，在不同层次（尺度）的特征图上都进行分类和回归。所谓多尺度指的是采用大小不同的特征图，CNN网络一般前面的特征图比较大，后面会逐渐采用stride=2的卷积或者pool来降低特征图大小，一个比较大的特征图和一个比较小的特征图，它们都用来做检测。这样做的好处是较大的特征图来用来检测相对较小的目标，而小的特征图负责检测大目标，</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li>使用CNN提取图片特征</li><li>对得到的图片特征继续进行多尺度的卷积</li><li>对每个不同尺度的卷积核输出分别用两个3*3卷积核卷积</li><li>两个卷积核一个输出分类用的confidence，一个生成回归用的localization（四个坐标值：x,y,w,h）</li></ol><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><h4 id="模型预训练-amp-多尺度卷积"><a href="#模型预训练-amp-多尺度卷积" class="headerlink" title="模型预训练&amp;多尺度卷积"></a>模型预训练&amp;多尺度卷积</h4><p>首先训练一个VGG分类模型，用来提取图片的特征。然后在基础的VGG提取特征的基础上，将全连接删除，添加了5个卷积层（卷积层的输出大小逐渐减小）。并且对每个卷积层的输出feature map，都进行 <strong>边界框生成+分类回归</strong></p><h4 id="边界框生成"><a href="#边界框生成" class="headerlink" title="边界框生成"></a>边界框生成</h4><p>SSD使用了faster-RCNN中的anchors机制，每次卷积根据feature map大小，分割多个基础网格，然后每个网格设置尺度或者长宽比不同的多个先验框，预测的边界框（bounding boxes）是以这些先验框为基准的。<br>一般情况下，每个单元会设置多个先验框，其尺度和长宽比存在差异，如图5所示，可以看到每个单元使用了4个不同的先验框，图片中猫和狗分别采用最适合它们形状的先验框来进行训练。<br><img src="/images/deep_learning/ssd2.jpg" alt="ssd2"></p><h4 id="边界框回归"><a href="#边界框回归" class="headerlink" title="边界框回归"></a>边界框回归</h4><p>对于每个网格的每个边界框，都输出一个预测值，主要包括两部分：1.该边界框属于各个类别的置信度（包括背景）在预测过程时，置信度最高的那个类别就是边界框所属类别；2.该边界框的位置，包含四个值(x,y,w,h)用来定位边界框。</p><p>例如，对于一个大小为M <em> N的特征图，共有M</em>N个网格，设每个网格生成k个边界框，那么每个网格共生成（c+4）<em> k个预测值。所以对于整个特征图，一共生成（c+4）</em> k <em> M </em> N个预测值。<br>由于SSD采用卷积做检测，所以就需要 (c+4)k 个卷积核完成这个特征图的检测过程。</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图5所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。<br><img src="/images/deep_learning/ssd3.png" alt="ssd3"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>SSD对每个Stage的输出特征图都进行边框回归和分类操作，所以设计了一个联合损失函数：<br>L(x, c, l, g) = 1/N * (Lconf(x, c) + ßLloc(x, l, g))<br>其中：</p><ul><li>Lconf代表的是分类误差，采用的是softmax loss</li><li>Lloc表示的是location回归误差，采用的是Smooth L1 loss</li><li>ß 表示平衡权重<br>论文中对损失函数的描述如下所示：<br><img src="/images/deep_learning/ssd4.png" alt="ssd4"></li></ul>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Version </tag>
            
            <tag> Deep learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>论文阅读之《Neural Code Completion》</title>
      <link href="/2018/10/11/Paper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8ANeural%20Code%20Completion%E3%80%8B/"/>
      <url>/2018/10/11/Paper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8ANeural%20Code%20Completion%E3%80%8B/</url>
      <content type="html"><![CDATA[<p>论文《Neural Code Completion》的阅读笔记。<br><a id="more"></a></p><h2 id="Problem-Setup"><a href="#Problem-Setup" class="headerlink" title="Problem Setup"></a>Problem Setup</h2><p>该论文构建一个DL模型，输入是一个partial AST，输出是预测针对输入AST的下一个node。</p><h3 id="input-a-partial-AST"><a href="#input-a-partial-AST" class="headerlink" title="input: a partial AST"></a>input: a partial AST</h3><p>论文中，定义一个AST T的partial AST T’是T中的一个左子树。<br>对于给定节点n，该节点的左子树构成了一个partial tree，而这个partial tree的所有节点也恰好是对整个AST进行中序遍历时，早于节点n的节点子集。</p><h3 id="Next-node-prediction"><a href="#Next-node-prediction" class="headerlink" title="Next node prediction"></a>Next node prediction</h3><p>给定一个partial AST，预测该AST的下一个token。根据需要预测的token是non-terminal token还是terminal token可以将问题分成分别对两种token的预测。</p><h3 id="predicting-next-token-versus-predicting-next-node"><a href="#predicting-next-token-versus-predicting-next-node" class="headerlink" title="predicting next token versus predicting next node"></a>predicting next token versus predicting next node</h3><p>使用AST进行预测两种terminal token和non-terminal token相对于仅仅使用token sequence预测下一个token，前者可以利用更多的结构信息，更有利于预测。</p><h3 id="Joint-prediction"><a href="#Joint-prediction" class="headerlink" title="Joint prediction"></a>Joint prediction</h3><p>predict the next non-terminal and terminal together.被叫做joint prediction。</p><h3 id="Denying-prediction"><a href="#Denying-prediction" class="headerlink" title="Denying prediction"></a>Denying prediction</h3><p>因为有无穷多中的terminal token，所以对它们进行准确的预测时不现实的，论文提出了一个alternative scenario：如果programmer输入一个非常少见的terminal token，模型可以预测出该token的出现并拒绝给出准确的预测。<br>所以论文使用最常见的terminal token构建vocabulary，对于不常见的token设为UNK。</p><h2 id="models"><a href="#models" class="headerlink" title="models"></a>models</h2><h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><ul><li>对于一个给定的partial AST作为输入，首先将其转换为LCRS二叉树，然后中序遍历，得到一个node sequence。</li><li>对于一个长度为k的node sequence，结构为：[(N1, T1), (N2, T2), ..(Nk, Tk)]。其中Ni表示一个non-terminal token，Ti表示该token Ni的一个terminal 子token。</li><li>对于每个non-terminal token，我们不仅仅encode该token的种类，还有该token是否包含最少一个non-terminal child, and/or one right-sibling.</li><li>对于没有terminal child的non-terminal token，标注该token有一个EMPTY child。</li><li>该输入序列：(N1,T1),(N2,T2),…,(Nk,Tk) 只被用作除了预测next terminal token 之外的所有任务。对于next terminal token：Tk+1的预测，除了使用之前的输入序列，还包括Tk+1的non-terminal parent token，也就是Nk+1。</li><li>对于Ni和Ti都使用one-hot-encoding表示。non-terminal的vocabulary和terminal的vocabulary是分开的。<h3 id="Next-non-terminal-prediction"><a href="#Next-non-terminal-prediction" class="headerlink" title="Next non-terminal prediction"></a>Next non-terminal prediction</h3>给定一个输入序列，模型首先预测next non-terminal。该模型被命名为NT2N(using the sequence of Non-terminal and terminal pairs To predict the next Non-terminal)<br>该NT2N的结构如下：<br><img src="/images/paper/nt2n.png" alt="nt2n"><br>从图中可以看到，在LSTM的最后一个输出后连接一个softmax层，进行下一个non-terminal的预测。<h3 id="Using-only-non-terminal-inputs"><a href="#Using-only-non-terminal-inputs" class="headerlink" title="Using only non-terminal inputs"></a>Using only non-terminal inputs</h3>NT2N的一个变种是，在输入层省略掉所有terminal，只是用non-terminal进行训练，在预测时也只预测non-terminal。该变种被称为N2N。<h3 id="Next-terminal-and-non-terminal-prediction"><a href="#Next-terminal-and-non-terminal-prediction" class="headerlink" title="Next terminal and non-terminal prediction"></a>Next terminal and non-terminal prediction</h3>使用NT2N模型，不仅可以预测next non-terminal，也可以预测next terminal。结合LSTM的最后输出，经过一个线性层和softmax层：softmax(WHt+b)得到最后对Tk+1的预测。该模型被称为NT2NT。<h3 id="Next-terminal-prediction"><a href="#Next-terminal-prediction" class="headerlink" title="Next terminal prediction"></a>Next terminal prediction</h3>对下个terminal Tk+1的预测结合了上一个时刻LSTM的输出Ht以及当前时刻k+1的non-terminal Nk+1。对Tk+1的预测如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tk+1 = softmax(W1Hk + W2Nk+1 + b)</span><br></pre></td></tr></table></figure></li></ul><p>该模型被称为NTN2T。模型的结构如下：<br><img src="/images/paper/ntn2t.png" alt="ntn2t"></p><h3 id="Joint-Prediction"><a href="#Joint-Prediction" class="headerlink" title="Joint Prediction"></a>Joint Prediction</h3><p>使用两种方法一起预测next non-terminal和next terminal。</p><ol><li>使用NT2NT模型一起预测</li><li>用一个方法X去预测next non-terminal，然后将预测的non-terminal和input sequence输入进NTN2T中预测next terminal。这种方法被称为X+NTN2T。<h3 id="Denying-Prediction"><a href="#Denying-Prediction" class="headerlink" title="Denying Prediction"></a>Denying Prediction</h3>对于一些不经常使用的terminal token，将其转化为UNK。但为了防止模型过多的预测UNK，从新构建了loss function。当label=UNK时，该loss等于0.</li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ol><li>一共包含44种non-terminal，在每个non-terminal后加上两个bit表示是否有child和是否有right-sibling，最后一共有176个non-terminal。</li><li>选择出现最频繁的50,000个terminal。</li><li>加入三种特殊的terminal：UNK表示out-of-vocab；EOF表示end-of-program；Empty表示non-terminal which does not have a terminal.</li><li>值得一提的是，45% terminal 是 Empty terminal<h3 id="training-details"><a href="#training-details" class="headerlink" title="training details"></a>training details</h3></li><li>single layer LSTM， size of hidden layer:1500</li><li>Adam Algorithms</li><li>learning rate:0.001,并且每0.2个epoch会乘0.9</li><li>梯度裁剪阈值为5</li><li>batch size：80</li><li>use truncated backpropagation through time, by unrolling the LSTM model s = 50 times to take an input sequence of length 50 in each batch (and therefore each batch contains b × s = 4000 tokens).<h3 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h3></li><li>使用随机初始化初始LSTM的h0和c0。并且将前一个segment的final_state作为下一个segment的初始如果这两个segment属于一个program。如果不属于一个则仍然用h0和c0进行初始化。 <strong>We observe that resetting the hidden states for every new program improves the performance a lot.</strong></li><li>对于LSTM模型，一次训练五种不同超参的模型，然后集成到一起进行投票。<strong>we find that the ensemble improves the accuracy by 1 to 3 points in general.</strong><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2>Raychev et al.(2016a)</li></ol>]]></content>
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>论文阅读之《Automatic Code Completion》</title>
      <link href="/2018/10/10/Paper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8AAutomatic%20Code%20Completion%E3%80%8B/"/>
      <url>/2018/10/10/Paper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8AAutomatic%20Code%20Completion%E3%80%8B/</url>
      <content type="html"><![CDATA[<p>论文《Automatic Code Completion》的阅读笔记。</p><a id="more"></a><p>本论文主要介绍了如何使用带有Attention机制的LSTM模型对代码进行预测。实验结果中：对non-terminal token的预测准确率达到了77%，对terminal token的预测准确率达到了46%。<br>论文的一个核心思想是：将编程语言看做自然语言，进而可以把NLP技术应用到编程语言的预测上。</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><ol><li>使用了JavaScript的源码数据集，大小为11G，一共包含了100,000个源码用于训练和50,000个源码用于测试。并且所有的源码都被处理成了AST形式。</li><li>将AST转换为Left-Child-Right-Sibling的二叉树，然后进行中序遍历，得到一个token sequence表示这个AST。每个token sequence包含49个tokens，每个tokens包含一个non-terminal 和 terminal node pair。</li><li>Non-terminal token仅包含type（ie IfStatement，LiteralString）。同时，对于每个non-terminal token增加2bit的信息：1.是否包含一个terminal child；2.是否包含一个right sibling。最后，一共有176种可能的non-terminal nodes</li><li>对于terminal token，理论上有无穷多总可能，这里使用了最频繁的50,000个token和一个additional UNK。</li><li>将terminal token 用GloVe进行预训练得到word embedding表示，对于non-terminal token和UNK使用随机数进行初始化。</li><li>一个token的embedding表示是它的terminal和non-terminal embedding的拼接。<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3>对non-terminal token进行word embedding。<br>将embedded的non-terminal token序列和terminal token序列输入到一个LSTM模型中。一共构建了四种LSTM模型，一个是基础的LSTM作为benchmark，然后是Tail Attention，Sum Attention，Gated Attention。</li></ol><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h4><p>模型输入是部分code segement 包含49个tokens（non-terminal, terminal pairs)，输出是两个预测：1. 下一个non-terminal token 和下一个terminal token。</p><h4 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h4><p>分别对non-terminal token和terminal token计算两个准确率，其中：</p><ol><li>对于non-terminal token，计算两种准确率：standard accuracy表示对token的预测是否正确；Flex accuracy的计算只考虑是否预测对下一个token（忽略添加的用来表示是否有terminal child和right-sibling的2 bits信息）。</li><li>对于terminal tokens,具体计算准确率时忽略对UNK的预测。<h4 id="超参"><a href="#超参" class="headerlink" title="超参"></a>超参</h4>使用grid search 方法优化超参，论文显示对超参的优化对Attention的性能高较大。<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><img src="/images/paper/auto_code_complete.png" alt="picture"></li></ol><h3 id="future-work"><a href="#future-work" class="headerlink" title="future work"></a>future work</h3><ol><li>One augmentation to the terminal prediction models is a probabilistic copying function which would allow the model to copy terminals it had seen in the window with learned probabilities. We think this would be a promising approach as terminals are scoped and often repeated within functions and code blocks, and so it is often a good idea to copy terminals you have seen previously</li><li>to augment our models with syntactic features that exploit the structured nature of code. For example, we could reject predicting a terminal string when syntactically the code requires an integer.</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li>统计学习模型：5，7</li><li>使用AST结构的LSTM benchmark：10，12</li><li>N-Gram：11</li><li>probabilistic grammars：5</li><li>决策树模型：6</li><li>attention模型提高了对python代码的补全（sparse pointer network）：7，9</li></ul>]]></content>
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux常用命令整理</title>
      <link href="/2018/09/30/Knowledge/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"/>
      <url>/2018/09/30/Knowledge/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>整理常用的Linux命令。</p><a id="more"></a><h2 id="Linux指令"><a href="#Linux指令" class="headerlink" title="Linux指令"></a>Linux指令</h2><h3 id="基本指令"><a href="#基本指令" class="headerlink" title="基本指令"></a>基本指令</h3><h4 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h4><p>移动到指定目录</p><h4 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h4><p>列出当前文件中所有内容</p><pre><code>1. &apos;-l&apos; (long的缩写) 输出详细信息2. &apos;-a&apos; (all的缩写) 输出包括隐藏文件的所有文件3. &apos;-lh&apos; (human), 直接 -l 不方便人看, 这个指令是方便给人观看的. 注意这里的文件大小使用了 K, MB, GB 之类概括</code></pre><h5 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h5><p>像 ‘-rw-rw-r–’ 这种, 就是权限的说明. 这串字符得拆成4个部分</p><ul><li>Type: 很多种 (最常见的是 - 为文件, d 为文件夹, 其他的还有l, n … 这种东西, 真正自己遇到了, 网上再搜就好, 一次性说太多记不住的).</li><li>User: 后面跟着的三个空是使用 User 的身份能对这个做什么处理 (r 能读; w 能写; x 能执行; - 不能完成某个操作).</li><li>Group: 一个 Group 里可能有一个或多个 user, 这些权限的样式和 User 一样.</li><li>Others: 除了 User 和 Group 以外人的权限.</li></ul><h3 id="文件、文件夹的创建和移动"><a href="#文件、文件夹的创建和移动" class="headerlink" title="文件、文件夹的创建和移动"></a>文件、文件夹的创建和移动</h3><h4 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h4><p>后接文件名，新建一个文件<br>如果想同时建立多个文件, 输入多个文件的名字, 以空格分开.</p><h4 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h4><p>是复制文件或者文件夹的指令, 常用的方式是’copy oldFile newFile’</p><pre><code>1. -i (interactive) 如果 newFile 已经存在, 会直接覆盖已存在的newFile, 如果要避免直接覆盖, 我们在 cp 后面加一个选项 -i.表示如果newFile已经存在警告是否覆盖2. -R (Recursive) 复制 **文件夹** 到一个位置3. 复制多个文件. 复制名字部分相同的多个文件, \* 是说”你就找文件前面是 file 的文件, 后面是什么名字无所谓” &apos;cp file_pre_name* folder2/&apos;4. 或者你可以单独选定几个文件, cp 会默认最后一个选项是要复制去的文件夹. 比如下面把 file1 和 file2 复制去 folder1/ &apos;cp file1 file2 folder/&apos;</code></pre><h4 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h4><p>剪切：移动到另一个文件夹</p><pre><code>1. 因为移动文件到原始的地点, 但是以不同的文件名 &apos;mv file1 NewName&apos;</code></pre><h4 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h4><p>创建一个文件夹</p><h4 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h4><p>移除文件夹，前提条件是文件夹必须为空</p><h5 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h5><p>移除文件。</p><pre><code>1. -i 或 -I 有提示地移除文件 (为了避免误删)2. -r 或 -R (recursively)递归删除文件夹中的内容和文件夹，和 rmdir 不同, rm -r 可以在文件夹中有文件的情况下删除这个文件夹.</code></pre><h3 id="文件的编辑和显示"><a href="#文件的编辑和显示" class="headerlink" title="文件的编辑和显示"></a>文件的编辑和显示</h3><h4 id="vi"><a href="#vi" class="headerlink" title="vi"></a>vi</h4><p>使用vim打开文件进行编辑</p><h4 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h4><p>(catenate) 可以用来显示文件内容, 或者是将某个文件里的内容写入到其他文件里.</p><pre><code>1. &apos;&gt;&apos; 将文件的内容放到另一个文件中 &apos;cat file1 &gt; file2&apos;2. &gt; 将多个文件的内容打包一起放入另一个文件 &apos;cat file1 file2 &gt; file3&apos;3. &gt;&gt; 将内容添加在一个文件末尾</code></pre><h4 id="tac"><a href="#tac" class="headerlink" title="tac"></a>tac</h4><p>是cat的反向操作，从最后一行开始打印</p><h4 id="more"><a href="#more" class="headerlink" title="more"></a>more</h4><p>和cat不同的是more可以一页一页查看文件内容，比较适合大文件的查看，more会首先显示文件的一页内容，然后按Enter再显示一行，按空格在显示一页。</p><h4 id="head"><a href="#head" class="headerlink" title="head"></a>head</h4><p>[-n number] filename   取得文件的前几行，-n后接数字表示显示几行</p><h4 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h4><p>head的反向操作，显示文件的后几行</p><h3 id="文件搜索"><a href="#文件搜索" class="headerlink" title="文件搜索"></a>文件搜索</h3><h4 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h4><p>locate命令用于查找符合条件的文档，他会去保存文档和目录名称的数据库内，查找合乎范本样式条件的文档或目录。<br>一般情况我们只需要输入 locate your_file_name 即可查找指定文件。</p><h4 id="whereis"><a href="#whereis" class="headerlink" title="whereis"></a>whereis</h4><p>在特定目录中查找原始代码、二进制文件，或是帮助文件等特殊文件。<br>所以该指令只能用于查找二进制文件、源代码文件和man手册页，一般文件的定位需使用locate命令。</p><h4 id="find"><a href="#find" class="headerlink" title="find"></a>find</h4><p>find命令用来在指定目录下查找文件。可以用正则表达式进行文件匹配。</p><h5 id="find指令例子"><a href="#find指令例子" class="headerlink" title="find指令例子"></a>find指令例子</h5><ul><li>将目前目录及其子目录下所有延伸档名是 c 的文件列出来。<br>‘find . -name “*.c”‘</li><li>将目前目录其其下子目录中所有一般文件列出<br>‘find . -type f’</li><li>将目前目录及其子目录下所有最近 20 天内更新过的文件列出<br>‘find . -ctime 20’</li></ul><h3 id="系统相关"><a href="#系统相关" class="headerlink" title="系统相关"></a>系统相关</h3><h4 id="lsblk"><a href="#lsblk" class="headerlink" title="lsblk"></a>lsblk</h4><p>以树状图输出所有块设备，可以用来便捷的查找插入USB的名字</p><h4 id="uname-a"><a href="#uname-a" class="headerlink" title="uname -a"></a>uname -a</h4><p>是Unix Name的缩写，显示机器名，操作系统和内核详细信息</p><h4 id="history"><a href="#history" class="headerlink" title="history"></a>history</h4><p>显示在终端执行过的所有命令的历史，按ctrl+R可以搜索已经执行过的命令</p><h4 id="ifconfig"><a href="#ifconfig" class="headerlink" title="ifconfig"></a>ifconfig</h4><p>显示网络配置详细信息</p>]]></content>
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习算法之EM算法</title>
      <link href="/2018/09/26/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BEM%E7%AE%97%E6%B3%95/"/>
      <url>/2018/09/26/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BEM%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>总结EM算法的原理，以及用python实现的EM算法求解双硬币问题。</p><a id="more"></a><h2 id="EM算法概述"><a href="#EM算法概述" class="headerlink" title="EM算法概述"></a>EM算法概述</h2><p>EM算法被列为十大数据挖掘算法之一，必然有它的过人特性，但相比于其他的算法，EM算法更加抽象难懂。在这里，我尝试用最简单的语言解释EM算法到底是什么，干什么用的，他的运行过程和原理是什么，最后，结合一个EM算法求解双硬币问题的代码，彻底理解它，本文不涉及到具体的公式推导。</p><h3 id="基本抛硬币问题"><a href="#基本抛硬币问题" class="headerlink" title="基本抛硬币问题"></a>基本抛硬币问题</h3><p>想象如下问题：有一枚不均匀的硬币，现在拿该硬币进行十次的抛币实验，得到的结果序列为：正，正，反，正，反，正，正，正… 现要求该硬币正面向上的概率是多少？<br>这是一道非常给定观测序列估计参数的题，只需要假设正面向上的概率为p，反面向上的概率为(1-p)，然后利用极大似然估计，求解p即可。在此不进行详述。</p><h3 id="双硬币问题"><a href="#双硬币问题" class="headerlink" title="双硬币问题"></a>双硬币问题</h3><p>在上一题的基础上进一步加深，现在有两枚不均匀硬币A和B，每次随机从中拿出一枚进行连续十次的抛币实验。然后再将该硬币放回，继续随机从中拿去一枚进行10次抛币实验。往复5次，得到一个5 * 10的抛币实验观察，问两枚硬币A和B的正面向上的概率分别是多少。</p><p>该问题和第一个问题不同的地方就是第二个问题引入了隐变量：‘每次随机拿去的硬币’。因为我们不知道每次随机拿去的是哪枚硬币，就无法用极大似然估计来求这枚硬币的概率分布。所以预测每个硬币的概率分布（正面朝上的概率）就变成了两个问题：1.该数据来自哪个分布（随机选的哪个硬币）；2.该分布是什么样的（正面朝上的概率）</p><h3 id="迭代求解"><a href="#迭代求解" class="headerlink" title="迭代求解"></a>迭代求解</h3><p>直观上来看，这个问题是不可解的，是一个鸡生蛋，蛋生鸡的问题。<br>就双硬币实验来说，只有当我们知道了本次抛币属于同一个硬币的概率分布的时候，我们才能够对这个分布的参数作出靠谱的预测（极大似然）。但现在两枚硬币的概率分布混在了一起，而我们又没办法分开它们（隐变量，每次选取的硬币未知），所以就没法估计这两个分布的参数（正面朝上的概率）。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些抛币实验结果属于第一个分布，哪些属于第二个分布。<br>为了解决这个循环问题，提出了EM算法：<strong>先随便选出一个随机值来，根据这个值来不断调整，如此循环迭代相互推导最后EM算法收敛到一个解。</strong></p><p>假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。这就是EM算法的核心思想。</p><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>EM的意思是“Expectation Maximization”，在我们上面的双硬币问题里面，我们是先随便猜一下两枚硬币A和B正面朝上的概率，然后计算每次抛币实验更可能属于第一个硬币还是第二个硬币的分布，这个是属于Expectation一步。<br>有了这个估计，我们就可以根据最大似然估计，来重新估计第一枚硬币A分布的参数，和第二枚硬币B的参数。这个是Maximization。<br>然后根据新得到的更新的两个硬币的参数，在重复E步和M步，不断基于之前的参数更新结果进行计算和进一步的更新，直到参数收敛。</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol><li>随机初始化概率分布参数</li><li>E步：选取一个概率分布参数，求在该参数下的属于该分布的期望</li><li>M步：根据E步求解的期望，计算极大似然估计，并最大化该值</li><li>重复2，3步指导参数收敛。</li></ol><h3 id="EM求解双硬币问题"><a href="#EM求解双硬币问题" class="headerlink" title="EM求解双硬币问题"></a>EM求解双硬币问题</h3><p><img src="/images/machine_learning/EM1.png" alt="EM1"></p><p>下图表示双硬币问题两种条件下的解法，1为知道每次选取的是哪枚硬币（无隐变量），用极大似然估计求解；2为不知道每次选取的是哪枚硬币（包含隐变量），用EM算法求解。</p><p><img src="/images/machine_learning/EM2.png" alt="EM2"></p><p>对于EM算法，稍微解释一下上图的计算过程。首先初始值θA=0.6,θB=0.5。<br>由于两个硬币的初始值0.6和0.5，容易得出投掷出5正5反的概率是pA=C(10,5)(0.6^5)(0.4^5)，pB=C(10,5)(0.5^5)(0.5^5), pA/(pA+pB)=0.449, 0.449近似为0.45，表示第一组实验选择的硬币是A的概率为0.45。同理计算第一组实验选择的硬币是B的概率为0.55.<br>图中的2.2H，2.2T是怎么得来的呢？ 0.449 <em> 5H = 2.2H ，0.449 </em> 5T = 2.2T ，表示第一组实验选择A硬币且正面朝上次数的期望值是2.2。其他的值依次类推。</p><h3 id="实现源码"><a href="#实现源码" class="headerlink" title="实现源码"></a>实现源码</h3><p>在我的GitHub上有对上述双硬币问题的求解python实现源码，地址如下：<a href="https://github.com/YHfeather/ML_Note/blob/master/ML_Algorithm/EM_Algorithm.py" target="_blank" rel="noopener">python求解双硬币问题</a></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow之tfdbg</title>
      <link href="/2018/09/21/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8Btfdbg/"/>
      <url>/2018/09/21/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8Btfdbg/</url>
      <content type="html"><![CDATA[<p>我们知道TensorFlow在训练时是在后台运行的，所以常见的python debug工具没办法对运行中的TensorFlow进行debug，而TensorFlow提供了一个官方的debug工具：tfdbg。</p><a id="more"></a><p>tfdbg使用起来十分简单，常规命令以及界面都和idbg等工具相同，在TensorFlow的代码中也仅仅需要增加两行就可以使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.python import debug as tfdbg</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variable_initializer())</span><br><span class="line">debug_sess = tfdbg.LocalCLIDebugWrapperSession(sess=sess)</span><br><span class="line">debug_sess.run(train_op) ## 用加了 wrapper 的 session，来代替之前的 session 做训练操作</span><br></pre></td></tr></table></figure><p>使用加了 wrapper 的 session，来代替之前的 session 做训练操作，这样在运行时就会进入tfdbg的界面，我们可以清晰的看到各个tensor的值。</p><p>常见的debug命令：</p><ol><li>run： 执行一次debug_session.run()操作，并将涉及到的tensor值打印到debug界面<ol><li>-t <t>：执行 T - 1 次 Session.run（无需调试），接着执行一次运行（需要调试），进入debug界面。</t></li></ol></li><li>exit：退出debug界面</li><li>help：输出帮助信息</li><li>ri：  显示有关当前运行的信息，包括fetch和feed</li><li>pf：  输出session.run()的feed_dict中的一个值<ol><li>pf &lt;feed_tensor_name&gt; 输出feed的值</li></ol></li></ol><p>更多debug界面的命令可以参考官网：<a href="https://www.tensorflow.org/guide/debugger" target="_blank" rel="noopener">https://www.tensorflow.org/guide/debugger</a></p>]]></content>
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Word2Vec之gensim使用</title>
      <link href="/2018/09/15/Deep%20Learning/Word2Vec%E4%B9%8Bgensim%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/09/15/Deep%20Learning/Word2Vec%E4%B9%8Bgensim%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>在NLP任务中，我们可以自己搭建Word2Vec模型，但同时gensim封装了Word2Vec的实现，利用gesim模块提供的API可以方便快捷的构建Word2Vec模型。</p><a id="more"></a><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>直接看代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line"></span><br><span class="line">sentences = word2vec.Text8Corpus(text8_corpus_path) # 加载text8语料</span><br><span class="line"></span><br><span class="line"># 使用导入的语料库训练一个skip-gram模型，指定embedding size为200</span><br><span class="line">model = word2vec.Word2Vec(sentences, size=200)</span><br></pre></td></tr></table></figure></p><h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 模型训练好之后，有如下常用操作</span><br><span class="line">similarity = model.similarity(&apos;woman&apos;, &apos;man&apos;) #计算两个word的相似度</span><br><span class="line">top_similar = model.most_similar(&apos;good&apos;, topn=10) #计算topn各最相似的词，返回一个list，每个元素包含词和相似度</span><br><span class="line">not_match = model.doesnt_match(&quot;breakfast dinner lunch father&quot;.split()) #返回输入list中最不相似的词</span><br><span class="line">word_vector = model[&apos;word&apos;]  # 直接用类似字典的方法查询给定词的词向量</span><br></pre></td></tr></table></figure><h3 id="模型保存和加载"><a href="#模型保存和加载" class="headerlink" title="模型保存和加载"></a>模型保存和加载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.save(&apos;save_path.model&apos;)</span><br><span class="line">new_model = word2vec.Word2Vec.load(&apos;save_path.model&apos;)</span><br><span class="line"></span><br><span class="line">model.save_word2vec_format(&apos;save_path.model.bin&apos;, binary=True) # 以C语言可解析的形式存储词向量</span><br><span class="line">new_model = word2vec.Word2Vec.load_word2vec_format(&apos;save_path.model.bin&apos;, binary=True) #加载</span><br></pre></td></tr></table></figure><p>通过最后一种加载方式，可以在网上下载已经训练好的模型使用，进而省去了自己训练的过程。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java集合</title>
      <link href="/2018/09/11/Programming%20Language/Java/Java%E9%9B%86%E5%90%88/"/>
      <url>/2018/09/11/Programming%20Language/Java/Java%E9%9B%86%E5%90%88/</url>
      <content type="html"><![CDATA[<p>总结Java集合的一些常规用法。</p><a id="more"></a><p><img src="https://o70e8d1kb.qnssl.com/java-collection-hierarchy.png" alt="image"></p><h2 id="集合接口"><a href="#集合接口" class="headerlink" title="集合接口"></a>集合接口</h2><p>在集合Collection中，定义了如下集合通用方法：</p><h3 id="添加"><a href="#添加" class="headerlink" title="添加"></a>添加</h3><ul><li>add(E e) 将指定的元素添加到列表的尾部。</li><li>add(int index, E e) 在index位置插入element</li><li>addAll(Collection c) 将特定 Collection 中的元素添加到末尾</li><li>addAll(int index, Collection c) 添加指定collection到index处<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3></li><li>clear()</li><li>remove(E e)</li><li>remove(int index) 删除指定index处的元素，并返回被删除的值</li><li>remove(E e) 删除集合中第一个等于输入元素的元素，并返回boolean表示删除是否成功</li><li>removeAll(Collection c)<h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3></li><li>contains(E e)</li><li>equals(E e)</li><li>isEmpty()<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3></li><li>size()</li><li>toArray()</li></ul><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><p>List 是一个有序的接口，可以通过通过index 访问元素，且不限制重复元素。</p><h3 id="ArrayList"><a href="#ArrayList" class="headerlink" title="ArrayList"></a>ArrayList</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>ArrayList即动态数组，可以动态扩容。ArrayList是不同步的。其底层实现是一个数组，当存入的数据超过数组原来的大小后，进行扩容。</p><ul><li>可动态扩容（增大 50%）</li><li>允许插入 null</li><li>非同步。<h4 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h4></li></ul><ol><li>ArrayList(): 初始化一个容量为10的ArrayList</li><li>ArrayList(int initialCapacity): 指定容量</li><li>ArrayList(Collection c)：构造一个包含指定 collection 的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。<h4 id="array-set-int-index-E-element"><a href="#array-set-int-index-E-element" class="headerlink" title="array.set(int index, E element)"></a>array.set(int index, E element)</h4>该方法首先调用rangeCheck(index)来校验 index 变量是否超出数组范围，超出则抛出异常。而后，取出原 index 位置的值，并且将新的 element 放入 Index 位置，返回 oldValue。<h4 id="array-get-int-index"><a href="#array-get-int-index" class="headerlink" title="array.get(int index)"></a>array.get(int index)</h4>返回对应index的元素<h3 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h3>Vector 和 ArrayList 很类似，不同之处在于，Vector是同步的，线程安全，自动扩容时，容量是增大 100%。<h3 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h3>Stack是一个基于Vector的实现，实现了队列。<h3 id="LinkedList"><a href="#LinkedList" class="headerlink" title="LinkedList"></a>LinkedList</h3>LinkedList 是一个双向链表实现，对LinkedList内元素的索引都需要从它的头部或尾部遍历。因此 LinkedList 不能像 ArrayList和Vector那样根据元素索引位置进行随机访问，只能通过 LinkedList 里逐个 Node 依次向下（上）迭代进行查找，<strong>访问效率相比会差一点</strong>。</li></ol><h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h2><p>Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Queue接 口。Queue接口窄化了对LinkedList的方法的访问权限（即在方法中的参数类型如果是Queue时，就完全只能访问Queue接口所定义的方法 了，而不能直接访问 LinkedList的非Queue的方法），以使得只有恰当的方法才可以使用。</p><h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><ul><li>remove() 移除并返回队列头部的元素    </li><li>element() 返回队列头部的元素</li><li>offer(E e)     添加一个元素并返回true</li><li>poll()         移除并返问队列头部的元素</li><li>peek()       返回队列头部的元素</li></ul><h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>Set 是无序，不存在重复元素的集合类型。Set 支持null，最多也只能存在一个。</p><h3 id="HashSet"><a href="#HashSet" class="headerlink" title="HashSet"></a>HashSet</h3><p>凡是和 Hash 相关的都是查询效率很高的集合类型，HashSet也是。</p><h3 id="LinkedHashSet"><a href="#LinkedHashSet" class="headerlink" title="LinkedHashSet"></a>LinkedHashSet</h3><p>LinkedHashSet 继承了 HashSet，不同的是，LinkedHashSet 内部持有一个双向链表，这个链表决定了其元素遍历的顺序，即元素插入的顺序。因此 LinkedHashSet 是有序的集合。</p><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p><img src="https://o70e8d1kb.qnssl.com/java-map-hierarchy.png" alt="image"></p><h3 id="HashMap"><a href="#HashMap" class="headerlink" title="HashMap"></a>HashMap</h3><p>HashMap 的 key 和 value 允许 null，且它是非同步的。HashMap 是无序的</p><h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3><p>Hashtable 和 HashMap 类似，但它是同步的，且不支持 null。在单线程环境下，Hashtable 的性能要稍差于 HashMap。</p><h3 id="TreeMap"><a href="#TreeMap" class="headerlink" title="TreeMap"></a>TreeMap</h3><p>TreeMap 是基于红黑树实现，是有序集合，以元素key的自然顺序或者比较器作为排序依据。</p><h3 id="map方法"><a href="#map方法" class="headerlink" title="map方法"></a>map方法</h3><ul><li>map.clear()</li><li>map.remove(Object key)</li><li>map.keySet()</li><li>map.values()</li><li>map.containsKey(key)</li><li>map.containsValue(value)</li><li>map.isEmpty()</li><li>map.size()</li><li>map.getOrDefault(key,value) 如果指定的key存在，则返回该key对应的value，如果不存在，则返回指定值。</li><li>map.get(Object key)</li><li>map.put(key,value)</li><li>map.putAll(Map t)</li></ul><h2 id="集合方法"><a href="#集合方法" class="headerlink" title="集合方法"></a>集合方法</h2><ul><li>Collection.sort(list);</li><li>Collection.shuffle(list);</li><li>Collection.max(list);</li><li>Collection.min(list);</li></ul>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积核的种类</title>
      <link href="/2018/09/11/Deep%20Learning/%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E7%A7%8D%E7%B1%BB/"/>
      <url>/2018/09/11/Deep%20Learning/%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E7%A7%8D%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>总结常见的卷积核原理和特性。</p><a id="more"></a><h2 id="卷积操作演变过程"><a href="#卷积操作演变过程" class="headerlink" title="卷积操作演变过程"></a>卷积操作演变过程</h2><h3 id="传统卷积操作"><a href="#传统卷积操作" class="headerlink" title="传统卷积操作"></a>传统卷积操作</h3><p>最常规的卷积操作，通过用卷积核在输入特征矩阵上滑动+矩阵变换，得到输出的feature map。<br>代表模型有：LeNet, Alexnet, VGG</p><h3 id="多隐层非线性卷积（inception）"><a href="#多隐层非线性卷积（inception）" class="headerlink" title="多隐层非线性卷积（inception）"></a>多隐层非线性卷积（inception）</h3><p>使用多个尺度的卷积核对原始feature map进行卷积(包括1*1大小的卷积核)，然后将每个尺寸的卷积核的输出concat，这个结构被称为“inception”，由GoogleNet首先提出。</p><h3 id="空洞卷积（Dilation）"><a href="#空洞卷积（Dilation）" class="headerlink" title="空洞卷积（Dilation）"></a>空洞卷积（Dilation）</h3><p>也称卷积核膨胀操作。它是解决pixel-wise输出模型的一种常用的卷积方式。我们知道pooling下采样会到时原始信息丢失，这对于只需要输出预测概率的图片预测模型没有问题，但如果做像素级别的预测时，就需要一定的优化方式。<br>空洞卷积就是一种 <strong>用卷积代替pooling的操作</strong>，通过卷积核插“0”的方式（空洞），它可以比普通的卷积获得更大的感受野。</p><h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><p>标准的卷积过程可以看上图，一个2×2的卷积核在卷积时，对应图像区域中的所有通道均被同时考虑，问题在于，为什么一定要同时考虑图像区域和通道？我们为什么不能把通道和空间区域分开考虑？<br>首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道feature maps之后，这时再对这批新的通道feature maps进行标准的1×1跨通道卷积操作。</p><p>深度卷积是对输入的每一个channel独立的用对应channel的所有卷积核去卷积，假设卷积核的shape是[filter_height, filter_width, in_channels, channel_multiplier]，那么每个in_channel会输出channel_multiplier那么多个通道，最后的feature map就会有in_channels <em> channel_multiplier个通道了。反观普通的卷积，输出的feature map一般就只有channel_multiplier那么多个通道。<br>既然叫深度可分离卷积，光做depthwise convolution肯定是不够的，原文在深度卷积后面又加了pointwise convolution，这个pointwise convolution就是1</em>1的卷积，可以看做是对那么多分离的通道做了个融合。<br>这两个过程合起来，就称为Depthwise Separable Convolution了。<br>代表模型：Xception</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ol><li><p>大大减少参数个数，降低了模型计算量</p><blockquote><p>假设输入通道数为3，要求输出通道数为256，两种做法：<br>1.直接接一个3×3×256的卷积核，参数量为：3×3×3×256 = 6,912<br>2.DW操作，分两步完成，参数量为：3×3×3 + 3×1×1×256 = 795，又把参数量降低到九分之一</p></blockquote></li><li><p>效果更好</p></li><li>缺点：因为DSC减少了卷积操作的参数数量，如果你的NN已经比较小了，使用DSC很可能会的得到一个包含更少参数的小网络，进而导致整个网络无法学习到足够的信息。</li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java HashMap实现原理</title>
      <link href="/2018/09/11/Programming%20Language/Java/Java%20HashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>/2018/09/11/Programming%20Language/Java/Java%20HashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      <content type="html"><![CDATA[<p>总结HashMap的底层实现原理。</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>HashMap 是基于哈希表的 Map 接口的非同步实现。允许使用 null 值和 null 键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。<br>Hashmap 不是同步的，如果多个线程同时访问一个 HashMap，而其中至少一个线程从结构上（指添加或者删除一个或多个映射关系的任何操作）修改了，则必须保持外部同步，以防止对映射进行意外的非同步访问。</p><h2 id="HashMap的数据结构"><a href="#HashMap的数据结构" class="headerlink" title="HashMap的数据结构"></a>HashMap的数据结构</h2><p>HashMap底层是一个数组，数组中的元素是一个链表，链表中主要包含两个值：key和value，还有一个next指针。当新建一个HashMap时，会初始化一个数组。</p><h2 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ol><li>HashMap(): 无参数初始化会构建一个数组容量为16，负载因子为0.75的HashMap。</li><li>HashMap(int initialCapacity): 指定初始化容量</li><li>HashMap(int initialCapacity, float loadFactor): 指定初始化容量和负载因子。<h3 id="map-put-key-value"><a href="#map-put-key-value" class="headerlink" title="map.put(key, value)"></a>map.put(key, value)</h3>源码请百度，这里直接总结整体思路：</li><li>当key为null时，调用putForNullKey方法将其放到table[0]</li><li>计算key的hash值 hash(key)</li><li>根据hash值计算在数组中的索引：indexFor(hash, table.length)</li><li>开始遍历对应index的链表，如果key已经存在了，则用新的value代替，并返回旧value。</li><li>如果不存在，则将输入的key-value存到链表尾部，返回null<h3 id="map-get-key"><a href="#map-get-key" class="headerlink" title="map.get(key)"></a>map.get(key)</h3></li><li>当key为null，返回getForNullKey()</li><li>计算key对应的hash，并计算对应的索引</li><li>遍历索引对应的链表，找到key值相等的链表节点，返回value</li><li>如果链表没有对应值，返回null<h2 id="HashMap-resize"><a href="#HashMap-resize" class="headerlink" title="HashMap resize"></a>HashMap resize</h2>当 HashMap 中的元素越来越多的时候，hash 冲突的几率也就越来越高，因为数组的长度是固定的。各个链表的长度也变长，导致效率下降。所以当map大到一定程度时，需要对其扩容。<br>不同于ArrayList的扩容，HashMap 数组扩容时需要将原数组中的数据重新计算其在新数组中的位置，并放进去。这也是很消耗性能的一点。<br>那么 HashMap 什么时候进行扩容呢？当 HashMap 中的元素个数超过数组大小 loadFactor时，就会进行数组扩容，loadFactor的默认值为 0.75。也就是说，默认情况下，数组大小为 16，那么当 HashMap 中元素个数超过 16<em>0.75=12 的时候，就把数组的大小扩展为 2</em>16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单地说，HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。HashMap 底层采用一个 Entry[] 数组来保存所有的 key-value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该Entry。</p><h2 id="HashTable"><a href="#HashTable" class="headerlink" title="HashTable"></a>HashTable</h2><p>HashTable同HashMap，都是一种存储key-value的数据结构。两者有以下不同：</p><ol><li>HashTable中几乎所有public方法都是synchronized的，方法内部也是通过synchronized代码块来实现，因此其是同步的，线程安全的，HashMap相反。</li><li>HashTable的key和value都不可以为null，HashMap相反。</li></ol>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习调参经验</title>
      <link href="/2018/09/11/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/"/>
      <url>/2018/09/11/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/</url>
      <content type="html"><![CDATA[<p>总结深度学习模型构建中的调参经验。</p><a id="more"></a><h2 id="参数初始化方法"><a href="#参数初始化方法" class="headerlink" title="参数初始化方法"></a>参数初始化方法</h2><ol><li>normal初始化cnn的参数，最后acc只能到70%多，仅仅改成xavier，acc可以到98%。</li><li>给word embedding初始化，最开始使用了TensorFlow中默认的initializer（即glorot_uniform_initializer，也就是xavier），训练速度慢，结果也不好。改为uniform，训练速度飙升，结果也飙升。</li></ol><p>总结，初始化就跟黑科技一样，用对了超参都不用调；没用对，跑出来的结果就跟模型有bug一样不忍直视。</p><h2 id="编码风格"><a href="#编码风格" class="headerlink" title="编码风格"></a>编码风格</h2><p>由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。</p><h2 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h2><ol start="3"><li>刚开始, 先上小规模数据, 模型往大了放, 只要不爆显存, 能用256个filter你就别用128个. 直接训练一个过拟合网络. 目的是为了看整体训练流程没有错误，假如loss不收敛就要好好反思了。</li><li>观察loss剩余观察accuracy</li><li>对数据集进行shuffle</li><li>卷积核无脑用3*3，无脑用ReLU，无脑加BN，适当考虑加Dropout，无脑用Adam</li><li>第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要).</li></ol><h2 id="超参经验取值"><a href="#超参经验取值" class="headerlink" title="超参经验取值"></a>超参经验取值</h2><ol><li>learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。同时建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</li><li>网络层数： 先从1层开始。</li><li>每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。</li><li>batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</li><li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15</li><li>dropout： 0.5</li><li>L2正则：1.0，超过10的很少见。</li><li>词向量embedding大小：128，256<br>正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样。<br>在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。</li></ol><h2 id="自动调参"><a href="#自动调参" class="headerlink" title="自动调参"></a>自动调参</h2><p>人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：</p><ul><li>Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li><li>Random Search。Bengio在Random Search for Hyper-Parameter Optimization中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。</li><li>Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： Practical Bayesian Optimization of Machine Learning Algorithms ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：<br>jaberg/hyperopt, 比较简单。<br>fmfn/BayesianOptimization， 比较复杂，支持并行调参。</li></ul>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow之RNN</title>
      <link href="/2018/09/05/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8BRNN/"/>
      <url>/2018/09/05/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8BRNN/</url>
      <content type="html"><![CDATA[<p>总结TensorFlow中RNN常见的API使用方法和技巧。</p><a id="more"></a><h2 id="单步RNN：RNNCell"><a href="#单步RNN：RNNCell" class="headerlink" title="单步RNN：RNNCell"></a>单步RNN：RNNCell</h2><p>TensorFlow中的所有RNN都继承自tf.contrib.rnn.RNNCell这个抽象类，每个RNNCell都有一个cell方法，(output, next_state) = call(input, state)。<br>假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)。再调用一次call(x2, h1)就可以得到(output2, h2)。也就是说，<strong>每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。</strong></p><h2 id="tf-contrib-rnn-BasicLSTMCell"><a href="#tf-contrib-rnn-BasicLSTMCell" class="headerlink" title="tf.contrib.rnn.BasicLSTMCell()"></a>tf.contrib.rnn.BasicLSTMCell()</h2><p>BasicLSTMCell是比较基本的创建LSTM cell的一个类.</p><h3 id="构造函数参数"><a href="#构造函数参数" class="headerlink" title="构造函数参数"></a>构造函数参数</h3><ol><li>num_units: LSTM内部节点数目</li><li>forget_bias float类型, 遗忘门加上一个bias. 为了减少在训练早期的遗忘尺度，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记。</li><li>state_is_tuple 为True 的话, 接受和返回的states都是一个tuples,其中成员是和返回的状态都是一个2元元组,成员分别为 c_state and m_state.</li><li>activation 内部激活函数<h3 id="内部参数"><a href="#内部参数" class="headerlink" title="内部参数"></a>内部参数</h3></li><li>state_size  当state_is_tuple 为 true时，返回一个LSTMStateTuple(c=128, h=128)，c和h分别表示cell state的size和每个时刻输出的大小。</li><li>output_size 输出层的大小</li></ol><h3 id="zero-state-batch-size-dtype"><a href="#zero-state-batch-size-dtype" class="headerlink" title="zero_state(batch_size, dtype)"></a>zero_state(batch_size, dtype)</h3><p>返回一个填充零的状态state，通常用其作为initial state。<br>该state是一个BasicLSTMCellZeroState类，大小为[batch_size, n_units]，表示cell的初始state。</p><h3 id="call-inputs-state-scope"><a href="#call-inputs-state-scope" class="headerlink" title="__call__(inputs, state, scope)"></a>__call__(inputs, state, scope)</h3><p>在给定state和input上运行一次LSTM。返回本次运行的output和一个新的state。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">n_hidden = 128</span><br><span class="line">batch_size = 64</span><br><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)</span><br><span class="line">init_state = cell.zero_state(batch_size,dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">print(cell.state_size)</span><br><span class="line">print(cell.output_size)</span><br><span class="line">print(init_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LSTMStateTuple(c=128, h=128)</span><br><span class="line">128</span><br><span class="line">LSTMStateTuple(c=&lt;tf.Tensor &apos;BasicLSTMCellZeroState/zeros:0&apos; shape=(64, 128) dtype=float32&gt;, h=&lt;tf.Tensor &apos;BasicLSTMCellZeroState/zeros_1:0&apos; shape=(64, 128) dtype=float32&gt;)</span><br></pre></td></tr></table></figure><p>###LSTMCell和BasicLSTMCell的区别</p><ol><li><p>增加了use_peepholes, bool值，为True时增加窥视孔。</p></li><li><p>增加了cell_clip, 浮点值，把cell的值限制在 ±cell_clip内</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)</span><br></pre></td></tr></table></figure></li><li><p>增加了num_proj（int）和proj_clip(float), 相对于BasicLSTMCell，在输出m计算完之后增加了一层线性变换，并限制了输出的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m = _linear(m, self._num_proj, bias=False, scope=scope)</span><br><span class="line">m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)</span><br></pre></td></tr></table></figure></li></ol><h2 id="tf-contrib-rnn-MultiRNNCell-cells"><a href="#tf-contrib-rnn-MultiRNNCell-cells" class="headerlink" title="tf.contrib.rnn.MultiRNNCell(cells)"></a>tf.contrib.rnn.MultiRNNCell(cells)</h2><p>定义多层RNN，初始参数cells是一个list，元素为想要叠加在一起的cells。</p><h3 id="内置参数"><a href="#内置参数" class="headerlink" title="内置参数"></a>内置参数</h3><ul><li>state_size 输出一个list，list的长度等于LSTM隐藏层的大小，每个元素都是一个LSTMStateTuple，其中有cell和ht的size。</li><li>output_size 输出层size，大小等于最后一个LSTM层的n_units<h3 id="zero-state-batch-size"><a href="#zero-state-batch-size" class="headerlink" title="zero_state(batch_size)"></a>zero_state(batch_size)</h3>对MultiRNN中每个LSTM隐藏层返回一个初始state。是一个list，list大小等于MultiRNN中隐藏层的个数，list中每个元素为一个BasicLSTMZeroState，大小为[batch_size, n_units]，表示该LSTM隐藏层cell的初始state。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">n_hidden = 128</span><br><span class="line">batch_size = 64</span><br><span class="line"></span><br><span class="line">cell_list = []</span><br><span class="line">for i in range(3):</span><br><span class="line">    n_hidden += 64</span><br><span class="line">    c = tf.contrib.rnn.BasicLSTMCell(n_hidden)</span><br><span class="line">    cell_list.append(c)</span><br><span class="line"></span><br><span class="line">multiCell = tf.contrib.rnn.MultiRNNCell(cell_list)</span><br><span class="line">init_states = multiCell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">print(multiCell.state_size)</span><br><span class="line">print(multiCell.output_size)</span><br><span class="line">print(init_states)</span><br><span class="line"></span><br><span class="line">(LSTMStateTuple(c=192, h=192), LSTMStateTuple(c=256, h=256), LSTMStateTuple(c=320, h=320))</span><br><span class="line">320</span><br><span class="line">(LSTMStateTuple(c=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0&apos; shape=(64, 192) dtype=float32&gt;, h=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0&apos; shape=(64, 192) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0&apos; shape=(64, 256) dtype=float32&gt;, h=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0&apos; shape=(64, 256) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState_2/zeros:0&apos; shape=(64, 320) dtype=float32&gt;, h=&lt;tf.Tensor &apos;MultiRNNCellZeroState/BasicLSTMCellZeroState_2/zeros_1:0&apos; shape=(64, 320) dtype=float32&gt;))</span><br></pre></td></tr></table></figure></li></ul><h2 id="tf-nn-dynamic-rnn"><a href="#tf-nn-dynamic-rnn" class="headerlink" title="tf.nn.dynamic_rnn()"></a>tf.nn.dynamic_rnn()</h2><p>这个函数的作用就是通过指定的RNN Cell来展开计算神经网络.</p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>dynamic_rnn(cell,inputs,sequence_length=None,initial_state=None,dtype=None,parallel_iterations=None,swap_memory=False,time_major=False,scope=None)<br>其中常用的参数如下所示：</p><ul><li>cell: RNNCell的对象.</li><li>inputs: RNN的输入,当time_major == False (default) 的时候,必须是形状为 [batch_size, max_time, …] 的tensor, 要是 time_major == True 的话, 必须是形状为 [max_time, batch_size, …] 的tensor.</li><li>initial_state: RNN的初始状态. 一般用cell.zero_state()的返回结果作为输入。</li><li>time_major: 规定了输入和输出tensor的格式,如果 true, tensor为[max_time, batch_size, depth]. 若是false, 那么tensor的形状为[batch_size, max_time, depth].<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3></li><li>outputs: 表示RNN的输出tensor,要是time_major == False (default),那么这个tensor的形状为[batch_size, max_time, cell.output_size].</li><li>state: 最终state,输出该RNN中每个隐藏层的final state。对于只有一层的RNN，返回一个LSTMStateTuple，对于多层RNN，返回一个LSTMStateTuple的list，表示每个隐藏层的final state。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)</span><br><span class="line">init_state = cell.zero_state(batch_size)</span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,</span><br><span class="line">    initial_state=init_state, dtype=tf.float32)</span><br><span class="line"># output is a tensor of shape [batch_size, time_steps, hidden_size]</span><br><span class="line"># state is a tensor of shape [batch_size, hidden_size]</span><br></pre></td></tr></table></figure></li></ul><h2 id="RNN中的Dropout"><a href="#RNN中的Dropout" class="headerlink" title="RNN中的Dropout"></a>RNN中的Dropout</h2><p>RNN对时序序列上的连接不进行Dropout，因为RNN中的循环会放大噪声，扰乱它自己的学习。而仅仅在同一时刻不同隐藏层之间进行Dropout。</p><h3 id="tf-contrib-rnn-DropoutWrapper"><a href="#tf-contrib-rnn-DropoutWrapper" class="headerlink" title="tf.contrib.rnn.DropoutWrapper()"></a>tf.contrib.rnn.DropoutWrapper()</h3><p>对rnn进行Dropout，核心参数如下：</p><ul><li>cell</li><li>input_keep_prob 输入层的Dropout概率</li><li>output_keep_prob 输出层的Dropout概率</li><li>variational_recurrent=False 该参数用来控制Dropout方式，当为False时，仅在rnn层与层之间进行Dropout，在时序上不进行Dropout；当为True时，也在每个time step上进行Dropout。If this parameter is set, input_size must be provided.</li><li>input_size TensorShape objects containing the depth(s) of the input tensors expected to be passed in to the DropoutWrapper.</li></ul>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow之模型保存和回复</title>
      <link href="/2018/09/02/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8B%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%9B%9E%E5%A4%8D/"/>
      <url>/2018/09/02/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8B%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%9B%9E%E5%A4%8D/</url>
      <content type="html"><![CDATA[<p>模型的保存和回复是深度学习中非常重要的一部分，我们可以将训练好的模型保存到本地文件，并在任何想要使用该模型的时候从文件中加载，这样我们就不需要每次都重新训练模型。<br><a id="more"></a></p><h2 id="TensorFlow模型保存"><a href="#TensorFlow模型保存" class="headerlink" title="TensorFlow模型保存"></a>TensorFlow模型保存</h2><p>TensorFlow使用 ‘tf.train.Saver()’用来保存和回复模型</p><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;b&apos;, [])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">saver.save(sess, &apos;checkpoints/tftcp.model&apos;)</span><br></pre></td></tr></table></figure><p>打开保存路径，会发现里面包含四个文件：</p><ol><li>model.data-00000-of00001 包含模型中训练好的权重，可能是最大的文件</li><li>model.meta 模型的网络结构，包含重建模型的全部信息</li><li>model.index 连接前两个文件的索引结构，用于在数据文件中找到对应节点的参数</li><li>checkpoint 如果在整个训练过程中保存了多个版本的模型，该文件会对每个模型进行跟踪。</li></ol><h3 id="恢复模型"><a href="#恢复模型" class="headerlink" title="恢复模型"></a>恢复模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;b&apos;, [])</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver.restore(sess, &apos;checkpoints/tftcp.model&apos;)</span><br><span class="line">sess.run([a,b])</span><br></pre></td></tr></table></figure><p>通过如上方法即可将模型回复，可以发现在运行前并不需要初始化a和b，因为restore运算会将变量的值从文件复制到会话中。</p><h3 id="获取变量"><a href="#获取变量" class="headerlink" title="获取变量"></a>获取变量</h3><p>当一个 tf.train.Saver 程序初始化后，它会查看当前图形并获取变量列表；这是 saver「关心」的永久存储的变量列表。我们可以用._var_list 属性来检查：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;b&apos;, [])</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">c = tf.get_variable(&apos;c&apos;, [])</span><br><span class="line">print(saver._var_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&lt;tf.Variable &apos;a:0&apos; shape=() dtype=float32_ref&gt;, &lt;tf.Variable &apos;b:0&apos; shape=() dtype=float32_ref&gt;]</span><br></pre></td></tr></table></figure></p><p>因为在创建 saver 时 c 还没有出现，所以它并没有成为函数的一部分。一般来说，你要在创建 saver 之前确保已经创建了所有的变量。</p><p>当然，在某些特定的情况下，可能只需保存变量的一个子集。当创建 var_list 以期望它跟踪可用变量子集时，tf.train.Saver 允许传递 var_list。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;b&apos;, [])</span><br><span class="line">c = tf.get_variable(&apos;c&apos;, [])</span><br><span class="line">saver = tf.train.Saver(var_list=[a,b])</span><br><span class="line">print(saver._var_list)</span><br></pre></td></tr></table></figure><h3 id="模型局部加载"><a href="#模型局部加载" class="headerlink" title="模型局部加载"></a>模型局部加载</h3><p>有的时候，我们只想要从文件中加载模型的一部分当前模型下：<br>如下保存代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;b&apos;, [])</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">saver.save(sess, save_path)</span><br></pre></td></tr></table></figure></p><p>如下加载代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver = tf.restore(sess, saved_path)</span><br><span class="line">sess.run(a)</span><br></pre></td></tr></table></figure></p><p>从上述代码中，我们将变量a，和b保存在文件中。在加载代码中，我们仅新建了一个变量a并令其name=’a’，然后从文件中restore，会发现上述代码可以运行，加载过程自动按照变量名称进行匹配，丢弃没有被加载的变量。</p><p>但如果加载文件变成如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.get_variable(&apos;a&apos;, [])</span><br><span class="line">b = tf.get_variable(&apos;c&apos;, [])</span><br><span class="line">save_path = &apos;check/model.model&apos;</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver.restore(sess, save_path)</span><br><span class="line">print(sess.run(b))</span><br></pre></td></tr></table></figure></p><p>上述代码会报错，因为在保存的文件中，并不存在一个变量名为’c’的变量。代码不知道该将变量’c’赋值为文件中的哪个变量。<br>为了解决上述问题，’tf.train.Saver()’提供了参数用来指定变量的加载：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.get_variable(&apos;x&apos;, [])</span><br><span class="line">y = tf.get_variable(&apos;y&apos;, [])</span><br><span class="line">save_path = &apos;check/model.model&apos;</span><br><span class="line">saver = tf.train.Saver(var_list=&#123;&apos;a&apos;:x, &apos;b&apos;:y&#125;)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver.restore(sess, save_path)</span><br><span class="line">print(sess.run(x))</span><br></pre></td></tr></table></figure></p><p>通过Saver的参数var_list来指定文件中的变量值和模型的中变量的映射关系，key为文件中的变量名， value为当前模型中的变量。</p><h3 id="模型检查"><a href="#模型检查" class="headerlink" title="模型检查"></a>模型检查</h3><p>如果想加载的模型来源于网络或由自己创建（两个月前），那你很可能不知道原始变量是如何命名的。要检查保存的模型，需要使用官方 Tensorflow 库的一些工具。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.get_variable(&apos;x&apos;, [])</span><br><span class="line">y = tf.get_variable(&apos;y&apos;, [])</span><br><span class="line">save_path = &apos;check/model.model&apos;</span><br><span class="line">print(tf.contrib.framework.list_variables(save_path))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[(&apos;a&apos;, []), (&apos;b&apos;, [])]</span><br></pre></td></tr></table></figure></p><p>这样就可以获得保存文件中各个参数的名称和shape。</p>]]></content>
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>概率论之抛硬币</title>
      <link href="/2018/09/01/Mathematic/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B9%8B%E6%8A%9B%E7%A1%AC%E5%B8%81/"/>
      <url>/2018/09/01/Mathematic/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B9%8B%E6%8A%9B%E7%A1%AC%E5%B8%81/</url>
      <content type="html"><![CDATA[<p>算法面试中会经常遇到概率论问题，被问到最多的就是各种各样的抛硬币问题。<br><a id="more"></a></p><h2 id="概率题之抛硬币"><a href="#概率题之抛硬币" class="headerlink" title="概率题之抛硬币"></a>概率题之抛硬币</h2><h3 id="连续两次正面的期望"><a href="#连续两次正面的期望" class="headerlink" title="连续两次正面的期望"></a>连续两次正面的期望</h3><p>假设连续两次正面的期望为E，那么当我们抛一次硬币是，有如下两种情况：</p><ol><li>反面，这时候意味着接下来的期望仍然为E（相当于本次没抛），然后将本次抛硬币考虑进去，E = E+1</li><li>正面，这时候考虑下一次抛硬币，如果下一次也为正面，E=2，抛硬币结束。如果下一次为反面，意味着需要重新抛硬币，在将这两次抛硬币：E = E+2<br>所以，期望E就等于：<blockquote><p>E = 0.5<em>(E+1) + 0.25</em>2 + 0.25*(E+2)<br>最后得 E=6</p></blockquote></li></ol><p>可以将题干进行扩展，变为连续n个正面。推理过程是一样的。</p><h3 id="一堆硬币"><a href="#一堆硬币" class="headerlink" title="一堆硬币"></a>一堆硬币</h3><p>一堆硬币，每天都随便捡一枚抛，如果抛到正面，就把它翻过来；如果抛到反面，就再抛一下，问很长很长时间以后，硬币正面和反面的比例会趋近于多少？</p><h4 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h4><p>假设正面的比例为x，则反面为1-x，对于任意一次抛硬币：</p><ul><li>正面，将其翻过来，得到的结果为反面</li><li>反面，重新抛，所以0.5概率得到正面，0.5概率得到反面<br>所以有一下公式：<blockquote><p>x<em>0 + (1-x)\</em>0.5 = x</p></blockquote></li></ul><p>所以 x = 1/3，因此硬币正面和反面的比例会趋近于 x/(1-x) = 1/2</p><h3 id="第一次连续两次正面"><a href="#第一次连续两次正面" class="headerlink" title="第一次连续两次正面"></a>第一次连续两次正面</h3><p>连续抛硬币，直到第一次出现连续两次正面为止，恰好抛了 N 次的概率是多少？</p><h4 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h4><p>考虑“恰好”抛 N 次硬币，到底有多少种情况可以得出最后两次是连续出现了正面，而之前没有出现过连续正面。</p><ul><li>假设 f(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是反面；</li><li>假设 g(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是正面。<br>所以 f(1)=f(2)=0，g(1)=0，g(2)=1，而当 x&gt;2，</li><li>求 f(x+1)，因为第一次是反面，所以这新添加的第一次不影响结果，因此 f(x+1)=f(x)+g(x)</li><li>求 g(x+1)，因为第一次是正面，必须要保证第二次不能为正，所以 g(x+1)=f(x)<br>于是得到：</li><li>f(x+2)=f(x+1)+g(x+1)=f(x+1)+f(x)</li><li>g(x+1)=f(x)</li></ul><h3 id="抛-N-次硬币，正反两面出现次数相同的概率"><a href="#抛-N-次硬币，正反两面出现次数相同的概率" class="headerlink" title="抛 N 次硬币，正反两面出现次数相同的概率"></a>抛 N 次硬币，正反两面出现次数相同的概率</h3><p>其实就是从 N 个硬币的空位中，选出 N/2 个作为正面，余下 N/2 个作为反面，应用组合公式可得到：</p><blockquote><p>C(N,N/2)/2^N = N!/((N-N/2)!(N/2)!)/2^N</p></blockquote><p>正面出现次数超过反面的概率？</p><p>因为正反情况相同，因此正面次数超过反面的概率应当等于反面次数超过正面的概率，因此结果为 1 减去上面那一问的结果之后除以 2：</p><blockquote><p>(1-C(N,N/2)/2^N)/2</p></blockquote>]]></content>
      
      <categories>
          
          <category> Mathematic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mathematic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>经典算法总结之BitMap</title>
      <link href="/2018/08/30/Algorithms/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E4%B9%8BBitMap/"/>
      <url>/2018/08/30/Algorithms/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E4%B9%8BBitMap/</url>
      <content type="html"><![CDATA[<p>本文总结一些常见的经典算法，这些算法更重于思想，实现起来比较简单。故将其总结到了一起。</p><a id="more"></a><h2 id="BitMap"><a href="#BitMap" class="headerlink" title="BitMap"></a>BitMap</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>如何对大量整数进行去重？我们知道，一般情况下一个int占用4字节，现在有10亿个小于1亿的int数文件，要求输出0~1亿这些整数中，从未在文件中出现的整数。<br>10亿个数大概大小为3.72G，显然不能直接将如此庞大的数据一次性读入内存，而如果分批次读取有会大量消耗IO。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>构建一个长度为1亿的bit数组。我们知道1 bit有两个取值，1或0。这里，我们用1表示该位置表示的index出现过，0表示未出现。就可以将原 10亿<em>4</em>8 bit的数组大小压缩成10亿 bit。</p><h3 id="具体思路"><a href="#具体思路" class="headerlink" title="具体思路"></a>具体思路</h3><p>这时候会遇到一个问题：常见编程语言数据存储的最小单位为字节（即便是boolean类型根据java虚拟机规范，也是四个字节）。而我们为了压缩内存，用的数据类型是bit而不是字节。这就涉及到第一个问题，如何转换。<br>为了快速定位需要改变的bit，需要找到两个值：index和position，前者为对应的在字节数组的index，后者为在该字节对应的位。<br>比如说输入的数字为N，对应的</p><p>index(N) = N/8 = N&gt;&gt;&gt;3<br>position(N) = N%8 = N&amp;0x07</p><h4 id="add方法"><a href="#add方法" class="headerlink" title="add方法"></a>add方法</h4><p>在明确了定位index和position的方法后，向一个bits[]添加元素的方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private byte[] bits;</span><br><span class="line">public void add(int x)&#123;</span><br><span class="line">    int index = x&gt;&gt;&gt;3;</span><br><span class="line">    int position = x&amp;0x07;</span><br><span class="line">    bits[index] |= (1&lt;&lt;position);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意这里需要对’1’进行左移position位，然后和原元素取或。</p><h4 id="clear方法"><a href="#clear方法" class="headerlink" title="clear方法"></a>clear方法</h4><p>将给定元素N从bits[]数组中删除：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public void clear(int x)&#123;</span><br><span class="line">    int index = x&gt;&gt;&gt;3;</span><br><span class="line">    int position = x &amp; 0x07;</span><br><span class="line">    bits[index] &amp;= ~(1&lt;&lt;position) // 取与</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="contain方法"><a href="#contain方法" class="headerlink" title="contain方法"></a>contain方法</h4><p>判断输入int整数是否包含在bits[]中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public boolean contain(int x)&#123;</span><br><span class="line">    int index = x &gt;&gt;&gt; 3;</span><br><span class="line">    int position = x &amp; 0x07</span><br><span class="line">    if ((bits[index]&gt;&gt;position)&amp;1) == 1)</span><br><span class="line">        return true;</span><br><span class="line">    else</span><br><span class="line">        return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="java-util-BitSet类"><a href="#java-util-BitSet类" class="headerlink" title="java.util.BitSet类"></a>java.util.BitSet类</h3><p>在java中，直接提供了BitSet类方便我们使用，常见方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bits = BitSet(int size)</span><br><span class="line">bits.clear(int index) 将指定index设为false</span><br><span class="line">bits.get(int index) 返回对应index的值</span><br><span class="line">bits.set(int index, boolean v)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之lightGBM</title>
      <link href="/2018/08/27/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BlightGBM/"/>
      <url>/2018/08/27/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BlightGBM/</url>
      <content type="html"><![CDATA[<p>LightGBM是微软开发的一种boosting方法，主要针对xgboost的缺点进行优化</p><a id="more"></a><h2 id="xgboost的不足之处"><a href="#xgboost的不足之处" class="headerlink" title="xgboost的不足之处"></a>xgboost的不足之处</h2><ul><li>每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</li><li>预排序方法（pre-sorted）：空间消耗大，需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</li><li>对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。<h2 id="LightGBM特点"><a href="#LightGBM特点" class="headerlink" title="LightGBM特点"></a>LightGBM特点</h2>LightGBM正是针对xgboot的缺点进行优化，LightGBM有如下特点：</li></ul><ol><li>基于Histogram的决策树算法</li><li>带深度限制的leaf-wise的叶子生长策略</li><li>直方图做差加速</li><li>直接支持类别特征(Categorical Feature)</li><li>Cache命中率优化</li><li>基于直方图的稀疏特征优化</li><li>多线程优化</li></ol><p>前2个特点使我们尤为关注的。</p><h3 id="Histogram算法"><a href="#Histogram算法" class="headerlink" title="Histogram算法"></a>Histogram算法</h3><p>histogram算法不同于预排序，而是将特征值做装箱处理：对于连续型特征，浮点特征值离散化成k个整数段，相当于将某个区间的连续值映射成一个离散值。</p><h4 id="Histogram构建提升树流程"><a href="#Histogram构建提升树流程" class="headerlink" title="Histogram构建提升树流程"></a>Histogram构建提升树流程</h4><p><img src="/images/machine_learning/lightgbm_procedure.png" alt="Lightgbm_procedure"></p><ol><li>遍历当前树的每个叶子节点</li><li>对于每个叶子节点，选择最优分裂属性和对应的分裂值</li><li>对每个特征简历一个直方图，并对特征的各个连续特征值进行装箱操作。一次对于各特征，直方图的每个箱包含了一定的样本，此时 <strong>计算每个箱样本的梯度之和以及样本个数</strong></li><li>遍历每个箱，找到适合分类的最佳bin（箱）</li><li>找最佳bin的方法：SL是当前分裂 bin 左边所有 bin 的集合,对比理解SR，那么SP其中的 P 就是 parent 的意思，就是父节点，传统的决策树会有分裂前后信息增益的计算，典型的 ID3 或者 C45 之类，在这里我们也会计算，但是SR中所有 bin 的梯度之和不需要在额外计算了，直接使用父节点的减去左边的就得到。</li><li>在遍历完所有特种功能后计算各个loss，找到loss最小的特征和对应分裂点作为最佳分裂点。loss的计算公式为：<blockquote><p>loss = sl^2/nl + sr^2/nr - sp2/np</p></blockquote></li></ol><p>通过这种方法，LightGBM相当于通过bin减小了特征值的取值个数，也大大减小了最佳分割点的计算次数。</p><h3 id="带深度限制的Leaf-wise的叶子生长策略"><a href="#带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="带深度限制的Leaf-wise的叶子生长策略"></a>带深度限制的Leaf-wise的叶子生长策略</h3><p>xgboost可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上这是一种低效做法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p><p>Leaf-wise则是一种更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。</p><p>Leaf-wise的缺点：可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SVD分解在推荐系统中的应用</title>
      <link href="/2018/08/25/Machine%20Learning/SVD%E5%88%86%E8%A7%A3%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/08/25/Machine%20Learning/SVD%E5%88%86%E8%A7%A3%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>整理SVD在推荐系统中的应用<br><a id="more"></a></p><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>我们知道，任何一个’M * N’的矩阵，可以被分解成三个矩阵的乘积：</p><ol><li>U矩阵：M行M列的列正交矩阵</li><li>S矩阵：M*N的对角矩阵，矩阵元素非负</li><li>V矩阵：N<em>N的正交矩阵的倒置<br>即 A = U </em> S * V’</li></ol><h2 id="SVD在推荐系统中"><a href="#SVD在推荐系统中" class="headerlink" title="SVD在推荐系统中"></a>SVD在推荐系统中</h2><p>我们知道，在推荐系统中，user-item表是非常关键的数据，这个二维表存储了每个用户对每个item的喜欢程度。而在实际应用中，user-item表往往是一个极其巨大，且极其稀疏的表。而这个稀疏性往往也导致推荐系统很难直接应用user-item数据。<br>通过SVD分解，可以将一个大的矩阵降维，进行有损压缩。</p><h3 id="SVD对user-item矩阵进行分解"><a href="#SVD对user-item矩阵进行分解" class="headerlink" title="SVD对user-item矩阵进行分解"></a>SVD对user-item矩阵进行分解</h3><p>通过上文对SVD分解的定义，对于user-item矩阵A，我们可以将其拆分成三个矩阵的乘积。<br>其中S矩阵，是一个对角矩阵，对角上的每个值就是特征值， <strong>特征值用来表示该矩阵向着该特征值对应的特征向量方向的变化权重。</strong> S矩阵对角线上的值依次减小。值越大，说明矩阵在该特征向量方向上变化越大，该特征向量也就越大。也就是说，我们可以选择前k个大的特征值和特征向量来表示原始矩阵，抛弃剩余的小特征值以及对应的特征向量。这样虽然损失了一定的信息，但大大压缩了矩阵的大小。<br>举例：</p><blockquote><p>user-item矩阵A大小为6<em>4， 经过SVD分解，得到三个矩阵U= 6\</em>6，S= 6*4，V=4*4。通过对S矩阵的观察发现，S对角上前两个值特别大，所以选择k=2对矩阵进行压缩，压缩后的形状为：U= 6*2，S= 2*2，V= 2*4。</p></blockquote><p>得到压缩后的三个矩阵后，将三个矩阵相乘，可以得到压缩后的user-item矩阵。经过对比可以发现，原矩阵和压缩后的矩阵每个元素都非常接近。</p><h3 id="SVD矩阵数据相关性"><a href="#SVD矩阵数据相关性" class="headerlink" title="SVD矩阵数据相关性"></a>SVD矩阵数据相关性</h3><p>如上文例子所示，原始的user-item矩阵大小为6<em>4，即一共有6个user和4个item。经过SVD分解，产生的矩阵U=6</em>2，V=2*4。也就是说，矩阵U的每一行可以表示一个user，矩阵V的每一行表示一个item。<br>可以在坐标系中画出每个user和item的位置。</p><p><img src="/images/recommender_practice/svd_graph.png" alt="svd_graph"><br>从图中可以看出，Season5和Season6距离很近，用户Ben和用户Fred距离也很近。</p><p>至此我们通过SVD分解，将每个user和item的特征向量表示压缩到更小的维度空间，然后计算user之间或者item之间的相似度，进行推荐。</p><h2 id="SVD推荐流程"><a href="#SVD推荐流程" class="headerlink" title="SVD推荐流程"></a>SVD推荐流程</h2><ol><li>将原始的user-item矩阵进行SVD分解，得到三个矩阵U，S，V</li><li>选定压缩维度k，将对角矩阵S进行压缩。</li><li>矩阵U和矩阵V也根据压缩维度k进行对应的压缩</li><li>压缩后的矩阵U每一行都代表着一个用户的特征向量</li><li>矩阵V的每一列都表示一个item的特征向量</li><li>得到特征向量后，可以在用户之间或者item之间计算相似度，进而完成推荐任务</li></ol><h2 id="应用SVD的注意事项"><a href="#应用SVD的注意事项" class="headerlink" title="应用SVD的注意事项"></a>应用SVD的注意事项</h2><ol><li>在user-item矩阵较大时，SVD的时间消耗很大，可以使用梯度下降等方法进行近似计算，减少时间消耗。</li><li>SVD分解后，仍然需要进行相似度的计算，各种相似度计算方法的选择直接影响模型准确率。</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python变量作用域</title>
      <link href="/2018/08/21/Programming%20Language/Python/Python%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F/"/>
      <url>/2018/08/21/Programming%20Language/Python/Python%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F/</url>
      <content type="html"><![CDATA[<p>Python的作用域和其他常见的编程语言作用域不同，特记录于此。</p><a id="more"></a><h2 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(5):</span><br><span class="line">    res = 0</span><br><span class="line">print(i)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p>在上段代码中，在如C++和Java语言中，两个print会报错，很简单，因为变量i和变量res都是for循环体内的局部变量，外部无法引用。<br>而在Python中：</p><ol><li>能改变作用域的代码段是：’def’, ‘class’, ‘lambda’</li><li>‘if/elif/else’, ‘try/except/finally’, ‘for/while’并不能涉及作用域的更改，也就是代码块中的变量在外部也是可以访问的。</li><li>变量的搜索路径是：本地变量-&gt;全局变量<br>在Python中，以下代码可以正常运行：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    newvar=8</span><br><span class="line">    print(newvar)</span><br><span class="line">    break;</span><br><span class="line">print(newvar)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    newlocal=7</span><br><span class="line">    raise Exception</span><br><span class="line">except:</span><br><span class="line">    print(newlocal)</span><br></pre></td></tr></table></figure></li></ol><h2 id="变量搜索过程"><a href="#变量搜索过程" class="headerlink" title="变量搜索过程"></a>变量搜索过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def scopetest():</span><br><span class="line">    var=6;</span><br><span class="line">    print(var)#</span><br><span class="line">    def innerFunc():</span><br><span class="line">        print(var)#look here</span><br><span class="line">    innerFunc()</span><br><span class="line"></span><br><span class="line">var=5</span><br><span class="line">print(var)</span><br><span class="line">scopetest()</span><br><span class="line">print(var)</span><br></pre></td></tr></table></figure><p>输出结果：5 6 6 5<br>根据调用顺序反向搜索，先本地变量再全局变量，例如搜先在innerFunc中搜索本地变量，没有，好吧，找找调用关系上一级scopetest，发现本地变量var=6，所以打印var=6.</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LFM隐语义模型</title>
      <link href="/2018/08/21/Machine%20Learning/LFM%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/08/21/Machine%20Learning/LFM%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>隐语义模型LFM和LSI，LDA，Topic Model其实都属于隐含语义分析技术，本质上是找出潜在的主题或分类。这些技术一开始都是在文本挖掘领域中提出来的，近些年它们也被不断应用到其他领域中，并得到了不错的应用效果。比如，在推荐系统中它能够基于用户的行为对item进行自动聚类，也就是把item划分到不同类别/主题，这些主题/类别可以理解为用户的兴趣。</p><a id="more"></a><h2 id="商品分类问题"><a href="#商品分类问题" class="headerlink" title="商品分类问题"></a>商品分类问题</h2><p>在推荐系统中，最直接的想法就是将物品分类，如果用户经常购买某个类别的商品，就可以向其推荐该类别的其他商品。但在实际应用中，“分类”本身就是一个比较难的东西，因为同一样物品，根据分类粒度不同，可以被分到多种类别中。<br>比如说，一本书《离散数学》，既可以被分到数学类，也可以被分到计算机类，或者按照出版社分类，按照作者分类。都是可以的，而用户可能对其中的一类感兴趣，比如说用户A特别喜欢这个作者的书，但其实对这类书不感兴趣。这就让“人工分类”变得非常难。</p><h3 id="两个问题"><a href="#两个问题" class="headerlink" title="两个问题"></a>两个问题</h3><ol><li>我们可以根据用户的数据对喜欢物品进行归类，但不等于该用户就只喜欢这几类类，对其他类别的商品就一点兴趣也没有。也就是说，我们需要了解用户对于所有类别的兴趣度。</li><li>对于一个给定的类来说，我们需要确定这个类中每个商品属于该类别的权重。权重有助于我们确定该推荐哪些商品给用户。</li></ol><h2 id="LFM"><a href="#LFM" class="headerlink" title="LFM"></a>LFM</h2><p>对于一个给定的用户行为数据集（数据集包含的是所有的user, 所有的item，以及每个user有过行为的item列表），使用LFM对其建模后，我们可以得到如下图所示的模型：（假设数据集中有3个user, 4个item, LFM建模的分类数为4）<br><img src="/images/recommender_practice/lfm_matrix.png" alt="lfm_matrix"></p><h3 id="LFM效果"><a href="#LFM效果" class="headerlink" title="LFM效果"></a>LFM效果</h3><p>在使用了LFM后：</p><ol><li>不需要手动对物品进行分类，分类结果是基于用户行为统计自动聚类的</li><li>不需要关注分类粒度，通过设置LFM的最终分类数可以自动控制粒度</li><li>对于一个item，不需要明确划分到某各类，而是计算属于各个类的权值，是一种软分类</li><li>对于一个user，可以得到他对每一个类的兴趣度，而不是仅关心几个类</li><li>对于每个class，可以得到类中每个item的权重，越能代表这个class，这个item 的权重就越高。</li></ol><h3 id="LFM参数计算"><a href="#LFM参数计算" class="headerlink" title="LFM参数计算"></a>LFM参数计算</h3><p>有了如上定义，接下来的问题就是如何计算矩阵P和矩阵Q中的参数值。一般做法是最优化损失函数。<br>在数据集上， 我们假定用户其有过行为的item为正样本，规定兴趣度RUI=1，没有行为的item为负样本，兴趣度RUI=0。<br><img src="/images/recommender_practice/lfm_loss_function.png" alt="lfm_loss_function"></p><h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def LFM(user_item, F, N, alpha, lambda_value):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    user_item 矩阵记录所有user对所有item的行为</span><br><span class="line">    F 表示分类数</span><br><span class="line">    N 表示更新迭代次数</span><br><span class="line">    alpha 学习速率</span><br><span class="line">    lambda_value 表示正则化参数</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 初始化P，Q矩阵</span><br><span class="line">    [P, Q] = InitModel(user_item, F)</span><br><span class="line">    # 开始更新迭代</span><br><span class="line">    for step in range(0, N):</span><br><span class="line">        # 从数据集中依次取出一个user和该user所有喜欢的item集合</span><br><span class="line">        for user, items in user_item.items():</span><br><span class="line">            # 从该user为发生行为的item中随机采样，取得item的正样本和负样本</span><br><span class="line">            samples = randomSelectSamples(items)</span><br><span class="line">            # sample为一个字典，key是各个item，value是该user对该item的rui，也就是喜爱程度</span><br><span class="line">            for item, rui in samples.items():</span><br><span class="line">                # 计算LFM对user和item的预测结果，并计算误差</span><br><span class="line">                error = rui - predict(P[user,:], Q[:,item])</span><br><span class="line">                # 更新参数</span><br><span class="line">                for f in range(0, F):</span><br><span class="line">                    P[user][f] += alpha * (eui * Q[f][item] - lambda_value * P[user][f])</span><br><span class="line">                    Q[f][item] += alpha * (eui * P[user][f] - lambda_value * Q[f][item])</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/08/16/Machine%20Learning/%E5%B8%B8%E8%A7%81%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
      <url>/2018/08/16/Machine%20Learning/%E5%B8%B8%E8%A7%81%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
      <content type="html"><![CDATA[<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>在特征工程中，特征选择是指从原有特征集合中选出自己，不改变原有特征空间。特征降维是指将原有的特征重新组合成包含信息更多的特征，改变了原有的特征空间。</p><h3 id="降维方法"><a href="#降维方法" class="headerlink" title="降维方法"></a>降维方法</h3><p>常见的降维方法由一下几种：</p><h4 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h4><h4 id="SVD奇异值分解"><a href="#SVD奇异值分解" class="headerlink" title="SVD奇异值分解"></a>SVD奇异值分解</h4><h4 id="Sammon映射"><a href="#Sammon映射" class="headerlink" title="Sammon映射"></a>Sammon映射</h4><h3 id="特征选择方法"><a href="#特征选择方法" class="headerlink" title="特征选择方法"></a>特征选择方法</h3><p>卡方检验，信息增益，皮尔逊系数，互信息系数，相关系数。通过确定各个特征和label之间的相关度，或者各个特征之间的相关度来选择特征。<br>皮尔逊系数只能衡量线性相关性而互信息系数能很好地衡量各种相关性。<br>通过L1正则项来选择特征</p><h3 id="常见特征工程手法"><a href="#常见特征工程手法" class="headerlink" title="常见特征工程手法"></a>常见特征工程手法</h3><ol><li>去掉取值变化小的特征：最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。</li><li>单变量特征选择：单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。<br>这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。<ol><li>Pearson相关系数：Pearson系数是对两个向量进行中心化后求向量cos夹角的计算。该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。Pearson相关系数的一个明显缺陷是，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。</li><li>互信息和最大信息系数：想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。</li><li>距离相关系数：距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</li><li>基于学习模型的特征排序 (Model based ranking)这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。</li></ol></li><li>线性模型<br>单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。<br>下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。、</li><li>正则化<br>正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||·||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。<br>L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。<br>很多特征的系数都是0。如果继续增加alpha的值，得到的模型就会越来越稀疏，即越来越多的特征系数会变成0。<br>然而，L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异</li><li><p>随机森林<br>随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。</p><ol><li>平均不纯度减少 mean decrease impurity<br>随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用 基尼不纯度 或者 信息增益 ，对于回归问题，通常采用的是 方差 或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。<br>这里特征得分实际上采用的是 Gini Importance 。使用基于不纯度的方法的时候，要记住：1、这种方法存在 偏向 ，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。<br>特征随机选择 方法稍微缓解了这个问题，但总的来说并没有完全解决。下面的例子中，X0、X1、X2是三个互相关联的变量，在没有噪音的情况下，输出变量是三者之和。<br>当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。<br>需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。</li><li>平均精确率减少 Mean decrease accuracy<br>另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。</li></ol></li><li><p>两种顶层特征选择算法<br>之所以叫做顶层，是因为他们都是建立在基于模型的特征选择方法基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。</p><ol><li>稳定性选择 Stability selection<br>稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</li><li>递归特征消除 Recursive feature elimination (RFE)<br>递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。</li></ol></li></ol><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><ol><li>用平均值、中值、分位数、众数、随机值等替代。效果一般，因为等于人为增加了噪声。</li><li>用其他变量做预测模型来算出缺失变量。效果比方法1略好。有一个根本缺陷，如果其他变量和缺失变量无关，则预测的结果无意义。如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。</li><li>最精确的做法，把变量映射到高维空间。比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。<br>正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。<br>随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。<br>特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/08/14/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/08/14/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>我叫李东方，目前在日本东京工业大学计算机学院攻读硕士学位，研究方向为深度学习和编程环境，本科是大连理工大学软件工程专业。<br>我拥有扎实的数据机构和算法基础，熟练使用TensorFlow等深度学习框架，熟悉机器学习、深度学习算法的原理和应用。熟悉Java、Python编程语言。有MapReduce和CUDA使用经验。了解强化学习。<br>目前的研究项目为：基于深度学习的代码补全系统。已完成的个人项目有基于深度学习的电影推荐系统和基于OpenAI环境的强化学习应用。<br>外语能力方面：可以熟练使用英语和日语。</p><h2 id="代码补全系统介绍"><a href="#代码补全系统介绍" class="headerlink" title="代码补全系统介绍"></a>代码补全系统介绍</h2><p>该项目的核心原理是：使用深度学习方法，对代码进行预测。数据集是从GitHub上爬去的JavaScript源码。然后将这些源码处理成token字符流。并在此基础上进行模型学习和预测。为便于理解，可以将这个任务想象成一个NLP任务，token看做一个个单词，基于给定的单词序列，预测接下来的单词是什么。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>模型方面，使用的是CNN+MultiRNN的结构。CNN with sliding window的作用是提取特征，RNN的作用是接收CNN提取的特征，并在时序上进行预测。<br>其中对于源码中的每个字符，构建embedding vector表示（dim=32），然后设定15个token为一个上下文（可以理解为一个定长的句子），然后分别用4个不同大小的卷积核对这个上下文进行扫描，在不从层次上提取特征图，然后将这个特征图拼接到一起，作为该上下文的特征表示。然后将其输入到RNN中，RNN使用的是双层LSTM的核，输入为CNN得到的一个上下文的特征表示，输出为一个token的one-hot encoding表示，作为整个模型的预测输出。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>对于原始的JavaScript源码，首先使用一个lexer处理成token 字节流，每个token有两个属性，一个是type，表示的是token的类别，如：变量名，关键字，字符串等等，另一个属性是value，表示该token 的取值。<br>考虑到有一些token如同数值变量，字符串等，有无数种取值，在一些情况下直接预测的难度非常大所以在目前阶段，将一些特定种类token的value进行统一处理，比如说数字的值直接领其为1.等。<br>然后将统一化token的type和value拼接到一起得到一种token。然后定义一个embedding matrix表示每种token的embedding 向量。</p><h4 id="表现"><a href="#表现" class="headerlink" title="表现"></a>表现</h4><p>目前的准确率是70%左右，同时建立一个bi-gram模型作为对照，该bi-gram的准确率接近40%。</p><h4 id="future-work"><a href="#future-work" class="headerlink" title="future work"></a>future work</h4><p>考虑到该研究本质上是一个类NLP项目，所以目前的想法是将seq2seq应用到项目中，对比准确率。</p><h2 id="推荐系统项目介绍"><a href="#推荐系统项目介绍" class="headerlink" title="推荐系统项目介绍"></a>推荐系统项目介绍</h2><p>整个系统的核心是使用MovieLen的电影数据集，通过对电影数据，用户数据和评分数据进行处理。得到每个电影和用户的特征。然后分别对每个特征构建embeding matrix以及神经网络，实现对各个特征进行提取。针对电影的标题使用有包含滑动窗口的CNN，对电影的种类，用户年龄，用户职业，用户性别，分别构建多个双层神经网络。这样就可以通过神经网络将每个用户的每个特征提取成一个向量。训练完成后模型的绝对值误差可以降到0.8左右</p><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><p>这样讲就可以得到可以用来表示所有用户和电影的特征矩阵。<br>整个神经网络训练完成后，通过给定用户和电影，对模型进行正向传播就可以预测出用户给该电影的评分。而如果给定一个电影，通过计算该电影与其他电影的余弦相似度，可以统计该电影topk个相似电影。同理给定用户可以找到topk个相似用户。同时，给定一个用户，我们可以通过用该用户的特征向量和电影特征矩阵相乘，计算出该用户对每个电影的估计评分，进而找出该用户可能喜欢的电影。<br>最后可以利用协同过滤思想，给定电影，找到可能喜欢该电影的n个用户，在找到这n个用户可能喜欢的电影，进而找到给定电影的相似电影。</p><h4 id="提问"><a href="#提问" class="headerlink" title="提问"></a>提问</h4><ol><li>如何下载数据？ 使用urllib.request根据url下载数据并使用zipfile解压。</li><li>如何处理用户数据？ 用户数据一共包含5个字段，使用userID，jobID，GenderID，Age作为用户特征，舍弃zip-code。</li><li>如何对用户数据进行处理？ userId和JobID不做处理，gender字段转换为0，1值。将age离散化，分割成7个区间，分别用0~6表示</li><li>如何处理movie数据？电影数据一共包含3个字段，movieid保留不作处理，genres字段表示电影的类别。构建一个映射字典将其数字化，同时每个电影有多个genres组合，所以genres属性是一个list。title字段中，首先去除掉电影时间，构建word2int映射字典，将title中的每个单词转换为int值。同时统一genres和title字段的长度，空白部分用定义的特殊符号表示</li><li>如何处理rating数据？ rating数据包括userID，movieId，ratings值和timestamps，这里仅将timestamps删除。然后将userid和movieid作为外键连接movie和user两个表组成整个feature表。<br>然后将整个大表切割成feature表和target表。</li><li>整个神经网络是什么样的？神经网络的整体被分成两个部分：user神经网络和movie神经网络。</li><li>moive神经网络是什么样的？分别对movieid，movie的genres和movie的title构建embedding matrix。对于输入的movie的各个特征分别做lookup得到embedding vector。然后将genres和id的vector分别输入到两个不同的MLP中。对于title的embedding matrix，构建CNN with sliding windows神经网络，分别定义filter 宽度为2，3，4，5在title embedding matrix上滑动提取多个feature map。使用最大池化，和dropout层减小过拟合。然后将多个feature map连接到一起组成title representation vector。最后分别将title vector，id vector, genres vector连接到一起组成movie feature representation。</li><li>user神经网络是什么样的？和movie神经网络类似，分别对userid，age，occupation，gender构建embedding matrix和多个MLP，每个MLP的输出可以做是对各个属性的特征提取。然后将多个MLP的输出拼接到一起组成user feature representation。</li><li>训练过程是怎样的？ 从user nn和movie nn可以得到user feature vector和movie feature vector。这里我通过通过将向量乘法，将这两个向量相乘得到一个值，这个值就是NN对给定user和给定movie的评分预测。通过和真实的ratings做平方损失函数，计算损失使用Adam优化算法对模型参数进行更新。</li><li>说说都有什么推荐功能，如何实现的？<ol><li>首先是给定user对给定movie的评分预测，只需要对NN进行正向传播即可</li><li>对所有user和movie进行正向传播，可以得到user feature matrix和movie feature matrix。</li><li>找topk相似电影：根据两个特征矩阵，对于给定movie，计算所有movie中和其余弦相似度最大的topk个movie作为最相似电影推荐。</li><li>同理可以找topk最相似用户。</li><li>给定用户推荐其喜欢的电影：用该user的特征向量和每个movie的向量相乘，找出topk个预测评分最高的电影。</li><li>看过这个电影的人还可能喜欢哪些？首先预测出对该电影评分最高的topk个人，然后预测这几个人对所有电影的评分，找到评分最高的n个电影返回。</li></ol></li><li>和协同过滤对比？ 在原始的协同过滤算法，当数据量过大时容易产生稀疏矩阵，这样的矩阵无论是存储还是推荐都会产生较大问题。而通过多个神经网络，可以将每个movie和user的表示压缩到一个小矩阵中。</li><li>数据缺失问题？</li></ol><h2 id="基于OpenAI-Gym的DQN应用介绍"><a href="#基于OpenAI-Gym的DQN应用介绍" class="headerlink" title="基于OpenAI Gym的DQN应用介绍"></a>基于OpenAI Gym的DQN应用介绍</h2><p>本项目主要使用OpenAI Gym作为环境。RL模型方面，构建了deep q network模型，在模型中使用了Double DQN的思想减小overestimate。使用priority Policy思想，构建了SumTree结构作为Replay Memory实现了基于数据优先级的模型训练，大大加快了模型的训练过程。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/08/13/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
      <url>/2018/08/13/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>62.梯度下降法找到的一定是下降最快的方向么？<br>梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。</p><p>63.牛顿法和梯度下降法有什么不同<br>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。<br>关于牛顿法和梯度下降法的效率对比：<br>a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。<br>b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><p>64.什么是拟牛顿法（Quasi-Newton Methods）<br>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。<br>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p><p>68.什么最小二乘法？<br>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：<br>使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。</p><p>75.简单介绍下logistics回归<br>Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。</p><p>86.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是(C)<br>A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小<br>B、在经主分量分解后,协方差矩阵成为对角矩阵<br>C、主分量分析就是K-L变换<br>D、主分量是通过求协方差矩阵的特征值得到<br>@BlackEyes_SGC：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。</p><p>92.影响基本K-均值算法的主要因素有(ABD）。<br>A.样本输入顺序；<br>B.模式相似性测度；<br>C.聚类准则；<br>D.初始类中心的选取</p><p>93.在统计模式分类问题中，当先验概率未知时，可以使用（BD）。<br>A. 最小损失准则；<br>B. 最小最大损失准则；<br>C. 最小误判概率准则；<br>D. N-P判决</p><p>94.如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（BC）。<br>A. 已知类别样本质量； B. 分类准则； C. 特征选取； D. 量纲</p><p>95.欧式距离具有（A B ）；马式距离具有（A B C D ）。<br>A. 平移不变性； B. 旋转不变性； C. 尺度缩放不变性； D. 不受量纲影响的特性</p><p>111.随机森林如何处理缺失值 方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。<br>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。</p><p>112.随机森林如何评估特征重要性 衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：<br>1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。<br>2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。</p><p>113.优化Kmeans 使用kd树或者ball tree<br>将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。</p><p>114.KMeans初始类簇中心点的选取 k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。</p><ol><li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li><li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li><li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li><li>重复2和3直到k个聚类中心被选出来</li><li>利用这k个初始的聚类中心来运行标准的k-means算法</li></ol><p>116.如何进行特征选择？ 特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解<br>常见的特征选择方式：</p><ol><li>去除方差较小的特征</li><li>正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。</li><li>随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。</li><li>稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</li></ol><p>133.SVD和PCA<br>PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。</p><p>134.数据不平衡问题<br>采样，对小样本加噪声采样，对大样本进行下采样<br>进行特殊的加权，如在Adaboost中或者SVM中<br>采用对不平衡数据集不敏感的算法<br>改变评价标准：用AUC/ROC来进行评价<br>采用Bagging/Boosting/ensemble等方法<br>考虑数据的先验分布</p><p>Bagging与Boosting的区别：<br>取样方式不同。<br>Bagging采用均匀取样，而Boosting根据错误率取样。<br>Bagging的各个预测函数没有权重，而Boosting是有权重的。<br>Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。</p><p>191.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m&lt;n&lt;p&lt;q，以下计算顺序效率最高的是（）<br>A.(AB)C<br>B.AC(B)<br>C.A(BC)<br>D.所以效率都相同<br>正确答案：A<br>首先，根据简单的矩阵知识，因为 A<em>B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项，<br>然后，再看 A 、 C 选项。在 A 选项中，m∗n 的矩阵 A 和n∗p的矩阵 B 的乘积，得到 m∗p的矩阵 A</em>B ，而 A∗B的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 m∗n∗p次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 m∗p∗q次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 m∗n∗p+m∗p∗q 。同理分析， C 选项 A (BC) 需要的乘法次数是 n∗p∗q+m∗n∗q。<br>由于m∗n∗p&lt;m∗n∗q，m∗p∗q&lt;n∗p∗q，显然 A 运算次数更少，故选 A 。</p><p>200.特征比数据量还大时，选择什么样的分类器？<br>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。</p><p>186.关于 logit 回归和 SVM 不正确的是（）<br>A.Logit回归目标函数是最小化后验概率<br>B. Logit回归可以用于预测事件发生概率的大小<br>C. SVM目标是结构风险最小化<br>D.SVM可以有效避免模型过拟合<br>正确答案： A<br>A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误   B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确    C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。    D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。</p><p>227.什么是共线性, 跟过拟合有什么关联?<br>共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。<br>共线性会造成冗余，导致过拟合。<br>解决方法：排除变量的相关性／加入权重正则。</p><p>230.机器学习中，有哪些特征选择的工程方法？</p><ol><li>计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；</li><li>构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；</li><li>通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；</li><li>训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；</li><li>通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。</li><li>通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。</li></ol><p>221.带核的SVM为什么能分类非线性问题？<br>核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积</p><p>222.常用核函数及核函数的条件：<br>核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。<br>RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。<br>线性核：主要用于线性可分的情况<br>多项式核</p><p>223.Boosting和Bagging<br>（1）随机森林<br>　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：<br>1）Boostrap从袋内有放回的抽取样本值<br>2）每次随机抽取一定数量的特征（通常为sqr(n)）。<br>　　分类问题：采用Bagging投票的方式选择类别频次最高的<br>　　回归问题：直接取每颗树结果的平均值。<br>常见参数误差分析优点缺点<br>1、树最大深度<br>2、树的个数<br>3、节点上的最小样本数<br>4、特征数(sqr(n))oob(out-of-bag)<br>将各个树的未采样样本作为预测样本统计误差作为误分率可以并行计算<br>不需要特征选择<br>可以总结出特征重要性<br>可以处理缺失数据<br>不需要额外设计测试集在回归上不能输出连续结果<br>（2）Boosting之AdaBoost<br>　　Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。<br>（3）Boosting之GBDT<br>　　将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。<br>　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。<br>（4）Xgboost<br>这个工具主要有以下几个特点：<br>支持线性分类器<br>可以自定义损失函数，并且可以用二阶偏导<br>加入了正则化项：叶节点数、每个叶节点输出score的L2-norm<br>支持特征抽样<br>在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。</p><p>1点到平面距离公式推导</p><p>235.对于k折交叉验证, 以下对k的说法正确的是 :<br>A. k越大, 不一定越好, 选择大的k会加大评估时间<br>B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)<br>C. 在选择k时, 要最小化数据集之间的方差<br>D. 以上所有<br>答案：D<br>k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.</p><p>255.对于PCA说法正确的是 :</p><ol><li>我们必须在使用PCA前规范化数据</li><li>我们应该选择使得模型有最大variance的主成分</li><li>我们应该选择使得模型有最小variance的主成分</li><li>我们可以使用PCA在低维度上做数据可视化<br>答案:  1, 2 and 4<br>1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).<br>2）我们总是应该选择使得模型有最大variance的主成分<br>3）有时在低维度上左图是需要PCA的降维帮助的</li></ol><h3 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h3><p>文章给的定义是： 数据集的标签是不可靠的，如（x，y），y对于x的标记是不可靠的。<br>这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。<br>在实际应用中的学习问题往往以混合形式出现,如多标记多示例、半监督多标记、弱标记多标记等。针对监督信息不完整或不明确对象的学习问题统称为弱监督学习<br>弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或是多个元素的<br>分别表示x没有标记，有一个标记，和有多个标记。在此统一声明为一个标记的集合。<br>在实际的使用中多标记的使用是很常见的，在图像文本语音中是很容易找到多个标记的。<br>举个图像中的例子<br>一般机器学习算法，每一个训练样本都需要类别标号（对于二分类：1/-1）。实际上那样的数据其实已经经过了抽象，实际的数据要获得这样的标号还是很难，图像就是个典型。还有就是数据标记的工作量太大，我们想偷懒了，所以多只是给了正负样本集。负样本集里面的样本都是负的，但是正样本里面的样本不一定都是正的，但是至少有一个样本是正的。比如检测人的问题，一张天空的照片就可以是一个负样本集；一张某某自拍照就是一个正样本集（你可以在N个区域取N个样本，但是只有部分是有人的正样本）。这样正样本的类别就很不明确，传统的方法就没法训练。<br>疑问来了，图像的不是有标注吗？有标注就应该有类别标号啊?这是因为图片是人标的，数据量特大，难免会有些标的不够好,这就是所谓的弱监督集（weakly supervised set）。所以如果算法能够自动找出最优的位置，那分类器不就更精确吗？ 标注位置不是很准确，这个例子不是很明显，还记得前面讲过的子模型的位置吗？比如自行车的车轮的位置，是完全没有位置标注的，只知道在bounding box区域附件有一个车轮。不知道精确位置，就没法提取样本。这种情况下，车轮会有很多个可能的位置，也就会形成一个正样本集，但里面只有部分是包含轮子的。</p><p>270.对于维度极低的特征，选择线性还是非线性分类器？ 非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。</p><ol><li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li><li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li><li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</li></ol><p>237.模型的高bias是什么意思, 我们如何降低它 ?<br>A. 在特征空间中减少特征<br>B. 在特征空间中增加特征<br>C. 增加数据点<br>D. B和C<br>E. 以上所有<br>答案: B<br>bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !</p><p>296.一般，k-NN最近邻方法在（ A）的情况下效果较好。<br>A．样本较多但典型性不好<br>B．样本呈团状分布<br>C．样本较少但典型性好<br>D．样本呈链状分布</p><p>297.下列哪些方法可以用来对高维数据进行降维（A B C D E F）<br>A LASSO<br>B 主成分分析法<br>C 聚类分析<br>D 小波分析法<br>E 线性判别法<br>F 拉普拉斯特征映射<br>解析：lasso通过参数缩减达到降维的目的；<br>pca就不用说了<br>线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；<br>小波分析有一些变换的操作降低其他干扰可以看做是降维<br>拉普拉斯请看这个<a href="http://f.dataguru.cn/thread-287243-1-1.html" target="_blank" rel="noopener">http://f.dataguru.cn/thread-287243-1-1.html</a></p><p>299.以下说法中正确的是（C）<br>A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性<br>B 在adaboost算法中，所有被分错样本的权重更新比例相同<br>C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重<br>D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少</p><p>294.机器学习中做特征选择时，可能用到的方法有？ （E）<br>A、卡方<br>B、信息增益<br>C、平均互信息<br>D、期望交叉熵<br>E 以上都有</p><p>281.在 k-均值算法中，以下哪个选项可用于获得全局最小？<br>A. 尝试为不同的质心（centroid）初始化运行算法<br>B. 调整迭代的次数<br>C. 找到集群的最佳数量<br>D. 以上所有<br>答案（D）：所有都可以用来调试以找到全局最小。</p><p>284.下面哪个选项中哪一项属于确定性算法？<br>A.PCA<br>B.K-Means<br>C. 以上都不是<br>答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。</p><p>285.特征向量的归一化方法有哪些？<br>线性函数转换，表达式如下：<br>y=(x-MinValue)/(MaxValue-MinValue)<br>对数函数转换，表达式如下：<br>y=log10 (x)<br>反余切函数转换 ，表达式如下：<br>y=arctan(x)*2/PI<br>减去均值，除以方差：<br>y=(x-means)/ variance</p><p>331.在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？<br>A.具有1百万项300个特征的数据集<br>B.具有100000项310个特征的数据集<br>C.具有10,000项8个特征的数据集<br>D.具有10,000项200个特征的数据集<br>解答：（C）<br>t-SNE具有二次时空复杂度。</p><p>331题、哪些机器学习算法不需要做归一化处理？<br>概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。<br>我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。<br>至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。</p><p>332题、对于树形结构为什么不需要归一化？<br>数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。<br>另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。</p><p>333题、在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别<br>欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为：<br>欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。<br>曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：<br>要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。<br>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。<br>曼哈顿距离和欧式距离一般用途不同，无相互替代性。</p><p>334题、数据归一化（或者标准化，注意归一化和标准化不同）的原因<br>要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。<br>有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。<br>有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。<br>补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。</p><h4 id="adaboost如何并行计算"><a href="#adaboost如何并行计算" class="headerlink" title="adaboost如何并行计算"></a>adaboost如何并行计算</h4><ol><li>最优基分类器的选择</li><li>每个数据权重的计算和更新<h4 id="Adaboost调参"><a href="#Adaboost调参" class="headerlink" title="Adaboost调参"></a>Adaboost调参</h4></li><li>base_estimator: 基分类器的种类，默认决策树</li><li>n_estimators：基学习器的最大迭代数，个数</li><li>learning_rate：基学习器的权重衰减系数</li><li>基学习器参数 max_feature：划分时考虑的最大特征函数</li><li>基学习器参数 max_depth：最大深度</li></ol><h4 id="规则学习"><a href="#规则学习" class="headerlink" title="规则学习"></a>规则学习</h4><ol><li>快排，最大堆</li><li>给出二叉树的前序和中序序列恢复二叉树的结构</li></ol><p>密度聚类的方法<br>问CRF、HMM知道不？</p><p>关联推荐<br>排序算法的时间空间复杂度<br>WordVec 的原理</p><p>圆上三个点组成锐角三角形的概率<br>1/4</p><p>将一条线段在两个地方随机切断可以组成三角形的概率<br>假设我们选择的两个点的坐标是x和y（先假设x &lt; y）：<br>那么由三角形两边和大于第三边的性质，有：<br>x+y-x&gt;1-y<br>x+1-y&gt;y-x<br>1-y+y-x&gt;x<br>由上述不等式得到：x &lt; 0.5, 0.5 &lt; y &lt; x+0.5<br>然后画个图就能得到联合概率为1/8，不要忘了这个结果是在假设x &lt; y时得来的，所以再乘以2，得到1/4。</p><p>共线性(collinearity)<br>当数据矩阵Xn×(p+1)不是列满秩时，正规方程中的XTX不可逆，任何满足正规方程的参数都能够最小化均方误差。满足要求的解有无限多个，因此得出的解容易过拟合(overfitting)。这个问题又称数据共线性。<br>如果n⩽p，则数据一定共线性。否则，数据很少精确地满足共线性。即使数据不是精确地满足共线性，也可能会非常接近列不满秩。此时XTX虽然可逆，但是数值求解其逆矩阵(inverse matrix)非常不稳定，这样得到的不稳定解也容易产生过拟合。<br>解决共线性的方法主要有两种。第一种是找出共线性的列并且删除其中一列直到数据列满秩。第二种是使用正则化(regularization)。</p><p>其中项目是重点，我相信看到这篇文章的你，应该有相关的NLP项目，那么你应该对你简历所写上去的东西负责任（也就是对细节非常熟），对方可能会问到你：</p><p>1）具体参数设置，为什么要这样设置（掌握一下调参玄学）<br>2）你的模型，为什么这么做，为什么能work，和xx方法比怎么样<br>3）可能根据你的项目及模型，提出某个可能存在的深藏不露的问题，问你如何解决<br>4）项目难点是什么，又如何解决，从哪几方面解决，效果提升多少<br>5）如今的你再来看从前的这个任务，有没有更好的解决思路<br>6）给你一个新的业务场景，你怎么把你的模型移植上去，怎么重新设计模型，和你之前项目的区别是什么，需要注意哪些问题<br>7）项目分工，你做了哪部分工作<br>8）你这个任务的state of the art</p><p>Part II：深度学习</p><p>首先关于基础原理，你至少要知道这些：<br>1）CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM<br>2）RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进。<br>3）Word2vec工具，怎么训练词向量，skip-gram和cbow，可以参考一下：一篇通俗易懂的word2vec（也可能并不通俗易懂）<br>4）Attention机制，比较常见的方法，可以参考一下：Attention用于NLP的一些小结<br>5）NLP基础任务，比如分词算法（序列标注任务），分类算法</p><p>关于实战部分，你至少也要知道这些：</p><p>1）数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练</p><p>2）过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？</p><p>3）调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。</p><p>4）模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估</p><p>5）优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法</p><p>8）如何处理数据不均衡问题，也是从数据和模型两方面出发解决。</p><p>你至少要掌握的算法原理：</p><p>5）SVM，核函数选择，不同SVM形式</p><p>6）HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？</p><p>7）最大熵原理，图解最大熵原理（The Maximum Entropy Principle）</p><p>8）KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解</p><p>以上列出的算法都需要掌握其基本原理以及优缺点，可以参考：机器学习算法优缺点及其应用领域 - CSDN博客</p><p>你必须要会写的公式：</p><p>1）BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。</p><p>2）softmax和交叉熵推导，分成i=j 和 ij 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导</p><p>3）各种Loss函数</p><p>4）似然函数，负对数似然函数的推导</p><p>5）最小二乘法，利用矩阵的秩进行推导</p><p>7）贝叶斯定理，拉普拉斯平滑</p><p>你最好也要掌握一下的公式：</p><p>1）RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。</p><p>2）各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园</p><p>3）Batch Normalization，就是个归一化过程，再加一个scale操作</p><p>4）SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法</p><p>5）最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园</p><p>Part IV：算法编程</p><p>不管你面试什么公司，请记住coding几乎是必考的，这是工程师的基本功。</p><p>编程分成三种：普通算法编程，海量数据编程，模型编程。</p><p>普通算法编程，一般用C++，需要掌握数组，链表，二叉树，递归，贪心，动态规划，各种容器，各种排序算法，在时间或者空间上的优化思路，以及复杂度的分析。</p><p>容器是个好东西，用vector代替数组，用map实现桶思想，用set排序，用queue写bfs，用stack写dfs等等。</p><p>推荐大家刷：剑指offer，这本书两天就可以看完（如果仅仅是看题目以及思路），然后上牛客网做一下题：剑指Offer_编程题_牛客网，66道原题全在这，而且评论区有大神出没，某些题的解法我觉得比书上的要巧妙。或者刷LeetCode也可以。</p><p>随手列几道常考的代码题：</p><p>1）复杂链表的复制，链表的删除</p><p>2）最长公共子序列，逆序对</p><p>3）快排，归并排序，堆排序</p><p>4）二分查找，以及衍生的题目</p><p>5）深度优先搜索</p><p>海量数据编程，这种用python写比较方便一点，可以把大文件划分成小文件，或者分治加哈希：十道海量数据处理面试题与十个方法大总结 - CSDN博客</p><p>模型编程，有时候可能会让你用某个深度学习框架搭某个模型，不过这种比较少。</p><p>做科研比较推荐pytorch，业界用tf 比较多，不过也得看组看个人，如果项目需要上线很有可能就是要用tf了。我个人比较喜欢用pytorch，方便搭模型，对RL也很友好。但tf 还是要掌握一下的，指不定哪天这个项目就是要用tf来上线呢。</p><p>总之，刷题即可，多写代码多搭模型。</p><p>L2 范数不但可以防止过拟合，提高模型的泛化能力，还可以让我们的优化求解变得稳定和快速。L2 范数对大数和 outlier 更敏感！</p><p>填空题<br>经过下列卷积操作后，3×3 conv -&gt; 3×3 conv -&gt; 2×2 maxpool -&gt; 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？<br>100×100×3，3×3 卷积核，输出是 50×50×10，算进行了多少次乘-加操作？</p><p>简答题<br>简述梯度下降法和牛顿法的优缺点？<br>正样本 10000，负样本 1000，怎样训练<br>Relu 相对于 sigmoid 函数的优缺点？</p><p>编程题<br>输入序列 a, 判断是否存在 i &lt; j &lt; k, 满足 a[i] &lt; a[k] &lt; a[j]，并写出算法复杂度？<br>输入多边形顶点坐标 List，判断是否为凸多边形(如果把一个多边形的所有边中，任意一条边向两方无限延长成为一直线时，其他各边都在此直线的同旁，那么这个多边形就叫做凸多边形)?</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>强化学习之Actor Critic</title>
      <link href="/2018/08/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BActor%20Critic/"/>
      <url>/2018/08/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BActor%20Critic/</url>
      <content type="html"><![CDATA[<p>Actor Critic 是目前来说表现最好的强化学习方法， 它结合了value_based和policy_based两种模型的特点。<br><a id="more"></a><br>我们知道value_based算法有一个缺点就是不能对连续值进行预测，只能预测每一个状态每一个动作对应的Q值。而传统 policy_based 虽然可以对连续值进行预测，却是回合更新，没办法按步更新，回合更新的学习小于要低很多。</p><h2 id="Actor和Critic"><a href="#Actor和Critic" class="headerlink" title="Actor和Critic"></a>Actor和Critic</h2><p>我们可以用两个不同的神经网络构造Actor-Critic，一个网络是Actor（Policy Network），一个网络是Critic（Q_Learning）。<strong>Actor_Net用于实际做各种动作，输入状态，预测出每个动作的概率。Critic Net会执行的动作的好坏进行预测，并将预测结果告知Actor Net，然后Actor Net根据Critic Net的结果计算loss函数，进而更新模型。</strong> 如果这个状态-动作是一个有益的动作就加大幅度更新，否则减小更新。</p><p>Critic Net输入状态，输出是每个动作的值，然后根据Q_Learning的Q值更新方法来计算Actor Net在状态s执行的动作a的Q(s, a)。并把这个值传递给Actor Net。<br>Critic Net传递给Actor Net的值就是Policy Network中的loss function：’log(poss)*f(s,a)’中的f(s,a)表示在当前状态s下执行动作a的优劣。</p><p>一句话概括Actor Critic方法：<strong>结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.</strong></p><h3 id="优势和劣势"><a href="#优势和劣势" class="headerlink" title="优势和劣势"></a>优势和劣势</h3><p>相比于传统policy gradient方法，Actor Critic方法的优势是：可以进行单步更新，比传统policy gradient要快。<br>劣势：取决于Critic的价值判断，但Critic难收敛，再加上actor的更新会更难收敛。<br>基于上面的劣势，提出了新的方法DDPG</p><h2 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient(DDPG)"></a>Deep Deterministic Policy Gradient(DDPG)</h2><p>因为Actor-Critic是在连续状态上进行更新，状态之间的相关性很高，会导致模型在连续动作上无法学习的问题。DeepMind团队将Actor-Critic和DQN结合到一起，成功解决了在连续动作预测上学不到东西的问题。</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>利用字典树过滤敏感词</title>
      <link href="/2018/08/11/Algorithms/%E5%88%A9%E7%94%A8%E5%AD%97%E5%85%B8%E6%A0%91%E8%BF%87%E6%BB%A4%E6%95%8F%E6%84%9F%E8%AF%8D/"/>
      <url>/2018/08/11/Algorithms/%E5%88%A9%E7%94%A8%E5%AD%97%E5%85%B8%E6%A0%91%E8%BF%87%E6%BB%A4%E6%95%8F%E6%84%9F%E8%AF%8D/</url>
      <content type="html"><![CDATA[<p>字典树，又称单词查找树，广泛应用于搜索引擎的词频统计和敏感词过滤。字典树的原理是：利用字符串的公共前缀来减少查询时间，最大限度的减少无谓的比较，运行效率很高。</p><a id="more"></a><h2 id="字典树特性"><a href="#字典树特性" class="headerlink" title="字典树特性"></a>字典树特性</h2><ol><li>树形结构，每个节点有多个子节点</li><li>每个节点仅仅保存一个字符</li><li>根节点不包含任何字符</li><li>节点表示的字符串是从根节点到该节点所经历节点路径对应字符连接在一起的字符串</li><li>每个节点子节点所包含的字符都不相同</li><li>每个路径保存的字符串不相同</li></ol><h2 id="算法运行原理"><a href="#算法运行原理" class="headerlink" title="算法运行原理"></a>算法运行原理</h2><ol><li>通过敏感词集合构建一个字典树，并在之后用该字典树进行敏感词过滤</li><li>创建字典树节点<ol><li>使用一个boolean变量isEnd表示该节点是否为子节点，也就是说该节点经过的路径是否一个完整的敏感词</li><li>每个节点包含一个Map成员表示所有子节点</li><li>对外提供方法</li></ol></li></ol><ul><li><p>节点类的java代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class TrieNode&#123;</span><br><span class="line">    public boolean isEnd = false; // 表示是否为叶节点</span><br><span class="line">    private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); // 该节点所有子节点的表示</span><br><span class="line">    public void addSubNode(Character key, TrieNode node)&#123; // 向指定位置添加子树</span><br><span class="line">        subNodes.put(key, node);</span><br><span class="line">    &#125;</span><br><span class="line">    public TrieNode getSubNode(Character key)&#123;</span><br><span class="line">        return subNodes.get(key);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>字典树类的java代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class TrimTree&#123;</span><br><span class="line">    private static final String DEFAULT_REPLACEMENT = &quot;NotAllow&quot;;</span><br><span class="line">    private TrieNode rootNode = new TrieNode();</span><br><span class="line">    //判断是否是一个符号</span><br><span class="line">    private boolean isSymbol(char c) &#123;</span><br><span class="line">        int ic = (int) c;</span><br><span class="line">        // 0x2E80-0x9FFF 东亚文字范围</span><br><span class="line">        return !((c &gt;= &apos;0&apos; &amp;&amp; c &lt;= &apos;9&apos;) || (c &gt;= &apos;a&apos; &amp;&amp; c &lt;= &apos;z&apos;)|| (c &gt;= &apos;A&apos; &amp;&amp; c &lt;= &apos;Z&apos;)) &amp;&amp; (ic &lt; 0x2E80 || ic &gt; 0x9FFF);</span><br><span class="line">    &#125;</span><br><span class="line">    public void addDirTreeNode(String textLine)&#123;</span><br><span class="line">        // 构建字典树方法， 根据输入字符串，逐步构建字典树</span><br><span class="line">    &#125;</span><br><span class="line">    public String filterWords(String text)&#123;</span><br><span class="line">        // 过滤文本中的敏感词</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>字典树里面有两个核心方法，一个是根据输入敏感字符串，构建字典树：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public void addDirTreeNode(String textLine)&#123;</span><br><span class="line">    // 构建字典树方法， 根据输入字符串，逐步构建字典树</span><br><span class="line">    if(textLine == null)&#123;return;&#125;</span><br><span class="line">    TrieNode tempNode = this.rootNode; // 临时节点指向根节点</span><br><span class="line">    for (int i = 0; i&lt;textLine.length(); i++) &#123;</span><br><span class="line">        char word = textLine.charAt(i);</span><br><span class="line">        if (isSymbol(c)) &#123;contiue;&#125; // 直接删除掉非法字符</span><br><span class="line">        TrieNode node = tempNode.getSubNode(c);</span><br><span class="line">        if (node == null) &#123; // 当前节点没有对应char的子节点</span><br><span class="line">            node = new TrieNode()</span><br><span class="line">            tempNode.addSubNode(c, node);</span><br><span class="line">        &#125;</span><br><span class="line">        tempNode = node;</span><br><span class="line">        if (i == textLine.length() - 1) &#123; // 当前敏感词已经遍历结束，将当前节点设为叶节点</span><br><span class="line">            tempNode.isEnd = true;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>另一个核心方法是对给定的一些输入文档，将文档中的敏感词过滤掉</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public String filterWords(String text)&#123;</span><br><span class="line">    // 过滤文本中的敏感词</span><br><span class="line">    if (text.trim().length == 0) &#123;return text;&#125;</span><br><span class="line">    StringBuilder result = new StringBuilder();</span><br><span class="line">    TrieNode tempNode = rootNode;</span><br><span class="line">    int begin = 0; // 回滚数</span><br><span class="line">    int position = 0; // 当前比较的位置</span><br><span class="line">    while (position &lt; text.length())&#123;</span><br><span class="line">        char c = text.charAt(position);</span><br><span class="line">        // 直接跳过空格</span><br><span class="line">        if (isSymbol(c)) &#123;</span><br><span class="line">            if (tempNode == rootNode) &#123;</span><br><span class="line">                result.append(c);</span><br><span class="line">                begin++;</span><br><span class="line">            &#125;</span><br><span class="line">            position++;</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        tempNode = tempNode.getSubNode(c);</span><br><span class="line">        if (tempNode == null) &#123; // 以begin开始的当前字符串匹配结束，并不是敏感词</span><br><span class="line">            result.append(text.charAt(begin));</span><br><span class="line">            // 调到下一个字符开始测试</span><br><span class="line">            position = begin + 1;</span><br><span class="line">            begin = position;</span><br><span class="line">            tempNode = rootNode; // 回到树的初始节点</span><br><span class="line">        &#125;else if (tempNode.isEnd) &#123; //从begin开始的字符串为敏感词，用replace替换掉</span><br><span class="line">            result.append(this.DEFAULT_REPLACEMENT);</span><br><span class="line">            position = position + 1;</span><br><span class="line">            begin = position;</span><br><span class="line">            tempNode = tempNode;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            position++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(text.substring(begin));</span><br><span class="line">    return result.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>从背包问题理解动态规划</title>
      <link href="/2018/07/28/Algorithms/%E4%BB%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>/2018/07/28/Algorithms/%E4%BB%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      <content type="html"><![CDATA[<p>动态规划是程序设计中非常重要的一个编程思想，基本上所有大厂的笔试面试都会遇到对动态规划的考察。而因为动态规划的思想和我们日常思考问题的角度不太相同，因此动态规划也一直是一个比较难掌握的思想。本文是对经典的入门教程：背包十讲的学习笔记。</p><a id="more"></a><p>动态规划最关键的解题步骤就是找到所有状态，以及状态转移方程。找到这两个东西，基本上问题也就解决了。</p><h2 id="背包问题基本定义"><a href="#背包问题基本定义" class="headerlink" title="背包问题基本定义"></a>背包问题基本定义</h2><p>背包问题主要是给定背包大小，可以将一些物品放入背包中，让背包中物品的总价值最大。这里定义背包类如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public class Knapsack&#123;</span><br><span class="line">    private int[] value; // 每个商品的价值</span><br><span class="line">    private int[] weight; // 每个商品所占的体积</span><br><span class="line">    private int[] nums;  // 每个商品的数量</span><br><span class="line">    private int variety;  // 商品种类数量</span><br><span class="line">    private int volume;  // 背包大小</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>背包问题涉及到的变量如上注释。</p><h2 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h2><p>01背包是最简单的背包问题，但其后所有复杂背包问题本质上都是对01背包问题的转化。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 件物品和一个容量为 V 的背包。放入第 i 件物品耗费的费用是 Ci，得到的 价值是 Wi。求解将哪些物品装入背包可使价值总和最大。</p><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>最基本的背包问题，特点是：<strong>每种物品仅有一件，可以选择放或者不放。</strong><br>直接上状态转移方程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX(table[i-1][j], table[i-1][j-Ci]+Wi)</span><br></pre></td></tr></table></figure></p><p>table[i][j]表示表示前 i 种物品恰放入一个容量为 j 的背包时，背包里物品总价值的最大值。<br>这样，状态转移方程就可以解释为：<strong>当前总价值的最大值在物品i在背包大小为j时的取值，等于不放入第i件物品(table[i-1][j])，和放入第i件物品(table[i-1][j-Ci]+Wi) 的最大值。</strong></p><p>若只考虑第 i 件物品的策略(放或不放)，那么就可以转化为一个只和前 i − 1 件物品相关的问题。<br>如果不放第 i 件物品，那么问题就转化为“前 i − 1 件物品放入容量为 v 的背包中”，价值为 F[i−1,v];<br>如果放第 i 件物品，那么问题就转化为“前 i − 1 件物品放入剩下的容量为v−Ci 的背包中”，<br>此时能获得的最大价值就是F[i−1,v−Ci]再加上通过放入第 i 件物品获得的价值 Wi</p><p>Java代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int zeroOnePackV1()&#123;</span><br><span class="line">    int[][] table = new int[variety+1][volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = 1; j&lt;=volume; j++) &#123;</span><br><span class="line">            table[i][j] = Math.max(table[i-1][j], table[i-1][j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume][variety];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="空间复杂度优化"><a href="#空间复杂度优化" class="headerlink" title="空间复杂度优化"></a>空间复杂度优化</h3><p>通过上面的状态转移方程我们发现，考察物品i时背包的价值仅取决于考察物品i-1时的各个背包大小时的值。所以，我们可以将原01背包的空间复杂度O(NV)优化成O(V)。<br>我们知道循环更新每个table[i][j]时需要用到之前的值table[i-1][j]和table[i-1][j-c]<br>所以可以通过从后向前遍历容量来计算table[v]，这样就可以保证在计算f[v]是f[v-c]是f[i-1, v-c]的值。</p><p>Java代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int zeroOnePackV2()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1;i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume;j&gt;=weight[i-1];j--) &#123;</span><br><span class="line">            table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 种物品和一个容量为 V 的背包，每种物品都有无限件可用。放入第 i 种物品 的费用是 Ci，价值是 Wi。 和01背包最大的不同是：<strong>每个物品可以使用无限次</strong></p><h3 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h3><p>仍然按照01背包的解题思路，状态转移方程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX&#123;table[i-1][j-k*Ci]+k*Wi&#125;   0&lt;=kCi&lt;=V</span><br></pre></td></tr></table></figure></p><p>将前i件物品放入大小为j的背包时的最大价值，就等于：1.不将第i件商品放入；2. 将第i件商品放入1,2,3,4…个的最大值。</p><h3 id="简单优化"><a href="#简单优化" class="headerlink" title="简单优化"></a>简单优化</h3><p>完全背包问题有一个很简单有效的优化，是这样的:若两件物品 i、j 满足 Ci ≤ Cj 且 Wi ≥ Vj，则将可以将物品 j 直接去掉，不用考虑。这个优化思想是非常显而易见的。</p><p>Java实现代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public int completePackV1()&#123;</span><br><span class="line">    int[] table = new int[volume];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume;j&gt;=weight[i-1];j-- ) &#123;</span><br><span class="line">            for (int k = 0; volume &gt; k*weight[i-1];k++ ) &#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-weight[i-1]*k] + value[i-1]*k);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面的代码用了三层循环，时间复杂度为O(NCK)，我们不妨转换一下思想，可以将时间复杂度变为O(VN).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public int completePackV2()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = weight[i-1];j&lt;=volume ; j++) &#123;</span><br><span class="line">            table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个伪代码与 01 背包问题的伪代码只有 v 的循环次序不同而已。 为什么这个算法就可行呢?首先想想为什么 01 背包中要按照 v 递减的次序来循环。 让 v 递减是为了保证第 i 次循环中的状态 table[i][j] 是由状态 F[i−1][j−Ci] 递推而来。<br>换句话说，这正是为了保证每件物品只选一次，保证在考虑“选入第 i 件物品”这件策略时，依据的是一个绝无已经选入第 i 件物品的子结果 F [i − 1, v − Ci]。而现在完全背包的特点恰是每种物品可选无限件，所以在考虑“加选一件第 i 种物品”这种策略时， 却正需要一个可能已选入第 i 种物品的子结果 F [i, v − Ci ]，所以就可以并且必须采用 v 递增的顺序循环。这就是这个简单的程序为何成立的道理。<br>值得一提的是，上面的伪代码中两层 for 循环的次序可以颠倒。这个结论有可能会 带来算法时间常数上的优化。<br>这个算法也可以由另外的思路得出。例如，将基本思路中求解 table[i][j-Ci] 的状态转移方程显式地写出来，代入原方程中，会发现该方程可以等价地变形成这种形式:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX(table[i−1][j], table[i][j−Ci] + Wi)</span><br></pre></td></tr></table></figure></p><h2 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h2><p>有 N 种物品和一个容量为 V 的背包。第 i 种物品最多有 Mi 件可用，每件耗费的空间是 Ci，价值是 Wi。 与完全背包不同的是，<strong>每个物品的个数是有限的</strong></p><h3 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h3><p>这题目和完全背包问题很类似。基本的方程只需将完全背包问题的方程略微一改 即可。<br>因为对于第 i 种物品有 Mi +1 种策略:取 0 件，取 1 件……取 Mi 件。<br>状态转移方程:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table[i][j] = MAX&#123;table[i-1][j-k*Ci]+k*Wi&#125;   0&lt;=k&lt;=Mi</span><br></pre></td></tr></table></figure></p><p>原始解法的Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public int multiplePackV1()&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for (int i = 1; i&lt;=variety; i++) &#123;</span><br><span class="line">        for (int j = volume; j&gt;=weight[i-1];j-- ) &#123;</span><br><span class="line">            for (int k = 0; volume&gt;weight[i-1] &amp;&amp; k&lt;nums[i-1];k++ ) &#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-k*weight[i-1]]+k*value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return table[volume];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h3><p>仍然考虑二进制的思想，我们考虑把第 i 种物品换成若干件物品，使得原问题中第 i 种物品可取的每种策略——取 0…Mi 件——均能等价于取若干件代换以后的物品。 另外，取超过 Mi 件的策略必不能出现。<br>方法是:将第 i 种物品分成若干件 01 背包中的物品，其中每件物品有一个系数。这件物品的费用和价值均是原来的费用和价值乘以这个系数。令这些系数分别为 1,2,22 …2k−1,Mi −2k +1，且 k 是满足 Mi −2k +1 &gt; 0 的最大整数。例如，如果 Mi 为 13，则相应的 k = 3，这种最多取 13 件的物品应被分成系数分别为 1, 2, 4, 6 的四件物品。分成的这几件物品的系数和为 Mi，表明不可能取多于 Mi 件的第 i 种物品。另外 这种方法也能保证对于0…Mi 间的每一个整数，均可以用若干个系数的和表示。这里 算法正确性的证明可以分0…2k−1 和2k…Mi 两段来分别讨论得出。</p><h2 id="混合三种背包问题"><a href="#混合三种背包问题" class="headerlink" title="混合三种背包问题"></a>混合三种背包问题</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>如果将前面1、2、3中的三种背包问题混合起来。也就是说，有的物品只可以取一 次(01 背包)，有的物品可以取无限次(完全背包)，有的物品可以取的次数有一个上限<br>(多重背包)。</p><h3 id="01背包和完全背包混合"><a href="#01背包和完全背包混合" class="headerlink" title="01背包和完全背包混合"></a>01背包和完全背包混合</h3><p>考虑到 01 背包和完全背包中给出的伪代码只有一处不同，故如果只有两类物品: 一类物品只能取一次，另一类物品可以取无限次，那么只需在对每个物品应用转移方程 时，根据物品的类别选用顺序或逆序的循环即可，复杂度是 O(V N )。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public int mixturePackV1(boolean[] nums)&#123;</span><br><span class="line">    int[] table = new int[volume+1];</span><br><span class="line">    for(int i = 1;i&lt;=variety; i++)&#123;</span><br><span class="line">        if(nums[i-1])&#123; // 当前物品只许取一次</span><br><span class="line">            for(int j = volume; j&gt;=weight[i-1];j--)&#123;</span><br><span class="line">                table[j] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            for(int j = weight[i-1]; j&lt;=volume; j++)&#123;</span><br><span class="line">                table[i] = Math.max(table[j], table[j-weight[i-1]]+value[i-1]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="加入多重背包"><a href="#加入多重背包" class="headerlink" title="加入多重背包"></a>加入多重背包</h3><p>很简单，只需要多加入一个判断即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fori ←1toN</span><br><span class="line">    if第 i 件物品属于01背包</span><br><span class="line">        ZeroOnePack(F,Ci,Wi)</span><br><span class="line">    else if 第 i 件物品属于完全背包</span><br><span class="line">        CompletePack(F,Ci,Wi)</span><br><span class="line">    else if 第 i 件物品属于多重背包</span><br><span class="line">        MultiplePack(F,Ci,Wi,Ni)</span><br></pre></td></tr></table></figure></p><h2 id="二维费用背包问题"><a href="#二维费用背包问题" class="headerlink" title="二维费用背包问题"></a>二维费用背包问题</h2><h3 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h3><p>对于每件物品，具有两种不同的费用，选择这件物品必 须同时付出这两种费用。对于每种费用都有一个可付出的最大值(背包容量)。<br>设第 i 件物品所需的两种费用分别为 Ci 和 Di。两种费用可付出的最大值(也即两 种背包容量)分别为 V 和 U。物品的价值为 Wi。</p><h3 id="解题思路-3"><a href="#解题思路-3" class="headerlink" title="解题思路"></a>解题思路</h3><p>费用加了一维，只需状态也加一维即可。设 F [i, v, u] 表示前 i 件物品付出两种费用<br>分别为 v 和 u 时可获得的最大价值。状态转移方程就是:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F[i,v,u] = max&#123;F[i − 1,v,u],F[i − 1,v − Ci,u − Di] + Wi&#125;</span><br></pre></td></tr></table></figure></p><p>如前述优化空间复杂度的方法，可以只使用二维的数组:当每件物品只可以取一次 时变量 v 和 u 采用逆序的循环，当物品有如完全背包问题时采用顺序的循环，当物品 有如多重背包问题时拆分物品。</p><h3 id="物品总个数的限制"><a href="#物品总个数的限制" class="headerlink" title="物品总个数的限制"></a>物品总个数的限制</h3><p>有时，“二维费用”的条件是以这样一种隐含的方式给出的:最多只能取 U 件物品。 这事实上相当于每件物品多了一种“件数”的费用，每个物品的件数费用均为 1，可以付出的最大件数费用为 U。换句话说，设 F[v,u] 表示付出费用 v、最多选 u 件时可得到的最大价值，则根据物品的类型(01、完全、多重)用不同的方法循环更新，最后在 f[0…V,0…U] 范围内寻找答案。</p><h2 id="分组背包问题"><a href="#分组背包问题" class="headerlink" title="分组背包问题"></a>分组背包问题</h2><h3 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h3><p>有 N 件物品和一个容量为 V 的背包。第 i 件物品的费用是 Ci，价值是 Wi。这些 物品被划分为 K 组，每组中的物品互相冲突，最多选一件。求解将哪些物品装入背包 可使这些物品的费用总和不超过背包容量，且价值总和最大。</p><h3 id="解题思路-4"><a href="#解题思路-4" class="headerlink" title="解题思路"></a>解题思路</h3><p>这个问题变成了每组物品有若干种策略:是选择本组的某一件，还是一件都不选。<br>也就是说设 F [k, v] 表示前 k 组物品花费费用 v 能取得的最大权值，则有:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F[k,v]=max&#123;F[k−1,v],F[k−1,v−Ci]+Wi |itemi∈groupk&#125;</span><br></pre></td></tr></table></figure></p><p>核心思想是：<strong>将该分组背包问题拆解为k个01背包问题</strong><br>使用一维数组的伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fork ←1toK</span><br><span class="line">    forv ←V to0</span><br><span class="line">        for all item i in group k</span><br><span class="line">            F[v] ←max&#123;F[v],F[v−Ci]+Wi&#125;</span><br></pre></td></tr></table></figure></p><h2 id="有依赖背包问题"><a href="#有依赖背包问题" class="headerlink" title="有依赖背包问题"></a>有依赖背包问题</h2><h3 id="问题描述-5"><a href="#问题描述-5" class="headerlink" title="问题描述"></a>问题描述</h3><p>这种背包问题的物品间存在某种“依赖”的关系。也就是说，物品 i 依赖于物品 j， 表示若选物品 i，则必须选物品 j。为了简化起见，我们先设没有某个物品既依赖于别 的物品，又被别的物品所依赖;另外，没有某件物品同时依赖多件物品。我们将不依赖于别的物品的物品称为“主件”，依赖于某主件的物品称为“附件”。</p><h2 id="泛化物品"><a href="#泛化物品" class="headerlink" title="泛化物品"></a>泛化物品</h2><h3 id="问题描述-6"><a href="#问题描述-6" class="headerlink" title="问题描述"></a>问题描述</h3><p>考虑这样一种物品，它并没有固定的费用和价值，而是它的价值随着你分配给它的 费用而变化。这就是泛化物品的概念。<br>更严格的定义之。在背包容量为V 的背包问题中，泛化物品是一个定义域为0…V 中的整数的函数 h，当分配给它的费用为 v 时，能得到的价值就是 h(v)。<br>这个定义有一点点抽象，另一种理解是一个泛化物品就是一个数组 h[0 . . . V ]，给它 费用 v，可得到价值 h[v]。</p>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>自制编程语言</title>
      <link href="/2018/07/27/Reading/%E8%87%AA%E5%88%B6%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
      <url>/2018/07/27/Reading/%E8%87%AA%E5%88%B6%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</url>
      <content type="html"><![CDATA[<p>本文是通过对《2週間でできる! スクリプト言語の作り方》这本书的学习，完成一个简单的编译器。</p><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>身在Programming Language Research Lab，虽然自身的研究是深度学习&amp;programming environment，但也有义务对PL有一定了解，虽然看了《SICP》，自己也试着写了一个特别小的Scheme解释器，对编程语言设计整体有了个大概的认识，但因为大三学的《编译原理》早早的就都还给了老师，所以对PL还是缺少一个清晰的认识。正好看到东工大编程语言研究室前任教授：千葉滋教授的编译器入门书籍：《2週間でできる! スクリプト言語の作り方》，而且书评不错，花时间把书中代码实现了一遍。</p><h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><p>本章是关于编程设计语言和语言处理器的一些基本概念。</p><h2 id="机器语言和汇编语言的区别"><a href="#机器语言和汇编语言的区别" class="headerlink" title="机器语言和汇编语言的区别"></a>机器语言和汇编语言的区别</h2><p>两者经常混淆，其实机器语言只由一串很长的01二进制组成，而汇编语言就是用来表示这串01数字的，便于理解。所以要想执行汇编语言程序仍然需要将其转换为机器语言。</p><h2 id="编译器和解释器的区别"><a href="#编译器和解释器的区别" class="headerlink" title="编译器和解释器的区别"></a>编译器和解释器的区别</h2><ul><li>解释器就是根据程序中的算法执行运算并返回结果。简单来说，计算器就是一个解释器，我们输入一串数字和运算符到计算器中，计算器返回计算结果，这就是个简单的解释器。</li><li>编译器能将某种语言的程序转换为另一种语言的程序，通常会转换为机器语言程序。</li><li>现代编程语言经常会混用二者，比如Java语言会首先通过编译器将源码转换为java二进制码，并将这种虚拟的语言保存在文件中。之后，Java虚拟机的解释器会执行这段代码。</li><li>大多数Java虚拟机为了提高性能，会在执行过程中通过编译器将一部分Java二进制代码直接转换为机器语言，在执行过程中进行的机器语言转换称为冬天编译或者JIT编译，转换后得到的机器语言程序将被载入内存，由硬件执行，不需要使用解释器。</li></ul><h2 id="语言处理器的架构"><a href="#语言处理器的架构" class="headerlink" title="语言处理器的架构"></a>语言处理器的架构</h2><p>无论是解释器还是编译器，语言处理器的前半部分大同小异，首先源码会进行词法分析，被分割成多个小字符串单元称为单词，之后处理器会执行语法分析，把单词排列转换为AST。之后编译器会将AST转换为其他语言，而解释器会一边分析AST，一边执行运算。</p><h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><p>本章是关于一种名叫Stone语言的语法设计。</p><h2 id="Stone语言语法"><a href="#Stone语言语法" class="headerlink" title="Stone语言语法"></a>Stone语言语法</h2><ol><li>包含数的四则运算</li><li>提供变量支持</li><li>基本的循环判断控制语句</li><li>动态语言（不需要显式声明变量类型）</li></ol><h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><p>本章是关于如何将源码分割成token，也是语言处理器的第一个组成部分：词法分析器（Lexer）</p><p>分为如下几步：</p><ol><li>定义token类</li><li>借助regex构建词法分析器</li><li>搭建UI</li></ol><h1 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h1><p>本章是语法分析，关于如何从sequence of token构建AST。</p><p>语法分析的主要任务是分析token之间的关系，如判断哪些token属于同一个表达式或语句，以及处理左右括号配对问题等。同时也会检查程序中是否含有语法错误。</p><h2 id="构建ASTree类"><a href="#构建ASTree类" class="headerlink" title="构建ASTree类"></a>构建ASTree类</h2><ol><li>构建抽象类ASTree，</li><li>并声明两个实现ASTLeaf和ASTList，前者表示AST的叶节点，后者表示AST的中间节点。</li><li>Name类继承自ASTLeaf，表示变量token</li><li>NameLiteral类继承自ASTLeaf，表示整数token</li><li>BinaryExpr类继承自ASTList，表示二元操作符</li></ol><ul><li>要构造AST，处理器首先需要知道会接收到那些token sequence，并确定希望构造出怎样的AST。语法规定了token的组合规则。本章讨论的语法较为浅显，进通过判断语句从哪个token开始，中途能够出现哪些token，又以什么token结束。</li><li>使用BNF巴科斯范式，与上下文无关文法等价。</li></ul><h2 id="BNF"><a href="#BNF" class="headerlink" title="BNF"></a>BNF</h2><p>BNF(Backus-Naur Form)是描述编程语言的文法。巴科斯范式是一种用于表示上下文无关文法的语言，上下文无关文法描述了一类形式语言。</p><p>自然语言存在不同程度的二义性。这种模糊、不确定的方式无法精确定义一门程序设计语言。必须设计一种准确无误地描述程序设计语言的语法结构，这种严谨、简洁、易读的形式规则描述的语言结构模型称为文法。</p><h4 id="BNF语法"><a href="#BNF语法" class="headerlink" title="BNF语法"></a>BNF语法</h4><p>&lt; &gt;     : 内包含的为必选项。<br>[ ]     : 内包含的为可选项。<br>{ }     : 内包含的为可重复0至无数次的项。<br>|       : 表示在其左右两边任选一项，相当于”OR”的意思。<br>::=     : 是“被定义为”的意思<br>“…”   : 术语符号<br>[…]   : 选项，最多出现一次<br>{…}   : 重复项，任意次数，包括 0 次<br>(…)   : 分组<br>|       : 并列选项，只能选一个</p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>Java语言总的for语句的BNF范式定义如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FOR_STATEMENT ::=</span><br><span class="line">    &quot;for&quot; &quot;(&quot; ( variable_declaration |</span><br><span class="line">    ( expression &quot;;&quot; ) | &quot;;&quot; )</span><br><span class="line">    [ expression ] &quot;;&quot;</span><br><span class="line">    [ expression ]</span><br><span class="line">    &quot;)&quot; statement</span><br></pre></td></tr></table></figure></p><h1 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h1><p>本章是关于如何设计语法分析器，即将token sequence与语法规则定义的模式进行匹配，并构造AST。</p><h4 id="Stone语言语法规则"><a href="#Stone语言语法规则" class="headerlink" title="Stone语言语法规则"></a>Stone语言语法规则</h4><p>利用BNF定义Stone语言的语法规则。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Project </tag>
            
            <tag> Reading </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Policy gradient</title>
      <link href="/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPolicy%20Gradients/"/>
      <url>/2018/07/25/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BPolicy%20Gradients/</url>
      <content type="html"><![CDATA[<p>我们已经知道DQN是一个基于价值value的方法。换句话说就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法。Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 那样学习每个 action 的 value, 而是给定每个action对应的概率，直接输出动作. 而且 Policy gradient 有一个优势是: 输出的这个 action 可以是一个连续的值, 而 value-based 方法输出的都是不连续的值。<br><a id="more"></a><br>举一个例子，车辆的速度选择，我们知道速度是一个连续值，而value-based是针对每一个动作给出reward进行学习，这样就不得不将速度离散化。而如果使用policy gradient方法则可以学习连续动作，直接给出车辆速度。</p><h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>想要使用梯度下降算法训练策略网络，就需要一个目标损失函数。策略网络使用策略梯度作为更新，具体的意思就是 <strong>直接改变动作的出现概率</strong>。仅从概率的角度来思考问题：我们有一个策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到reward，或者result。那么这个时候，我们有个非常简单的想法： <strong>如果某一个动作得到reward多，那么我们就使其出现的概率增大，如果某一个动作得到的reward少，那么我们就使其出现的概率减小。</strong></p><p>如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！通常情况下，使用对数似然来表示动作的评价指标。<br>因此，可以构造一个损失函数如下：<br><img src="images/reinforce_learning/policy_gradient_loss.png" alt="policy_gradient_loss"><br>这里f(s,a)表示最后的结果，也就是说一个游戏回合结束后，如果赢了，就认为这轮游戏中执行的每个动作都是好的，f(s,a)就为1，如果输了，每个动作都是不好的，f(s, a)为-1.</p><h2 id="Policy-Network"><a href="#Policy-Network" class="headerlink" title="Policy Network"></a>Policy Network</h2><p>什么是策略网络？Policy Network就是一个神经网络，输入是状态，输出直接就是动作（而不是Q值）。或者输出每个动作的概率。<br>以下是Policy Network的组成部分：</p><ol><li>一个神经网络（区别于DQN），网络的输入是状态，输出是每个动作的概率</li><li>损失函数时plicy gradient（log(poss)*f(s,a))</li><li>基于回合更新，也就是在整个回合结束后再更新，而不像value_based每一步一更新</li></ol><h2 id="Policy-Network工作流程"><a href="#Policy-Network工作流程" class="headerlink" title="Policy Network工作流程"></a>Policy Network工作流程</h2><ol><li>初始化神经网络模型</li><li>输入一个状态s到网络中，网络预测状态时每个动作的执行概率，并根据该概率返回一个动作a</li><li>将s和a输入到环境中，得到对应的reward，下一个状态s_，以及终止符t</li><li>如果t为未终止<ol><li>将[s, a, r]存入到网络中（可以理解为网络的记忆库）</li><li>将新的状态s_输入到网络中，重复第2步到第4步</li></ol></li><li>如果t为终止<ol><li>表示游戏结束，Policy Network根据之前存储的[s, a, r]数据对网络进行训练</li><li>重置环境，重复2~5步</li></ol></li></ol><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>使用OpenAI Gym的爬山小车作为环境，训练一个Policy Network模型进行训练。URL:<img src="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/policy%20gradient" alt="Policy Network"></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DQN的常见优化方法</title>
      <link href="/2018/07/23/Reinforce%20Learning/DQN%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2018/07/23/Reinforce%20Learning/DQN%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在实际应用DQN的过程中，会出现各种问题，本文介绍DQN的三种常见改进方法<br><a id="more"></a></p><h2 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h2><h3 id="过估计的原因"><a href="#过估计的原因" class="headerlink" title="过估计的原因"></a>过估计的原因</h3><p>DQN会遇到的第一个问题就是过估计（Overestimate），如果在DQN训练过程中试着输出Q值，可能发现Q值会非常大，这就是过估计。而Double DQN就是对传统DQN的一个改进方式，可以很好的解决过估计问题。<br>在DQN中，我们知道是用Target_Net的输出值中的最大值作为Q_Learning更新公式中的Max(Q(s’, a’))来得到Q(s, a)的真实label值，进而和Q_Net的预测值做比较更新神经网络。而Target_Net本身就是一个没有训练完成的，包含误差的神经网络，这就导致了Q值的过估计。</p><h3 id="Double-DQN的解决方法"><a href="#Double-DQN的解决方法" class="headerlink" title="Double DQN的解决方法"></a>Double DQN的解决方法</h3><p>Double DQN的想法是引入另一个神经网络来打消这些最大误差的影响。而DQN中本身就有两个神经网络，所以我们直接Q_Net估计Target_Net中的最大动作值，然后用这个被Q_Net估计出来的动作选择Target_Net的Q(s’,).<br>对比一下DQN的更新方法和Double的更新方法会更好理解：</p><ul><li>传统DQN：gamma<em>Q_next = gamma </em> Max(Q(s’, a’))   //取Target_Net对s’状态预测的所有动作最大值用来更新</li><li>Double DQN：gamma<em>Q_next = gamma </em> Q(s’, argmax(Q_Net(s’, a’))) //先用Q_Net对s’状态进行估计，找到Q值最大的动作，再用Target_Net中对应的最大动作的值Target_Net(s’, a’)来更新。<br>Double DQN的更新公式也就变成了：<br><img src="/images/reinforce_learning/double_DQN_update.png" alt="double_DQN_update"></li></ul><h3 id="Double-DQN更新流程"><a href="#Double-DQN更新流程" class="headerlink" title="Double DQN更新流程"></a>Double DQN更新流程</h3><p>DoubleDQN在整体上和DQN没有任何区别，只是在训练Q_Net时计算真实label值进而计算误差反向传播时有区别，我们只着眼于这个区别：<br>更新流程如下：</p><ol><li>每次训练有数据：[s, a, r, s_]，分别表示当前状态s，要采取的行动a，获得的奖励r，以及到达的新状态s_</li><li>对于输入状态s，Q_Net预测所有动作的Q值，并找到动作a对应的Q值设为 q_pre</li><li>对于输入状态s_, Q_Net预测所有动作的Q值，并找到最大Q值对应的动作 a_next</li><li>将状态s_输入到Target_Net中，得到所有动作的Q值。我们找到在第2步得到的动作a_next对应的q值, 设为q_up。</li><li>根据更新公式计算真实label的值：q_label = r + gamma * q_up</li><li>计算误差：sqrt( (q_pre - q_label)^2 )，对Q_Net进行反向传播</li><li>重复2至6步，不断训练Q_Net<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3>这里借用了OpenAI的gym中的一个Pendulum环境。主要参考的是<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-5-double_DQN/" target="_blank" rel="noopener">莫烦Python</a>的教学实例。</li></ol><p>该项目在我的GitHub上。具体项目URL：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Double_DQN" target="_blank" rel="noopener">Double DQN Example</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DQN是一个非常强大的算法，它成功的将深度学习和强化学习相结合。但在具体应用中会遇到各种各样的问题，而Double DQN就是一种解决Q值过大导致过估计的方法。Double DQN在结构和运行机制上和DQN误差，主要区别就在于训练Q_Net时对真实label的计算不同于传统DQN。</p><h2 id="Prioritized-Memory"><a href="#Prioritized-Memory" class="headerlink" title="Prioritized Memory"></a>Prioritized Memory</h2><p>实际应用DQN会遇到的第二个问题就是记忆库中样本不均衡问题。</p><h3 id="记忆库样本不均衡问题"><a href="#记忆库样本不均衡问题" class="headerlink" title="记忆库样本不均衡问题"></a>记忆库样本不均衡问题</h3><p>想想一下个简单的RL场景：机器人在走迷宫。我们设定没有成功到达出口的奖励值为-1， 成功到达出口的奖励值为100，同时只有一个出口。这时我们会发现DQN的训练过程非常缓慢，因为在DQN的记忆库中，只有非常少的几条记忆对应的奖励值时100，而且因为传统DQN是从记忆库中随机采样，DQN会很难学习到这几条“有价值”的记忆。也就是说，因为记忆库中的正负样本不均衡，会导致DQN训练时间过长。<br>而Prioritized Replay通过更加重视这些少量的记忆样本，达到加快学习速度的目的。</p><h3 id="Prioritized-Replay原理"><a href="#Prioritized-Replay原理" class="headerlink" title="Prioritized Replay原理"></a>Prioritized Replay原理</h3><p>PR的重点在于训练Q_Net时，每次从记忆库中选取batch个记忆是不再使用随机抽样，而是按照Memory中的样本优先级来抽取，这样可以更加高效的进行学习。</p><h4 id="优先级"><a href="#优先级" class="headerlink" title="优先级"></a>优先级</h4><p>而样本优先级p的设定可以根据TD-error来计算，每次更新Q_Net时计算的误差值（具体计算方法请查看之前的DQN笔记或者Double DQN笔记）。TD-error值越大，说明Q_Net的估计误差越大，越需要用这个样本进行学习，优先级p也就越高。</p><h4 id="SumTree"><a href="#SumTree" class="headerlink" title="SumTree"></a>SumTree</h4><p>在有了优先级p的计算方式之后，下一个问题就是如何根据p进行采样。如果Q_Net每个batch的训练都对记忆库根据p进行排序会非常消耗计算力。<a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener">Prioritized Experience Replay</a>这篇论文提出了SumTree结构，可以不需要对记忆库中的记忆排序而获取高优先级的训练记忆。<br>SumTree是一个二叉树，节点保存一个int值，该值等于左右子树节点的和。每个叶节点表示一个记忆样本。容易推断出根节点的值是所有数据p值的和。<br>一个SumTree的例子如下：<br><a href="/images/reinforce_Learning/sumtree.png">SumTree</a><br>进行抽样时， 将p的总和（也就是根节点的值）除batch size。也就是将p值分为batch size个区间。拿上图举例, 假设batch size为6, 这时的区间拥有的 priority 可能是这样.<br>[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]<br>然后在每个区间里随机选取一个数. 比如在第区间 [21-28] 里选到了24, 就按照这个 24 从最顶上的42开始向下搜索. 首先看到最顶上 42 下面有两个 child nodes, 拿着手中的24对比左边的 child 29, 如果 左边的 child 比自己手中的值大, 那我们就走左边这条路, 接着再对比 29 下面的左边那个点 13, 这时, 手中的 24 比 13 大, 那我们就走右边的路, 并且将手中的值根据 13 修改一下, 变成 24-13 = 11. 接着拿着 11 和 13 左下角的 12 比, 结果 12 比 11 大, 那我们就选 12 当做这次选到的 priority, 并且也选择 12 对应的数据.</p><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><p>这里是一个应用Prioritized Replay DQN的小车爬山例子，代码来自于<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/" target="_blank" rel="noopener">莫烦Python</a>。代码的URL如下：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/DQN_Prioritied_Replay" target="_blank" rel="noopener">DQN with Prioritized Replay</a></p><h2 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h2><p>对于传统的DQN，存在训练时间太长，收敛速度较慢的问题，Dueling DQN是一个新的方法来提升学习速度，甚至提高模型准确率。</p><h3 id="Dueling-DQN结构"><a href="#Dueling-DQN结构" class="headerlink" title="Dueling DQN结构"></a>Dueling DQN结构</h3><p>Dueling DQN对DQN中神经网络的结构做出了修改，如下图所示：<br><img src="images/reinforce_learning/DuelingDQNstructure.png" alt="DuelingDQNstructure"><br>上图第一个模型为传统DQN，输入一个state到Q_Net中，Q_Net会预测在该state每个动作的Q值大小。<br>上图第二个模型即为Dueling DQN结构，其将卷积层提取的抽象特征分流到两个支路中，其中上路为状态值函数V(s)，<strong>表示当前输入状态本身具有的价值</strong>，下路为当前状态各个动作的优势函数A(a)，<strong>表示在这个状态选择摸个Action额外带来的价值。</strong><br>而Q值的计算包含当前state的价值和state中每个动作的advantage：<br><img src="images/reinforce_learning/dueling_DQN.png" alt="dueling_DQN"><br>这种竞争结构能学到在没有动作的影响下环境状态的价值 V(s)。Dueling DQN这么设计的直观意义是对于某些state，在当前的state无论做什么动作对下一个state都没有多大影响，这个时候动作的Q值更少的受动作的A(a)影响，更多的受状态的价值V(s)影响。具体解释请看这篇paper:<a href="https://arxiv.org/abs/1511.06581" target="_blank" rel="noopener"><br>Dueling Network Architectures for Deep Reinforcement Learning</a></p><p>在这篇paper中，有如下例子：图中红色区域代表 V(s) 和 A(a) 所关注的地方。V(s) 关注于地平线上是否有车辆出现（此时动作的选择影响不大）以及分数；A(a) 则更关心会立即造成碰撞的车辆，此时动作的选择很重要。<br><img src="images/reinforce_learning/DuelingDQNExample.png" alt="DuelingDQNExample"></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>参考的<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-7-dueling-DQN/" target="_blank" rel="noopener">莫烦Python</a>中的例子，具体代码URL：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Dueling_DQN" target="_blank" rel="noopener">Dueling DQN</a></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>面试常见的海量数据处理题</title>
      <link href="/2018/07/15/Algorithms/%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%A2%98/"/>
      <url>/2018/07/15/Algorithms/%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>总结面试常见的几种海量数据处理题<br><a id="more"></a></p><h2 id="十道海量数据处理面试题"><a href="#十道海量数据处理面试题" class="headerlink" title="十道海量数据处理面试题"></a>十道海量数据处理面试题</h2><h3 id="日志数据提取访问次数最多的IP。"><a href="#日志数据提取访问次数最多的IP。" class="headerlink" title="日志数据提取访问次数最多的IP。"></a>日志数据提取访问次数最多的IP。</h3><h4 id="算法思想：分治-Hash"><a href="#算法思想：分治-Hash" class="headerlink" title="算法思想：分治+Hash"></a>算法思想：分治+Hash</h4><ol><li>IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理；</li><li>可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址；</li><li>对于每一个小文件，可以构建一个IP为key，出现次数为value的Hash map，同时记录当前出现次数最多的那个IP地址；</li><li>可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP；</li></ol><h3 id="query中最热门的100个query。"><a href="#query中最热门的100个query。" class="headerlink" title="query中最热门的100个query。"></a>query中最热门的100个query。</h3><h4 id="算法思路：hash-topk排序"><a href="#算法思路：hash-topk排序" class="headerlink" title="算法思路：hash+topk排序"></a>算法思路：hash+topk排序</h4><ol><li>先对这批海量数据预处理，在O(N)的时间内用Hash的方法将所有query分割成多个小文件。</li><li>对每个小文件，使用字典树/HashMap 统计每个query出现的频率。（也可以对小文件先进行排序，然后返回每个小文件中频率最高的100个query）</li><li>然后使用最小堆，找出Top K个出现次数最多的query，时间复杂度为NlogK。</li><li>利用最小堆选出出现频率最大的k个query。即维护一个K大小的最小堆，堆顶为整个堆中出现频率最小的query。然后遍历所有query，分别和根元素进行对比并调整堆。</li></ol><h3 id="大文件数组排序"><a href="#大文件数组排序" class="headerlink" title="大文件数组排序"></a>大文件数组排序</h3><h4 id="多路归并外排序"><a href="#多路归并外排序" class="headerlink" title="多路归并外排序"></a>多路归并外排序</h4><p>将大文件拆分成M个小文件，用内部排序算法让每个小文件有序。然后构建M个队列，每个队列从对应的小文件中读取m个sorted数据。合并时每次找到M个队列头最小的值，将其写入结果文件中。</p><h3 id="找出两个大文件中共同的数据"><a href="#找出两个大文件中共同的数据" class="headerlink" title="找出两个大文件中共同的数据"></a>找出两个大文件中共同的数据</h3><p>给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？<br>估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。</p><h4 id="算法思路：hash-分治"><a href="#算法思路：hash-分治" class="headerlink" title="算法思路：hash+分治"></a>算法思路：hash+分治</h4><ol><li>遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。</li><li>遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</li><li>求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</li></ol><h3 id="在2-5亿个整数中找出不重复的整数。"><a href="#在2-5亿个整数中找出不重复的整数。" class="headerlink" title="在2.5亿个整数中找出不重复的整数。"></a>在2.5亿个整数中找出不重复的整数。</h3><h4 id="算法思路：bitmap统计"><a href="#算法思路：bitmap统计" class="headerlink" title="算法思路：bitmap统计"></a>算法思路：bitmap统计</h4><p>采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。</p><h4 id="hash-分治"><a href="#hash-分治" class="headerlink" title="hash+分治"></a>hash+分治</h4><p>也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。</p><h3 id="判断一个数是否出现在一个大文件中"><a href="#判断一个数是否出现在一个大文件中" class="headerlink" title="判断一个数是否出现在一个大文件中"></a>判断一个数是否出现在一个大文件中</h3><p>给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？</p><h4 id="算法思路：快速排序-二分查找"><a href="#算法思路：快速排序-二分查找" class="headerlink" title="算法思路：快速排序+二分查找"></a>算法思路：快速排序+二分查找</h4><h4 id="BitMap"><a href="#BitMap" class="headerlink" title="BitMap"></a>BitMap</h4><p>申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。</p><h4 id="方案3"><a href="#方案3" class="headerlink" title="方案3"></a>方案3</h4><ol><li>首先把40亿个数中的每一个用32位的二进制来表示</li><li>假设这40亿个数开始放在一个文件中。然后将这40亿个数分成两类:1.最高位为0；2.最高位为1</li><li>将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20亿，而另一个&gt;=20亿（这相当于折半了）；</li><li>与要查找的数的最高位比较并接着进入相应的文件再查找</li><li>然后用次高位的取值再次将文件分为两个更小文件。不断进行二分查找</li><li>以此类推，就可以找到了,而且时间复杂度为O(logn)</li></ol><h3 id="数据中重复次数最多的一个"><a href="#数据中重复次数最多的一个" class="headerlink" title="数据中重复次数最多的一个"></a>数据中重复次数最多的一个</h3><p>先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。</p>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Big Data </tag>
            
            <tag> Interview </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hive学习笔记</title>
      <link href="/2018/07/11/Hadoop/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/07/11/Hadoop/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><p>Hive是Hadoop的一个重要子项目，利用了MapReduce编程技术，实现了部分SQL语句，提供了类SQL的编程接口。<br><a id="more"></a><br>Hive 是一个基于Hadoop文件系统的数据仓库架构，定义了类SQL语言—Hive QL。HQL允许用户进行和SQL相似的操作，同时还允许开发人员方便的使用mapper和reducer操作。<br>由于Hadoop是批量处理系统，任务高延迟，所以Hive即使处理小数据执行时也会出现延迟现象。<br><strong>Hive可以将外部命令解析成一个MapReduce可执行计划，并按照该计划生成MapReduce任务后交给Hadoop集群处理。</strong><br>Hive本身没有专门的数据存储格式，也不能为数据建立索引，用户可以自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据的列分隔符和行分隔符就可以。</p><h2 id="Hive主要数据类型"><a href="#Hive主要数据类型" class="headerlink" title="Hive主要数据类型"></a>Hive主要数据类型</h2><ol><li>表</li><li>外部表</li><li>分区</li><li>桶</li></ol><h2 id="Hive-QL"><a href="#Hive-QL" class="headerlink" title="Hive QL"></a>Hive QL</h2><h3 id="数据定义操作（DDL）"><a href="#数据定义操作（DDL）" class="headerlink" title="数据定义操作（DDL）"></a>数据定义操作（DDL）</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE table_name</span><br><span class="line">[(col_name data_type ..)]</span><br><span class="line">STORED AS ...</span><br><span class="line">PARTITIONED BY ...</span><br><span class="line"></span><br><span class="line">CREATE [EXTERNAL] TABLE table_name</span><br><span class="line">LIKE existing_table_name</span><br></pre></td></tr></table></figure><ol><li>CREATE TABLE 创建一个指定名字的表，如果已经存在则抛出异常</li><li>EXTERNAL 创建一个外部表，创建表的同时指定一个指向实际数据的路径。Hive创建内部表时，会将数据移动到指定路径，创造外部表时则仅记录数据所在的路径，不对数据的位置坐任何改变。</li><li>LIKE 修饰create table命令允许复制一个已经存在表的定义，而不复制他的数据内容。</li><li>STORED AS 表明存储方式，’STORED AS TEXTFILE’表示将数据存储为纯文本文件。’STORED AS SEQUENCEFILE’表示压缩存储</li><li>‘PARTITIONED BY’ 创建有分区的表，一个表可以有一个或者多个分区。每个分区单独存在于一个目录下。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE page_view (viewTime INT, userid BIGINT,</span><br><span class="line">    page_url STRING, ip STRING COMMENT &apos;IP address of the user&apos;)</span><br><span class="line">COMMENT &apos;This is the page view table&apos;</span><br><span class="line">PARTITIONED BY(dt STRING, country STRING)</span><br><span class="line">STORED AS SEQUENCEFILE;</span><br></pre></td></tr></table></figure></li></ol><p>上面是一个使用Hive创建表的例子，创建了page_view表，并包括viewTime，userid，page_url，ip列以及注释</p><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE table_name</span><br></pre></td></tr></table></figure><h4 id="修改表、分区语句"><a href="#修改表、分区语句" class="headerlink" title="修改表、分区语句"></a>修改表、分区语句</h4><ul><li><p>向表中增加分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name ADD partition_spec [LOCATION &apos;location1&apos;] ...</span><br></pre></td></tr></table></figure></li><li><p>删除分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name DROP partition_spec, ...</span><br></pre></td></tr></table></figure></li><li><p>重命名表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure></li><li><p>改变列名字、类型、位置、注释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name CHANGE [COLUMN]</span><br><span class="line">    col_old_name col_new_name column_type</span><br><span class="line">    [COMMENT col_comment]</span><br><span class="line">    [FIRST|AFTER column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加、更新列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name ADD|REPLACE</span><br><span class="line">    COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br></pre></td></tr></table></figure></li><li><p>创建、删除试图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE VIEW view_name [(column_name [COMMENT column_comment], ...)]</span><br><span class="line">[COMMENT view_comment]</span><br><span class="line">AS SELECT ...</span><br></pre></td></tr></table></figure></li><li><p>创建、删除函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY FUNCTION function_name AS class_name</span><br><span class="line"></span><br><span class="line">DROP TEMPORARY FUNCTION function_name</span><br></pre></td></tr></table></figure></li></ul><p>创建、删除一个由类名实现的函数。</p><h3 id="数据操作（DML）"><a href="#数据操作（DML）" class="headerlink" title="数据操作（DML）"></a>数据操作（DML）</h3><h4 id="向数据表中加载文件"><a href="#向数据表中加载文件" class="headerlink" title="向数据表中加载文件"></a>向数据表中加载文件</h4><p>当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制、移动到Hive表对应的位置上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE]</span><br><span class="line">    INTO TABLE tablename</span><br><span class="line">    [PARTITION (partcol1=val1, partcol2=val2 .. )]</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之DQN</title>
      <link href="/2018/07/02/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/"/>
      <url>/2018/07/02/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/</url>
      <content type="html"><![CDATA[<p>DQN全称Deep Q Network，是一种将传统强化学习算法Q-Learning和Deep Learning相结合的算法。最有名的DQN应用应该就是在围棋上打败全人类的AlphaGo了。</p><a id="more"></a><h2 id="传统强化学习"><a href="#传统强化学习" class="headerlink" title="传统强化学习"></a>传统强化学习</h2><p>在<a href="">强化学习之Q_Learning</a>中我们已经了解到强化学习的基本概念以及Q_Learning的思想和。而Q_Learning算法面临一个巨大的问题：当State个数过于庞大时，Q_table会变得特别大，以至于每次执行学习循环时table的查找会消耗大量时间，严重影响效率，同时过大的Q_table也会造成存储问题。对于这个问题，DQN是一个非常好的解决办法。</p><h2 id="Q-Learning和Deep-Neural-Network"><a href="#Q-Learning和Deep-Neural-Network" class="headerlink" title="Q_Learning和Deep Neural Network"></a>Q_Learning和Deep Neural Network</h2><p>我们可以通过一个神经网络学习并预测每个State和Action对应的Q值，这样我们就不在需要记录巨大的Q表，只需要训练一个神经网络即可。具体流程如下：输入一个状态，神经网络输出所有动作对应的预测Q值，然后按照Q Learning的原则，使用梯度下降方法对网络进行更新。<br>形象的理解，就是Agent接收当前State的种种信息，然后在神经网络大脑中对自己可以做的每个Action默默评估，选择自认为最优的行为执行，做出动作后然后通过环境的反馈不断更新神经网络。</p><p>整个DQN算法结构如下：<br><img src="/images/reinforce_learning/DQN_alg.jpg" alt="DQN_Algorithm"></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>在DQN中，一共有三个组件：包括两个神经网络Target_Net、Q_Net和用来存储训练数据的记忆库（ReplayMemory）。</p><h4 id="Q-Net和Target-Net"><a href="#Q-Net和Target-Net" class="headerlink" title="Q_Net和Target_Net"></a>Q_Net和Target_Net</h4><p><strong>结构上这两个神经网络完全相同</strong>（可以包含卷积层，全连接层）。<strong>两个神经网络的输入都是当前状态的一些信息，输出是每一个action对应的预测Q值</strong>。<br>在最开始学习DQN的时候，一定会有这样的困惑，为什么要有两个神经网络？这两个神经网络的作用是什么？在这里我做个简单的不严谨说明便于理解：Q_Net是一个实时的网络（每次输入状态，就是用Q_Net的预测作为输出动作）。而Target_Net是用来计算Q更新公式求训练label的（将s的下一个状态s_输入Target_Net，输出的最大值作为Q(s’, a’)，计算更新公式得到Q(s, a)作为真实label）。<br>所以要想理解DQN两个网络工作机制的核心，只需要记住这一句话：<strong>Q_Net做出预测，Target_Net计算真实label，然后计算loss反向传播Q_Net</strong><br>在训练时，我们只对Q_Net做反向传播更新权重，Target_Net的权重更新来自于对Q_Net的直接复制。</p><h4 id="记忆库"><a href="#记忆库" class="headerlink" title="记忆库"></a>记忆库</h4><p>（ReplayMemeory）记忆库本质上是一个二维表，表的每一行都是一个马尔科夫节点表示一条记忆。举个例子：比如当前Agent处在状态s_t下，并执行action a_t到达下一个状态s_t_1，同时获得奖励reward值r_t。那么由：[s_t, a_t, r_t, s_t_1]构成的一个list就是一个马尔科夫节点。说明当前状态和下一个状态之间的关系。<br>记忆库是作为对神经网络的训练数据集存在，可以理解为一小段Q表信息。每次从环境获得一个[状态，动作，奖励，新状态]，就将其更新到记忆库中。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>在训练时我们只训练Q_Net，并不训练Target_Net。Target_Net权值的更新来自于对Q_Net权值的直接复制。</p><ol><li>每一次对Q_Net的迭代训练，就会从记忆库中随机抽取一个batch的马尔科夫节点[s_t, a_t, s_t_1, r_t, t_t]记忆。</li><li>然后将s_t输入到Q_Net中，得到Q_Net对s_t状态下每个action预测的Q值，然后取出 action a_t 对应的Q值这就是Q_Net的预测值Q(a_t|s_t)。</li><li>然后将s_t_1喂入Target-Net, 得到Target_Net在状态s_t_1下，对每一个action对应的Q值，然后取最大的Q值作为Max(Q(s’, a’))</li><li>根据贝尔曼公式（Q Learning更新方法）计算对应的Q(s, a)作为真实label值。</li><li>最后用上一步计算的Q(s, a)减去Q_Net的预测值Q(a_t|s_t)作为误差反向传播，更新Q_Net。</li><li>然后再每次Q_Net的训练迭代到一定次数的时候更新Target_Net（将Q_Net的参数完全复制到Target_Net）中。</li></ol><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol><li>初始化状态、环境以及DQN</li><li>DQN通过当前状态s的观测值选择行为a<ol><li>将输入状态s的种种观测值observation输入到Q_Net，Q_Net预测每个动作对应的Q值</li><li>大概率返回预测Q值最大的动作，小概率随机返回一个动作。增加DQN的探索性</li></ol></li><li>环境根据s和a，返回下一个新状态s_，奖励值r以及是否结束t</li><li>DQN存储记忆，将[当前状态s，行为a，奖励r，新状态s_]存储到记忆库中<ol><li>将[s, a, r, s_]存储到DQN的记忆库中</li><li>若已经达到记忆库的大小，则不断删除旧记忆，实现记忆库更新</li></ol></li><li>每隔指定步骤对神经网络进行一次训练<ol><li>从记忆库中随机选取batch个记忆</li><li>将每个记忆的s输入到Q_Net中，每个记忆的s_输入到Target_Net中。从Q_Net得到在s时每个动作的预测Q值（一个list），从Target_Net中得到s_的预测Q值（一个list）。</li><li>从记忆[s, a, r, s_]中找到动作a，同时找到该动作在Q_Net的预测输出中对应的Q值q_pre，以及Target_Net的预测输出中最大的Q值q_next。</li><li>根据更新公式：Q(s, a) = r + gamma * Max(Q(s_, a_))，该值作为实际的label值</li><li>计算误差，对Q_Net进行反向传播：loss = q_pre - (r + gamma*q_next)</li><li>重复1至5步，实现对Q_Net的训练，每训练指定个step后对Target_Net进行更新。更新方法是将Q_Net的参数直接copy给Target_Net。</li></ol></li><li>更新状态s = s_</li><li>循环2至5步直至环境返回结束状态</li></ol><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>在我的GitHub上实现了一个简单的DQN走迷宫的例子，（参考<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">莫烦Python</a>教程的例子）。为了便于理解，我在代码中尽可能详细的写了注释。这是项目地址：<a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/DQN" target="_blank" rel="noopener">DQN走迷宫</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DQN是将传统的强化学习方法Q_Learning和深度学习相结合，得到的一个强力RL模型。DQN在很多任务上取得了非常优秀的表现，著名的AlphaGo即使DQN的代表应用之一，当然AlphaGo的模型要复杂的多的多。</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop家族各个组件的关系</title>
      <link href="/2018/06/27/Hadoop/Hadoop%E5%AE%B6%E6%97%8F%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
      <url>/2018/06/27/Hadoop/Hadoop%E5%AE%B6%E6%97%8F%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
      <content type="html"><![CDATA[<p>Hadoop是一个庞大的家族，用于多个组件，新手刚入门时会搞不懂各个组件是干什么的，它们之间的关系如何，这篇文章介绍了Hadoop家族中主要组件的功能。</p><a id="more"></a><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>大数据，首先你要能存的下大数据。<br>传统的文件系统是单机的，不能横跨不同的机器。HDFS的设计本质上是为了大量的数据能横跨成百上千台机器，但是你看到的是一个文件系统而不是很多文件系统。比如你说我要获取/hdfs/tmp/file1的数据，你引用的是一个文件路径，但是实际的数据存放在很多不同的机器上。你作为用户，不需要知道这些，就好比在单机上你不关心文件分散在什么磁道什么扇区一样。HDFS为你管理这些数据。</p><h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><p>存的下数据之后，你就开始考虑怎么处理数据。虽然HDFS可以为你整体管理不同机器上的数据，但是这些数据太大了。一台机器慢慢跑也许需要好几天甚至好几周。对于很多公司来说，单机处理是不可忍受的，比如微博要更新24小时热博，它必须在24小时之内跑完这些处理。那么我如果要用很多台机器处理，我就面临了如何分配工作，机器之间如何互相通信交换数据以完成复杂的计算等等。这就是MapReduce / Tez / Spark的功能。MapReduce是第一代计算引擎，Tez和Spark是第二代。MapReduce的设计，采用了很简化的计算模型，只有Map和Reduce两个计算过程（中间用Shuffle串联），用这个模型，已经可以处理大数据领域很大一部分问题了。</p><h4 id="什么是Map什么是Reduce？"><a href="#什么是Map什么是Reduce？" class="headerlink" title="什么是Map什么是Reduce？"></a>什么是Map什么是Reduce？</h4><p>考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。Map阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的Pair；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个Reducer都如上处理，你就得到了整个文件的词频结果。</p><h4 id="Tez-amp-Spark"><a href="#Tez-amp-Spark" class="headerlink" title="Tez &amp; Spark"></a>Tez &amp; Spark</h4><p>Map＋Reduce的简单模型很黄很暴力，虽然好用，但是很笨重。第二代的Tez和Spark除了内存Cache之类的新feature，本质上来说，是让Map/Reduce模型更通用，让Map和Reduce之间的界限更模糊，数据交换更灵活，更少的磁盘读写，以便更方便地描述复杂算法，取得更高的吞吐量。</p><h4 id="Pig-amp-Hive"><a href="#Pig-amp-Hive" class="headerlink" title="Pig &amp; Hive"></a>Pig &amp; Hive</h4><p>有了MapReduce，Tez和Spark之后，程序员发现，MapReduce的程序写起来真麻烦。他们希望简化这个过程。希望有个更高层更抽象的语言层来描述算法和数据处理流程。于是就有了Pig和Hive。 <strong>Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL。</strong> 它们把脚本和SQL语言翻译成MapReduce程序，丢给计算引擎去计算，而你就从繁琐的MapReduce程序中解脱出来，用更简单更直观的语言去写程序了。</p><p>有了Hive之后，人们发现SQL对比Java有巨大的优势。一个是它太容易写了。刚才词频的东西，用SQL描述就只有一两行，MapReduce写起来大约要几十上百行。Hive逐渐成长成了大数据仓库的核心组件。甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。</p><h4 id="Impala-amp-Presto-amp-Drill"><a href="#Impala-amp-Presto-amp-Drill" class="headerlink" title="Impala &amp; Presto &amp; Drill"></a>Impala &amp; Presto &amp; Drill</h4><p>自从数据分析人员开始用Hive分析数据之后，它们发现，Hive在MapReduce上跑，慢！流水线作业集也许没啥关系，比如24小时更新的推荐，反正24小时内跑完就算了。但是数据分析，人们总是希望能跑更快一些。</p><p>于是Impala，Presto，Drill诞生了（当然还有无数非著名的交互SQL引擎，就不一一列举了）。三个系统的核心理念是，MapReduce引擎太慢，因为它太通用，太强壮，太保守，我们SQL需要更轻量，更激进地获取资源，更专门地对SQL做优化，而且不需要那么多容错性保证（因为系统出错了大不了重新启动任务，如果整个处理时间更短的话，比如几分钟之内）。这些系统让用户更快速地处理SQL任务，牺牲了通用性稳定性等特性。如果说MapReduce是大砍刀，砍啥都不怕，那上面三个就是剔骨刀，灵巧锋利，但是不能搞太大太硬的东西。</p><h4 id="Hive-on-Tez-Spark-amp-SparkSQL"><a href="#Hive-on-Tez-Spark-amp-SparkSQL" class="headerlink" title="Hive on Tez/Spark &amp; SparkSQL"></a>Hive on Tez/Spark &amp; SparkSQL</h4><p>这些系统，说实话，一直没有达到人们期望的流行度。因为这时候又两个异类被造出来了。他们是Hive on Tez / Spark和SparkSQL。它们的设计理念是，MapReduce慢，但是如果我用新一代通用计算引擎Tez或者Spark来跑SQL，那我就能跑的更快。而且用户不需要维护两套系统。</p><p>上面的介绍，基本就是一个数据仓库的构架了。底层HDFS，上面跑MapReduce／Tez／Spark，再上面跑Hive，Pig。或者HDFS上直接跑Impala，Drill，Presto。这解决了中低速数据处理的要求。</p><h4 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h4><p>那如果我要更高速的处理呢？</p><p>如果我是一个类似微博的公司，我希望显示不是24小时热博，我想看一个不断变化的热播榜，更新延迟在一分钟之内，上面的手段都将无法胜任。于是又一种计算模型被开发出来，这就是Streaming（流）计算。Storm是最流行的流计算平台。流计算的思路是，如果要达到更实时的更新，我何不在数据流进来的时候就处理了？比如还是词频统计的例子，我的数据流是一个一个的词，我就让他们一边流过我就一边开始统计了。流计算很牛逼，基本无延迟，但是它的短处是，不灵活，你想要统计的东西必须预先知道，毕竟数据流过就没了，你没算的东西就无法补算了。因此它是个很好的东西，但是无法替代上面数据仓库和批处理系统。</p><h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><p>还有一个有些独立的模块是KVStore，比如Cassandra，HBase，MongoDB等。所以KV Store就是说，我有一堆键值，我能很快速获取与这个Key绑定的数据。比如我用身份证号，能取到你的身份数据。这个动作用MapReduce也能完成，但是很可能要扫描整个数据集。而KV Store专用来处理这个操作，所有存和取都专门为此优化了。从几个P的数据中查找一个身份证号，也许只要零点几秒。这让大数据公司的一些专门操作被大大优化了。比如我网页上有个根据订单号查找订单内容的页面，而整个网站的订单数量无法单机数据库存储，我就会考虑用KV Store来存。KV Store的理念是，基本无法处理复杂的计算，大多没法JOIN，也许没法聚合，没有强一致性保证（不同数据分布在不同机器上，你每次读取也许会读到不同的结果，也无法处理类似银行转账那样的强一致性要求的操作）。但是丫就是快。极快。</p><p>每个不同的KV Store设计都有不同取舍，有些更快，有些容量更高，有些可以支持更复杂的操作。必有一款适合你。</p><h4 id="Mahout-amp-Protobuf-amp-ZooKeeper"><a href="#Mahout-amp-Protobuf-amp-ZooKeeper" class="headerlink" title="Mahout &amp; Protobuf &amp; ZooKeeper"></a>Mahout &amp; Protobuf &amp; ZooKeeper</h4><p>除此之外，还有一些更特制的系统／组件，比如Mahout是分布式机器学习库，Protobuf是数据交换的编码和库，ZooKeeper是高一致性的分布存取协同系统，等等。</p><h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><p>有了这么多乱七八糟的工具，都在同一个集群上运转，大家需要互相尊重有序工作。所以另外一个重要组件是，调度系统。现在最流行的是Yarn。你可以把他看作中央管理，好比你妈在厨房监工，哎，你妹妹切菜切完了，你可以把刀拿去杀鸡了。只要大家都服从你妈分配，那大家都能愉快滴烧菜。</p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP流水线</title>
      <link href="/2018/06/25/Project/NLP%E6%B5%81%E6%B0%B4%E7%BA%BF/"/>
      <url>/2018/06/25/Project/NLP%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
      <content type="html"><![CDATA[<p>NLP项目是一个庞大且复杂的项目，在机器学习中任何复杂的事情通常意味着需要建立一条流水线（pipeline），把复杂问题分解成各个小部分，然后用机器学习模型分别解决每个部分，最后通过吧几个互相馈送结果的模型连接在一起，就可以用来解决非常复杂的问题。<br><a id="more"></a></p><h2 id="NLP流水线"><a href="#NLP流水线" class="headerlink" title="NLP流水线"></a>NLP流水线</h2><h3 id="步骤1：句子分割"><a href="#步骤1：句子分割" class="headerlink" title="步骤1：句子分割"></a>步骤1：句子分割</h3><p>pipeline的第一步是把原始文本拆分成单独的句子。编写一个程序来理解一个句子比理解整个段落要容易的多。<br>编码一个句子分割模型可以很简单地在任何看到标点符号的时候拆分句子。但是，现代 NLP 流水线通常使用更为复杂的技术，以应对那些没有被格式化干净的文件。</p><h3 id="步骤2：词汇标记化"><a href="#步骤2：词汇标记化" class="headerlink" title="步骤2：词汇标记化"></a>步骤2：词汇标记化</h3><p>对于已经处理完的句子，下一步就是将句子拆分成不同的单词或者标记，这个步骤叫做标记化，通常也将标点符号当做单独的标记对待。<br>对于英文莱索标记化很容易做到，只因为英文单词之间有空格。</p><h3 id="步骤3：预测标记的词性"><a href="#步骤3：预测标记的词性" class="headerlink" title="步骤3：预测标记的词性"></a>步骤3：预测标记的词性</h3><p>对于每个标记，需要尝试猜测他的词性：名词，动词，形容词等等。知道每个单词的词性可以帮助计算机理解句子的意思。<br>每个单词（和它周围的一些额外的单词用于上下文）输入预先训练的词性分类模型：<br><img src="/images/NLP/cixingbiaoji.png" alt="NLP_cixingyuce"><br>词性模型最初是通过给它提供数以百万计的英语句子来训练的，每一个单词的词性都已经标注出来，并让它学会复制这种行为。<br>最后，对于句子中每个标记，我们可以得到它对应的词性预测。如：is-Verb；china-Noun</p><h3 id="步骤4：文本词形还原"><a href="#步骤4：文本词形还原" class="headerlink" title="步骤4：文本词形还原"></a>步骤4：文本词形还原</h3><p>在如英语等大多数语言中，单词会以不同形式出现，如动词的单三形式，过去式，过去分词等。在计算机中处理文本时，了解每个单词的基本形式是有意义的，这样才可以知道两个句子都在讨论一个概念。<br>所以，在NLP中，对单词的各种变换进行词形还原是非常重要的：<strong>找出句子中每个单词的基本形式</strong><br><strong>词形还原通常是通过基于词性的词条形式查表直接替换完成的</strong></p><h3 id="步骤5：识别停止词"><a href="#步骤5：识别停止词" class="headerlink" title="步骤5：识别停止词"></a>步骤5：识别停止词</h3><p>接下来，我们要考虑句子中每个词的重要性。英语有很多填充词，它们经常出现，如「and」、「the」和「a」。当对文本进行统计时，这些词引入了大量的噪声，因为它们比其他词更频繁地出现。一些 NLP 流水线将它们标记为「停止词」，也就是说，在进行任何统计分析之前，这可能是你想要过滤掉的单词。</p><h3 id="步骤6a：依赖解析"><a href="#步骤6a：依赖解析" class="headerlink" title="步骤6a：依赖解析"></a>步骤6a：依赖解析</h3><p>下一步是弄清楚我们句子中的所有单词是如何相互关联的，这叫做依赖解析。<br>我们的目标是构建一棵树，它给句子中的每个单词分配一个单一的父词。树的根结点是句子中的主要动词。下面是我们的句子的解析树一开始的样子：<br><img src="/images/NLP/yilaishu.png" alt="依赖树"><br>作者：机器之心<br>链接：<a href="https://zhuanlan.zhihu.com/p/41850756" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/41850756</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。</p><p>这棵解析树告诉我们，句子的主语是名词「London」，它与「capital」有「be」关系。我们终于知道了一些有用的东西——伦敦是一个首都！如果我们遵循完整的解析树的句子（除上方所示），我们甚至会发现，伦敦是英国的首都。</p><p>就像我们先前使用机器学习模型预测词性一样，依赖解析也可以通过将单词输入机器学习模型并输出结果来工作。但是解析单词的依赖项是一项特别复杂的任务，需要一篇完整的文章来详细说明。如果你想知道它是如何工作的，一个很好的开始阅读的地方是 Matthew Honnibal 的优秀文章「Parsing English in 500 Lines of Python」。</p><p>但是，尽管作者在 2015 的一篇文章中说这种方法在现在是标准的，但它实际上已经过时了，甚至不再被作者使用。在 2016，谷歌发布了一个新的依赖性分析器，称为 Parsey McParseface，它使用了一种新的深度学习方法并超越了以前的基准，它迅速地遍及整个行业。一年后，他们发布了一种新的叫做 ParseySaurus 的模型，它改进了更多的东西。换句话说，解析技术仍然是一个活跃的研究领域，在不断地变化和改进。</p><p>同样需要记住的是，很多英语句子都是模棱两可的，难以解析的。在这种情况下，模型将根据该句子的解析版本进行猜测，但它并不完美，有时该模型将导致令人尴尬的错误。但随着时间的推移，我们的 NLP 模型将继续以更好的方式解析文本。</p><h3 id="步骤6b：寻找名词短语"><a href="#步骤6b：寻找名词短语" class="headerlink" title="步骤6b：寻找名词短语"></a>步骤6b：寻找名词短语</h3><p>到目前为止，我们把句子中的每个词都看作是独立的实体。但是有时候把代表一个想法或事物的单词组合在一起更有意义。我们可以使用依赖解析树中的相关信息自动将所有讨论同一事物的单词组合在一起。是否做这一步取决于我们的最终目标。如果我们不需要更多的细节来描述哪些词是形容词，而是想更多地关注提取完整的想法，那么这是一种快速而简单的方法。</p><h3 id="步骤7：命名实体识别（NER）"><a href="#步骤7：命名实体识别（NER）" class="headerlink" title="步骤7：命名实体识别（NER）"></a>步骤7：命名实体识别（NER）</h3><p>命名实体识别（NER）的目标是用它们所代表的真实世界的概念来检测和标记这些名词。以下是我们在使用 NER 标签模型运行每个标签之后的句子：<br><img src="/images/NLP/ner.png" alt="NER"><br>NER 系统不仅仅是简单的字典查找。相反，他们使用的是一个单词如何出现在句子中的上下文和一个统计模型来猜测单词代表的是哪种类型的名词。一个好的 NER 系统可以通过上下文线索来区分「Brooklyn Decker」这个人名和「Brooklyn」这个位置。<br>下面是一些典型的 NER 系统可以标记的对象类型：</p><ul><li>人名</li><li>公司名称</li><li>地理位置（物理和政治）</li><li>产品名称</li><li>日期与时间</li><li>金钱数量</li><li>事件名称<br>NER 有大量的用途，因为它可以很容易地从文本中获取结构化数据。这是从 NLP 流水线中快速获取有价值信息的最简单方法之一。<h3 id="步骤8：共指解析"><a href="#步骤8：共指解析" class="headerlink" title="步骤8：共指解析"></a>步骤8：共指解析</h3>到此，我们对句子已经有了一个很好的表述。我们知道每个单词的词性、单词如何相互关联、哪些词在谈论命名实体。<br>然而，我们还有一个大问题。英语里充满了人称代词，比如他、她，还有它。这些是我们使用的快捷表述方法，而不需要在每个句子中一遍又一遍地写名字。人类可以根据上下文来记录这些词所代表的内容。但是我们的 NLP 模型不知道人称代词是什么意思，因为它一次只检查一个句子。<blockquote><p>「It was founded by the Romans, who named it Londinium.」</p></blockquote></li></ul><p>如果我们用 NLP 流水线来解析这个句子，我们就会知道「it」是由罗马人建立的。但知道「London」是由罗马人建立的则更为有用。</p><p>人类阅读这个句子时，可以很容易地理解「it」的意思是「London」。共指解析的目的是通过追踪句子中的代词来找出相同的映射。我们想找出所有提到同一个实体的单词。<br>共指解析是 NLP 流水线实现中最困难的步骤之一。这比句子分析更困难。深度学习的最新进展研究出了更精确的新方法，但还不完善。</p>]]></content>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习优化算法总结</title>
      <link href="/2018/06/21/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>/2018/06/21/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>总结深度学习常见的优化算法。</p><a id="more"></a><p>优化算法是深度学习中最重要的组成部分，优化算法的选择直接关系到模型的性能和表现。本文总结常见的深度学习算法的原理。</p><p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。为了更好的理解各个优化算法的思想，定义如下概念：</p><ol><li>当前梯度gt：即当前训练批次，目标函数关于当前参数的梯度</li><li>一阶动量mt：根据历史梯度计算的值</li><li>二阶动量vt：根据历史梯度计算的值</li><li>更新梯度：即当前梯度或一阶动量 除以 二阶动量</li></ol><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>随机梯度下降算法，最常见最经典的优化算法，SGD中没有动量概念，直接用当前时刻的参数梯度进行更新。<br><strong>SGD最大的缺点</strong> 是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p><h2 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h2><p>为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。它在SGD的基础上引入了一阶动量：</p><blockquote><p>mt = a <em> m(t-1) + (1-a) </em> gt</p></blockquote><p>也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 a的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。</p><h2 id="SGD-with-Nesterov-Acceleration-NAG"><a href="#SGD-with-Nesterov-Acceleration-NAG" class="headerlink" title="SGD with Nesterov Acceleration (NAG)"></a>SGD with Nesterov Acceleration (NAG)</h2><p>SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。</p><p>NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向,然后用下一个点的梯度方向，与历史累积动量相结合，计算当前时刻的累积动量。</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p><p>怎么样去度量历史更新频率呢？那就是二阶动量—— <strong>该维度上，迄今为止所有梯度值的平方和</strong></p><blockquote><p>Vt = sum(gt^2)</p></blockquote><p>引入二阶动量后，我们发现学习率会从å 变为 å/sqrt(Vt)， 一般为了避免分母为0，会在分母上加一个小的平滑项。因此sqrt(Vt) 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。</p><p>这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为sqrt(Vt) 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。</p><h2 id="AdaDelta-RMSProp"><a href="#AdaDelta-RMSProp" class="headerlink" title="AdaDelta/RMSProp"></a>AdaDelta/RMSProp</h2><p>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。<br>修改的思路很简单。只需要用前n个时刻的梯度计算因此我们用这一方法来计算二阶累积动量。这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam即上述优化算法的集大成者。SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。</p><h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>在Adam中，并没有引入NAG的“向前看”思想，所以将NAG和Adam结合，即Nadam。</p><h2 id="Adam缺点"><a href="#Adam缺点" class="headerlink" title="Adam缺点"></a>Adam缺点</h2><p>如上文所讲，Adam理论上已经是所有SGD优化方法的集大成者，但Adam却也有自身的缺点。</p><h3 id="Adam可能不收敛"><a href="#Adam可能不收敛" class="headerlink" title="Adam可能不收敛"></a>Adam可能不收敛</h3><p>其中，SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。<br>但AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 Vt 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。<br>对于这一点，有一个修正的trick就是对Adam的二阶动量进行控制，避免上下波动使学习率单调递减。</p><h3 id="Adam可能错过全局最优解"><a href="#Adam可能错过全局最优解" class="headerlink" title="Adam可能错过全局最优解"></a>Adam可能错过全局最优解</h3><p>同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。<br>通过实验发现：Adam的收敛速度比SGD要快，但最终收敛的结果并没有SGD好。他们进一步实验发现，主要是后期Adam的学习率太低，影响了有效的收敛。而通过对Adam的学习率的下界进行控制，发现效果好了很多。<br>于是提出了一个用来改进Adam的方法：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。</p><h2 id="Adam-SGD组合策略"><a href="#Adam-SGD组合策略" class="headerlink" title="Adam + SGD组合策略"></a>Adam + SGD组合策略</h2><p>目前研究的主流观点是：Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。<br>所以，将两者相互结合变成了一个更好的选择：<strong>先用Adam快速下降，再用SGD调优</strong>。但该方法存在两个技术难题：</p><ol><li>什么时候切换优化算法？——如果切换太晚，Adam可能已经跑到自己的盆地里去了，SGD再怎么好也跑不出来了。</li><li>切换算法以后用什么样的学习率？——Adam用的是自适应学习率，依赖的是二阶动量的累积，SGD接着训练的话，用什么样的学习率？<br>对于第一个问题，目前给出的答案是：<strong>当SGD的相应学习率的移动平均值基本不变的时候，每次迭代完都计算一下SGD接班人的相应学习率，如果发现基本稳定了，那就SGD以 \tilde{\lambda}_t^{SGD} 为学习率接班前进。</strong><br>对于第二个问题，目前的答案是：<strong>SGD在Adam下降方向上的正交投影，应该正好等于Adam的下降方向（含步长）</strong></li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之Q_Learning</title>
      <link href="/2018/06/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BQ_Learning/"/>
      <url>/2018/06/12/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BQ_Learning/</url>
      <content type="html"><![CDATA[<p>Q_Learning是最典型，最简单的强化学习算法，但其蕴含的思想却影响了很多其它的RL算法，现将Q_Learning的思路和特点做以总结。</p><a id="more"></a><p>Agent，Environment，State，Reward，Action 是强化学习中最基本的几个概念，Agent在当前的State下执行Action，并从Environment处获得反馈和更新的State，Agent接收reward反馈，更新自己的策略模式并进入到下个State，进而不断循环上述过程打到学习的目的。这就是强化学习最一般的概念。</p><h2 id="Q-Learning概念"><a href="#Q-Learning概念" class="headerlink" title="Q_Learning概念"></a>Q_Learning概念</h2><p>Q-Learning是最直接反应上述过程的学习算法，通过构建Q_table，记录每个State状态下每个Action的Q值（相当于在该状态下采取该动作的奖惩值），并通过环境返回的Reward不断更新Q_table直至算法可以快速准确的在每个State下做出相对正确的Action。Q_Table的更新算法如下：<br><img src="/images/q_learning/q_update.png" alt="q_learning"><br>其中：</p><ul><li>Q(s, a) 表示在s状态下动作a的Q值</li><li>r 表示如果在s状态实行a，环境返回的reward值</li><li>gamma 表示衰减率，用来衡量当前状态的更新和之后状态的关系。gamma的存在让Q表的更新有了”前瞻性”。（如果下个状态s’最大的q值很低，说明新状态s’是一个很差的状态，Q(s, a)对应的q值就会很小，让agent尽可能不在s执行a动作到达差状态s’。</li><li>Max(Q(s’, a’)) 表示在状态s执行动作a后，到达新状态s’，并返回新状态s’时对应所有状态Q值的最大值。</li></ul><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>现在我们已经知道了Q_Learning的运行原理本质上是从Environment的反馈更新Q表，同时也理解了更新方法。下面看一下整个Q_Learning算法的具体流程：</p><ol><li>初始化一个Q表，行对应着所有的state，列对应着所有的action</li><li>初始化state</li><li>根据当前state选择动作action。（贪心选择当前state对应Q值最大的action，同时有一定概率随机选择action）</li><li>将当前state和选择的action输入到环境中，环境返回下一个状态s_ 、本次执行动作的奖励r、以及游戏是否终止t</li><li>找到新状态s_下所有动作q值最大的值，根据更新公式对Q(s, a)进行更新。</li><li>从新状态s_不断重复3至5步，直到游戏结束完成一个epoch。循环数个epoch会发现模型收敛。</li></ol><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里是两个我用Python写的Q_Learning简单例子，主要参考了<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">莫烦Python</a>的教程，为了便于理解，在代码里我写了尽可能多的注释以便将整个Q_Learning的运行原理解释清楚。具体例子请看下面的URL。<br><a href="https://github.com/YHfeather/ML_Engineer/tree/master/Reinforcement%20Learning/Q_learning" target="_blank" rel="noopener">Q_Learning Example</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上就是我整理的Q_Learning的简单教程，Q_Learning和很多强化学习算法的基础，比如Sarsa，DQN等。理解Q Learning在我看来只需要记住这句话： <strong>在状态s执行动作a到达新状态s_并获得奖励r，根据r和新状态最大的q值对目前状态s和动作a的q值Q(s, a)进行更新</strong></p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop之MapReduce</title>
      <link href="/2018/06/11/Hadoop/Hadoop%E4%B9%8BMapReduce/"/>
      <url>/2018/06/11/Hadoop/Hadoop%E4%B9%8BMapReduce/</url>
      <content type="html"><![CDATA[<p>介绍整理MapReduce的结构和使用方法<br><a id="more"></a></p><h1 id="MapReduce计算模型"><a href="#MapReduce计算模型" class="headerlink" title="MapReduce计算模型"></a>MapReduce计算模型</h1><p>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个JobTracker，一个TaskTracker。前者是用于调度工作的，一个Hadoop集群只有一个JobTracker。后者是用于执行工作的。</p><h2 id="MapReduce-Job"><a href="#MapReduce-Job" class="headerlink" title="MapReduce Job"></a>MapReduce Job</h2><p>每个MapReduce任务会被初始化为一个Job，每个Job分为两个阶段：map阶段和reduce阶段。</p><h4 id="InputFormat-和InputSplit"><a href="#InputFormat-和InputSplit" class="headerlink" title="InputFormat()和InputSplit"></a>InputFormat()和InputSplit</h4><p>InputSplit是Hadoop定义的用来传送给每个单独map数据，InputSplit存储的并非数据本身，而还是一个分片长度和一个记录数据位置的数组。当数据传送给map时，map会将输入分片传送到InputFormat上，InputFormat会创建可供map处理的key-value对。也就是说， <strong>InputFormat()方法是用来生成可供map处理的key-value对的</strong>。<br>Hadoop预定义了多种将不同类型的输入数据转化为key-value对的方法， 他们都继承自InputFormat。<br>其中FileInputFormat又有多个子类。<br>在key-value对中，<strong>key值时每个数据记录在数据分片中的字节偏移量，数据类型是LongWritable，value值时每行的内容，数据类型是Text</strong></p><h4 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h4><p>每个输入格式都有一种输出格式与其对应，outputFormat会将每条记录以一行的形式存入文本文件中，它的key-value对可以试任意的。</p><h4 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h4><p>Map函数接收InputFormat处理所产生的key-value对，经过map函数的处理后输出key-value对，Hadoop会将map输出的中间结果存储在磁盘上而不是HDFS上。Reduce会读取map的输出数据，这时的key-value数据中key相同的value已经被合并为一个list，reduce函数会对输入的数据进行处理并将结果存储在HDFS上。<br>自定义Map类时需要继承自Mapper类。自定义Reduce类继承自Reducer类。并分别在类中重写map()和reduce()函数。以下为WordCount的map和reduce实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountMapper</span><br><span class="line">        extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">        throws IOException, InterruptedException&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] words = line.split(&quot; &quot;);</span><br><span class="line">        for(String word: words)&#123;</span><br><span class="line">            context.write(new Text(word), new IntWritable(1));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Reduce类实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iteterable&lt;IntWritable&gt; values, Context context)</span><br><span class="line">            throws IOException, InterruptedException&#123;</span><br><span class="line">        Integer count = 0;</span><br><span class="line">        for(IntWritable value:values)&#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, new IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意：</p><ol><li>reduce task的数量可以通过程序制定，当有多个reduce task时，每个task都会生成一个输出文件</li><li>没有reduce任务的时候，系统会直接将map task的输出作为最终输出，有多少个map task文件就有多少个输出文件。</li></ol><h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><ol><li>编写代码</li><li>编译成.class文件</li><li>将所有class文件打包成jar文件</li><li>上传到Hadoop</li><li>运行jar文件</li></ol><p>Hadoop运行MapReduce任务时，JobTracker调度任务给TaskTracker，TaskTracker执行任务时会返回进度报告，JobTracker会记录进度的进行状况，如果某个TaskTracker上的任务失败，JobTracker会把这个任务分配给另一台TaskTracker。</p><h2 id="MapReduce优化"><a href="#MapReduce优化" class="headerlink" title="MapReduce优化"></a>MapReduce优化</h2><p>MapReduce模型的优化设计到很多方面，主要为：1. 计算性能优化；2. IO操作优化</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>MapReduce擅长处理少量的大数据，不擅长处理大量的小数据。所以可以在使用MapReduce前对数据进行预处理合并。当一个map任务只需要几秒就可以运行结束时，应该给他分配更多任务。一般而言，一个map任务的运行时间在一分钟比较好。</p><h3 id="map和reduce的数量"><a href="#map和reduce的数量" class="headerlink" title="map和reduce的数量"></a>map和reduce的数量</h3><p>定义两个概念：map任务槽和reduce任务槽，表示这个Hadoop集群能够同时运行的map/reduce任务的数量。</p><h3 id="combine函数"><a href="#combine函数" class="headerlink" title="combine函数"></a>combine函数</h3><p>combine函数用于在本地合并数据，也就是说，在map处理数据后，先将map的输出进行一个预合并，然后再将合并的结果传送给reduce，这样会大大减少网络IO的消耗。例如：在wordcount中，每个map可能都会生成大量的(the, 1)输出，如果将这些输出一个个的都传递个reduce，会浪费很多IO，可以在combine函数中，先将map的输出进行合并（the，10）。</p><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>可以对map的输出和最终的输出进行压缩。通常情况下map的输出会很多大，对其进行压缩可以有效的减少数据在网络的传输量。</p><h3 id="自定义comparator"><a href="#自定义comparator" class="headerlink" title="自定义comparator"></a>自定义comparator</h3><p>可以通过自定义comparator实现数据的二进制比较，这样可以省去数据序列化和反序列化的时间，提高运行效率。</p><h1 id="Hadoop中的IO"><a href="#Hadoop中的IO" class="headerlink" title="Hadoop中的IO"></a>Hadoop中的IO</h1><h2 id="IO操作中的数据检查"><a href="#IO操作中的数据检查" class="headerlink" title="IO操作中的数据检查"></a>IO操作中的数据检查</h2><p>Hadoop是由上千台主机集成的，这么多主机同时运行时，难免会有主机损坏，所以对于Hadoop来说，进行数据完整性检查是非常重要的。</p><h3 id="校验和方式"><a href="#校验和方式" class="headerlink" title="校验和方式"></a>校验和方式</h3><p>通过对比新旧校验和来确定数据是否损坏。循环冗余校验被广泛应用在Hadoop网络IO检查，数据完整性检查等方面。</p><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><p>对于HDFS来说，文件压缩是必须的，它会带来两个好处：1.减少文件所需的存储空间；2.加快文件在网络上的传输速度。<br>Hadoop文件压缩模块都在’package org.apache.hadoop.io.compress’中。<br>Hadoop支持多种压缩格式和压缩算法。</p><h3 id="MapReduce中使用压缩"><a href="#MapReduce中使用压缩" class="headerlink" title="MapReduce中使用压缩"></a>MapReduce中使用压缩</h3><p>只需要在它的Job配置时配置好conf就可以了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 设置map处理后数据的压缩代码</span><br><span class="line">JobConf conf = new Jobconf();</span><br><span class="line">conf.setBoolean(&quot;mapred.compress.map.output&quot;, true);</span><br><span class="line"></span><br><span class="line">// 设置output输出压缩代码</span><br><span class="line">JobConf conf = new JobConf();</span><br><span class="line">conf.setBoolean(&quot;mapred.output.compress&quot;, true);</span><br><span class="line">conf.setClass(&quot;mapred.output.compression.codec&quot;, GzipCodec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python线程和进程</title>
      <link href="/2018/05/26/Programming%20Language/Python/Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/"/>
      <url>/2018/05/26/Programming%20Language/Python/Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>Python多线程和多进程教程。</p><a id="more"></a><h1 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h1><p>Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。</p><p>子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。</p><h2 id="os-模块创建子进程"><a href="#os-模块创建子进程" class="headerlink" title="os 模块创建子进程"></a>os 模块创建子进程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">print(os.getpid())</span><br><span class="line">pid = os.fork()</span><br><span class="line">if pid == 0:</span><br><span class="line">    print(&apos;this is child &#123;&#125;&apos;.format(os.getpid()))</span><br><span class="line">else:</span><br><span class="line">    print(&apos;this is father &#123;&#125;&apos;.format(os.getpid()))</span><br></pre></td></tr></table></figure><p>由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的<br>有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。</p><h2 id="multiprocessing模块"><a href="#multiprocessing模块" class="headerlink" title="multiprocessing模块"></a>multiprocessing模块</h2><h3 id="Process类"><a href="#Process类" class="headerlink" title="Process类"></a>Process类</h3><p>multiprocessing模块提供了跨平台多进程支持。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">def run_proc(name):</span><br><span class="line">    print(&quot;run a processing %s (%s)&quot; % (name, os.getpid()))</span><br><span class="line"></span><br><span class="line">p = multiprocessing.Process(target=run_proc, args=(&apos;test&apos;,))</span><br><span class="line">print(&apos;child porcess will start&apos;)</span><br><span class="line">p.start()</span><br><span class="line">p.join()</span><br><span class="line">print(&apos;Child process end&apos;)</span><br></pre></td></tr></table></figure></p><p>join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。</p><h3 id="Pool类"><a href="#Pool类" class="headerlink" title="Pool类"></a>Pool类</h3><p>如果要启动大量的子进程，可以用进程池的方式批量创建子进程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import os, time, random</span><br><span class="line"></span><br><span class="line">def long_time_task(name):</span><br><span class="line">    print(&quot;Run task %s (%s)..&quot;%(name, os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * 3)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(&quot;task %s runs %.2f seconds.&quot;%(name, end-start))</span><br><span class="line"></span><br><span class="line">pool = multiprocessing.Pool(4)</span><br><span class="line">for i in range(5):</span><br><span class="line">    pool.apply_async(long_time_task, args=(i,))</span><br><span class="line">print(&quot;waiting for all subprocesses done...&quot;)</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br><span class="line">print(&apos;all subprocess done&apos;)</span><br></pre></td></tr></table></figure></p><p>对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。</p><p>请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。<br>由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。</p><h2 id="subprocess子进程"><a href="#subprocess子进程" class="headerlink" title="subprocess子进程"></a>subprocess子进程</h2><p>很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。<br>subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。<br>下面的例子演示了如何在Python代码中运行命令nslookup <a href="http://www.python.org，这和命令行直接运行的效果是一样的：" target="_blank" rel="noopener">www.python.org，这和命令行直接运行的效果是一样的：</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import subprocess</span><br><span class="line">print(&apos;$ nslookup www.python.org&apos;)</span><br><span class="line">r = subprocess.call([&apos;nslookup&apos;, &apos;www.python.org&apos;])</span><br><span class="line">print(&apos;Exit code:&apos;, r)</span><br></pre></td></tr></table></figure></p><p>如果子进程还需要输入，则可以通过communicate()方法输入</p><h2 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h2><p>Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。</p><p>我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from multiprocessing import Process, Queue</span><br><span class="line">import os, time, random</span><br><span class="line"></span><br><span class="line"># 写数据进程执行的代码</span><br><span class="line">def write(q):</span><br><span class="line">    print(&apos;Process to write: %s &apos; % os.getpid())</span><br><span class="line">    for value in [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]:</span><br><span class="line">        print(&apos;Put %s to queue..&apos;%value)</span><br><span class="line"></span><br><span class="line"># 读数据进程执行的代码</span><br><span class="line">def read(q):</span><br><span class="line">    print(&apos;Process to read: %s&apos;%os.getpid())</span><br><span class="line">    while True:</span><br><span class="line">        value = q.get(True)</span><br><span class="line">        print(&apos;Get %s from queue&apos; % value)</span><br><span class="line"></span><br><span class="line">q = Queue()</span><br><span class="line">pw = Process(target=write, args=(q,))</span><br><span class="line">pr = Process(target=read, args=(q, ))</span><br><span class="line">pw.start()</span><br><span class="line">pr.start()</span><br><span class="line">pr.join()</span><br><span class="line">pr.terminate() # pr是死循环，只能强行终止</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Unix/Linux下，可以使用fork()调用实现多进程。</p><p>要实现跨平台的多进程，可以使用multiprocessing模块。</p><p>进程间通信是通过Queue、Pipes等实现的。</p><h1 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h1><p>Python3通过引入threading包进行多线程操作<br>Python使用线程很简单，通过把一个函数传入并创建Thread实例，然后调用start()开始执行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line">def loop():</span><br><span class="line">    print(&quot;current thread&quot;, threading.current_thread().name)</span><br><span class="line">    for i in range(5):</span><br><span class="line">        print(i)</span><br><span class="line">thread1 = threading.Thread(target=loop, name=&apos;thread1&apos;)</span><br><span class="line">thread2 = threading.Thread(target=loop, name=&apos;thread2&apos;)</span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line">print(threading.current_thread())</span><br></pre></td></tr></table></figure></p><h2 id="Thread类"><a href="#Thread类" class="headerlink" title="Thread类"></a>Thread类</h2><p>通过新建Thread类实例，创建一个新的线程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Thread(target=func, args=(), name=name)</span><br></pre></td></tr></table></figure></p><p>其中，参数target表示该thread要运行的函数，name为声明的名称。args表示调用函数需要的参数</p><h2 id="threading-current-thread"><a href="#threading-current-thread" class="headerlink" title="threading.current_thread()"></a>threading.current_thread()</h2><p>返回当前线程，调用.name()可以直接返回该线程的名字。</p><h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><p>多个线程如何同时抢占一个共有资源时，容易出现资源混乱的情况。为了防止这种事情，可以给对修改共有资源的函数上锁。只有持有这个锁的线程，才可以修改这个资源，该线程退出这个函数时，锁释放。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">balance = 0</span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line">def run_thread(n):</span><br><span class="line">    for i in range(100000):</span><br><span class="line">        # 先要获取锁:</span><br><span class="line">        lock.acquire()</span><br><span class="line">        try:</span><br><span class="line">            # 放心地改吧:</span><br><span class="line">            change_it(n)</span><br><span class="line">        finally:</span><br><span class="line">            # 改完了一定要释放锁:</span><br><span class="line">            lock.release()</span><br></pre></td></tr></table></figure></p><p>当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p><p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try…finally来确保锁一定会被释放。</p><p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</p><h2 id="Python对多核CPU的设计缺陷"><a href="#Python对多核CPU的设计缺陷" class="headerlink" title="Python对多核CPU的设计缺陷"></a>Python对多核CPU的设计缺陷</h2><p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p><p>如果写一个死循环的话，会出现什么情况呢？</p><p>打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。</p><p>我们可以监控到一个死循环线程会100%占用一个CPU。</p><p>如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。</p><p>要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p><p>试试用Python写个死循环：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import threading, multiprocessing</span><br><span class="line"></span><br><span class="line">def loop():</span><br><span class="line">    x = 0</span><br><span class="line">    while True:</span><br><span class="line">        x = x ^ 1</span><br><span class="line"></span><br><span class="line">for i in range(multiprocessing.cpu_count()):</span><br><span class="line">    t = threading.Thread(target=loop)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure></p><p>启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</p><p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p><p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p><p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p><p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p><p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p><h2 id="ThreadLocal线程内参数传递"><a href="#ThreadLocal线程内参数传递" class="headerlink" title="ThreadLocal线程内参数传递"></a>ThreadLocal线程内参数传递</h2><p>在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。</p><p>但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦。ThreadLocal可以解决这个问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line"></span><br><span class="line"># 创建全局ThreadLocal对象:</span><br><span class="line">local_school = threading.local()</span><br><span class="line"></span><br><span class="line">def process_student():</span><br><span class="line">    # 获取当前线程关联的student:</span><br><span class="line">    std = local_school.student</span><br><span class="line">    print(&apos;Hello, %s (in %s)&apos; % (std, threading.current_thread().name))</span><br><span class="line"></span><br><span class="line">def process_thread(name):</span><br><span class="line">    # 绑定ThreadLocal的student:</span><br><span class="line">    local_school.student = name</span><br><span class="line">    process_student()</span><br><span class="line"></span><br><span class="line">t1 = threading.Thread(target= process_thread, args=(&apos;Alice&apos;,), name=&apos;Thread-A&apos;)</span><br><span class="line">t2 = threading.Thread(target= process_thread, args=(&apos;Bob&apos;,), name=&apos;Thread-B&apos;)</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line"></span><br><span class="line">执行结果：</span><br><span class="line">Hello, Alice (in Thread-A)</span><br><span class="line">Hello, Bob (in Thread-B)</span><br></pre></td></tr></table></figure></p><p>全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。<br>可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。<br>ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。<br>一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。</p><p>Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python补充知识点</title>
      <link href="/2018/05/26/Programming%20Language/Python/Python%E8%A1%A5%E5%85%85%E7%9A%84%E7%82%B9/"/>
      <url>/2018/05/26/Programming%20Language/Python/Python%E8%A1%A5%E5%85%85%E7%9A%84%E7%82%B9/</url>
      <content type="html"><![CDATA[<p>总结python使用过程中比较有用但却不常用的技巧。</p><a id="more"></a><h1 id="Python基础"><a href="#Python基础" class="headerlink" title="Python基础"></a>Python基础</h1><h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><ol start="4"><li>tuple(); list();</li><li>chr() 将一个int转换为字符（ASCII）</li><li>unichr() 将一个int转为字符（Unicode）</li><li>ord() 将一个字符转换成对应的ASCII值</li><li>hex() 将一个整数转换成十六进制字符串</li><li>oct() 将一个整数转换成八进制字符串<h3 id="round-x-n-对变量进行四舍五入"><a href="#round-x-n-对变量进行四舍五入" class="headerlink" title="round(x, n)对变量进行四舍五入"></a>round(x, n)对变量进行四舍五入</h3>在python3中，可以使用round()函数进行四舍五入输出：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(round(3.4)) #3</span><br><span class="line">print(round(3.5)) #4</span><br><span class="line">print(round(1.95555, 2)) #1.86 保留小数点后2位</span><br><span class="line">print(round(1241757, -3)) #1242000 舍去小数点前3位</span><br></pre></td></tr></table></figure></li></ol><h3 id="print函数"><a href="#print函数" class="headerlink" title="print函数"></a>print函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(*value, sep=&apos; &apos;, end=&apos;\n&apos;, file=sys.stdout, flush=False)</span><br></pre></td></tr></table></figure><ol><li>value 表示要打印的值，可以为任意个参数</li><li>sep 当要打印多个value时，各个value之间的分隔方式。默认值为空格</li><li>end value打印结束后输出的符号，默认为回车换行，如果不希望打印后换行，可以将其修改为空字符串。</li><li>file 设置输出设备，默认为输出到终端，可以这是file=文件存储对象，将内容打印到文件中。</li><li>flush 刷新，默认为False，不刷新<h3 id="locals-函数"><a href="#locals-函数" class="headerlink" title="locals()函数"></a>locals()函数</h3>locals()内置函数会以字典类型返回当前位置的全部局部变量。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def func(arg):</span><br><span class="line">    z = 1</span><br><span class="line">    print(locals())</span><br><span class="line">&gt;&gt;&gt; func(4)</span><br><span class="line">&#123;&apos;z&apos;: 1, &apos;arg&apos;: 4&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="map-函数"><a href="#map-函数" class="headerlink" title="map() 函数"></a>map() 函数</h3><p>map(function, iterable)，对输入序列中的每个元素调用func。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">temp_list = [1,2,3,4]</span><br><span class="line">def func(x):</span><br><span class="line">    return x * 2</span><br><span class="line"></span><br><span class="line">list1 = list(map(func, temp_list))</span><br><span class="line">list2 = list(map(lambda x:x*2, temp_list))</span><br></pre></td></tr></table></figure></p><h3 id="reduce-函数"><a href="#reduce-函数" class="headerlink" title="reduce()函数"></a>reduce()函数</h3><p>reduce(function, iterable)，函数声明式和map一样，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)</span><br></pre></td></tr></table></figure></p><p>reduce由于不常用，已经被冲内置函数中删除，放到了functools中。如果要把序列[1, 3, 5, 7, 9]变换成整数13579，reduce就可以派上用场：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from functools import reduce</span><br><span class="line">&gt;&gt;&gt; def fn(x, y):</span><br><span class="line">...     return x * 10 + y</span><br><span class="line">&gt;&gt;&gt; reduce(fn, [1, 3, 5, 7, 9])</span><br><span class="line">13579</span><br></pre></td></tr></table></figure></p><h3 id="filter-函数"><a href="#filter-函数" class="headerlink" title="filter()函数"></a>filter()函数</h3><p>filter(function, iterable) 函数用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换。</p><p>该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def is_odd(n):</span><br><span class="line">    return n%2 == 1</span><br><span class="line">temp_list = [1,2,3,4]</span><br><span class="line">list1 = list(filter(is_odd, temp_list))</span><br><span class="line">list2 = list(filter(lambda x:x%2==1, temp_list))</span><br></pre></td></tr></table></figure><h3 id="sorted-函数"><a href="#sorted-函数" class="headerlink" title="sorted()函数"></a>sorted()函数</h3><p>sorted(iterable, key=None, reverse=False) 函数对所有可迭代的对象进行排序操作。<br>参数key可以为一个lambda表达式，进而指定排序规则<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp_list = [(7,2), (4,3), (6,1), (9,1)]</span><br><span class="line">list1 = sorted(temp_list, key=lambda x:x[1], reverse=True)</span><br></pre></td></tr></table></figure></p><h4 id="sort-与-sorted-区别："><a href="#sort-与-sorted-区别：" class="headerlink" title="sort 与 sorted 区别："></a>sort 与 sorted 区别：</h4><p>sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。<br>list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。</p><h2 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;]</span><br><span class="line">[&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;]</span><br></pre></td></tr></table></figure><h3 id="插入元素"><a href="#插入元素" class="headerlink" title="插入元素"></a>插入元素</h3><p>list.insert(index, obj)  向指定位置插入一个对象</p><h3 id="删除元素"><a href="#删除元素" class="headerlink" title="删除元素"></a>删除元素</h3><ul><li>del myList[index] 根据index删除</li><li>value = myList.pop(index) 效果和del一样</li><li>myList.remove(value) 根据值进行删除，只删除第一个指定值，可以用一个while循环来删除所有指定value<h3 id="list排序"><a href="#list排序" class="headerlink" title="list排序"></a>list排序</h3></li><li>myList.reverse()</li><li>myList.sort(reverse=True) 在list内部排序，也就意味着不保留原始list<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><h4 id="对字典进行循环"><a href="#对字典进行循环" class="headerlink" title="对字典进行循环"></a>对字典进行循环</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = [1:&apos;a&apos;, 2:&apos;b&apos;, 3:&apos;c&apos;]</span><br><span class="line">for i in a: # 该遍历将自动遍历dict的key</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">for k, v in a.items(): # 遍历每个key和value</span><br><span class="line">    print(k, v)</span><br></pre></td></tr></table></figure></li></ul><h4 id="删除字典中指定元素"><a href="#删除字典中指定元素" class="headerlink" title="删除字典中指定元素"></a>删除字典中指定元素</h4><p>使用del即可</p><h4 id="字典表达式"><a href="#字典表达式" class="headerlink" title="字典表达式"></a>字典表达式</h4><p>也可以用表达式方法构建字典<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [0, 1, 2, 3, 4]</span><br><span class="line">even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;</span><br><span class="line">print(even_num_to_square)  # Prints &quot;&#123;0: 0, 2: 4, 4: 16&#125;&quot;</span><br></pre></td></tr></table></figure></p><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>从集合中删除一个指定元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">animals = &#123;&apos;cat&apos;, &apos;dog&apos;&#125;</span><br><span class="line">animals.add(&apos;fish&apos;)       # Add an element to a set</span><br><span class="line">animals.remove(&apos;cat&apos;)     # Remove an element from a set</span><br></pre></td></tr></table></figure></p><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><ol><li>string.title() 大写每个单词首字母</li><li>string.upper()</li><li>string.lower()</li><li>string.rstrip(); string.lstrip(); string.strip() 删除左右空格</li><li>string.split(); 按照空格将string分割成list<h3 id="转义符"><a href="#转义符" class="headerlink" title="转义符"></a>转义符</h3>Python 使用反斜杠()转义特殊字符，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r，表示原始字符串：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(r&apos;Ru\noob&apos;)</span><br><span class="line">Ru\noob</span><br></pre></td></tr></table></figure></li></ol><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="形参和实参关系"><a href="#形参和实参关系" class="headerlink" title="形参和实参关系"></a>形参和实参关系</h3><p>在python中，如果直接将一个list作为实参传到函数中，而该函数对传入的list直接修改的话，会修改原函数中list的值。<br>如果想在被调用函数中修改传入list的值，并且不改变原函数中的list，可以传入原函数中list的一个‘副本’：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func(alist[:])</span><br></pre></td></tr></table></figure></p><p>但需要注意的是，该方法只对基本类型数据有用（int，float等），如果alist中的元素是复杂对象，例如 <strong>类类型，list, map等</strong> 等，即是以这种方式传入被调用函数，其实仍然没有产生副本，在被调用函数中仍然修改的是原函数中list中的元素。这就是python中的浅拷贝。<br>如果需要完全达到对于任意类型数据，都不修改原函数中list的值的话， 需要import copy中的deepcopy()</p><h3 id="传递任意数量实参"><a href="#传递任意数量实参" class="headerlink" title="传递任意数量实参"></a>传递任意数量实参</h3><p>有时候，你预先不知道函数需要接受多少个实参，Python允许函数从调用语句中收集任意数量的实参。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def func(*args):</span><br><span class="line">    for arg in args:</span><br><span class="line">        print(arg)</span><br><span class="line"></span><br><span class="line">func(&apos;a&apos;)</span><br><span class="line">func(&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;)</span><br></pre></td></tr></table></figure></p><p>以上两个函数调用都是合法的，在函数定义时可以声明参数为*，表示任意形参。</p><h3 id="关键字实参"><a href="#关键字实参" class="headerlink" title="关键字实参"></a>关键字实参</h3><p>有时候，需要接受任意数量的实参，但预先不知道传递给函数的会是什么样的信息。在这种情况下，可将函数编写成能够接受任意数量的键—值对——调用语句提供了多少就接受多少。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def func(**key_args):</span><br><span class="line">    for key, value in key_args.items():</span><br><span class="line">        print(key, &apos; &apos;, value)</span><br><span class="line"></span><br><span class="line">func(key1=&apos;value1&apos;, key2=&apos;value2&apos;)</span><br></pre></td></tr></table></figure></p><p>在调用拥有关键字形参的函数时，可以声明任意任意的参数名，并对该参数进行赋值。这些都将保存在关键字参数**key_args中，key_args本质上是一个字典。保存调用该函数时传入的键值对。</p><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><p>在Python中，所有以“__”双下划线包起来的方法，都统称为“Magic Method”</p><h2 id="内部方法"><a href="#内部方法" class="headerlink" title="内部方法"></a>内部方法</h2><h3 id="new-方法"><a href="#new-方法" class="headerlink" title="new 方法"></a><strong>new</strong> 方法</h3><p>当我们创建一个对象时，最开始调用的并不是 <strong>init</strong> 而是 <strong>new</strong> 方法，该方法会创建类并返回这个类的实例。</p><h3 id="init-self"><a href="#init-self" class="headerlink" title="init(self,)"></a><strong>init</strong>(self,)</h3><p>在new方法被调用并创建一个实例后，init函数会将参数传入到实例中，初始化该实例。</p><h3 id="del-self"><a href="#del-self" class="headerlink" title="del(self)"></a><strong>del</strong>(self)</h3><p>当对象生命周期结束时，del方法会被调用</p><h3 id="call-self"><a href="#call-self" class="headerlink" title="call(self)"></a><strong>call</strong>(self)</h3><p>Python中，如果在创建class的时候写了call()方法， 那么该class实例化出实例后， 实例名()就是调用call()方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Animal(object):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        self.name = name</span><br><span class="line">    def __call__(self):</span><br><span class="line">        print(&apos;my name is: &#123;&#125;&apos;.format(self.name))</span><br><span class="line"></span><br><span class="line">cat = Animal(&apos;cat&apos;)</span><br><span class="line">cat()</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;my name is: cat</span><br></pre></td></tr></table></figure></p><h3 id="copy-self"><a href="#copy-self" class="headerlink" title="copy(self)"></a><strong>copy</strong>(self)</h3><p>定义了当对你的类的实例调用copy.copy()时所产生的行为。copy.copy()返回了你的对象的一个浅拷贝——这意味着，当实例本身是一个新实例时，它的所有数据都被引用了——例如，当一个对象本身被复制了，它的数据仍然是被引用的（因此，对于浅拷贝中数据的更改仍然可能导致数据在原始对象的中的改变）。</p><h3 id="deepcopy-self-memodict"><a href="#deepcopy-self-memodict" class="headerlink" title="deepcopy(self, memodict={})"></a><strong>deepcopy</strong>(self, memodict={})</h3><p>定义了当对你的类的实例调用copy.deepcopy()时所产生的行为。copy.deepcopy()返回了你的对象的一个深拷贝——对象和其数据都被拷贝了。memodict是对之前被拷贝的对象的一个缓存——这优化了拷贝过程并且阻止了对递归数据结构拷贝时的无限递归。当你想要进行对一个单独的属性进行深拷贝时，调用copy.deepcopy()，并以memodict为第一个参数。</p><h3 id="slots-限定属性添加"><a href="#slots-限定属性添加" class="headerlink" title="slots 限定属性添加"></a><strong>slots</strong> 限定属性添加</h3><p>由于python支持对对象动态添加成员，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">student1 = Student()</span><br><span class="line">student1.age = 11 # Student中原来并没有age成员</span><br></pre></td></tr></table></figure></p><p>为了防止属性添加混乱，使用 <strong>slots</strong> 限制属性添加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class Student():</span><br><span class="line">    __slots__ = (&apos;name&apos;, &apos;age&apos;)</span><br></pre></td></tr></table></figure></p><p>上述代码限制该Student类只可以添加这两个变量</p><h3 id="使用-property对成员进行get和set"><a href="#使用-property对成员进行get和set" class="headerlink" title="使用@property对成员进行get和set"></a>使用@property对成员进行get和set</h3><p>通常情况下， 对于成员进行get和set时都需要单独编写getter和setter函数，便于进行检查，但这样会导致代码繁杂。python支持使用内置装饰器@property把一个属性变为可调用的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Student(object):</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def score(self):</span><br><span class="line">        return self._score</span><br><span class="line"></span><br><span class="line">    @score.setter</span><br><span class="line">    def score(self, value):</span><br><span class="line">        if not isinstance(value, int):</span><br><span class="line">            raise ValueError(&apos;score must be an integer!&apos;)</span><br><span class="line">        if value &lt; 0 or value &gt; 100:</span><br><span class="line">            raise ValueError(&apos;score must between 0 ~ 100!&apos;)</span><br><span class="line">        self._score = value</span><br></pre></td></tr></table></figure></p><p>把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值。</p><p>通过仅使用@property作为get方法而不定义setter方法，可以实现只读属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Student(object):</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def birth(self):</span><br><span class="line">        return self._birth</span><br><span class="line"></span><br><span class="line">    @birth.setter</span><br><span class="line">    def birth(self, value):</span><br><span class="line">        self._birth = value</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def age(self):</span><br><span class="line">        return 2015 - self._birth</span><br><span class="line"></span><br><span class="line">a = Student()</span><br><span class="line">a.age = 1111</span><br></pre></td></tr></table></figure></p><p>上一段代码会报错</p><h2 id="外部方法"><a href="#外部方法" class="headerlink" title="外部方法"></a>外部方法</h2><h3 id="getattr-obj-variable-name"><a href="#getattr-obj-variable-name" class="headerlink" title="getattr(obj, variable_name)"></a>getattr(obj, variable_name)</h3><p>获取对象中指定成员变量的值</p><h3 id="setattr-obj-variable-name-value"><a href="#setattr-obj-variable-name-value" class="headerlink" title="setattr(obj, variable_name, value)"></a>setattr(obj, variable_name, value)</h3><p>向一个对象中添加一个成员变量，并为其赋值</p><h3 id="hasattr-obj-variable-name"><a href="#hasattr-obj-variable-name" class="headerlink" title="hasattr(obj, variable_name)"></a>hasattr(obj, variable_name)</h3><p>返回boolean值，表示该对象中是否有指定的成员变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class A():</span><br><span class="line">    def __init__(self, a, b):</span><br><span class="line">        self.__aa = a</span><br><span class="line">        self.__bb = b</span><br><span class="line">        self.aaa = 1</span><br><span class="line">obj = A(1,2)</span><br><span class="line">hasattr(obj, &apos;aaa&apos;) #True</span><br><span class="line">getattr(obj, &apos;aaa&apos;) #1</span><br><span class="line">setattr(obj, &apos;x&apos;, 1)</span><br></pre></td></tr></table></figure></p><h3 id="使用dir"><a href="#使用dir" class="headerlink" title="使用dir()"></a>使用dir()</h3><p>获得对象的属性和方法<br>如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class A():</span><br><span class="line">    def __init__(self, a, b):</span><br><span class="line">        self.__aa = a</span><br><span class="line">        self.__bb = b</span><br><span class="line">        self.aaa = 1</span><br><span class="line">obj = A(1,2)</span><br><span class="line">print(dir(A))</span><br></pre></td></tr></table></figure></p><h2 id="类属性"><a href="#类属性" class="headerlink" title="类属性"></a>类属性</h2><h3 id="private-属性"><a href="#private-属性" class="headerlink" title="private 属性"></a>private 属性</h3><p>如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线<strong>，在Python中，实例的变量名如果以</strong>开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class A():</span><br><span class="line">    def __init__(self, a, b):</span><br><span class="line">        self.__aa = a</span><br><span class="line">        self.__bb = b</span><br><span class="line"></span><br><span class="line">temp = A(1,2)</span><br><span class="line">print(temp.aa)</span><br></pre></td></tr></table></figure></p><p>上述代码运行时会报错。</p><p>有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。</p><p>双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问<strong>name是因为Python解释器对外把</strong>name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量</p><h3 id="类属性，静态成员变量"><a href="#类属性，静态成员变量" class="headerlink" title="类属性，静态成员变量"></a>类属性，静态成员变量</h3><p>直接在class中定义的属性被称为类属性，归整个类所有，类似于static。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Student():</span><br><span class="line">    name = &apos;Student&apos;</span><br><span class="line">    def __init__():</span><br><span class="line">        pass</span><br><span class="line">print(Student.name)</span><br></pre></td></tr></table></figure></p><h2 id="多重继承"><a href="#多重继承" class="headerlink" title="多重继承"></a>多重继承</h2><p>通过多重继承，一个子类就可以同时获得多个父类的所有功能。</p><h3 id="MixIn"><a href="#MixIn" class="headerlink" title="MixIn"></a>MixIn</h3><p>在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。</p><p>为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。</p><p>MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。</p><h1 id="标准库"><a href="#标准库" class="headerlink" title="标准库"></a>标准库</h1><h2 id="math"><a href="#math" class="headerlink" title="math"></a>math</h2><ol><li>log(x, y); log10();  pow(); sqrt()</li><li>math.ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回5</li><li>math.exp(x) 返回e的x次幂</li><li>math.floor(x) 返回数字的下舍整数，如math.floor(4.9) 返回4<h3 id="内置常量"><a href="#内置常量" class="headerlink" title="内置常量"></a>内置常量</h3></li><li>math.pi 圆周率</li><li>math.e 自然常数</li></ol><h2 id="random"><a href="#random" class="headerlink" title="random"></a>random</h2><ol><li>choose(seq) 从序列seq中随机选择一个元素返回</li><li>randrange(start, stop, step) 从指定范围内，按指定基数递增的集合中获取一个随机数</li><li>seed() 改变随机数种子</li><li>shuffle(seq) 将序列所有元素洗牌</li><li>uniform(x, y) 随机生成一个实数，取值范围在[x,y]之间</li></ol><h2 id="collection"><a href="#collection" class="headerlink" title="collection"></a>collection</h2><p>collections是Python内建的一个集合模块，提供了许多有用的集合类。</p><h3 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple"></a>namedtuple</h3><p>namedtuple 是一个函数，它用来创建一个自定义的元组对象，并且规定了元组元素的个数，并可以用属性而不是索引来引用元组的某个元素。可以通过 namedtuple 来定义一种数据类型，它具备元组的不变性，又可以根据属性来引用，十分方便。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from collections import namedtuple</span><br><span class="line">&gt;&gt;&gt; Mytuple = namedtuple(&apos;Mytuple&apos;, [&apos;x&apos;,&apos;y&apos;])</span><br><span class="line">&gt;&gt;&gt; n = Mytuple(11,22)</span><br><span class="line">&gt;&gt;&gt; n.x</span><br><span class="line">11</span><br><span class="line">&gt;&gt;&gt; n.y</span><br><span class="line">22</span><br></pre></td></tr></table></figure></p><h3 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h3><p>使用字典时，如果引用的Key不存在，就会抛出 KeyError，如果希望key不存在时，返回一个默认值，就可以用 defaultdict.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from collections import defaultdict</span><br><span class="line">&gt;&gt;&gt; Mydict = defaultdict(lambda: &apos;N/A&apos;)</span><br><span class="line">&gt;&gt;&gt; Mydict[&apos;key1&apos;] = &apos;abc&apos;</span><br><span class="line">&gt;&gt;&gt; Mydict[&apos;key1&apos;]    # 字典的key1存在</span><br><span class="line">&apos;abc&apos;</span><br><span class="line">&gt;&gt;&gt; Mydict[&apos;key2&apos;]    # 字典的key2不存在，返回默认值为‘N/A’</span><br><span class="line">&apos;N/A&apos;</span><br></pre></td></tr></table></figure></p><h3 id="OrderedDict"><a href="#OrderedDict" class="headerlink" title="OrderedDict"></a>OrderedDict</h3><p>有序字典的应用。OrderedDict 的有序性是按照插入的顺序，而不是KEY的顺序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from collections import OrderedDict</span><br><span class="line">&gt;&gt;&gt; d = dict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])</span><br><span class="line">&gt;&gt;&gt; d # dict的Key是无序的</span><br><span class="line">&#123;&apos;a&apos;: 1, &apos;c&apos;: 3, &apos;b&apos;: 2&#125;</span><br><span class="line">&gt;&gt;&gt; od = OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])</span><br><span class="line">&gt;&gt;&gt; od # OrderedDict的Key是有序的</span><br><span class="line">OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])</span><br></pre></td></tr></table></figure></p><h3 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h3><p>简单的计数器，例如，统计字符出现的个数。</p><h4 id="构建函数"><a href="#构建函数" class="headerlink" title="构建函数"></a>构建函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter()                           # a new, empty counter</span><br><span class="line">&gt;&gt;&gt; c = Counter(&apos;gallahad&apos;)                 # a new counter from an iterable</span><br><span class="line">&gt;&gt;&gt; c = Counter(&#123;&apos;red&apos;: 4, &apos;blue&apos;: 2&#125;)      # a new counter from a mapping</span><br><span class="line">&gt;&gt;&gt; c = Counter(cats=4, dogs=8)             # a new counter from keyword args</span><br></pre></td></tr></table></figure><p>或者也可以new一个空的Counter，然后用for向里面添加元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; cnt = Counter()</span><br><span class="line">&gt;&gt;&gt; for word in [&apos;red&apos;, &apos;blue&apos;, &apos;red&apos;, &apos;green&apos;, &apos;blue&apos;, &apos;blue&apos;]:</span><br><span class="line">...     cnt[word] += 1</span><br><span class="line">&gt;&gt;&gt; cnt</span><br><span class="line">Counter(&#123;&apos;blue&apos;: 3, &apos;red&apos;: 2, &apos;green&apos;: 1&#125;)</span><br></pre></td></tr></table></figure></p><h4 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h4><ol><li><p>counter.elements()  返回所有key元素，每个元素出现次数等于它的value。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2)</span><br><span class="line">&gt;&gt;&gt; sorted(c.elements())</span><br><span class="line">[&apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;b&apos;, &apos;b&apos;]</span><br></pre></td></tr></table></figure></li><li><p>counter.most_common(n) 返回一个list，表示出现次数最多的n个元素，以及他们的出现次数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; Counter(&apos;abracadabra&apos;).most_common(3)  </span><br><span class="line">[(&apos;a&apos;, 5), (&apos;r&apos;, 2), (&apos;b&apos;, 2)]</span><br></pre></td></tr></table></figure></li><li><p>counter.substract([iterable-or-mapping]) 对两个counter或者一个map中的元素个数相减。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2)</span><br><span class="line">&gt;&gt;&gt; d = Counter(a=1, b=2, c=3, d=4)</span><br><span class="line">&gt;&gt;&gt; c.subtract(d)</span><br><span class="line">&gt;&gt;&gt; c</span><br><span class="line">Counter(&#123;&apos;a&apos;: 3, &apos;b&apos;: 0, &apos;c&apos;: -3, &apos;d&apos;: -6&#125;)</span><br></pre></td></tr></table></figure></li></ol><h3 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h3><p>使用list 存储数据时，按照索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from collections import deque</span><br><span class="line">&gt;&gt;&gt; q = deque([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])</span><br><span class="line">&gt;&gt;&gt; q.append(&apos;x&apos;)    # 默认添加列表最后一项</span><br><span class="line">&gt;&gt;&gt; q.appendleft(&apos;y&apos;)  # 添加到列表第一项</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([&apos;y&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;x&apos;])</span><br><span class="line">&gt;&gt;&gt; q.pop()  # 默认删除列表最后一个元素</span><br><span class="line">&apos;x&apos;</span><br><span class="line">&gt;&gt;&gt; q.popleft()  # 删除列表的第一个元素</span><br><span class="line">&apos;y&apos;</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</span><br></pre></td></tr></table></figure></p><h4 id="常用内置函数-1"><a href="#常用内置函数-1" class="headerlink" title="常用内置函数"></a>常用内置函数</h4><ol><li>append(x)<br>Add x to the right side of the deque.</li><li>appendleft(x)<br>Add x to the left side of the deque.</li><li>clear()<br>Remove all elements from the deque leaving it with length 0.</li><li>copy()<br>Create a shallow copy of the deque.</li><li>count(x)<br>Count the number of deque elements equal to x.</li><li>extend(iterable)<br>Extend the right side of the deque by appending elements from the iterable argument.</li><li>extendleft(iterable)<br>Extend the left side of the deque by appending elements from iterable. Note, the series of left appends results in reversing the order of elements in the iterable argument.</li><li>index(x[, start[, stop]])<br>Return the position of x in the deque (at or after index start and before index stop). Returns the first match or raises ValueError if not found.</li><li>insert(i, x)<br>Insert x into the deque at position i.</li><li>pop()<br>Remove and return an element from the right side of the deque. If no elements are present, raises an IndexError.</li><li>popleft()<br>Remove and return an element from the left side of the deque. If no elements are present, raises an IndexError.</li><li>reverse()<br>Reverse the elements of the deque in-place and then return None.</li></ol><h1 id="第三方库"><a href="#第三方库" class="headerlink" title="第三方库"></a>第三方库</h1><h2 id="os模块"><a href="#os模块" class="headerlink" title="os模块"></a>os模块</h2><p>os模块用来提供便利的使用操作系统函数的方法，直观理解就是os负责与操作系统的交互，提供了访问操作系统的接口，尤其是文件系统。</p><h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><ol><li>os.remove(‘path/filename’) 删除文件</li><li>os.mkdir(‘dirname’)创建目录</li><li>os.rmdir(‘dirname’)删除目录</li><li>os.listdir(‘dirname’) 列出指定目录的文件，返回文件名组成的列表</li><li>os.getcwd() 取得当前工作目录</li><li>os.path.join(path1[,path2[,…]]) 将分离的各部分组合成一个路径名</li><li>os.path.getsize() 返回文件大小</li><li>os.path.exists() 判断路径是否存在</li><li>os.path.isdir() 是否为目录</li><li>os.path.isfile() 是否为文件</li></ol><h2 id="sys模块"><a href="#sys模块" class="headerlink" title="sys模块"></a>sys模块</h2><p>sys模块提供访问由解释器使用或者维护的变量，以及与解释器交互的函数。</p><h3 id="常用函数-1"><a href="#常用函数-1" class="headerlink" title="常用函数"></a>常用函数</h3><ol><li>sys.argv 返回由命令行输入参数组成的List，argv[0]是程序本身路径</li><li>sys.exit(n) 退出程序，正常退出时exit(0)</li><li>sys.maxint 最大的Int值</li></ol><h2 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a>tqdm</h2><p>tqdm是一个便捷的，可扩展的python进度条模块，可以在python长循环添加一个进度提示信息，使用方法很简单，只需要封装任意迭代器即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from tqdm import tqdm</span><br><span class="line">for i in tqdm(range(1000))</span><br><span class="line">    sleep(0.01)</span><br></pre></td></tr></table></figure></p><p>运行如上代码，在控制台就会输出一个进度条，表示当前循环的运行进程。tqdm传入的参数可以是任意iterable的对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from tqdm import trange</span><br><span class="line">for i in trange(1000):</span><br><span class="line">    sleep(0.01)</span><br></pre></td></tr></table></figure></p><p>trange是tqdm(range())的简略写法，效果相同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tqdm(total=100) as pbar:</span><br><span class="line">     for i in range(10):</span><br><span class="line">        pbar.update(10)</span><br></pre></td></tr></table></figure></p><p>可以指定tqdm的total，然后调用update方法手动更新进度条的进度。</p><h2 id="warnings"><a href="#warnings" class="headerlink" title="warnings"></a>warnings</h2><p>warnings.filterwarnings(‘ignore’)</p><h1 id="Python-3-6新特性"><a href="#Python-3-6新特性" class="headerlink" title="Python 3.6新特性"></a>Python 3.6新特性</h1><h2 id="f-string"><a href="#f-string" class="headerlink" title="f-string"></a>f-string</h2><p>过往，如果想在字符串输出变量，需要如下操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = &apos;aaa&apos;</span><br><span class="line">print(&apos;my name is &#123;name&#125;&apos;.format(name=name))</span><br><span class="line">print(&apos;my name is &#123;&#125;&apos;.format(name))</span><br></pre></td></tr></table></figure></p><p>在3.6中，引入f-strings特性，只需要在字符串前加上f，就可以将变量直接嵌入到字符串中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">name = &apos;aaa&apos;</span><br><span class="line">print(f&apos;my name is &#123;name&#125;&apos;)</span><br></pre></td></tr></table></figure></p><p>嵌入的内容可以是任何对象，包括变量，列表推导式等等。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 小数精度</span><br><span class="line">&gt;&gt;&gt; PI = 3.141592653</span><br><span class="line">&gt;&gt;&gt; f&quot;Pi is &#123;PI:.2f&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; &apos;Pi is 3.14&apos;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; f&apos;Numbers from 1-10 are &#123;[_ for _ in range(1, 11)]&#125;&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;Numbers from 1-10 are [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&apos;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习补充知识</title>
      <link href="/2018/05/22/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86/"/>
      <url>/2018/05/22/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<p>深度学习知识点复杂繁多，同时各个点之间往往有很深的联系，这篇文章总结了我在学习CNN和RNN过程中遇到的小知识点的补充。</p><a id="more"></a><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="BP算法推导过程"><a href="#BP算法推导过程" class="headerlink" title="BP算法推导过程"></a>BP算法推导过程</h3><h3 id="常见参数初始化方法"><a href="#常见参数初始化方法" class="headerlink" title="常见参数初始化方法"></a>常见参数初始化方法</h3><p>全零初始化方法在前向传播过程中会使得隐层神经元的激活值均未0，在反向过程中根据BP公式，不同维度的参数会得到相同的更新。 需要破坏这种“对称性”。</p><h4 id="标准初始化"><a href="#标准初始化" class="headerlink" title="标准初始化"></a>标准初始化</h4><p>标准初始化方法通过对方差乘以一个系数确保每层神经元的输出具有相同的方差，提高训练收敛速度。<br>标准均匀初始化方法保证了激活函数的输入值的均值为0，方差为常量1/3，和网络的层数和神经元的数量无关。<br>标准正态初始化方法保证激活函数的输入均值为0，方差为1。</p><h4 id="Xavier初始化（glorot初始化）"><a href="#Xavier初始化（glorot初始化）" class="headerlink" title="Xavier初始化（glorot初始化）"></a>Xavier初始化（glorot初始化）</h4><p>为了使得模型更好的训练，Xavier假设神经网络每一层的输出的方差尽可能相等。</p><h4 id="He-初始化"><a href="#He-初始化" class="headerlink" title="He 初始化"></a>He 初始化</h4><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Batch Normalization是将分散的数据统一的一种方法。<br>神经网络的训练效率受数据分布影响，如果输入神经元的数值过大或过小，该输入值会在激活函数的饱和区，也就是说这个时候的梯度几乎为0，导致梯度消失，很难更新权重。<br>Batch Normalization相当于在每层hidden layer之间对数据进行归一化，让数据分布集中在（-1， 1）之间，这样下一层神经元的输入大部分会落到激活函数的工作区，BP时更新梯度较大，模型可以更快收敛。<br><img src="/images/nn_knowledge/batch_normalization.jpg" alt="batch_normalization"></p><p>一般应用Batch Normalization时往往还进行反向Normalization，将Normalized的数据用可训练的参数再扩展和平移回去，目的是让神经网络自己学习扩展平移参数，进而检测此次的Batch Normalization是否起到优化作用。</p><p>(BN首先是把所有的samples的统计分布标准化，降低了batch内不同样本的差异性，然后又允许batch内的各个samples有各自的统计分布)，BN最大的优点为允许网络使用较大的学习速率进行训练，加快网络的训练速度（减少epoch次数），提升效果。</p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>层数比较多的神经网络模型在训练时会出现一些问题，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。</p><h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><h4 id="梯度消失原因"><a href="#梯度消失原因" class="headerlink" title="梯度消失原因"></a>梯度消失原因</h4><p>对于多层神经网络，当梯度消失发生时，接近输出层的hidden layer权重正常更新，但接近输入层的权重更新会非常缓慢，导致前几层的权值几乎不变，这就导致了前几层就相当于一个映射层。这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p><p>对于梯度消失的具体原因，我们可以将反向传播的公式进行展开，这里激活函数我们用Sigmoid举例:<br><img src="/images/deep_learning/sigmoid_bp.png" alt="sigmoid_bp"><br>可以看到，整个反向传播过程是由几个sigmoid函数的导数和权重相乘组成的，而对sigmoid求导发现，sigmoid的导数最大值为0.25，同时我们初始化网络权值时通常也会小于1，也就是说对于上面的链式法则求导，层数越多，求导结果越小，对前几个隐藏层权重的更新也越小。从而导致了梯度消失。</p><h4 id="梯度消失解决办法"><a href="#梯度消失解决办法" class="headerlink" title="梯度消失解决办法"></a>梯度消失解决办法</h4><ul><li>放弃sigmoid函数，使用Relu是最直接的方法</li><li>在网络中加入Batch Normalization</li><li>残差神经网络可以防止梯度消失的发生</li></ul><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><h4 id="梯度爆炸原因"><a href="#梯度爆炸原因" class="headerlink" title="梯度爆炸原因"></a>梯度爆炸原因</h4><p>在深层网络或递归神经网络中，误差梯度在更新中累积得到一个非常大的梯度，这样的梯度会大幅度更新网络参数，进而导致网络不稳定。在极端情况下，权重的值变得特别大，以至于权重溢出（NaN值）。当梯度爆炸发生时，网络层之间反复乘以大于1.0的梯度值使得梯度值成倍增长，网络不稳定，无法收敛。</p><p>根据链式法则我们知道，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大</p><h4 id="梯度爆炸现象"><a href="#梯度爆炸现象" class="headerlink" title="梯度爆炸现象"></a>梯度爆炸现象</h4><ul><li>模型无法在训练数据上收敛（比如，损失函数值非常差）；</li><li>模型不稳定，在更新的时候损失有较大的变化；</li><li>模型的损失函数值在训练过程中变成NaN值；</li><li>模型在训练过程中，权重变化非常大；</li><li>模型在训练过程中，权重变成NaN值；<h4 id="梯度爆炸解决办法"><a href="#梯度爆炸解决办法" class="headerlink" title="梯度爆炸解决办法"></a>梯度爆炸解决办法</h4></li></ul><ol><li>重新设计网络。通过减少模型隐藏层数或者训练时使用较小的batch。</li><li>使用Relu。</li><li>对于RNN使用LSTM。虽然LSTM无法完全消除梯度爆炸，但有效的减小了发生的概率</li><li>使用梯度裁剪。当梯度大于某一值时将其裁剪成固定值。</li><li>使用L1或L2正则化</li><li>也可以使用Batch Normalization</li></ol><h2 id="为什么Dropout可以解决过拟合"><a href="#为什么Dropout可以解决过拟合" class="headerlink" title="为什么Dropout可以解决过拟合"></a>为什么Dropout可以解决过拟合</h2><ul><li>解除特定神经网络元之间的依赖 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。</li><li>类集成学习， Dropout本质上是在训练多个子神经网络，而在测试时，完整神经网络的输出相当于是对多个子神经网络预测结果的组合。</li></ul><h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><p>激活函数给神经元引入了非线性因素，如果不用激活函数的话，无论神经网络有多少层，输出都是输入的线性组合。<br>激活函数的发展经历了Sigmoid -&gt; Tanh -&gt; ReLU -&gt; Leaky ReLU -&gt; Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。</p><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><ul><li>最具代表性的激活函数，可以将输入数据压缩到0~1之间，但现在已经很少有人使用sigmoid</li><li>缺点1：sigmoid饱和使得梯度消失</li><li>缺点2：sigmoid求导复杂</li><li>缺点3：输出不是0中心<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3></li><li>将输入数据压缩到[-1, 1]之间</li><li>解决了sigmoid输出不是零中心的问题。但仍然存在饱和导致梯度消失的问题。<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3></li><li>现在的主流激活函数</li><li>因为大于0的部分导数恒等于1，不会出现梯度消失并且可以大大加快模型训练速度。</li><li>函数简单，不需要指数运算</li><li>缺点：可能会导致神经元死掉。<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3></li><li>在ReLU的基础上做了改进，让函数的非正数部分导数恒等于一个非常小的正数。</li><li>解决了神经元死掉的问题。</li></ul><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h3><p>对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。</p><h3 id="池化操作"><a href="#池化操作" class="headerlink" title="池化操作"></a>池化操作</h3><p>降维的方法，按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。</p><h3 id="CNN卷积后特征图大小"><a href="#CNN卷积后特征图大小" class="headerlink" title="CNN卷积后特征图大小"></a>CNN卷积后特征图大小</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：<br>答案：97</p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。<br>（200-5+2<em>1）/2+1 为99.5，取99<br>（99-3）/1+1 为97<br>（97-3+2</em>1）/1+1 为97</p><p>这里提供一个计算每一层输出图像的size的公式。无论是卷积层还是pooling层，公式都是这样的：</p><p>( input_size + 2*padding - kernel_size ) / stride + 1 = output_size</p><h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><ol><li>01损失</li><li>平方损失</li><li>绝对值损失</li><li>对数损失</li><li>交叉熵损失</li></ol><h2 id="TensorFlow的优缺点"><a href="#TensorFlow的优缺点" class="headerlink" title="TensorFlow的优缺点"></a>TensorFlow的优缺点</h2><ul><li>缺点<ol><li>API接口更新过快，不断挪位置。接口不灵活</li><li>调试困难，tensorflow是静态图框架，打印中间结果必须借助Session，或者额外的debug工具    </li></ol></li><li>优点<ol><li>文档最全，资源最多，用户基数最大</li><li>可视化组件tensorboard</li><li>静态图结构虽然调试困难，但部署方便，可以部署到移动端</li><li>和Numpy完美结合</li><li>支持多GPU</li></ol></li></ul><h2 id="其它考点"><a href="#其它考点" class="headerlink" title="其它考点"></a>其它考点</h2><ol><li>Relu缺点：负值的那部分永无翻身之日，而且改变了输出的分布状态，最好可以加上一个BN来改进</li><li>lstm参数数量计算：（1+cell_size+embed_size) <em> cell_size </em> 4</li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop之HDFS</title>
      <link href="/2018/05/21/Hadoop/Hadoop%E4%B9%8BHDFS/"/>
      <url>/2018/05/21/Hadoop/Hadoop%E4%B9%8BHDFS/</url>
      <content type="html"><![CDATA[<p>介绍HDFS的结构和相关操作<br><a id="more"></a></p><h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><p>HDFS是Hadoop的分布式文件系统。HDFS具有如下特点：</p><ol><li>处理超大文件</li><li>流式地访问数据</li><li>运行于廉价的商用机器集群上<br>由于以上设计的种种考虑，HDFS存在着一定的缺点：</li><li>不适合低延迟数据访问</li><li>无法高效存储大量小文件（由于NameNode大小限制）</li><li>不支持多用户写入及任意修改文件（在HDFS的一个文件只有一个写入者，而且只能在文件末尾执行追加操作）<h2 id="HDFS体系结构"><a href="#HDFS体系结构" class="headerlink" title="HDFS体系结构"></a>HDFS体系结构</h2><h3 id="HDFS相关概念"><a href="#HDFS相关概念" class="headerlink" title="HDFS相关概念"></a>HDFS相关概念</h3><h4 id="块（Block）"><a href="#块（Block）" class="headerlink" title="块（Block）"></a>块（Block）</h4>在HDFS中，文件是以block的形式存储在各个主机上，HDFS默认一个文件块大小为64MB。而且当一个文件存储在HDFS时，该文件是按照逻辑块来进行分割存储的，而不是物理块。也就是说，一个文件的不同块可能被存储在不同的主机上，但逻辑上他们是存储在一起的。<br>在HDFS中，为了处理节点故障，默认将文件块副本数设定为3份，分别存储在集群的不同节点上，当一个块损坏时，NameNode会在另外的主机上读取一个副本并存储。<h4 id="NameNode和DataNode"><a href="#NameNode和DataNode" class="headerlink" title="NameNode和DataNode"></a>NameNode和DataNode</h4>NameNode和DataNode分别承担Master和Worker任务。NameNode管理文件系统的命名空间，维护整个文件系统的文件目录树以及文件的索引目录。从NameNode中你可以获得每个文件的每块所在的DataNode。<br>DataNode是文件系统的Worker节点，用来执行具体任务：存储文件块，被客户端和NameNode调用。同时，它通过心跳通信定时向NameNode发送存储文件的信息。<br>一个HDFS是由一个NameNode和一定数目的DataNode组成的。</li></ol><h2 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h2><ol><li><p>从本地将一个文件复制到HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal localPath hdfsPath</span><br></pre></td></tr></table></figure></li><li><p>从HDFS复制到本机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyToLocal hdfsPath localPath</span><br></pre></td></tr></table></figure></li><li><p>创建文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir DirName</span><br></pre></td></tr></table></figure></li><li><p>用命令行查看HDFS文件列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr DirName // 查看文件夹下的所有文件详细信息</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS常用Java-API"><a href="#HDFS常用Java-API" class="headerlink" title="HDFS常用Java API"></a>HDFS常用Java API</h2><h3 id="使用Hadoop-URL读取数据"><a href="#使用Hadoop-URL读取数据" class="headerlink" title="使用Hadoop URL读取数据"></a>使用Hadoop URL读取数据</h3><p>想从HDFS中读取数据，最简单的办法就是使用java.net.URL对象打开一个数据流，从中读取数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">InputStream in = null;</span><br><span class="line">try&#123;</span><br><span class="line">    in = new URL(&quot;hdfs://NameNodeIP/path&quot;).openStream();</span><br><span class="line">&#125;finally&#123;</span><br><span class="line">    IOUtils.closeStream(in);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Scheme基本语法</title>
      <link href="/2018/05/21/Programming%20Language/Scheme%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2018/05/21/Programming%20Language/Scheme%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>因为研究室要进行EOPL（《Essentials of Programming Languages》）的轮讲，而这本书是用Scheme作为教学语言，以至于不得不学习一下Scheme的基本语法。作为一种教学性质的函数式编程语言，Scheme语法简单，非常通俗易懂，可以帮助程序员理解函数式编程思想，还是值得一学的。而且著名的编程神书SICP（《Structure and Interpretation of Computer Programs》）也是用Scheme作为教学语言。所以为了方便更好的理解这两本书，对Scheme的基本语法有一个简单的了解还是有必要的。</p><a id="more"></a><h2 id="Scheme-Basic"><a href="#Scheme-Basic" class="headerlink" title="Scheme Basic"></a>Scheme Basic</h2><p>与一般的编程语言最大的区别，Scheme使用前缀表达式。什么意思呢，看如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2)</span><br><span class="line">(+ 1 2 3 4 5)</span><br></pre></td></tr></table></figure></p><p>上面这行代码就是一句最简单的Scheme程序，返回1+2的结果。可以看到Scheme使用前缀表达式，先输入运算符‘+’，然后输入运算符的两个参数‘1’，‘2’。使用前缀表达式的一个最直接的优点就是运算符的运算对象个数不受限制，我们可以用一个‘+’计算任意个数的和。</p><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><ul><li><p>Scheme使用分号”;”表示单行注释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">; this is a scheme comment</span><br></pre></td></tr></table></figure></li><li><p>标准Scheme中未定义多行注释方法。</p></li></ul><h3 id="块-form"><a href="#块-form" class="headerlink" title="块(form)"></a>块(form)</h3><ul><li><p>块是Scheme中最小单元，用”( )”表示一个form，一个form可以是一个表达式，一个过程，一个变量声明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2)</span><br><span class="line">(define x 2)</span><br></pre></td></tr></table></figure></li><li><p>块可以通过嵌套完成复杂的表达式计算。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(+ (* 2 3) (/ 5 (- 3 2)))</span><br><span class="line">; == 2*3 + 5/(3-2)</span><br></pre></td></tr></table></figure></li></ul><h3 id="基本数值运算"><a href="#基本数值运算" class="headerlink" title="基本数值运算"></a>基本数值运算</h3><p>+, -, *, 和/分别代表加、减、乘、除。由于Scheme使用的是前缀表达式，所以这些操作都接受任意多的参数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(+ 1 2 3 4) ;→ 10</span><br><span class="line">(- 10 1 2)  ;→ 7</span><br><span class="line">(* 2 3 4)   ;→ 24</span><br><span class="line">(/ 29 3 7)  ;→ 29/21</span><br></pre></td></tr></table></figure></p><h3 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h3><p>Scheme有很多扩展库定义了一些有用的过程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(max 1 2 3 4)</span><br><span class="line">(min 1 2 3 4)</span><br><span class="line">(abs -2)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>高级编程语言的一个重要特性就是变量，我们可以通过定义变量从而记录对应的数据，进而可以进行更复杂的计算。在Scheme中使用’define’定义一个变量：</p><ul><li><p>定义一个变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define x 2)</span><br></pre></td></tr></table></figure></li><li><p>使用set!来改变变量的值:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(set! x &quot;hello&quot;)</span><br></pre></td></tr></table></figure></li><li><p>Scheme和python一样，它的变量类型是不固定的，可以随时改变。</p></li></ul><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="布尔型"><a href="#布尔型" class="headerlink" title="布尔型"></a>布尔型</h3><ul><li>Scheme使用“#t”表示真(True)，使用“#f”表示假(False)</li><li><p>“not” 表示取反:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(not #f) -&gt;True</span><br><span class="line">(not 1)  -&gt;False</span><br></pre></td></tr></table></figure></li><li><p>“not”后面的参数只要不是布尔型，都返回#f</p></li></ul><h3 id="数值型"><a href="#数值型" class="headerlink" title="数值型"></a>数值型</h3><p>Scheme支持四种数值类型：整型(integer)，有理数型(rational)，实型(real)，复数型(complex)</p><h3 id="字符型"><a href="#字符型" class="headerlink" title="字符型"></a>字符型</h3><p>Scheme语言中的字符型数据均以符号组合 “#\” 开始，表示单个字符，可以是字母、数字或”[ ! $ % &amp; * + - . / : %lt; = &gt; ? @ ^ _ ~ ]”等等其它字符，如：</p><p>#\A 表示大写字母A，#\0表示字符0，<br>其中特殊字符有：#\space 表示空格符和 #\newline 表示换行符。</p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>字符串(string) 由多个字符组成的数据类型，可以直接写成由双引号括起的内容，如：”hello” 。</p><h3 id="点对-pair"><a href="#点对-pair" class="headerlink" title="点对(pair)"></a>点对(pair)</h3><p>pair是Scheme中非常重要的一个数据结构，它是由一个点和被它分隔开的两个所值组成的。形如： (1 . 2) 或 (a . b) ，注意的是点的两边有空格。<br>它用cons来定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(cons 8 9) =&gt;(8 . 9)</span><br></pre></td></tr></table></figure></p><p>其中在点前面的值被称为 car ，在点后面的值被称为 cdr ，car和cdr同时又成为取pair的这两个值的过程，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(define p (cons 4 5))   =&gt; (4 . 5)</span><br><span class="line">(car p)         =&gt; 4</span><br><span class="line">(cdr p)         =&gt; 5</span><br></pre></td></tr></table></figure></p><p>还可以用set-car! 和 set-cdr! 来分别设定这两个值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(set-car! p &quot;hello&quot;)</span><br><span class="line">(set-cdr! p &quot;good&quot;)</span><br></pre></td></tr></table></figure></p><h3 id="列表-list"><a href="#列表-list" class="headerlink" title="列表(list)"></a>列表(list)</h3><p>Scheme中的列表结构和数据结构中的链表比较相似具体方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(define la (list 1 2 3 4 ))   -&gt;(1 2 3 4)</span><br><span class="line">(length la)  ; 取得列表的长度</span><br><span class="line">(list-ref la 3)  ; 取得列表第3项的值（从0开始）</span><br><span class="line">(list-set! la 2 99)  ; 设定列表第2项的值为99</span><br><span class="line">(define y (make-list 5 6))  ;创建列表</span><br><span class="line">(6 6 6 6 6)</span><br></pre></td></tr></table></figure></p><ul><li>make-list用来创建列表，第一个参数是列表的长度，第二个参数是列表中添充的内容；还可以实现多重列表，即列表的元素也是列表，如：(list (list 1 2 3) (list 4 5 6))。</li><li>列表是在点对的基础上形成的一种特殊格式，所以可以通过pair构建list。用于pair的操作过程大多可以用于list。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(cadr ls)   ; 此&quot;点对&quot;对象的cdr的car</span><br><span class="line">(cddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr</span><br><span class="line">(caddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr的car</span><br><span class="line">(cdddr ls)   ; 此&quot;点对&quot;对象的cdr的cdr的cdr</span><br></pre></td></tr></table></figure></li></ul><h3 id="类型判断-转换"><a href="#类型判断-转换" class="headerlink" title="类型判断/转换"></a>类型判断/转换</h3><ul><li>Scheme语言中所有判断都是用类型名加问号再加相应的常量或变量构成：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(type? variable)</span><br><span class="line">(boolean? #f)</span><br><span class="line">(char? #\a)</span><br><span class="line">(integer? 1)</span><br><span class="line">(real? 4/5)</span><br><span class="line">(null? &apos;())</span><br></pre></td></tr></table></figure><ul><li>Scheme中可以使用 “=”, “eq? “, “eqv? “, “equal? “判断是否相等</li><li>Scheme使用”-&gt;”表示类型转换：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(number-&gt;string 123)  ; 数字转换为字符串</span><br><span class="line">(string-&gt;number &quot;456&quot;)  ; 字符串转换为数字</span><br><span class="line">(char-&gt;integer #\a)   ;字符转换为整型数，小写字母a的ASCII码值为97</span><br><span class="line">(char-&gt;integer #\A)  ;大写字母A的值为65</span><br><span class="line">(integer-&gt;char 97)  ;整型数转换为字符#\a</span><br></pre></td></tr></table></figure></li></ul><h2 id="过程-Procedure"><a href="#过程-Procedure" class="headerlink" title="过程(Procedure)"></a>过程(Procedure)</h2><p>在Scheme语言中，过程相当于C语言中的函数，不同的是Scheme语言过程是一种数据类型，这也是为什么Scheme语言将程序和数据作为同一对象处理的原因。也正是因为如此，define不仅可以定义变量，还可以定义过程，不过标准过程定义要使用lambda关键字来标识。</p><h3 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h3><p>Scheme语言中可以用lambda来定义过程，其格式如下：<br>(define 过程名 ( lambda (参数 …) (操作过程 …)))<br>例：定义自增1的过程 add1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define add1 (lambda (x) (+ x 1)))</span><br></pre></td></tr></table></figure></p><p>该过程需要传入一个参数x，返回x+1的值。</p><h3 id="另一种过程定义方法"><a href="#另一种过程定义方法" class="headerlink" title="另一种过程定义方法"></a>另一种过程定义方法</h3><p>在Scheme语言中，也可以不用lambda，而直接用define来定义过程，它的格式为：<br>(define (过程名 参数) (过程内容 …))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(define (add1 x) (+ x 1))</span><br></pre></td></tr></table></figure></p><h3 id="过程的嵌套定义"><a href="#过程的嵌套定义" class="headerlink" title="过程的嵌套定义"></a>过程的嵌套定义</h3><p>在Scheme语言中，过程定义也可以嵌套，一般情况下，过程的内部过程定义只有在过程内部才有效，相当C语言中的局部变量。</p><h2 id="常用结构"><a href="#常用结构" class="headerlink" title="常用结构"></a>常用结构</h2><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h3><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p>用法如同常规语言，Scheme的具体语法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(if (predicate)</span><br><span class="line">    (consequent)</span><br><span class="line">    (alternative))</span><br></pre></td></tr></table></figure></p><h4 id="cond"><a href="#cond" class="headerlink" title="cond"></a>cond</h4><p>用法如同C语言中的switch关键字，具体语法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(cond</span><br><span class="line">  (predicate1) (consequent1)</span><br><span class="line">  (predicate2) (consequent2)</span><br><span class="line">  ...</span><br><span class="line">  else (consequentn))</span><br><span class="line">; else 可写可不写</span><br></pre></td></tr></table></figure></p><h3 id="逻辑判断词"><a href="#逻辑判断词" class="headerlink" title="逻辑判断词"></a>逻辑判断词</h3><p>在Scheme中，使用逻辑关键字and，or，not表示布尔逻辑判断与或非：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(not #f)</span><br><span class="line">(and (#f) (#t))</span><br><span class="line">(or (#f) (#t))</span><br></pre></td></tr></table></figure></p><h3 id="case"><a href="#case" class="headerlink" title="case"></a>case</h3><p>case类似于枚举型的cond，具体用法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(case (* 2 4)</span><br><span class="line">  ((1 3 5 7) &apos;odd)</span><br><span class="line">  ((2 4 6 8) &apos;even))</span><br><span class="line">; 结果返回 even</span><br></pre></td></tr></table></figure></p><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>在Scheme中， 并没有定义循环关键字，但如同其他高级语言一样，Scheme支持递归调用，所以一般情况下Scheme通过递归来实现循环</p><h2 id="变量和过程的绑定"><a href="#变量和过程的绑定" class="headerlink" title="变量和过程的绑定"></a>变量和过程的绑定</h2><p>使用let, let*, letrec在lambda里面定义局部变量。</p><h3 id="let"><a href="#let" class="headerlink" title="let"></a>let</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(let (x 2) (y 5) (* x y))</span><br></pre></td></tr></table></figure><ul><li>letrec<br>letrec是将内部定义的过程或变量间的相互引用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(letrec ((countdown</span><br><span class="line">          (lambda (i)</span><br><span class="line">          (if (= i 0)</span><br><span class="line">              &apos;()</span><br><span class="line">              (countdown (- i 1)))))))</span><br></pre></td></tr></table></figure></li></ul><h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><p>apply的功能是为数据赋予某一个操作过程，他的第一个参数必须是一个过程，随后的阐述必须是一个列表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(apply + (list 1 2 3))</span><br><span class="line">&gt;&gt; 6</span><br><span class="line"></span><br><span class="line">(define scrum</span><br><span class="line">  (lambda (x)</span><br><span class="line">    (apply + x)))</span><br></pre></td></tr></table></figure></p><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>map的功能和apply有些相似，它的第一个参数也必需是一个过程，随后的参数必需是多个列表，返回的结果是此过程来操作列表后的值</p><h2 id="顺序结构"><a href="#顺序结构" class="headerlink" title="顺序结构"></a>顺序结构</h2><h3 id="begin"><a href="#begin" class="headerlink" title="begin"></a>begin</h3><p>通过begin过程实现顺序结构，用begin来将多个form放在一对小括号内，最终形成一个form。格式为：(begin form1 form2 …)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(begin</span><br><span class="line">    (display &quot;hello world&quot;)</span><br><span class="line">    (display &quot;goodbye world&quot;)</span><br><span class="line">    (newline))</span><br></pre></td></tr></table></figure></p><h3 id="scheme-quote函数"><a href="#scheme-quote函数" class="headerlink" title="scheme quote函数"></a>scheme quote函数</h3><p>引用（Quotation）</p><p>语法：(quote obj) 或者简写为 ‘obj</p><p>(+ 2 3)      ; 返回 5<br>‘(+ 2 3)     ; 返回列表 (+ 2 3)<br>(quote (+ 2 3)) ; 返回列表 (+ 2 3)</p>]]></content>
      
      <categories>
          
          <category> Programming Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>强化学习之算法种类</title>
      <link href="/2018/05/13/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A7%8D%E7%B1%BB/"/>
      <url>/2018/05/13/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A7%8D%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>本小节主要介绍了强化学习的分类</p><a id="more"></a><h2 id="Model-free-amp-Model-based"><a href="#Model-free-amp-Model-based" class="headerlink" title="Model_free &amp; Model_based"></a>Model_free &amp; Model_based</h2><p>不理解环境和理解环境<br>Model_free是指Agent不会尝试理解环境，环境返回什么就是什么，然后根据环境返回的状态和奖励进行学习，而不试图去理解环境本身。Model_based值Agent理解环境如何运行，本质上是学习出一个模型来表示环境。<br>Model_free的代表算法有：Q_Learning，Sarsa，Policy Gradients。<br>Model_based RL只是多了一道程序：为真实世界建模，model_based RL可以通过想象来预判接下来将发生的情况，并选择这些想象中最优解。而Model_free只能按部就班等待环境的反馈。</p><h2 id="Policy-Based-amp-Value-Based"><a href="#Policy-Based-amp-Value-Based" class="headerlink" title="Policy_Based &amp; Value_Based"></a>Policy_Based &amp; Value_Based</h2><p>基于概率和基于价值<br>基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同. 而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作。<br>我们现在说的动作都是一个一个不连续的动作, 而对于选取连续的动作, 基于价值的方法是无能为力的. 我们却能用一个概率分布在连续动作中选取特定动作, 这也是基于概率的方法的优点之一.<br><br>Policy_Based代表算法：Policy Gradients<br>Value_Based代表算法那：Q_learning, Sarsa</p><h2 id="回合更新和逐步更新"><a href="#回合更新和逐步更新" class="headerlink" title="回合更新和逐步更新"></a>回合更新和逐步更新</h2><p>回合制更新是在本轮学习过程结束后，对学习过程中的每个决策进行更新. 而逐步更新则是学习过程中的每一步都在更新, 不用等待本轮学习结束，这样的好处是可以边学习边更新。7因为逐步更新更有效率, 所以现在大多方法都是基于逐步更新. 而且有的强化学习问题并不属于回合问题.</p><p>回合制更新算法：Monte-carlo learning 和基础版的policy gradients<br>逐步更新算法： Q_learning, Sarsa, 升级版的 policy gradients 等都是单步更新制.</p><h2 id="在线学习和离线学习"><a href="#在线学习和离线学习" class="headerlink" title="在线学习和离线学习"></a>在线学习和离线学习</h2><p>所谓在线学习, 就是指Agent必须“亲自”从环境的反馈中学习, 而离线学习则是可以通过观察别的Agent的学习过程来学习。同样是从过往的经验中学习, 离线学习中的过往经历没必要是自己的经历, 任何人的经历都能被学习.</p><p>在线学习算法：Sarsa 以及优化版Sarsa：Sarsa lambda<br>离线学习算法：Q_learning, Deep-Q-Network.</p>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SICP第二章:构造数据抽象</title>
      <link href="/2018/04/27/Reading/SICP%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/27/Reading/SICP%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>SICP的第二章：构造数据抽象<br><a id="more"></a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="为什么需要复合数据？"><a href="#为什么需要复合数据？" class="headerlink" title="为什么需要复合数据？"></a>为什么需要复合数据？</h3><p>与我们需要复合过程一样的原因一样：同样是为了提升我们在设计程序时所位于的概念层次，提高设计的模块性，增强语言的表达能力。正如定义过程的能力使我们有可能在更高的概念层次上处理计算工作一样，复合数据的能力，也将使我们得以在比语言提供的基本数据对象更高的概念层次上，处理与数据有关的各种问题。</p><h3 id="复合数据提高模块性"><a href="#复合数据提高模块性" class="headerlink" title="复合数据提高模块性"></a>复合数据提高模块性</h3><p>如果我们可以直接在将有理数本身当作对象的方式下操作他们，那么也就可能把处理有理数的那些程序部分，与有理数如何表示的细节隔离开。也就是说：将程序中处理数据对象的表示部分，和处理数据对象的使用的部分相互隔离。数据抽象技术能使程序更容易设计，维护和修改。</p><h2 id="2-1-数据抽象索引"><a href="#2-1-数据抽象索引" class="headerlink" title="2.1 数据抽象索引"></a>2.1 数据抽象索引</h2><ul><li><p>数据抽象使一种方法学，它使我们能将一个 <strong>复合数据对象的使用</strong>，与该数据对象怎样由 <strong>更基本的数据对象构造</strong> 起来的细节隔离开。（对于过程抽象，我们可以理解为：构造一个抽象，它将这一过程的 <strong>使用方式</strong>，和该过程究竟如何通过 <strong>更基本的过程实现的具体细节</strong> 相互分离）  </p></li><li><p>数据抽象的基本思想，就是设法构造出一些使用复合数据对象的程序。我们的程序使用数据的方式应该是这样的：除了完成当前工作所必要的东西之外，它们不对所有数据作任何假设，与此同时，一种‘具体’数据表示的定义，也应该与程序中使用数据的方式无关。这两个部分之间的界面是一组过程，称为 <strong>选择函数和构造函数</strong></p></li></ul><h3 id="2-1-1-实例：有理数的运算"><a href="#2-1-1-实例：有理数的运算" class="headerlink" title="2.1.1 实例：有理数的运算"></a>2.1.1 实例：有理数的运算</h3><p>假定我们要作有理数的算术，包括加减乘除等<br>假定已经有了一种从分子和分母构造有理数的方法，并进一步假定如果有了一个有理数，我们有一种方法取得他的分子和分母。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(make-rat &lt;n&gt; &lt;d&gt;) 返回一个有理数， 分子是n，分母是d</span><br><span class="line">(numer &lt;x&gt;) 返回分子</span><br><span class="line">(denom &lt;x&gt;) 返回分母</span><br></pre></td></tr></table></figure></p><p>目前，我们并不考虑上面的三个过程的具体实现，我们仅根据他们的功能，完成有理数的加减乘除：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(define (add-rat x y)</span><br><span class="line">  (make-rat (+ (* (numer x) (denom y))</span><br><span class="line">               (* (numer y) (denom x)))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (sub-rat x y)</span><br><span class="line">  (make-rat (- (* (numer x) (denom y))</span><br><span class="line">               (* (numer y) (denom x)))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (mul-rat x y)</span><br><span class="line">  (make-rat (* (numer x) (numer y))</span><br><span class="line">            (* (denom x) (denom y))))</span><br><span class="line"></span><br><span class="line">(define (div-rat x y)</span><br><span class="line">  (make-rat (* (numer x) (denom y))</span><br><span class="line">            (* (numer y) (denom x))))</span><br><span class="line"></span><br><span class="line">(define (equal-rat? x y)</span><br><span class="line">  (let ((a (* (numer x) (denom y)))</span><br><span class="line">        (b (* (numer y) (denom x))))</span><br><span class="line">    (if (eqv? a b)</span><br><span class="line">        #t</span><br><span class="line">        #f)))</span><br></pre></td></tr></table></figure></p><p>这样，我们有了定义在三个构造过程基础之上的各种运算。而这些基础还没有定义。我们可以使用序对cons进行构造：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(define (make-rat n d) (cons n d))</span><br><span class="line">(define (numer x) (car x))</span><br><span class="line">(define (denom x) (cdr x))</span><br><span class="line">(define (print-rat x))</span><br></pre></td></tr></table></figure></p><h3 id="2-1-2-抽象屏障"><a href="#2-1-2-抽象屏障" class="headerlink" title="2.1.2 抽象屏障"></a>2.1.2 抽象屏障</h3><ul><li>一般而言，数据抽象的基本思想就是为每一类数据对象标示出一组 <strong>基本操作</strong>，使得对这类数据对象的所有操作都可以基于他们表述，而且在操作这些数据对象时也只能使用他们。</li><li>对于数据，可以有多个抽象层次，也就是多个抽象屏障，如有理数的定义：<blockquote><p>使用有理数的程序将仅仅通过有理数包提供的“API”（add-rat, sub-rat, mul-rat, div-rat..)去完成有理数操作；而这些过程转而又是完全基于构造函数和选择函数make-rat, numer, denom实现的。而这些函数又是基于序对实现的，只要序对可以通过cons,car和cdr操作。，有关序对如何实现的细节与有理数包的其余部分都完全没有关系。</p></blockquote></li></ul><h3 id="2-1-3-数据意味着什么"><a href="#2-1-3-数据意味着什么" class="headerlink" title="2.1.3 数据意味着什么"></a>2.1.3 数据意味着什么</h3><ul><li>可以考虑 <strong>序对</strong> ，我们从来没有说过序对是什么，只是说所有的语言为序对的操作提供了三个过程cons，car，cdr。有关这三个操作，我们需要知道的全部东西就是，三个过程的功能。</li><li><p>进一步思考，我们能发现一个令人吃惊的事实：我们完全可以不用任何数据结构，只使用过程就可以实现序对：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(define (cons x y)</span><br><span class="line">  (define (dispatch m)</span><br><span class="line">    (cond ((= m 0) x)</span><br><span class="line">          ((= m 1) y)</span><br><span class="line">          (else (error &quot;argument no 0 or 1&quot; m))))</span><br><span class="line">  dispatch)</span><br><span class="line">(define (car z) (z 0))</span><br><span class="line">(define (cdr z) (z 1))</span><br><span class="line"></span><br><span class="line">(car (cons 0 1)) -&gt; 0</span><br><span class="line">(cdr (cons 0 1)) -&gt; 1</span><br><span class="line"></span><br><span class="line">; cons过程会返回一个过程，而过程car，cdr会使用cons返回的过程并传入；参数，而在cons返回的过程中针对传入的参数进行操作。</span><br></pre></td></tr></table></figure></li><li><p>可以看出，cons返回一个过程，而对这三个方法的定义，完全满足了序对的定义。从这个例子可以看出，我们无法把这一实现和“真正的”数据结构区分开。</p></li><li><strong>数据的过程性表示</strong> 将在我们的程序设计中扮演一种核心角色。有关的程序设计风格通常称为 <strong>消息传递</strong></li></ul><h2 id="2-2-层次性数据和闭包性质"><a href="#2-2-层次性数据和闭包性质" class="headerlink" title="2.2 层次性数据和闭包性质"></a>2.2 层次性数据和闭包性质</h2><ul><li>序对为我们提供了一种用于构造复合数据的基本“粘合剂”。我们可以建立元素本身也是序对的序对，这就是表结构得以作为一种表示工具的根本基础。我们将这种能力成为cons的闭包性质。</li><li>一般的说，某种组合数据对象的操作满足闭包性质，那就是说，通过它组合起来的数据对象得到的结果本身还可以通过同样的操作再进行组合。</li></ul><h3 id="2-2-1-序列的表示"><a href="#2-2-1-序列的表示" class="headerlink" title="2.2.1 序列的表示"></a>2.2.1 序列的表示</h3><ul><li><p>可以使用cons构造序列，同时Scheme提供过程list用以快速构造序列：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(list &lt;a1&gt; &lt;a2&gt; ..) = (cons (a1 (cons a2 (cons ..))))</span><br></pre></td></tr></table></figure></li><li><p>使用map过程，对一个list的所有元素进行操作映射。</p></li></ul><h3 id="2-2-2-层次性结构"><a href="#2-2-2-层次性结构" class="headerlink" title="2.2.2 层次性结构"></a>2.2.2 层次性结构</h3><p>使用list和cons可以构建复合形式的数据，我们可以将它看作树结构，进行递归访问。</p><h3 id="2-2-3-序列作为一种约定的界面"><a href="#2-2-3-序列作为一种约定的界面" class="headerlink" title="2.2.3 序列作为一种约定的界面"></a>2.2.3 序列作为一种约定的界面</h3><p>对于一个复杂的过程，我们可以将其拆分成一个信号流系统，将这个复杂的过程分割成不同的子过程，并且各个子过程之间用信号表示流动。<br>例：给定一个树，计算值为奇数的叶子的平方和。最原始的思想解题过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define (sum-odd-squares tree)</span><br><span class="line">  (cond ((null? tree) 0)</span><br><span class="line">        ((not (pair? tree))</span><br><span class="line">         (if (odd? tree)</span><br><span class="line">             (square tree)</span><br><span class="line">             0))</span><br><span class="line">        (else (+ (sum-odd-squares (car tree))</span><br><span class="line">                 (sum-odd-squares (cdr tree))))))</span><br></pre></td></tr></table></figure></p><p>我们可以将上面这个复杂的过程归纳为信号流结构：</p><ol><li>枚举出一棵树的每个树叶</li><li>过滤它们，选出其中的奇数树叶</li><li>对选出的每一个数求平方</li><li>用+累加起来得到结果，从0开始</li></ol><p>但在上面的原始过程中，我们并没有体现出信号流结构。我们需要重新组织这些程序，使之能够清晰的反应上面信号流的结构，其中最关键的一点就是将注意力集中在处理过程中从一个步骤流向下一个步骤的“信号”。   </p><ul><li><p>枚举一棵树的所有树叶：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define (enumerate-tree tree)</span><br><span class="line">  (cond ((null? tree) nil)</span><br><span class="line">        ((not (pair? tree)) (list tree))</span><br><span class="line">        (else (append (enumerate-tree (car tree))</span><br><span class="line">                      (enumerate-tree (cdr tree))))))</span><br><span class="line"></span><br><span class="line">(enumerate-tree (list 1 (list 2 (list 3 4)) 5))</span><br><span class="line">(1 2 3 4 5)</span><br></pre></td></tr></table></figure></li><li><p>按照给定谓词过滤元素：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(define (filter predicate sequence)</span><br><span class="line">  (cond ((null? sequence) nil)</span><br><span class="line">        ((predicate (car sequence))</span><br><span class="line">         (cons (car sequence)</span><br><span class="line">               (filter predicate (cdr sequence))))</span><br><span class="line">        (else (filter predicate (cdr sequence)))))</span><br><span class="line"></span><br><span class="line">(filter odd? (list 1 2 3 4 5))</span><br><span class="line">(1 3 5)</span><br></pre></td></tr></table></figure></li><li><p>实现信号流图中的映射步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(map square (list 1 2 3 4 5))</span><br></pre></td></tr></table></figure></li><li><p>实现流图中的累加过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(define (accumulate op initial sequence)</span><br><span class="line">  (if (null? sequence)</span><br><span class="line">      initial</span><br><span class="line">      (op (car sequence)</span><br><span class="line">          (accumulate op initial (cdr sequence)))))</span><br></pre></td></tr></table></figure></li><li><p>最后，将整个流程整合起来形成一个过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(define (sum-odd-squares tree)</span><br><span class="line">  (accumulate +</span><br><span class="line">              0</span><br><span class="line">              (map square</span><br><span class="line">                   (filter odd?</span><br><span class="line">                           (enumerate-tree tree)))))</span><br></pre></td></tr></table></figure></li></ul><p>将程序表示为一些针对序列的操作，这样做的价值就在于能帮助我们得到模块化的程序设计，也就是说，得到由一些比较独立的片段的组合构成的设计。在工业设计中，模块化结构是控制复杂性的一种威力强大的策略。</p><h3 id="2-2-4-实例分析：一个图形语言"><a href="#2-2-4-实例分析：一个图形语言" class="headerlink" title="2.2.4 实例分析：一个图形语言"></a>2.2.4 实例分析：一个图形语言</h3><p>在描述一种语言时，应该将注意力集中到语言的基本原语，它的组合手段以及它的抽象手段。详细内容见书P86.</p><h2 id="2-3-符号数据"><a href="#2-3-符号数据" class="headerlink" title="2.3 符号数据"></a>2.3 符号数据</h2><p>到目前为止，我们已经使用过的所有复合数据都是从数值出发构造起来的，在这一节，我们要扩充所用语言的表述能力，引进将任意符号作为数据的功能。</p><h3 id="2-3-1-使用引号表示字符"><a href="#2-3-1-使用引号表示字符" class="headerlink" title="2.3.1 使用引号表示字符"></a>2.3.1 使用引号表示字符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(define a 1)</span><br><span class="line">(define b 2)</span><br><span class="line">(list a b)</span><br><span class="line">&gt;&gt;(1 2)</span><br><span class="line">(list &apos;a &apos;b)</span><br><span class="line">&gt;&gt;(a b)</span><br><span class="line">(list &apos;a b)</span><br><span class="line">&gt;&gt;(a 2)</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> SICP </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python编码规范</title>
      <link href="/2018/04/27/Programming%20Language/Python/Python%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"/>
      <url>/2018/04/27/Programming%20Language/Python/Python%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/</url>
      <content type="html"><![CDATA[<p>整理Python编码规范PEP8的内容以及一些其他约定俗成的编码规范。</p><a id="more"></a><h2 id="python编码风格"><a href="#python编码风格" class="headerlink" title="python编码风格"></a>python编码风格</h2><p>PEP8 是python代码风格指南。</p><h3 id="pep8包"><a href="#pep8包" class="headerlink" title="pep8包"></a>pep8包</h3><h4 id="pycodestyple"><a href="#pycodestyple" class="headerlink" title="pycodestyple"></a>pycodestyple</h4><p>现在有多个python包来检查python代码风格是否符合PEP8.<br>‘pip3 install pycodestyple’<br>然后，对一个python文件运行，会返回任何违反PEP8的报告：<br>‘pycodestyple python_test.py’</p><h4 id="autopep8"><a href="#autopep8" class="headerlink" title="autopep8"></a>autopep8</h4><p>程序autopep8能将代码自动格式化成pep8风格：<br>‘pip3 install autopep8’<br>用一下指令格式化一个文件：<br>‘autopep8 –in-place python_file.py’<br>不包含 –in-place 标志将会使得程序直接将更改的代码输出到控制台，以供审查。 –aggressive 标志则会执行更多实质性的变化，而且可以多次使用以达到更佳的效果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autopep8 --in-place --aggressive python_code.py</span><br></pre></td></tr></table></figure></p><h4 id="代码编排"><a href="#代码编排" class="headerlink" title="代码编排"></a>代码编排</h4><ol><li>类和top-level函数之间空两行，类中的方法定义之间空一行，函数内部逻辑无关段落之间空一行，其他地方尽量不要空行。</li><li>模块内容的顺序：模块说明—&gt;import—&gt;globals&amp;constants—&gt;其他定义。其中import部分，又按标准、三方和自己编写顺序依次排放，之间空一行。</li><li>不要在一句import中多个库，比如import os, sys不推荐。</li><li>如果采用from XX import XX引用库，可以省略‘module.’，都是可能出现命名冲突，这时就要采用import XX。<h4 id="空格使用"><a href="#空格使用" class="headerlink" title="空格使用"></a>空格使用</h4>总体原则：避免使用不必要的空格。</li><li>各种右括号之间不要加空格</li><li>逗号，冒号，分号之前不要加空格</li><li>函数和序列的左括号前不要加空格</li><li>各种操作符左右各加一个空格，不要为了对齐而加空格</li><li>函数调用时，默认参数使用的赋值符省略空格<h4 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h4></li><li>模块命名尽量短小，全部小写，可以使用下划线</li><li>包命名尽量短小，全部小写，不使用下划线</li><li>类命名遵守CapWord方式，异常命名使用CapWordError的形式</li><li>函数命名使用全部小写+下划线形式</li><li>常量命名使用全部大写+下划线形式</li><li>类属性命名使用全部小写，类属性若与关键字名字冲突，后缀加一个下划线。<h4 id="编码建议"><a href="#编码建议" class="headerlink" title="编码建议"></a>编码建议</h4></li><li>尽量使用is， is not代替 ==， !=，比如：if x is not None要优于if x:</li><li>异常处理try中的代码要尽可能少</li><li><p>使用startswith() 和 endswith() 代替切片进行序列前缀或后缀的检查：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if string.startswith(&apos;abc&apos;):</span><br><span class="line">要优于：</span><br><span class="line">if string[:3] == &apos;abc&apos;:</span><br></pre></td></tr></table></figure></li><li><p>使用isinstance()比较对象类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if isinstance(obj, int):</span><br><span class="line">要优于：</span><br><span class="line">if type(obj) is type(1):</span><br></pre></td></tr></table></figure></li></ol><h3 id="避免在列表中修改"><a href="#避免在列表中修改" class="headerlink" title="避免在列表中修改"></a>避免在列表中修改</h3><p>请记住，赋值永远不会创建新对象。如果两个或多个变量引用相同的列表，则修改其中一个变量意味着将修改所有变量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [3,4,5]</span><br><span class="line">b = a</span><br><span class="line">for i in range(len(a)):</span><br><span class="line">    a[i] += 3   # 这个时候b也被改变了</span><br></pre></td></tr></table></figure></p><p>优雅的代码格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [3,4,5]</span><br><span class="line">b = a</span><br><span class="line">a = [i+3 for i in a]</span><br><span class="line">a = list(map(lambda i:i+3, a)) # 或者</span><br></pre></td></tr></table></figure></p><h3 id="使用enumerate-获得当前列表计数"><a href="#使用enumerate-获得当前列表计数" class="headerlink" title="使用enumerate()获得当前列表计数"></a>使用enumerate()获得当前列表计数</h3><p>在遍历序列时，要避免使用range(len(seq))的方法，优雅的做法是使用enumerate()获得列表中当前位置的计数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i, value in enumerate(seq):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN模型演化过程</title>
      <link href="/2018/04/27/Deep%20Learning/CNN%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8C%96%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/04/27/Deep%20Learning/CNN%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8C%96%E8%BF%87%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>本文介绍了从LeNet到ResNet的CNN模型演化过程</p><a id="more"></a><h2 id="几个经典CNN模型"><a href="#几个经典CNN模型" class="headerlink" title="几个经典CNN模型"></a>几个经典CNN模型</h2><h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><p>卷积神经网络的鼻祖，由两个卷积层、两个池化层和两个全连接层组成，卷积都是5*5的模板，stride=1，池化都是MAX。</p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet最大的贡献是证明了CNN在复杂模型上的有效性。</p><ul><li>在LeNet的基础上使用了5个卷积层，五个最大池化层和三个全连接层。输出端是一个softmax。</li><li>为了加快训练速度，使用了ReLU和GPU并行</li><li>为了减少过拟合，采用了dropout和data augmentation（数据增强）以及LRN（局部相应归一化）</li><li>AlexNet确立了CNN在CV中的统治地位</li></ul><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>相当于AlexNet的增强版，有着结构简单但深度更深，精度更强的优势。特点是连续卷积多，计算量巨大。</p><ul><li>VGG 与 AlexNet 最鲜明的对比是卷积层、卷积核设计的变化。VGGNet 探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠 3x3 的小型卷积核和 2x2 的最大池化层，成功构筑了 16~19 层深的卷积神经网络。</li><li>证明了可以用多个小卷积核堆叠等价于一个大卷积核，而多个小卷积核的参数量要更少</li><li>VGG性能优异：同 AlexNet 提升明显，同 GoogleNet, ResNet 相比，表现相近</li><li>VGG是选择最多的基本模型，方便进行结构的优化、设计，SSD, RCNN，等其他任务的基本模型(base model)<h4 id="VGGNet卷卷积群和感知野"><a href="#VGGNet卷卷积群和感知野" class="headerlink" title="VGGNet卷卷积群和感知野"></a>VGGNet卷卷积群和感知野</h4>VGGNet 有 5 个卷积群，每一群内有 2~3 个卷积层，每个群连接一个 max-pooling 层来缩小图片尺寸。每个卷积群内的卷积核数量一样，越靠后的卷积群的卷积核数量越多：64 – 128 – 256 – 512 – 512。其中经常出现多个完全一样的 3x3 的卷积层堆叠在一起的情况，为什么呢？可以发现两个 3x3 的卷积层串联其实相当于 1 个 5x5 的卷积层，也就是说一个像素会跟周围 5x5 的像素产生关联。类似的，3 个 3x3 的卷积层串联的效果则相当于 1 个 7x7 的卷积层。为什么要堆叠卷积层而不直接用 7x7 的呢？一是因为 3 个串联的 3x3 的卷积层的参数比 1 个 7x7 的卷积层更少，二是因为 3 个 3x3 的卷积层比 1 个 7x7 的卷积层有更多的非线性变换（前者可以用三次 ReLU 激活函数，而后者只有一次），对特征的学习能力更强</li></ul><h3 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h3><ul><li>GoogleNet的特点是结构复杂，多分辨率融合，证明了用更多的卷积，更深的层次可以得到更好的效果</li><li>GoogleNet主要目标是找到最优的稀疏结构单元，也就是 Inception module。Inception 结构是将不同的卷积层通过并联的方式结合在一起，它主要改进了网络内部计算资源的利用率，让我们能在固定计算资源下增加神经网络深度和宽度。</li><li>GooLeNet 的 Inception 对特征图进行了三种不同的卷积(1x1, 3x3, 5x5)来提取多个尺度的信息，也就是提取更多的特征。举个例子，一张图片有两个人，近处一个远处一个，如果只用 5x5，可能对近处的人的学习比较好，而对远处那个人，由于尺寸的不匹配，达不到理想的学习效果，而采用不同卷积核来学习，相当于融合了不同的分辨率，可以较好的解决这个问题。把这些卷积核卷积后提取的 feature map进行聚合操作合并作为输出，但会发现这样结构下的参数暴增，耗费大量的计算资源。</li><li>改进方案，在 3x3，5x5 之前，以及 pooling 以后都跟上一个 1x1 的卷积用以降维，就可以在提取更多特征的同时，大量减少参数，降低计算量。1x1 的卷积核性价比很高，很小的计算量就能增加一层特征变换和非线性化(如果后面接 ReLU)，另外，这也是一种降维的方式，可以减少过拟合。</li><li>GoogleNet实现了参数数量更少，但准确率更高的效果。</li><li>提出了Batch Normalization<h4 id="GoogleNet的Inception-module"><a href="#GoogleNet的Inception-module" class="headerlink" title="GoogleNet的Inception module"></a>GoogleNet的Inception module</h4>GooLeNet 主要目标是找到最优的稀疏结构单元，也就是 Inception module。Inception 结构是将不同的卷积层通过并联的方式结合在一起，它主要改进了网络内部计算资源的利用率，让我们能在固定计算资源下增加神经网络深度和宽度。<br>GooLeNet 的 Inception 对特征图进行了三种不同的卷积(1x1, 3x3, 5x5)来提取多个尺度的信息，也就是提取更多的特征。举个例子，一张图片有两个人，近处一个远处一个，如果只用 5x5，可能对近处的人的学习比较好，而对远处那个人，由于尺寸的不匹配，达不到理想的学习效果，而采用不同卷积核来学习，相当于融合了不同的分辨率，可以较好的解决这个问题。把这些卷积核卷积后提取的 feature map (再加多一个 max pooling 的结果)进行聚合操作合并(在输出通道数这个维度上聚合)作为输出。会发现这样结构下的参数暴增，耗费大量的计算资源，在 3x3，5x5 之前，以及 pooling 以后都跟上一个 1x1 的卷积用以降维，就可以在提取更多特征的同时，大量减少参数，降低计算量。1x1 的卷积核性价比很高，很小的计算量就能增加一层特征变换和非线性化(如果后面接 ReLU 等 activation layer)，另外，这也是一种降维(dimension reductionality)的方式，可以减少过拟合。</li></ul><h2 id="ResNet网络"><a href="#ResNet网络" class="headerlink" title="ResNet网络"></a>ResNet网络</h2><p>深度卷积网络自然的整合了低中高不同层次的特征，特征的层次可以靠加深网络的层次来丰富。从而，在构建卷积网络时，网络的深度越高，可抽取的特征层次就越丰富。<br>但是当使用更深层的网络时，会发生梯度消失、爆炸问题，这个问题很大程度通过标准的初始化和正则化层来基本解决，这样可以确保几十层的网络能够收敛，但是随着网络层数的增加，梯度消失或者爆炸的问题仍然存在。</p><h3 id="网络退化问题"><a href="#网络退化问题" class="headerlink" title="网络退化问题"></a>网络退化问题</h3><p><strong>网络退化问题</strong>，举个例子，假设已经有了一个最优化的网络结构，是18层。当我们设计网络结构的时候，并不知道多少层的网络最优化网络结构，假设设计了34层网络结构。那么多出来的16层其实是冗余的，我们希望训练网络的过程中，模型能够自己训练这五层为恒等映射，也就是经过这层时的输入与输出完全一样。但是往往模型很难将这16层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。它不是由过拟合产生的，而是由冗余的网络层学习了不是恒等映射的参数造成的。</p><h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>ResNet希望网络可以自动训练出哪些隐藏层是冗余的，并将冗余层的输入输出相等。具体方法是将原网络的基层改成一个残差块，残差块的构造如下：<br><img src="images/deep_learning/resnet_block.png" alt="ResNet_block"><br>X是这一层残差块的输入，也称作F(x)为残差，x为输入值，F（X）是经过第一层线性变化并激活后的输出，该图表示在残差网络中，第二层进行线性变化之后激活之前，F(x)加入了这一层输入值X，然后再进行激活后输出。在第二层输出值激活前加入X，这条路径称作shortcut连接。</p><h3 id="残差块原理"><a href="#残差块原理" class="headerlink" title="残差块原理"></a>残差块原理</h3><p>假设该层是冗余的，我们想让该层的映射h(x)=x，但这是比较困难的。ResNet想到避免去学习该层恒等映射的参数，使用了如上图的结构，让h(x)=F(x)+x;这里的F(x)我们称作残差项，我们发现，要想让该冗余层能够恒等映射，我们只需要学习F(x)=0。学习F(x)=0比学习h(x)=x要简单，因为一般每层网络中的参数初始化偏向于0，这样在相比于更新该网络层的参数来学习h(x)=x，该冗余层学习F(x)=0的更新参数能够更快收敛。</p><h3 id="ResNet使用技巧"><a href="#ResNet使用技巧" class="headerlink" title="ResNet使用技巧"></a>ResNet使用技巧</h3><p>如果遇到了h(x) = F(x)+x中，F(x)和x维度不同，需要对x进行线性变换。可以使用zero-padding方法增加维度。或者采用一个新的1*1卷积映射但这样会增加参数。<br>ResNet由于自身网络结构原因，进行反向传播时的梯度恒大于1，解决了梯度消失问题。</p><h3 id="ResNet特点"><a href="#ResNet特点" class="headerlink" title="ResNet特点"></a>ResNet特点</h3><ul><li>由微软提出。ResNet的层数最多，训练用了8个GPU三周完成。</li><li>最大特性是允许原始输入信息传输到后面的层中，可以将一个卷积层的输出和输入相融合后再输入到下一个卷积层。这样使得后面的层可以直接学习残差，整个神经网络只需要学习输入输出差别的那部分，简化了学习目标和难度。</li></ul><h2 id="CNN知识点补充"><a href="#CNN知识点补充" class="headerlink" title="CNN知识点补充"></a>CNN知识点补充</h2><h3 id="为什么使用1-1大小的卷积核"><a href="#为什么使用1-1大小的卷积核" class="headerlink" title="为什么使用1*1大小的卷积核"></a>为什么使用1*1大小的卷积核</h3><ol><li>在大多数情况下它作用是升/降特征的维度，这里的维度指的是通道数（厚度），而不改变图片的宽和高。</li><li>用另一种角度去理解1*1卷积，可以把它看成是一种全连接。输入层大小等于输入通道数，输出层大小等于输出通道数。</li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/04/26/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BOpenAI%20Gym/"/>
      <url>/2018/04/26/Reinforce%20Learning/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BOpenAI%20Gym/</url>
      <content type="html"><![CDATA[<p>介绍OpenAI Gym的使用方法。</p><a id="more"></a><h2 id="OpenAI-Gym"><a href="#OpenAI-Gym" class="headerlink" title="OpenAI Gym"></a>OpenAI Gym</h2><p>OpenAI Gym 是一个用于开发和比较 RL 算法的工具包。现在主要支持的是 python 语言，以后将支<br>持其他语言。gym 文档 <a href="https://gym.openai.com/docs。" target="_blank" rel="noopener">https://gym.openai.com/docs。</a><br>Openai gym 包含 2 部分：<br>1、gym 开源库：包含一个测试问题集，每个问题成为环境（environment），可以用于自己的 RL 算法开发。这些环境有共享的接口，允许用户设计通用的算法。其包含了 deep mind 使用的 Atari 游戏测试床。<br>2、Openai gym 服务：提供一个站点和 api 允许用户对他们训练的算法进行性能比较。</p><h3 id="核心接口"><a href="#核心接口" class="headerlink" title="核心接口"></a>核心接口</h3><p>Gym提供了RL运行环境，同时提供了统一的API接口：</p><ol><li>reset(): 重置环境的状态，返回初始观察。</li><li>step(action): 根据输入action推进一个步长，返回observation，reward，done，info</li><li>render(mode=’human’, close=False): 重绘环境的一帧。<h3 id="构建第一个环境"><a href="#构建第一个环境" class="headerlink" title="构建第一个环境"></a>构建第一个环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import gym</span><br><span class="line">env = gym.make(&apos;CartPole-v0&apos;)</span><br><span class="line">env.reset()</span><br><span class="line"></span><br><span class="line">action_space = env.action_space</span><br><span class="line">observation_space = env.observation_space</span><br><span class="line">env_hight = env.observation_space.high</span><br><span class="line">env_low = env.observation_space.low</span><br><span class="line">observation_shape = env.observation_space.shape</span><br><span class="line"></span><br><span class="line">for _ in range(1000):</span><br><span class="line">    env.render()</span><br><span class="line">    action = env.action_space.sample()</span><br><span class="line">    observation, reward, done, info = env.step(action)</span><br><span class="line">    if done:</span><br><span class="line">        print(&apos;game finish&apos;)</span><br><span class="line">        break</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      <categories>
          
          <category> Reinforce Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforce Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之GBDT</title>
      <link href="/2018/04/22/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/"/>
      <url>/2018/04/22/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>Gradient Boosting主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。 <strong>GBDT树内部是多个回归CART树</strong><br>主要由三个概念组成：回归决策树，Gradient Boosting，Shrinkage。</p><h3 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h3><p>GBDT的核心在于每个决策树是对上一个决策树预测“误差”的学习，然后累加所有树的结果作为最终结果，而 <strong>分类树的结果累加是没有意义的， 所以GBDT中的树都是回归树，不是分类树</strong>。<br>GBDT调整后可以用于分类问题，但内部还是回归树。<br><strong>回归树在每个节点（不一定是叶子节点）都会得到一个预测值。</strong> 该预测值为属于这个节点所有人label值的平均值。构建回归树进行分割属性选择时按照回归树的属性选择方式。</p><h3 id="梯度迭代"><a href="#梯度迭代" class="headerlink" title="梯度迭代"></a>梯度迭代</h3><p><img src="/images/other_ML_knowledge/gbdt_alg.png" alt="GBDT"><br>GBDT学习过程如上图所示，主要流程是：</p><ol><li>初始化f0(x) = 0</li><li>对于m=1,2,..M<ol><li>计算之前所有树的残差</li><li>拟合残差学习一颗回归树（选用平方损失函数时，对其求偏导的负梯度等于残差近似值）</li><li>更新回归树集合（进行相加时通常会使用Shrinkage）</li></ol></li><li>M次迭代后，得到GBDT模型，为M个回归树的加性模型<br>从GBDT的算法过程可以发现 <strong>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，而当损失函数为平方损失函数时，梯度值正好等于这个残差值，也就是说残差向量都是它的全局最优方向，这就是 Gradient。</strong><br>比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。<h3 id="GBDT-工作实例"><a href="#GBDT-工作实例" class="headerlink" title="GBDT 工作实例"></a>GBDT 工作实例</h3>还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下所示结果：<br><img src="/images/other_ML_knowledge/decision_tree.jpg" alt="decision_tree"><br>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果：<br><img src="/images/other_ML_knowledge/boosting_tree.jpg" alt="decision_tree"><br>在GBDT的第一棵树中，(A,B)和(C,D)被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。<h3 id="GBDT优点"><a href="#GBDT优点" class="headerlink" title="GBDT优点"></a>GBDT优点</h3>正在上述例子中，决策树和GBDT可以达到同样的效果，而GBDT的优点就是减小 <strong>过拟合问题。</strong><br>图1为了达到100%精度使用了3个feature（上网时长、时段、网购金额），其中分枝“上网时长&gt;1.1h” 很显然已经过拟合了，这个数据集上A,B也许恰好A每天上网1.09h, B上网1.05小时，但用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的；<br>相对来说图2的boosting虽然用了两棵树 ，但其实只用了2个feature就搞定了，后一个feature是问答比例，显然图2的依据更靠谱。 <strong>Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。</strong><h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即<h4 id="没用Shrinkage"><a href="#没用Shrinkage" class="headerlink" title="没用Shrinkage"></a>没用Shrinkage</h4>y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) =  y真实值 - y(1 ~ i)<br>y(1 ~ i) = SUM(y1, …, yi)<h4 id="使用Shrinkage"><a href="#使用Shrinkage" class="headerlink" title="使用Shrinkage"></a>使用Shrinkage</h4>Shrinkage不改变第一个方程，只把第二个方程改为：<br>y(1 ~ i) = y(1 ~ i-1) + step * yi  </li></ol><p><strong>Shrinkage对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001，导致各个树的残差是渐变的而不是陡变的。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight</strong>。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。</p><h3 id="GBDT的适用范围"><a href="#GBDT的适用范围" class="headerlink" title="GBDT的适用范围"></a>GBDT的适用范围</h3><ul><li>GBDT 可以适用于回归问题（线性和非线性）；</li><li>GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。</li></ul><h3 id="GBDT用于分类"><a href="#GBDT用于分类" class="headerlink" title="GBDT用于分类"></a>GBDT用于分类</h3><p>首先，无论是回归任务还是分类任务，GBDT内部的决策树都是CART回归树。而由于GBDT是不断拟合上一棵树的残差，而在分类任务中类别的相减是没有意义的。所以GBDT应用于分类任务时，需要对模型进行一定的修改。</p><h4 id="为每一个类构建GBDT"><a href="#为每一个类构建GBDT" class="headerlink" title="为每一个类构建GBDT"></a>为每一个类构建GBDT</h4><p>GBDT用于分类任务的方法核心思想是：<strong>对每一个类都构建一个GBDT</strong>。<br>举个例子：分类任务一共有三个类，我们可以用one-hot-encoding来表示每个样本的类别，设样本x属于第二类，则x对应的label为：[0, 1, 0]。<br>针对这个有三个类的分类，实质上是在 <strong>每轮训练的时候同时训练三棵树，第一棵树针对样本x的第一类，输入为(x, 0)，第二棵树针对样本的第二类，输入为(x, 1)，第三棵树针对样本第三类，输入为(x, 0)</strong>，而这三棵树就将原问题转换为了三个回归问题，拟合的label分别为：0，1，0。<br>在测试阶段，将测试数据同时输入到三个GBDT中，得到三个预测值，然后可以通过softmax将其转换为对应每个label预测的概率值。</p><h3 id="GBDT和随机森林比较"><a href="#GBDT和随机森林比较" class="headerlink" title="GBDT和随机森林比较"></a>GBDT和随机森林比较</h3><h4 id="GBDT和随机森林的相同点"><a href="#GBDT和随机森林的相同点" class="headerlink" title="GBDT和随机森林的相同点"></a>GBDT和随机森林的相同点</h4><ul><li>都是由多棵树组成，最终的结果都由多棵树共同决定。<h4 id="GBDT和随机森林的不同点"><a href="#GBDT和随机森林的不同点" class="headerlink" title="GBDT和随机森林的不同点"></a>GBDT和随机森林的不同点</h4></li><li>组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；</li><li>组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；</li><li>对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；</li><li>随机森林对异常值不敏感，GBDT 对异常值非常敏感；</li><li>随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；<h3 id="GBDT可调参数"><a href="#GBDT可调参数" class="headerlink" title="GBDT可调参数"></a>GBDT可调参数</h3><h4 id="框架参数"><a href="#框架参数" class="headerlink" title="框架参数"></a>框架参数</h4></li></ul><ol><li>n_estimators：基学习器的个数</li><li>learning_rate: 即每个基学习器的缩减权重。</li><li>subsample：不放回子采样</li><li>init：初始弱学习器</li><li>loss：损失函数，对于分类模型可以选择对数损失和指数损失。对于回归模型有均方差，绝对损失等<h4 id="基学习器参数"><a href="#基学习器参数" class="headerlink" title="基学习器参数"></a>基学习器参数</h4></li><li>max_feature：划分时考虑的最大特征数（类似于随机森林的随机特征选择）</li><li>max_depth：决策树最大深度，可以不输入，但这样在构建子树时不会限制子树的深度。通常情况下推荐限制该值</li><li>min_sample_split：内部节点再划分所需要最小样本数</li><li>min_samples_leaf：叶节点最少样本数<br>…<h3 id="GBDT常见问题"><a href="#GBDT常见问题" class="headerlink" title="GBDT常见问题"></a>GBDT常见问题</h3></li><li>GBDT 相比于决策树有什么优点<br>泛化性能更好！GBDT 的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于 0。这样后面就更加专注于那些分错的样本。</li><li>Gradient 体现在哪里？<br>可以理解为残差是全局最优的绝对方向，类似于求梯度。<h3 id="GBDT用作特征选择"><a href="#GBDT用作特征选择" class="headerlink" title="GBDT用作特征选择"></a>GBDT用作特征选择</h3>GBDT用于特征选择时，主要通过计算特征j在单棵树中重要度的平均值：</li></ol><p><img src="/images/other_ML_knowledge/gbdt_feature_select.png" alt="feature_select"><br>其中，M是树的数量，L为树的叶子节点数量，L-1为树的非叶子节点数量，vt是和节点t相关联的特征，hat(ti2)是节点t分裂后平凡损失的减少值。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之xgboost</title>
      <link href="/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8Bxgboost/"/>
      <url>/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8Bxgboost/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h2 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h2><p>目前最好最快的boosting tree</p><p>如果不考虑工程实现、解决问题上的一些差异，xgboost与gbdt比较大的不同就是目标函数的定义。<br><img src="/images/other_ML_knowledge/xgboost_fomula.jpg" alt="xgboost_fomula"><br>注：红色箭头指向的l即为损失函数；红色方框为正则项，包括L1、L2正则项；红色圆圈为常数项。xgboost将损失函数做二阶泰勒展开做一个近似，我们可以看到，最终的目标函数只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。</p><h3 id="原理详解"><a href="#原理详解" class="headerlink" title="原理详解"></a>原理详解</h3><p><img src="/images/other_ML_knowledge/xgboost_math1.jpg" alt="gbdt_math"><br>对GBDT的目标函数（平方损失）进行泰勒2阶展开，得到如上图推导过程。相当于对每个叶节点的数据计算一阶二阶导数。</p><p><img src="/images/other_ML_knowledge/xgboost_math2.png" alt="gbdt_math2"><br>如上图所示，然后obj(t)对w求偏导得0，最后得到obj(t)的表示。wi为当前树对数据i的预测值。<br>推导上述目标函数公式后，单棵决策树的学习过程如下：</p><ol><li>枚举所有可能的树结构q（<strong>xgboost对该步骤进行了并行</strong>）</li><li>用上述目标函数为每个q计算目标函数分数，分数越小说明对应的树结构越好</li><li>根据上一步的结果，找到最佳树结构，并计算每个叶节点的预测值wj<h3 id="xgboost的并行"><a href="#xgboost的并行" class="headerlink" title="xgboost的并行"></a>xgboost的并行</h3>xgboost支持并行。xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<h3 id="xgboost对比GBDT"><a href="#xgboost对比GBDT" class="headerlink" title="xgboost对比GBDT"></a>xgboost对比GBDT</h3></li></ol><ul><li>GBDT缺点明显：boost是一个串行过程，不好并行化，计算复杂度高。而XGB在每一个基决策树的构造时并行遍历所有可能的树，计算obj(t)的值找到最有树。</li><li>GBDT是直接拟合负梯度（等于残差），xgboost对目标函数进行泰勒展开，引入二阶导。</li><li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</li><li>两者都用到了Shrinkage（缩减），</li><li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。<h3 id="xgboost进行特征选择"><a href="#xgboost进行特征选择" class="headerlink" title="xgboost进行特征选择"></a>xgboost进行特征选择</h3>XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+…)。</li></ul><h3 id="xgboost流程"><a href="#xgboost流程" class="headerlink" title="xgboost流程"></a>xgboost流程</h3><ol><li>对于当前树，枚举所有可能的树结构</li><li>对于每个树结构计算目标损失函数</li><li>找到损失函数最小的树，作为本轮的最佳树结构<br>由于可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。</li><li>从深度为 0 的树开始，对每个叶节点枚举所有的可用特征</li><li>针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）</li><li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集</li><li>回到第 1 步，递归执行到满足特定条件为止</li></ol><h3 id="分裂收益计算"><a href="#分裂收益计算" class="headerlink" title="分裂收益计算"></a>分裂收益计算</h3><p>如何计算每次分裂的收益呢？假设当前节点记为 C,分裂之后左孩子节点记为 L，右孩子节点记为 R，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：</p><blockquote><p>Gain = Objc = Objl - Objr</p></blockquote><h3 id="xgboost总结"><a href="#xgboost总结" class="headerlink" title="xgboost总结"></a>xgboost总结</h3><ol><li>算法每次迭代生成一颗新的决策树 ;</li><li>在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数 ;</li><li>通过贪心策略生成新的决策树，通过等式(7) 计算每个叶节点对应的预测值4. 把新生成的决策树添加到模型中</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树学习算法总结之决策树</title>
      <link href="/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/04/21/Machine%20Learning/%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>在机器学习中，决策树扮演了很重要的决策，不仅仅是传统的决策树，bagging集成方法的代表随机森林以及Boosting集成方法Adaboot，GBDT，xgboost都是基于决策树。决策树广泛的应用于机器学习的各个领域并且取得了良好的效果。本文就是对整个决策树体系的整理：从决策树到xgboost</p><a id="more"></a><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><ul><li>优点：计算量小，可解释性强，比较适合处理有缺失值的样本，能够处理不相关特征</li><li>容易过拟合（集成树方法减少了过拟合现象）<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3></li><li>信息熵的计算</li><li>信息熵值越小，纯度越高。<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3></li><li>信息增益的计算</li><li>一般而言，信息增益越大，意味着使用属性a来进行划分所获得的“纯度提升”越大。</li><li>缺点：信息增益倾向于选择可取值数目较多的属性。</li><li>ID3决策树生成算法采用信息增益生成决策树（见P75）</li><li>基于信息增益的划分法会偏向选取取值多的属性（如果用ID进行划分，也就是每个分治只有一个样本，每个分治纯度最大，但这样显然不具备泛化能力）<h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3></li><li>增益率为：分割属性的信息增益/分割属性的信息熵</li><li>缺点：增益率倾向于选择取值数目较少的属性</li><li>为克服基于信息增益的算法偏向选择属性多的缺点，C4.5基于信息增益率生成决策树</li><li>增益率准则倾向选择取值少的属性，因此C4.5算法并不是简单选用增益率划分，而是使用了一个启发式方法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<h3 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h3></li><li>CART树使用基尼系数进行属性划分</li><li>如果是二分类问题，基尼系数为：Gini(p) = 2p(1-p)</li><li>直观来说，基尼系数反应了从数据集随机抽取两个样本，其类别标记不一致的概率，因此Gini系数越小，数据集的纯度越高。</li></ul><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><ul><li>决策树剪枝的基本策略有“预剪枝”和“后剪枝”。<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3></li></ul><ol><li>预剪枝是指在决策树生成过程中，首先从数据集中分割出验证集，然后对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；<strong>比较划分前后在验证集的精度是否提高</strong></li><li>预剪枝有欠拟合风险<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3></li><li>后剪枝则是先训练一颗完整的决策树，然后自底向上的对非叶节点进行考察，若将该节点对应的子节点替换为叶节点能带来决策树泛化性能的提升（在验证集上准确率提高），则将该子树替换为叶节点。 <strong>比较划分前后在验证集的精度是否提高</strong></li><li>后剪枝也可以通过构建决策树loss function，最小化loss实现。基于损失函数的后剪枝算法见《统计学习方法》</li><li>后剪枝欠拟合风险小，泛化性能优于预剪枝，但需要先生成再剪枝，开销大。</li></ol><h2 id="连续值和缺失值"><a href="#连续值和缺失值" class="headerlink" title="连续值和缺失值"></a>连续值和缺失值</h2><h3 id="连续值"><a href="#连续值" class="headerlink" title="连续值"></a>连续值</h3><ul><li>对于取值为连续值的属性，考察该属性的所有取值，并从小到大排列。分别以任意相邻的取值的中值作为分割点，计算信息增益，并选取信息增益最大的点作为分割点。（其中可以将原节点的信息熵改为期望，因为每个取值的概率都为1/n） （具体公式见书84页）</li><li>对于分类树，直接按照该连续属性取值的切割点，将该点的数据分为两部分，然后计算各个切割方案的信息增益。</li><li>对于回归树的连续型属性，考察该属性的所有取值，分别以任意相邻节点取值的中值作为分割点，<strong>计算该节点输出c和标签y的平方损失</strong>，找到平方损失最小的连续型属性和该属性的分割节点。</li><li>与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3>三种情况：</li></ul><ol><li>如何在属性值缺失的情况下进行划分属性的选择（如何重构信息增益）<br>假如使用ID3算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的熵增。然后结果乘0.9（新样本占raw样本的比例），就是a属性最终的熵。<strong>可以理解为对各个属性的信息增益进行加权计算</strong></li><li>分类属性选择完成，对分类属性缺失的样本，如何进行划分？<br>比如该节点是根据a属性划分，但是待分类样本a属性缺失，就把该样本分配到两个子节点中去，但是权重由1变为（每个子节点中实例数目/父节点实例数目），直观的看，就是让同一个样本以不同的概率划入到不同的子节点中去。在子节点的继续分裂时，计算比例时缺失值的使用权重而不是为1.</li><li>训练完成，给测试集样本分类，有缺失值怎么办？<br>这时候不能按比例分配，因为必须给该样本一个确定的label。这时候可以 <strong>根据投票来确定，或者填充缺失值。</strong></li></ol><h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><ul><li>通常决策树相当于用多个平行于轴线的线段对特征空间进行分割。这样的分类边界使得学习结果具有较好的可解释性，因为每一段划分都直接对应了某个属性的取值，<strong>但在分类边界比较复杂时，对于同一个属性，必须使用很多段划分才能获得较好的近似，此时决策树会相当复杂</strong>。</li><li><strong>若能使用斜的划分边界，则决策树模型会大大简化</strong>，“多变量决策树” 就是能实现这样斜划分甚至更复杂划分的决策树。</li><li>以斜划分为例，在此类决策树中，<strong>非节点不再是仅针对某个属性，而是对属性的线性组合进行测试，换言之，每个非叶节点是一个形如 wa=t 的线性分类器，w表示属性a的权重，w和t可以在节点所含的样本集和属性集上学得</strong></li><li>于是，与传统的单变量决策树不同，在多变量决策树的学习过程中，不是为每个节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</li></ul><h2 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h2><p>CART树是分类决策树的缩写，有如下特点：</p><ol><li>一定是二叉树</li><li>既可用于分类也可以用于回归</li><li>对于分类树，使用 <strong>基尼指数</strong> 作为划分标准</li><li>对于回归树，使用 <strong>平方损失最小</strong> 作为划分标准，叶节点的预测值为叶节点所有label的平均值。</li><li>CART剪枝方法：从生成的决策树上不断剪枝，直到根节点，形成一个子树序列。然后用交叉验证集对每个子树进行验证，找出泛化误差最小的子树。</li></ol><h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p>Adaboost本质上是一个加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类算法。<br>基本思想是不断调整所有训练数据的训练权重，增加上一个基学习器分类错误数据的权重。<br>主要步骤如下：</p><ol><li>初始化训练数据的权重分布为1/n</li><li>训练一个基分类器，对所有数据进行分类</li><li>根据每个数据的分类结果和权重，计算分类误差率。</li><li>计算当前基分类器的权重：’a = 1/2 log((1-e)/e)’ (误差率越小，a越大)</li><li>更新所有数据的权重</li><li>重复2~5步，直至模型收敛。</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop入门</title>
      <link href="/2018/04/18/Hadoop/Hadoop%E5%85%A5%E9%97%A8/"/>
      <url>/2018/04/18/Hadoop/Hadoop%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<p>Hadoop如今已经成为了大数据处理的代名词，无论是云计算，机器学习还是后端开发都离不开大数据的支持，这篇文章就是我整理的关于Hadoop的入门概念。<br><a id="more"></a><br>Hadoop是一个分布式数据存储和分析系统，存储是由HDFS实现，分析是由MapReduce实现。纵然Hadoop还有其他功能，但这些功能是它的核心所在。</p><h4 id="大数据的三个‘V’："><a href="#大数据的三个‘V’：" class="headerlink" title="大数据的三个‘V’："></a>大数据的三个‘V’：</h4><ul><li>volume 数据量非常大</li><li>variety 数据有各种来源，也就导致了数据以各种形式存储</li><li>velocity 系统可能需要以非常快的速度接受数据，所以存储速度和处理速度都非常重要</li></ul><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>MapReduce是一种用于数据处理的编程模型，MapReduce支持多种编程语言，MapReduce本质上是并行的。</p><h3 id="map和reduce"><a href="#map和reduce" class="headerlink" title="map和reduce"></a>map和reduce</h3><p>MapReduce的工作过程分为两个阶段：<strong>map阶段和reduce阶段</strong>，每个阶段都有键值对作为输入和输出，并且他们的类型可以由程序员选择，程序员还需要定义两个函数map函数和reduce函数。</p><p>用一个列子说明MapReduce的工作过程：从海量的天气数据中找到气温最高的记录。<br>运行过程如下：</p><ol><li>在map阶段输入原始数据，对数据进行处理，使得reduce可以在此基础上进行工作：找出每年的最高气温。在map过程中，将筛选掉缺失的，不可靠的或者错误的气温数据。</li><li>map经过对数据的处理，输出（年份，气温）键值对。</li><li>map函数的输出先由MapReduce框架处理，然后再发送到reduce函数，这一处理过程根据键值对进行排序和分组。</li><li>reduce函数会看到（年份，[气温1， 气温2，。。。]）的输入，每个年份后都跟着一系列的气温。</li><li>reduce函数重复这个列表并找出每年中最高温度。<br><img src="iamges/hadoop_introduce/example1.png" alt="temperature"><h3 id="Java-MapReduce"><a href="#Java-MapReduce" class="headerlink" title="Java MapReduce"></a>Java MapReduce</h3>在理解MapReduce工作流程后，尝试使用Java来实现这个流程，我们需要实现三样东西：map函数，reduce函数以及一些运行作业的代码。map函数时由一个Mapper接口来实现的，声明了一个map()方法。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperatureMapper extends MapReduceBase</span><br><span class="line">    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    private static final int MISSING = 9999;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void map(LongWritable key, Text value, Context context) throws IOException&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String year = line.substring(15, 19);</span><br><span class="line">        int airTemperature;</span><br><span class="line">        if (line.charAt(87) == &apos;+&apos;) //parseInt doesnot like leading plus signs</span><br><span class="line">            airTemperature = Integer.parseInt(line.substring(88, 92));</span><br><span class="line">        else</span><br><span class="line">            airTemperature = Integer.parseInt(line.substring(87, 92));</span><br><span class="line">        String quality = line.substring(92, 93);</span><br><span class="line">        if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;))</span><br><span class="line">            context.write(new Text(year), new IntWritable(airTemperature));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>Mapper是一个泛型类型，有四个参数，分别为输入键，输入值，输出键和输出值的类型<br>map方法需要传入一个键一个值。我们将Text转换成Java的String类型，然后利用substring提取感兴趣的列<br>map方法还提供了一个OutputCollector实例来写入输出内容。<br>Hadoop 规定了一套可用于网络序列优化的基本类型，不同于Java内置类型，可以在org.apache.hadoop.io包找到<br>LongWritable相当于Java的Long型，Text相当于String，IntWritable相当于Integer</p><p>接下来reduce方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void reduce(Text key, Interator&lt;IntWritable&gt; values, Context context)</span><br><span class="line">        throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        int maxValue = Integer.MIN_VALUE;</span><br><span class="line">        while(values.hasNext())&#123;</span><br><span class="line">            maxValue = Math.max(maxValue, values.next().get());</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, new IntWritable(maxValue));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后是主函数，负责设定和启动整个MapReduce<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MaxTemperature &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        if (args.length != 2)&#123;</span><br><span class="line">            System.err.println(&quot;Usage: MaxTemperature &lt;input path&gt;</span><br><span class="line">                &lt;output path&gt;&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = new Job();</span><br><span class="line">        //JobConf对象指定了作业执行规范，授予你对整个作业如何运行的控制权</span><br><span class="line">        job.setJarByClass(MaxTemperature.class);</span><br><span class="line">        job.setJobName(&quot;Max temperature&quot;);</span><br><span class="line"></span><br><span class="line">        //指定文件的输入和输出路径,可以多次调用addInputPath添加多路径，addOutputPath只能有一个</span><br><span class="line">        FileInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.addOutputPath(job, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        //指定要使用的map和reduce类</span><br><span class="line">        job.setMapperClass(MaxTemperatureMapper.class);</span><br><span class="line">        job.setReduceClass(MaxTemperatureReducer.class);</span><br><span class="line"></span><br><span class="line">        //指定map和reduce函数的输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0:1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>具体解释已经在代码中注明。<br>在Hadoop上运行这个作业时，要把代码打包成一个JAR文件，不必明确指定JAR文件的名称，在Job对象setJarByClass()方法中传递一个类即可，Hadoop利用这个类来查找包含它的JAR文件。</p><h4 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h4><ol><li>客户端提交编写好的MapReduce代码到Hadoop，Hadoop会先到HDFS中查看目标文件的大小，了解要获取数据的规模，然后形成任务分配的规划，例如：<blockquote><p>a.txt 0-128M交给一个task，128-256M 交给一个task，b.txt 0-128M交给一个task，128-256M交给一个task …，形成规划文件job.split。</p></blockquote></li></ol><p>然后把规划文件job.split、jar、配置文件xml提交给yarn（Hadoop集群资源管理器，负责为任务分配合适的服务器资源）</p><ol start="2"><li>启动appmaster<br>appmaster是本次job的主管，负责maptask和reducetask的启动、监控、协调管理工作。<br>yarn找一个合适的服务器来启动appmaster，并把job.split、jar、xml交给它。</li><li>启动maptask<br>Appmaster启动后，根据固化文件job.split中的分片信息启动maptask，<strong>一个分片对应一个maptask。</strong><br>分配maptask时，会尽量让maptask在目标数据所在的datanode上执行。</li><li>执行maptask<br>maptask会一行行地读目标文件，交给我们写的map程序，读一行就调一次map方法进行数据统计处理。<br>然后map调用context.write把处理结果写出去，保存到本机的一个结果文件，这个文件中的内容是分区且有序的。<br>分区的作用就是定义哪些key在一组，一个分区对应一个reducer。</li><li>启动reducetask<br>maptask都运行结束后，appmaster再启动reducetask，maptask的结果中有几个分区，就启动几个reducetask。</li><li>执行reducetask<br>reducetask读取maptask的结果文件自己对应的需要处理的数据。reducetask把读取的数据按照key组织好，传给reduce方法进行处理，最后把处理结果写到指定的输出路径中。</li></ol><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>这里我们只是使用MapReduce，现在我们需要把数据存在HDFS上，由此允许Hadoop将MapReduce计算转移到存储有部分数据的各台机器上。</p><h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h3><p>MapReduce是在客户端需要执行的一个工作单元，包括多个任务。</p><p>有两类节点控制MapReduce作业的执行过程：一个jobtracker一个tasktracker。前者通过调度后者上运行的任务来协调所有运行在系统上的作业。后者在运行任务的同时将运行进度报告发给jobtracker，jobtracker由此记录每项作业任务的整体进度情况。如果其中一个任务失败，jobtrracker可以在另一个tasktracker节点上重启该任务。</p><h4 id="map过程"><a href="#map过程" class="headerlink" title="map过程"></a>map过程</h4><p><strong>Hadoop将MaprReduce的输入数据划分成等长的数据块，称为‘分片’。Hadoop为每一个分片构建一个map任务，并由该任务来运行用户自定义的map函数从而处理分片中的任务。 因为Hadoop并行运行多个map处理每个分片，整体时间花费较小</strong></p><p>一个合理的分片大小趋向于HDFS的一个块大小，默认是64MB。Hadoop在存储有输入数据的节点上运行map任务，可以获得最佳性能，因为它不需要集群带宽资源。</p><p>map任务将其输出写入本地硬盘，而非HDFS，因为map的输出是中间结果，该结果还需要由reduce任务处理才产生最终结果，而一旦整体任务完成，中间结果就可以被删除，所以存在HDFS上有点小题大做。</p><h4 id="reduce过程"><a href="#reduce过程" class="headerlink" title="reduce过程"></a>reduce过程</h4><p>reduce的输入往往来自所有mapper的输出。因此，排过序的map输出需要通过网络传输发送到运行reduce任务的节点，数据再reduce端合并，然后由用户定义的reduce函数处理。reduce的输出通常存储在HDFS中。<br><img src="/images/hadoop_introduce/mapreduce_flow.png" alt="mapreduce_flow"><br>上图是整个MapReduce的数据流。<br>reduce任务的数量并非由输入数据大小决定，是需要独立指定的。</p><p>如果有多个reduce任务，map任务会针对输出进行分区（partition），即为每个reduce任务建一个分区。分区由用户定义的partition函数控制。<br><img src="/images/hadoop_introduce/mapreduce_flow2.png" alt="mapreduce_flow"></p><h3 id="combiner函数"><a href="#combiner函数" class="headerlink" title="combiner函数"></a>combiner函数</h3><p>Hadoop允许用户对map任务的输出指定一个combiner函数，该函数的输出作为reduce的输入。由于combiner属于优化方案，所以Hadoop无法确定map任务输出记录调用多少次combiner。</p><p>我们可以理解combiner函数是对map函数输出数据的进一步处理，通过这种方法优化map和reduce任务之间的数据传递效率。<br>我们可以再之前的程序中修改Job的设置来指定combiner类<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setJarByClass(MaxTemperatureWithCombiner.class);</span><br><span class="line">job.setCombinerClass(MaxTemperatureReducer.class);</span><br></pre></td></tr></table></figure></p><h1 id="Hadoop分布式文件系统"><a href="#Hadoop分布式文件系统" class="headerlink" title="Hadoop分布式文件系统"></a>Hadoop分布式文件系统</h1><h2 id="HDFS概念"><a href="#HDFS概念" class="headerlink" title="HDFS概念"></a>HDFS概念</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>HDFS定义每个数据块大小为64MB，HDFS上的文件被划分为块大小的多个分块(chunk)，作为独立的存储单元。<br>这么做的优点是文件的所有块并不需要存在同一个磁盘上，可以利用集群上任意个磁盘进行存储。</p><h3 id="NameNode和DataNode"><a href="#NameNode和DataNode" class="headerlink" title="NameNode和DataNode"></a>NameNode和DataNode</h3><p>HDFS集群有两类节点以 <strong>一个管理者(NameNode)多个工作者(DataNode)模式运行。</strong></p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode管理文件系统的命名空间，维护者文件系统树及整棵树内所有文件和目录。同时NameNode也记录着每个文件中各个块所在数据节点信息。<br>NameNode在内存中保存文件系统中每个文件和数据块的引用关系，如果NameNode损坏，意味着我们失去了整个HDFS的存储信息。<br>如果集群过大，NameNode会成为限制系统横向扩展的瓶颈，Hadoop引入联邦HDFS允许系统通过添加NameNode实现扩展，每个NameNode管理HDFS中的一部分。</p><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode是HDFS的工作节点，它们根据需要存储并检索数据块并定期向NameNode发送它们所存储块的列表。</p><h3 id="通过FileSystem-API读取数据"><a href="#通过FileSystem-API读取数据" class="headerlink" title="通过FileSystem API读取数据"></a>通过FileSystem API读取数据</h3><h3 id=""><a href="#" class="headerlink" title="???"></a>???</h3><p>Hadoop文件系统通过Hadoop Path对象来表示文件，可以将路径视为一个Hadoop文件系统的URI，如’hdfs://localhost/user/tom/text.txt’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class FileSystemCat&#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String uri = args[0];</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        FIleSystem fs = FileSystem.get(URI.create(uri), conf);</span><br><span class="line">        InputStream in = null;</span><br><span class="line">        try &#123;</span><br><span class="line">            in = fs.open(new Path(uri));</span><br><span class="line">            IOUtils.copyBytes(in, System.out, 4096, false);</span><br><span class="line">        &#125;finally&#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow之常见debug方法</title>
      <link href="/2018/04/18/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8B%E5%B8%B8%E8%A7%81debug%E6%96%B9%E6%B3%95/"/>
      <url>/2018/04/18/Deep%20Learning/TensorFlow/TensorFlow%E4%B9%8B%E5%B8%B8%E8%A7%81debug%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>本文总结常见TensorFlow的debug方法。<br><a id="more"></a></p><h2 id="打印信息"><a href="#打印信息" class="headerlink" title="打印信息"></a>打印信息</h2><p>将每个tensor中的信息打印出来，进而判断bug位置是最直接最常见的方法。然而，由于TensorFlow是基于计算图模型的，直接print对应tensor只能得到tensor的基本信息，不能得到tensor的值。</p><h3 id="sess-run-tensor"><a href="#sess-run-tensor" class="headerlink" title="sess.run(tensor)"></a>sess.run(tensor)</h3><p>最直接的打印方法是用Session运行要打印的tensor，然后print返回值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = tf.constant(1.0)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">res = sess.run(t1)</span><br><span class="line">print(res.shape)</span><br></pre></td></tr></table></figure></p><h3 id="tensor-get-shape"><a href="#tensor-get-shape" class="headerlink" title="tensor.get_shape()"></a>tensor.get_shape()</h3><p>用Session运行对应的tensor然后再打印该tensor的shape很麻烦，TensorFlow中，tensor的内置函数get_shape()可以在计算图构建时直接打印tensor的shape。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1 = tf.constant(1.0)</span><br><span class="line">shape1 = t1.get_shape()</span><br><span class="line">print(shape1)</span><br></pre></td></tr></table></figure></p><h3 id="tf-Print-函数"><a href="#tf-Print-函数" class="headerlink" title="tf.Print()函数"></a>tf.Print()函数</h3><p>tf.Print实际上是TensorFlow的节点，有两个参数：要复制的节点和要输出的内容列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Print(input, data, message=None, first_n=None, summarize=None, name=None)</span><br></pre></td></tr></table></figure></p><ul><li>input是需要打印的变量的名字</li><li>data要求是一个list，里面包含要打印的内容。</li><li>message是需要输出的错误信息</li><li>first_n指只记录前n次</li><li>summarize是对每个tensor只打印的条目数量，如果是None，对于每个输入tensor只打印3个元素</li><li>name是op的名字<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x=tf.constant([2,3,4,5])</span><br><span class="line">x=tf.Print(x,[x,x.shape,&apos;test&apos;, x],message=&apos;Debug message:&apos;,summarize=100)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(x)</span><br><span class="line"></span><br><span class="line">#Debug message:[2 3 4 5][4][test][2 3 4 5]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x=tf.Print(x,[x,x.shape,&apos;test&apos;, x],message=&apos;Debug message:&apos;,summarize=2)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(x)</span><br><span class="line"></span><br><span class="line">#Debug message:[2 3...][4][test][2 3...]</span><br></pre></td></tr></table></figure></li></ul><h3 id="tf-Assert-函数"><a href="#tf-Assert-函数" class="headerlink" title="tf.Assert()函数"></a>tf.Assert()函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Assert(condition, data, summarize=None, name=None)</span><br></pre></td></tr></table></figure><p>函数在condition为False时会输出data的值，并抛出异常。summarize决定输出多少变量值。</p><h3 id="获取每一层的信息"><a href="#获取每一层的信息" class="headerlink" title="获取每一层的信息"></a>获取每一层的信息</h3><p>通常，我们用一个函数构建一个多层的神经网络并返回最后的输出值，而如果想要得到神经网络每一行的信息的话，就需要return每个网络层。这样会让函数变得非常冗杂。<strong>可以通过传入一个字典到函数中，并将每层的信息保存在该字典中。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def alexnet(x, net=&#123;&#125;):</span><br><span class="line">    net[&apos;conv1&apos;] = tf.conv2d(..)</span><br><span class="line">    net[&apos;pool1&apos;] = tf.max_pool(..)</span><br><span class="line">    ...</span><br><span class="line">    net[&apos;output&apos;] = tf.softmax()</span><br><span class="line">    return net[&apos;output&apos;]</span><br><span class="line"></span><br><span class="line">net = &#123;&#125;</span><br><span class="line">output = alexnet(x, net)</span><br></pre></td></tr></table></figure></p><p>通过这种方法就可以得到网络每层的信息。</p>]]></content>
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sklearn学习笔记</title>
      <link href="/2018/04/16/Machine%20Learning/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/16/Machine%20Learning/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>Sklearn是机器学习领域最知名的python模块之一，广泛的应用到各种机器学习项目中。Sklearn使用方便，大多数ML算法的调用形式相同，极其易于入门。</p><a id="more"></a><p><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">http://scikit-learn.org/stable/</a><br>Sklearn包含了多种机器学习方式：</p><ul><li>Classification分类</li><li>Regression回归</li><li>Clustering非监督聚类</li><li>Dimensionality reduction数据降维</li><li>Preprocessing数据预处理</li></ul><h2 id="Sklearn安装"><a href="#Sklearn安装" class="headerlink" title="Sklearn安装"></a>Sklearn安装</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>Linux可以直接使用pip进行安装，方便快捷。需要说明的是，sklearn需要Numpy和Scipy模块的支持。也就是说，在安装sklearn前，需要确定电脑已经安装了依赖模块。</p><h3 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h3><p>windows安装建议通过Anaconda来安装所有科学计算需要的模块，方便快捷。</p><h2 id="Sklearn模型选择"><a href="#Sklearn模型选择" class="headerlink" title="Sklearn模型选择"></a>Sklearn模型选择</h2><p>在官网上，有一个机器学习模型选择流程图，在进行机器学习模型选择时可以用作参考：<br><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a></p><h2 id="Sklearn初探"><a href="#Sklearn初探" class="headerlink" title="Sklearn初探"></a>Sklearn初探</h2><p>Sklearn把所有及其模型的使用模式整合在了一起，所有模型的调用方式都是一样的。也就是说，学会了一种机器学习模型的使用方法，也就掌握了所有模型的使用方式。</p><h3 id="K近邻方法"><a href="#K近邻方法" class="headerlink" title="K近邻方法"></a>K近邻方法</h3><p>让我们使用Sklearn自带的数据集，使用k近邻算法，探索Sklearn模块的使用模式：</p><ol><li><p>首先导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br></pre></td></tr></table></figure></li><li><p>载入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_X = iris.data</span><br><span class="line">iris_y = iris.target</span><br><span class="line"></span><br><span class="line">print(iris_X[:3,:])</span><br><span class="line">print(iris_y[:3])</span><br></pre></td></tr></table></figure></li><li><p>训练数据集测试数据集分离并训练K近邻模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)</span><br><span class="line"></span><br><span class="line">kNN = KNeighborsClassifier()</span><br><span class="line">kNN.fit(X_train, y_train)</span><br></pre></td></tr></table></figure></li><li><p>预测和评分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prediction = kNN.predict(X_test)</span><br><span class="line">score = kNN.score(X_test, y_test)#分类准确率</span><br><span class="line">probability = kNN.predict_proba(X_test)</span><br><span class="line">neighborpoint=knn.kneighbors(iris_x_test[-1],5,False)</span><br><span class="line">#计算与最后一个测试样本距离在最近的5个点，返回的是这些样本的序号组成的数组</span><br></pre></td></tr></table></figure></li></ol><h2 id="模型常用属性和功能"><a href="#模型常用属性和功能" class="headerlink" title="模型常用属性和功能"></a>模型常用属性和功能</h2><h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><p>如上文代码所示：</p><ul><li>model.fit(x_train, y_labels)可以对模型进行训练</li><li>model.predict(x_test)使用训练好的模型进行预测</li><li>每个模型在训练和预测时，都有非常多的参数可以调整，对于新手来说，可以直接使用默认参数。当熟练掌握模型后，可以通过对参数的调整来优化模型。</li></ul><h3 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h3><p>Sklearn对每个模型都提供方法，用以取出模型的具体参数信息：</p><ul><li>model.coef_, model.intercept_就属于model的属性。例如对线性回归来说，这两个参数分别代表模型的斜率和截距。</li><li>model.get_params()函数可以返回在模型定义时，定义的参数。</li></ul><h3 id="预测评分"><a href="#预测评分" class="headerlink" title="预测评分"></a>预测评分</h3><ul><li>model.score(x_test, y_test)可以输出模型对测试集合的预测评分。</li></ul><p>以上就是最常见的模型使用方法：1.创建模型 2.训练模型 3.预测/测试模型</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>Sklearn提供便捷强大的数据预处理模块，方便工程师对数据进行快速处理</p><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line">data = preprocessing.scale(data)</span><br></pre></td></tr></table></figure><h2 id="常用机器学习模型"><a href="#常用机器学习模型" class="headerlink" title="常用机器学习模型"></a>常用机器学习模型</h2><h3 id="k近邻"><a href="#k近邻" class="headerlink" title="k近邻"></a>k近邻</h3><p>sklearn.neighbors封装了所有与k近邻相关的算法模型，包括非监督kNN，分类kNN，回归kNN，以及kNN等几个实现算法等</p><h4 id="常用参数："><a href="#常用参数：" class="headerlink" title="常用参数："></a>常用参数：</h4><ul><li>n_neighbors：kNN中的k值</li><li>radius：限定半径最近邻中的半径</li><li>algorithm：实现算法，提供’ball_tree’, ‘kd_tree’, ‘auto’, ‘brute’</li><li>metric：距离度量方法，提供’euclidean’, ‘manhattan’, ‘chebyshev’, ‘minkowski’等，一般使用默认欧式距离<h4 id="非监督k近邻"><a href="#非监督k近邻" class="headerlink" title="非监督k近邻"></a>非监督k近邻</h4>‘from sklearn.neighbors import NearestNeighbors’</li></ul><h4 id="分类kNN"><a href="#分类kNN" class="headerlink" title="分类kNN"></a>分类kNN</h4><p>‘from sklearn.neighbors import KNeighborsClassifier’</p><h4 id="回归kNN"><a href="#回归kNN" class="headerlink" title="回归kNN"></a>回归kNN</h4><p>‘from sklearn.neighbors import KNeighborsRegressor’</p><h4 id="实现算法"><a href="#实现算法" class="headerlink" title="实现算法"></a>实现算法</h4><p>‘from sklearn.neighbors import KDTree’<br>‘from sklearn.neighbors import BallTree’</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python装饰器</title>
      <link href="/2018/04/12/Programming%20Language/Python/Python%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
      <url>/2018/04/12/Programming%20Language/Python/Python%E8%A3%85%E9%A5%B0%E5%99%A8/</url>
      <content type="html"><![CDATA[<p>python装饰器类似于java中的面向切面编程，是python的高级语法， 可以将各个函数中重复的操作（插入日志，性能测试，事务处理，缓存等）抽离出来，将这些于函数原本功能无关的雷同代码定义在装饰器中，概括的讲： <strong>装饰器的作用就是为已经存在的对象添加额外的功能</strong><br><a id="more"></a></p><h2 id="装饰器简单应用"><a href="#装饰器简单应用" class="headerlink" title="装饰器简单应用"></a>装饰器简单应用</h2><p>看如下代码：decorator函数即定义一个装饰器，foo为一个常规的函数。该代码即为用decorator对f函数进行装饰。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper():</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func()</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">def func():</span><br><span class="line">print(&apos;i am a normal function&apos;)</span><br><span class="line"></span><br><span class="line">foo = decorator(func)</span><br><span class="line">foo()</span><br><span class="line"># 调用装饰过得foo函数会首先运行wrapper()中的内容，然后返回foo函数</span><br></pre></td></tr></table></figure></p><h2 id="装饰器的语法糖"><a href="#装饰器的语法糖" class="headerlink" title="装饰器的语法糖"></a>装饰器的语法糖</h2><p>在上面对装饰器的简单实用中，我们发现每次想要对函数进行装饰时，都需要进行一步赋值操作 ‘foo = decorator(foo)’，这种操作让程序变得更为复杂难以调试，所以我们可以使用语法糖完成对函数进行装饰的过程。</p><p>将装饰器的语法糖‘@’放到函数定义的地方，这样就可以省略最后一步的赋值操作。可以将上面的例子改写为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper():</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func()</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo():</span><br><span class="line">print(&apos;i am a normal func&apos;)</span><br><span class="line"></span><br><span class="line">foo()</span><br></pre></td></tr></table></figure></p><p>如上所示，使用语法糖，我们就可以省略最后一步的赋值操作。foo函数不需要作任何修改，只需要在定义函数的地方加上装饰器的语法糖，调用的时候还是和以前一样。</p><h2 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h2><p>以上只是对装饰器最简单的应用，在现实应用中，业务逻辑函数会更加复杂，如果业务逻辑函数foo需要参数，这样在装饰器中我们似乎是需要修改参数，但通过在装饰器定义可变参数，可以更加便捷的实现该功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper(*args):</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func(*args)</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo(name, age)</span><br><span class="line">  print(&apos;....&apos;)</span><br></pre></td></tr></table></figure></p><p>这样以来，不管业务函数需要多少个参数，我们都不需要对装饰器进行过多的更改。<br>同时，如果在业务逻辑函数中存在指定关键字参数。我们可以在wrapper中定义指定关键字参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">def wrapper(*args, **kargs):</span><br><span class="line">print(&apos;this is a decorator&apos;)</span><br><span class="line">return func(*args, **kargs)</span><br><span class="line">return wrapper</span><br><span class="line"></span><br><span class="line">@decorator</span><br><span class="line">def foo(name, age, height=None)</span><br><span class="line">  print(&apos;....&apos;)</span><br></pre></td></tr></table></figure></p><h2 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h2><p>在装饰器中，如果对传入的不同业务逻辑函数需要作不同的装饰，可以在业务逻辑函数定义使用装饰器语法糖时指定参数，从而达到在装饰器中分别处理的目的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def use_logging(level):</span><br><span class="line">    def decorator(func):</span><br><span class="line">        def wrapper(*args, **kwargs):</span><br><span class="line">            if level == &quot;warn&quot;:</span><br><span class="line">                logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class="line">            elif level == &quot;info&quot;:</span><br><span class="line">                logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class="line">            return func(*args)</span><br><span class="line">        return wrapper</span><br><span class="line"></span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line">@use_logging(level=&quot;warn&quot;)</span><br><span class="line">def foo(name=&apos;foo&apos;):</span><br><span class="line">    print(&quot;i am %s&quot; % name)</span><br></pre></td></tr></table></figure></p><p>上面的 use logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=”warn”)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。<br>@use_logging(level=”warn”)等价于@decorator</p><h2 id="类装饰器"><a href="#类装饰器" class="headerlink" title="类装饰器"></a>类装饰器</h2><p>装饰器不仅仅是函数，也可以是类，使用类作为装饰器更加灵活强大。类装饰器主要依靠类的’<strong>call</strong>‘方法，当使用@形式将装饰器附加到函数上时，就会调用该方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Foo(object):</span><br><span class="line">  def __init__(self, func):</span><br><span class="line">    self._func = func</span><br><span class="line">  def __call__(self):</span><br><span class="line">    print(&apos;class decorator runing&apos;)</span><br><span class="line">    self._func()</span><br><span class="line">    print(&apos;class decorator ending&apos;)</span><br><span class="line"></span><br><span class="line">@Foo</span><br><span class="line">def bar():</span><br><span class="line">  print(&apos;bar&apos;)</span><br></pre></td></tr></table></figure></p><h2 id="装饰器顺序"><a href="#装饰器顺序" class="headerlink" title="装饰器顺序"></a>装饰器顺序</h2><p>对于一个函数，我们当然可以定义多个装饰器：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@a</span><br><span class="line">@b</span><br><span class="line">@c</span><br><span class="line">def f():</span><br><span class="line">  pass</span><br></pre></td></tr></table></figure></p><p>这个函数的执行顺序是从里向外的，最先调用定义在最后的装饰器，等效于：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f = a(b(c(f)))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习常见问题和知识点整理</title>
      <link href="/2018/04/11/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"/>
      <url>/2018/04/11/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>本文是我在学习过程中遇到的所有知识点的补充，文章的整体框架和基础知识点参考的周志华老师《机器学习》这本书。<br>在《机器学习》讲解的基础知识上，加入了我自己在学习过程中遇到的知识点作为补充。</p><a id="more"></a><h1 id="模型评估选择"><a href="#模型评估选择" class="headerlink" title="模型评估选择"></a>模型评估选择</h1><ul><li>学习器在训练集上的误差称为训练误差或者经验误差，在新样本上的误差称为泛化误差<h2 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h2>什么是生成模型，什么是判别模型<h2 id="模型评估方法"><a href="#模型评估方法" class="headerlink" title="模型评估方法"></a>模型评估方法</h2><h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3>直接将数据集分为两个不相交的子集，一个训练，一个测试。一般需要多次划分对评估结果取平均值。<h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3>将数据集分成k个大小相同的子集，每次用一个子集i作为测试，用剩下的k-1作为训练，然后遍历k个子集会产生k个训练/测试结果。留一法：让k=数据集大小，也就是说每次直选一个样本进行测试。<h3 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h3>有放回的从原始数据集采样，每次取一个数据复制放入新数据集，然后将该数据放回，重复采样直到新数据集大小为m，初始数据集中会有36%的样本未出现在新数据集中，用新数据训练，用未出现的数据集做测试。 自助法在数据集小，难以有效划分时有用，但改变了初始数据集的分布，引入了估计偏差。<h2 id="模型性能度量"><a href="#模型性能度量" class="headerlink" title="模型性能度量"></a>模型性能度量</h2></li><li>错误率和精度</li><li>查准率，查全率，F1值</li><li>真正例率：所有真实正例中被预测为正例的比例（对正例预测正确）</li><li>假正例率：所有真实反例中被预测为正例的比例（对反例预测错误）<h3 id="P-R曲线（查准率，查全率曲线）"><a href="#P-R曲线（查准率，查全率曲线）" class="headerlink" title="P-R曲线（查准率，查全率曲线）"></a>P-R曲线（查准率，查全率曲线）</h3></li></ul><ol><li>将预测结果从最可能为正例到最不可能进行排序，按照此顺序逐个把样本作为正例进行预测，并记录每一刻的P-R值</li><li>可以比较PR曲线下的面积来比较模型性能，或者比较平衡点（P==R）的大小</li><li>可以根据不同任务需求采用不同的截断点，更重视P则选择排序靠前的位置截断。</li><li>最开始的时候，由于只考虑将最可能是正例的几个实例预测为正例，所以P值非常高，而R值因为只有少数几个预测正例，所以接近于0。<h3 id="ROC曲线-amp-AUC"><a href="#ROC曲线-amp-AUC" class="headerlink" title="ROC曲线&amp;AUC"></a>ROC曲线&amp;AUC</h3></li></ol><ul><li>同样将样本从可能正例到不可能正例排序，首先个将所有样例均预测为反例，这时真正例率和假正例率都为0，然后调整阈值，依次将每个样例划分为正例。每次计算真正例率和假正例率绘制ROC曲线</li><li>RUC考虑的事样本预测的排序质量，因此它与排序误差有紧密联系</li><li>ROC曲线下方围成的面积即为AUC</li><li>因为AUC为点连成的非光滑曲线，所以可以通过计算各个相邻点组成的小矩形面积之和得出AUC<h3 id="代价敏感误差率与代价曲线"><a href="#代价敏感误差率与代价曲线" class="headerlink" title="代价敏感误差率与代价曲线"></a>代价敏感误差率与代价曲线</h3>构建有代价权重的混淆矩阵，然后可以依据此计算表示代价的ROC曲线</li></ul><h2 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h2><p>比较检验用来对学习器的泛化能力进行检验。</p><h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><p>假设检验可以推断出，如果在测试集上观察到的学习器A比B好，则A的泛化性能是否优于B，以及这个结论的把握有多大。</p><h3 id="交叉验证t检验"><a href="#交叉验证t检验" class="headerlink" title="交叉验证t检验"></a>交叉验证t检验</h3><h3 id="McNemar检验from"><a href="#McNemar检验from" class="headerlink" title="McNemar检验from"></a>McNemar检验from</h3><h3 id="Friedman检验和Nemenyi后续检验"><a href="#Friedman检验和Nemenyi后续检验" class="headerlink" title="Friedman检验和Nemenyi后续检验"></a>Friedman检验和Nemenyi后续检验</h3><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>在学习算法中，偏差指的是预测的期望值与真实值的差值，方差则是每一次预测值与预测值的期望之间的差均方。<br><strong>泛化误差可以分解为偏差，方差和噪声之和</strong><br><strong>偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。</strong> 噪声表达当前任务在任何学习算法所能达到的泛化误差的下界，通过对泛化误差的进行分解。可以得到：</p><ul><li>偏差度量算法的期望预测和真实结果的偏离程度，即刻画学习器的拟合能力</li><li>方差度量同样大小的训练集的变动所导致的学习性能的变化，即数据扰动所造成的影响。体现学习器的稳定性</li><li>噪音表达了当前任务上任何算法所能达到的期望泛化误差下界，刻画了问题本身的难度<br>方差和偏差具有矛盾性，这就是常说的偏差-方差窘伪境（bias-variance dilamma），<strong>随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。</strong> 换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练要适度辄止。高偏差意味着欠拟合，高方差意味着过拟合。</li></ul><h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="最小二乘法和其矩阵形式"><a href="#最小二乘法和其矩阵形式" class="headerlink" title="最小二乘法和其矩阵形式"></a>最小二乘法和其矩阵形式</h3><ol><li>基于均方误差最小化来进行模型求解的方法称为最小二乘法</li><li>在线性回归模型中最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小</li><li>最小二乘法即对每个参数求偏导，并令其等于0.进而求得w和b的闭式解。</li><li>最小二乘法的矩阵表示和解法（原理相同，只是变为对矩阵求偏导）<h3 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h3>可以不仅仅让wx+b逼近y，而是逼近y的衍生物。例如，我们认为wx+b的输出在指数上逼近y就可以得到对数线性回归模型：<br><strong>对数线性回归</strong> 实质上已是在求输入空间到输出空间的非线性函数映射：ln(y) = wx + b<br>进而可以推导出 <strong>广义线性模型</strong> 的形式：y = g-1(wx + b)</li></ol><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><p>逻辑斯蒂回归LR是广义线性模型的一种，是让wx+b逼近y的对数几率。</p><h3 id="几率和对数几率"><a href="#几率和对数几率" class="headerlink" title="几率和对数几率"></a>几率和对数几率</h3><ol><li>y/(1-y)称为几率，反映了样本作为正例的相对可能性。</li><li>对几率取对数为对数几率：ln(y/(1-y))</li><li>LR练一个线性模型，逼近y的对数几率，也就是数据作为正例的相对可能性。ln(y/(1-y)) = wx + b 等价于 y = 1/(1+exp(-wx-b))<h3 id="LR公式推导"><a href="#LR公式推导" class="headerlink" title="LR公式推导"></a>LR公式推导</h3>从LR是线性模型逼近y的对数几率来推导，使用极大似然估计推导w的一般形式，并使用梯度下降算法更新，进而得到w。<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3></li></ol><ul><li>优点：实现简单，分类时计算量非常小，速度快，存储资源低</li><li>缺点：容易欠拟合，准确度不高。只能处理二分类问题（衍生的softmax可解决），样本必须线性可分。</li></ul><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><ul><li>书第60页<br>LDA思想：给定训练数据集，设法将样例投影到一条直线上，是的同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。<strong>衡量同类样例距离尽可能小只需要让协方差尽可能小，异类样例距离尽可能大只需要让类中心之间的距离尽可能大。</strong><br>在对新样例进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。</li></ul><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="为什么不直接对loss求偏导-0，而要用梯度下降？"><a href="#为什么不直接对loss求偏导-0，而要用梯度下降？" class="headerlink" title="为什么不直接对loss求偏导=0，而要用梯度下降？"></a>为什么不直接对loss求偏导=0，而要用梯度下降？</h3><ol><li>逻辑回归没有解析解，就是说无法显式的表现函数的偏导，只能通过数值求解的方法迭代地找到最优解。</li><li>即使有解析解，大部分神经网络的loss为非凸函数，KKT条件（偏导为0是其中一项）仅仅是非凸函数最优化的必要非充分条件。</li><li>偏导为0可能并非是局部极值<h4 id="什么是解析解和数值解"><a href="#什么是解析解和数值解" class="headerlink" title="什么是解析解和数值解"></a>什么是解析解和数值解</h4>解析解就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值；数值解就是用数值方法求出解，给出一系列对应的自变量和解。<br>例子：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x^2 = 5</span><br><span class="line">解析解：x = sqrt(5)</span><br><span class="line">数值解：x = 2.236</span><br></pre></td></tr></table></figure></li></ol><ul><li>解析解(analytical solution)就是一些严格的公式,给出任意的自变量就可以求出其因变量,也就是问题的解, 他人可以利用这些公式计算各自的问题.  </li><li>数值解(numerical solution)是采用某种计算方法,如有限元的方法, 数值逼近,插值的方法, 得到的解.别人只能利用数值计算的结果, 而不能随意给出自变量并求出计算值.</li></ul><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>详情请见另一篇整理文章: <a href="https://yhfeather.github.io/2018/04/21/%E5%90%84%E7%A7%8DBoosting%E6%A0%91/" target="_blank" rel="noopener">各种树: 从决策树到xgboost</a></p><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h2><p>径向基网络是一种使用径向基函数作为隐层神经元激活函数而输出层是对隐层神经元输出进行线性组合的网络。</p><h2 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h2><p>竞争学习是神经网络中一种常用的无监督学习策略。网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。<br>自适应谐振网络（ART）是竞争学习的代表，该网络由比较层，识别层，识别阈值和重置模块构成。</p><h2 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h2><p>自组织映射网络（SOM）是一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构。即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。</p><h2 id="级联相关网络"><a href="#级联相关网络" class="headerlink" title="级联相关网络"></a>级联相关网络</h2><p>级联相关网络是结构自适应网络的代表，这种网络将网络结构也作为学习的目标之一，希望在训练过程中找到最符合数据特点的网络结构。</p><h2 id="玻尔兹曼机"><a href="#玻尔兹曼机" class="headerlink" title="玻尔兹曼机"></a>玻尔兹曼机</h2><p>玻尔兹曼机是一种基于能量的模型，包括显层和隐层两种神经元。</p><h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="线性可分SVM"><a href="#线性可分SVM" class="headerlink" title="线性可分SVM"></a>线性可分SVM</h2><p>距离超平面最近的几个训练样本点使距离等式成立，这几个样本点称为支持向量。两个异类支持向量到超平面的距离之和称为“间隔”。</p><h3 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h3><p>掌握</p><h3 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h3><p>掌握方法思想和原理</p><h3 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h3><p>KKT条件是一个具有很强几何意义的结论。需要掌握</p><h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO的基本思路是先固定ai之外的所有参数，然后求ai上的极值，由于存在约束sum(aiyi)=0，若固定ai之外的其他变量则ai可以由其他变量导出。于是SMO每次选择两个变量ai，aj。并固定其他参数，这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><pre><code>1. 选取一对需要更新的变量ai和aj2. 固定剩余的参数，求解SVM对偶式的公式获得更新后的ai和aj</code></pre><p>注意到只要ai和aj有一个不满足KKT条件，目标函数就会在迭代后增大，KKT条件违背程度越大，变量更新后可能导致的目标函数值增幅越大。<br>于是SMO先选取违背KKT条件程度最大的变量，第二个选取使目标函数值增幅最快的变量，但比较变量复杂度过大，SMO采用启发式方法：选取两个变量所对应样本之间的间隔最大，这样的两个变量有很大差别，对目标函数的更新有更大帮助。   </p><h2 id="SVM多分类"><a href="#SVM多分类" class="headerlink" title="SVM多分类"></a>SVM多分类</h2><ul><li>一对多，选择预测结果最大值</li><li>一对一，然后采用投票</li><li>层次SVM，层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。</li></ul><h2 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h2><p>在现实任务中我们很难恰到好处的找个某个核函数使得训练集在特征空间线性可分，也很难推是否是由过拟合造成的。所以我们允许SVM在一些样本上出错，引入软间隔概念。同时引入松弛变量的概念。</p><h4 id="软间隔SVM推导"><a href="#软间隔SVM推导" class="headerlink" title="软间隔SVM推导"></a>软间隔SVM推导</h4><p>掌握（软间隔的对偶形式和硬间隔的对偶形式唯一区别在于对ai的约束，软间隔的约束为’0&lt;=ai&lt;=C’, 硬间隔的约束仅为’0&lt;=ai’</p><h4 id="软间隔SVM的KKT条件"><a href="#软间隔SVM的KKT条件" class="headerlink" title="软间隔SVM的KKT条件"></a>软间隔SVM的KKT条件</h4><p>理解</p><h2 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h2><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>可将样本从原始空间映射到一个高维空间，使得样本在这个高维空间线性可分。如果原始空间是有限维，即属性有限，那么一定存在一个高维空间使样本可分。<br>我们发现SVM的对偶式中包含xi和xj的内积运算，使用映射方法只需要将原始空间内的内积运算转换为高维映射空间内的映射运算即可。但映射空间往往维度过高，计算复杂，所以假设存在核函数k(xi, xj) = xi,xj的內积，有了这个核函数在对偶式中只需将內积替换成核函数即可。</p><h4 id="核函数条件"><a href="#核函数条件" class="headerlink" title="核函数条件"></a>核函数条件</h4><p>只要一个对称函数所对应的和矩阵半正定，它就能作为核函数使用。</p><h4 id="核函数的种类"><a href="#核函数的种类" class="headerlink" title="核函数的种类"></a>核函数的种类</h4><p>核函数选择成了SVM的最大变数，若核函数不适合，样本被映射到了一个不适合的空间，会导致性能不佳。<br>常见的核函数有：</p><ol><li>线性核</li><li>多项式核</li><li>高斯核</li><li>拉普拉斯核</li><li>Sigmoid核<br>此外，还可以通过函数线性组合得到新的核函数。</li></ol><h2 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h2><p>对于传统回归模型通常直接计算模型输出和真实输出之间的差别来计算损失，只有当两个输出完全相同时loss才为0。SVR容忍预测和label之间最多有e的偏差，即当f预测和label之间的差别大于e才计算损失。相当于以f为中心，构建了一个宽度为2e的间隔带，样本落入该间隔带则被认为是分类正确的。</p><h4 id="SVR公式推导"><a href="#SVR公式推导" class="headerlink" title="SVR公式推导"></a>SVR公式推导</h4><p>假设我们能容忍f(x)和y之间最多有e的偏差，即当f(x)与y之间的差别绝对值大于e时才计算损失。相当于以f(x)为中心，构建了一个宽度为2e的间隔带，若样本落入此间隔带则认为被预测正确。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="SVM优点："><a href="#SVM优点：" class="headerlink" title="SVM优点："></a>SVM优点：</h3><ol><li>可用于线性/非线性分类，也可以用于回归</li><li>泛化能力强，泛化误差较低</li><li>小训练集上往往就可以得到较好的结果<h3 id="SVM缺点："><a href="#SVM缺点：" class="headerlink" title="SVM缺点："></a>SVM缺点：</h3></li><li>对参数和核函数的选择比较敏感，核函数选取比较难</li><li>原始的SVM只比较擅长处理二分类问题</li><li>时空开销都比较大<h3 id="SVM和LR的异同"><a href="#SVM和LR的异同" class="headerlink" title="SVM和LR的异同"></a>SVM和LR的异同</h3></li><li>两者都为线性模型</li><li>两者都是判别模型</li><li>本质上来讲是损失函数的不同，LR为交叉熵损失（对数损失），SVM为hinge损失</li><li>由于损失函数不同，LR直接依赖所有数据分布，SVM只关心支持向量</li><li>SVM依赖距离测度，数据需要归一化，LR不受影响</li><li>SVM自带正则项，LR需要在Loss Function上添加。</li></ol><h1 id="贝叶斯方法"><a href="#贝叶斯方法" class="headerlink" title="贝叶斯方法"></a>贝叶斯方法</h1><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>多个概率概念掌握</p><ul><li>P(Y|X) = P(X|Y)P(Y) / P(X)</li><li>y = argmaxP(Y=ci)连乘(P(X=xi|Y=ci))</li><li>联合概率：P(XY)</li><li>后验概率：P(Y|X)</li><li>先验概率：P(X)<h3 id="朴素贝叶斯处理连续属性"><a href="#朴素贝叶斯处理连续属性" class="headerlink" title="朴素贝叶斯处理连续属性"></a>朴素贝叶斯处理连续属性</h3>对于连续属性可以考虑概率密度函数，假定该属性遵循正态分布，由此得出连续属性的概率<h3 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h3>在测试过程中，如果某个属性的取值从未在训练集中出现过，会导致P(x|y)=0,进而让整个贝叶斯公式等于零。通过拉普拉斯平滑可以解决这个问题。通俗的说，拉普拉斯平滑即在计算概率的时候，<strong>分别在分子上加1，分母上加属性的取值种类。</strong><h3 id="朴素贝叶斯优化"><a href="#朴素贝叶斯优化" class="headerlink" title="朴素贝叶斯优化"></a>朴素贝叶斯优化</h3></li></ul><ol><li>可以将涉及到的所有概率先计算好存储起来，在进行预测时只需要“查表”即可进行判别。</li><li>当数据有多属性时，涉及到多属性连乘问题，可以将原连乘问题转换为对数连加问题加快计算速度。<h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3></li></ol><ul><li>优点：对小规模数据表现很好，适合多分类任务</li><li>缺点：对输入数据的表达形式很敏感<h3 id="为什么被称为“朴素”"><a href="#为什么被称为“朴素”" class="headerlink" title="为什么被称为“朴素”"></a>为什么被称为“朴素”</h3>朴素的意思是该分类方法建立在一个较强的独立假设上，即所有特征之间相互独立，所以被称为朴素，具体的说为：P(AB) = P(A)P(B)</li></ul><h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><ul><li>基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较性强的属性依赖关系。</li><li>“独依赖估计”是半朴素贝叶斯分类器最常用的一种策略，所谓“独依赖”就是假设每个属性在类别之外最多仅依赖一个其他属性。</li><li>半朴素贝叶斯不是很常见，具体细节见书p154</li></ul><h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>也称“信念网络”，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><p>吉布斯采样，EM算法</p><h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><p>通过构建并结合多个学习器来完成学习任务，总体思路是：1. 先产生一组“个体学习器”，再用某种策略将他们结合起来。</p><ul><li>同质的个体学习器也可以称为“基学习器”</li><li>异质集成中的个体学习器由不同算法组成，常称为“组件学习器”<br>集成学通过多个学习器的组合，通常会有比单一学习器更显著的泛化性能。<br>如何产生“好而不同”的个体学习器是集成学习的研究核心。<br>目前的集成学习方法大致可以分为两类：</li></ul><ol><li>个体学习器之间存在强依赖关系，必须串行生成序列化方法，代表是Boosting。</li><li>个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林</li></ol><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting是一族可将弱学习器提升为强学习器的方法：这族算法工作原理类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器的错误样本更受关注然后基于调整后的样本分布训练下一个基学习器。最后将这些基学习器组合到一起。</p><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol><li>低泛化误差</li><li>容易实现，准确率较高，不需要大量调参<h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3></li><li>对异常点过于敏感<br>在基学习器的选择上，大部分Boosting算法都选择决策树作为基学习器。<br>具体GBDT和xgboost细节参看另一篇文章</li></ol><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>基本思想是对训练集进行采样，产生出若干个不同的子集，再从每个子集中训练一个基学习器。常使用的采样方法为“自助采样法”。<br>我们可以采样出T个含有m个样本的采样集，用每个采样集训练出一个基学习器，再将这些基学习器进行结合就是Bagging的基本流程。<br>Bagging对于分类任务采用简单投票法，对于回归任务采用简单平均法。<br>对于每个基学习器的包外样本，可以用其进行泛化性能估计或者辅助剪枝（决策树）</p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是在Bagging的基础上进一步在决策树的训练过程中引入了 <strong>随机属性选择</strong>。<br>在RF中，对基决策树的每个节点，<strong>先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度，若k=d，则基决策树的构建和传统决策树相同；若k=1，则随机选择一个属性用于划分，一般情况下推荐k=log2d</strong><br>RF中的基学习器的多样性不仅仅来自于样本扰动（对原始数据集进行自助采样），还来自属性随机。</p><h3 id="随机森林进行特征选择"><a href="#随机森林进行特征选择" class="headerlink" title="随机森林进行特征选择"></a>随机森林进行特征选择</h3><p>在随机森林中某个特征X的重要性的计算方法如下：<br>1：对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB1.<br>2: 随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB2.<br>3：假设随机森林中有Ntree棵树,那么对于特征X的重要性=∑(errOOB2-errOOB1)/Ntree,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。</p><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><ol><li>对于回归问题：简单平均法、加权平均法</li><li>对于分类问题：绝对多数投票法，相对对数投票法，加权投票法。</li></ol><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>先从初始数据集训练出初始学习器，然后再生成一个新数据集用于训练次级学习器。在这个新的数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记被当做样例标记。</p><h2 id="集成学习的多样性度量"><a href="#集成学习的多样性度量" class="headerlink" title="集成学习的多样性度量"></a>集成学习的多样性度量</h2><p>见书P186页</p><h2 id="集成学习通常选用决策树的原因"><a href="#集成学习通常选用决策树的原因" class="headerlink" title="集成学习通常选用决策树的原因"></a>集成学习通常选用决策树的原因</h2><p>决策树可以认为是 if-then 规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征。<br>不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。因此，梯度提升方法和决策树学习算法可以互相取长补短</p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>对于一些依赖于距离测度的学习算法，在训练算法之前对数据进行预处理是非常有必要的</p><h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><p>指将属于映射到指定范围内，用于去除不同维度数据的量纲以及量纲单位。一般会映射到[0, 1]或者[-1, 1]</p><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>x = (x-u)/s. 其中u为均值，s为方差。标准化改变了数据的分布，某种程度上加快模型训练速度。</p><h2 id="为什么需要归一化"><a href="#为什么需要归一化" class="headerlink" title="为什么需要归一化"></a>为什么需要归一化</h2><ul><li>对于需要用距离度量的算法，如果不同特征的量纲不同会导致对预测结果的权重影响不同（如房价预测，kNN）</li><li>本质上是loss函数不同造成的，对于梯度下降来说，量纲不同会导致收敛速度过慢等问题。</li></ul><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><ul><li>用来评估聚类效果的好坏<ul><li>同一簇的样本尽可能的彼此相似，不同簇的样本尽可能不同。</li><li>簇内相似度尽可能高，簇间相似度尽可能低</li></ul></li><li>两类性能度量：<ol><li>将聚类结果与某个参考模型进行比较，称为”外部目标”</li><li>直接考察聚类结果而不利用任何参考模型，称为“内部目标”·</li></ol></li><li>三个聚类性能度量外部指标：<ol><li>Jaccard系数</li><li>FM系数</li><li>Rand指数</li></ol><ul><li>上述性能度量都在零一之间，且越大越好</li></ul></li><li>两个聚类性能度量内部指标<ol><li>DB指数</li><li>Dunn指数</li></ol><ul><li>DBI越小越好，DI越大越好</li></ul></li></ul><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><ul><li>闵可夫斯基距离度量：<ol><li>当p=2时，即为欧氏距离</li><li>当p=1时，即为曼哈顿距离</li></ol></li><li>使用闵可夫斯基距离时，注意只能针对有序属性特征</li><li>对于无序属性，可以采用VDM</li><li>对于一般问题，可以将闵可夫斯基度量和VDM相结合，前者处理有序属性，后者处理无序属性</li></ul><h2 id="k均值算法"><a href="#k均值算法" class="headerlink" title="k均值算法"></a>k均值算法</h2><ul><li>优点：思想简单，理论成熟，既可以回归也可以分类</li><li>缺点：计算量大，样本不平衡问题，需要大量内存<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4></li></ul><ol><li>设定聚类簇数为k，随机选取k个样本作为初始均值向量。</li><li>考察剩余样本，分别计算剩余样本到这k个均值向量的距离，并将其加入距离最近的聚类簇中</li><li>计算每个新聚类簇的均值向量，返回第二步直至收敛。</li></ol><h2 id="k近邻学习"><a href="#k近邻学习" class="headerlink" title="k近邻学习"></a>k近邻学习</h2><p>k近邻是最简单直接的算法之一，对于每个输入实例，找到在空间上距离该实例最近的k个数据，根据这k个数据的label对输入实例进行分类/回归。</p><ul><li>通常情况下，分类任务中使用投票法，回归任务中使用平均法，也可以使用加权平均法。</li><li>常见的距离测度方法为：闵可夫斯基距离，包括欧氏距离，曼哈顿距离等。</li><li>k近邻算法是懒惰学习 lazy learning的著名代表，没有显式的训练过程。（相应的，那些在训练阶段就对样本进行学习处理的方法称为急切学习 eager learning）</li><li>k值越大，模型越简单</li></ul><h1 id="常见基本问题"><a href="#常见基本问题" class="headerlink" title="常见基本问题"></a>常见基本问题</h1><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>多分类学习的基本思路是“拆解法”，即将多个分类任务拆为若干个二分类任务求解。经典的拆分策略有三种：‘一对一’，‘一对余’，‘多对多’</p><ol><li>一对一OvO：将N个类别两两配对，产生N*(N-1)/2个二分类任务。在 <strong>测试阶段</strong> ，新样本将同时提交给所有分类器，于是我们得到N(N-1)/2个分类结果，最终结果可以通过投票产生，即把被预测最多的类别作为最终分类结果。</li><li>一对余OvR：每次将一个类作为正例，剩下的N-1个类作为反例来训练N个分类器。在 <strong>测试阶段</strong> 若仅有一个分类器预测为正例，则对应的类别标记作为最终分类结果。若有多个分类器预测为正例，则通常考虑各个分类器的预测置信度，选择置信度最大的类别标记为分类结果。</li><li>多对多MvM：每次将若干类作为正类，若干类其他类作为反类，显然OvO和OvM是MvM的特例。MvM正反类的构造必须有特殊的设计，不能随意选取，通常使用“纠错输出码（ECOC）”技术。<ul><li>ECOE技术详见书第65页</li></ul></li></ol><h2 id="样本类别不平衡问题"><a href="#样本类别不平衡问题" class="headerlink" title="样本类别不平衡问题"></a>样本类别不平衡问题</h2><h3 id="再缩放"><a href="#再缩放" class="headerlink" title="再缩放"></a>再缩放</h3><p>解决类别不平衡学习的一个基本策略：<strong>再缩放</strong>，对预测出的几率乘以正负例的比值即为模型的预测输出。<br>对于线性分类y=wx+b, 我们通常认为y&gt;0.5为正例y&lt;0.5为反例，说明几率y/(1-y)的阈值为0.5，表示分类器认为正反例可能性相同。所以我们只需要修改分类器预测几率的阈值即可：若y/(1-y) &gt; m+/m-则判定为正例。</p><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>实际应用中共有三种做法： <strong>欠采样，过采样，阈值移动</strong></p><ul><li>欠采样即去除一些反例使得正反例数目接近，由于欠采样可能丢失重要信息，实践中使用EasyEnsemble利用集成学习将反例划分为若干个集合供不同学习器使用，这样对于每个学习器来说都是欠采样，但在全局看来不会丢失信息。</li><li>过采样即增加一些正例使得正反例数目接近（不能简单地对正例进行重复否则会产生严重的过拟合，SMOTE算法通过对训练集里的正例进行插值来产生额外的正例。<h3 id="调整正负样本的惩罚权重"><a href="#调整正负样本的惩罚权重" class="headerlink" title="调整正负样本的惩罚权重"></a>调整正负样本的惩罚权重</h3>对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。使用这种方法时不需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。<h3 id="集成学习，组合学习"><a href="#集成学习，组合学习" class="headerlink" title="集成学习，组合学习"></a>集成学习，组合学习</h3>组合/集成方法指的是：将多数类样本数据集分为几份，每份大小与少数类样本数据集大小相同，每次用一份多数类样本和所有少数类样本组成训练集训练模型，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。<h3 id="训练集中类别不均衡，哪个参数最不准确？"><a href="#训练集中类别不均衡，哪个参数最不准确？" class="headerlink" title="训练集中类别不均衡，哪个参数最不准确？"></a>训练集中类别不均衡，哪个参数最不准确？</h3>准确度（Accuracy）。 eg.训练集中class 1的样本数比class 2的样本数是60:1。使用逻辑回归进行分类，最后结果是其忽略了class 2，即其将所有的训练样本都分类为class 1。</li></ul><h2 id="过拟合解决办法"><a href="#过拟合解决办法" class="headerlink" title="过拟合解决办法"></a>过拟合解决办法</h2><ol><li>L1，L2正则</li><li>模型早停</li><li>增加样本，数据增强（比如说对图片进行平移旋转剪裁）</li><li>针对神经网络：Batch Normalization</li><li>针对神经网络：Dropout</li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>xgboost实战</title>
      <link href="/2018/03/25/Machine%20Learning/xgboost%E5%AE%9E%E6%88%98/"/>
      <url>/2018/03/25/Machine%20Learning/xgboost%E5%AE%9E%E6%88%98/</url>
      <content type="html"><![CDATA[<p>xgboost是一个非常强大的梯度提升树，在之前的文章中，详细介绍了xgboost对传统GBDT的优化。这篇文章介绍如何在项目中使用xgboost。<br><a id="more"></a></p><h2 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h2><p>xgboost支持三种数据格式的载入</p><ol><li>libsvm格式的文本数据</li><li>numpy的二维数组数据</li><li>xgboost的二进制缓存文件</li><li>scipy.sparse格式的数据<br>只需要将libsvm/numpy/scipy.sparse数据对象作为参数传递给函数’xgb.DMatrix(“fileObject”)’，就可以将原始格式数据转换为xgboost的DMatrix数据格式<br>例：numpy二维数组转换<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.rand(5,10) # 随机生成5个样本， 每个样本包含10个特征属性</span><br><span class="line">label = np.random.randint(2, size=5)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br></pre></td></tr></table></figure></li></ol><h3 id="样本设置权重参数"><a href="#样本设置权重参数" class="headerlink" title="样本设置权重参数"></a>样本设置权重参数</h3><p>xgboost可以对训练数据进行加权，使用方式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight = np.random.rand(5,1)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, weight=weight)</span><br></pre></td></tr></table></figure></p><h3 id="保存为二进制文件"><a href="#保存为二进制文件" class="headerlink" title="保存为二进制文件"></a>保存为二进制文件</h3><p>可以将DMatrix格式的数据保存为xgboost的二进制文件，这样下次加载时可以提高加载速度：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(&apos;train.svm.txt&apos;)</span><br><span class="line">dtrain.save_binary(&quot;train.buffer&quot;)</span><br></pre></td></tr></table></figure></p><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><p>对于训练完成的模型，可以将其保存在本地：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst.save_model(&apos;xgb.model&apos;)</span><br></pre></td></tr></table></figure></p><p>也可以导出训练好的模型到txt文件，或者导出模型的特征映射<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bst.dump_model(&apos;dump.raw.txt&apos;) # dump model</span><br><span class="line">bst.dump_model(&apos;dump.raw.txt&apos;,&apos;featmap.txt&apos;) # dump model with feature map</span><br></pre></td></tr></table></figure></p><p>加载模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bst = bgb.Booster(&#123;&apos;nthread&apos;:4&#125;) # init model</span><br><span class="line">bst.load_model(&quot;model.bin&quot;) # load model</span><br></pre></td></tr></table></figure></p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>xgboost是通过字典来存储参数的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    &apos;booster&apos;: &apos;gbtree&apos;,           # 使用的基学习器</span><br><span class="line">    &apos;objective&apos;: &apos;multi:softmax&apos;,  # 指定问题类型，multi:softmax表示多分类问题；reg:gamma表示回归问题</span><br><span class="line">    &apos;num_class&apos;: 10,               # 类别数，与 multisoftmax 并用</span><br><span class="line">    &apos;gamma&apos;: 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2</span><br><span class="line">    &apos;max_depth&apos;: 12,               # 构建树的深度，越大越容易过拟合</span><br><span class="line">    &apos;lambda&apos;: 2,                   # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span><br><span class="line">    &apos;subsample&apos;: 0.7,              # 随机采样训练样本</span><br><span class="line">    &apos;colsample_bytree&apos;: 0.7,       # 生成树时进行的列采样</span><br><span class="line">    &apos;min_child_weight&apos;: 3,</span><br><span class="line">    &apos;silent&apos;: 1,                   # 设置成1则没有运行信息输出，最好是设置为0.</span><br><span class="line">    &apos;eta&apos;: 0.007,                  # 如同学习率</span><br><span class="line">    &apos;seed&apos;: 1000,</span><br><span class="line">    &apos;nthread&apos;: 4,                  # cpu 线程数</span><br><span class="line">&#125;</span><br><span class="line">params_list = params.items()</span><br><span class="line">model = xgb.train(dtrain, params_list, num_rounds)</span><br></pre></td></tr></table></figure></p><p>xgboost包含三种参数：general parameters，booster parameters和task parameters。由于xgboost参数众多，所以这里只考虑常见的，需要调参的参数。</p><ul><li>General parameters<br>该参数参数控制在提升（boosting）过程中使用哪种booster，常用的booster有树模型（tree）和线性模型（linear model）。</li><li>Booster parameters<br>这取决于使用哪种booster。</li><li>Task parameters<br>控制学习的场景，例如在回归问题中会使用不同的参数控制排序。<h3 id="General-Parameters"><a href="#General-Parameters" class="headerlink" title="General Parameters"></a>General Parameters</h3></li><li>booster [default=gbtree]<br>有两中模型可以选择gbtree和gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。缺省值为gbtree</li><li>silent [default=0]<br>取0时表示打印出运行时信息，取1时表示以缄默方式运行，不打印运行时信息。缺省值为0。建议打印运行状况</li><li>nthread<br>XGBoost运行时的线程数。缺省值是当前系统可以获得的最大线程数<h3 id="Parameters-for-TreeBooster"><a href="#Parameters-for-TreeBooster" class="headerlink" title="Parameters for TreeBooster"></a>Parameters for TreeBooster</h3></li><li>eta [default=0.3]<br>为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3 取值范围为：[0,1]</li><li>gamma [default=0]<br>用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2。 取值范围为：[0,∞]</li><li>max_depth [default=6]<br>树的最大深度。缺省值为6 取值范围为：[1,∞]</li><li>min_child_weight [default=1]<br>孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该值越大，越保守，不容易过拟合。 取值范围为：[0,∞]</li><li>subsample [default=1]<br>用于训练模型的子样本占整个样本集合的比例。如果设置为0.5则意味着XGBoost将随机的从整个样本集合中随机的抽取出50%的子样本建立树模型，这能够防止过拟合。 取值范围为：(0,1]</li><li>colsample_bytree [default=1]<br>在建立树时对特征采样的比例。缺省值为1 取值范围为：(0,1]<h3 id="Parameters-for-LinearBooster"><a href="#Parameters-for-LinearBooster" class="headerlink" title="Parameters for LinearBooster"></a>Parameters for LinearBooster</h3></li><li>lambda [default=0]<br>L2 正则的惩罚系数</li><li>alpha [default=0]<br>L1 正则的惩罚系数</li><li>lambda_bias<br>在偏置上的L2正则。缺省值为0（在L1上没有偏置项的正则，因为L1时偏置不重要）<h3 id="Task-Parameters"><a href="#Task-Parameters" class="headerlink" title="Task Parameters"></a>Task Parameters</h3></li><li>objective [ default=reg:linear ]<br>定义学习任务及相应的学习目标，可选的目标函数如下：<ol><li>“reg:linear” —— 线性回归。</li><li>“reg:logistic”—— 逻辑回归。</li><li>“binary:logistic”—— 二分类的逻辑回归问题，输出为概率。</li><li>“binary:logitraw”—— 二分类的逻辑回归问题，输出的结果为wTx。</li><li>“count:poisson”—— 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7。(used to safeguard optimization)</li><li>“multi:softmax” –让XGBoost采用softmax目标函数处理多分类问题，同时需要设置参数num_class（类别个数）</li><li>“multi:softprob” –和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。</li><li>“rank:pairwise” rank任务，方法是最小化pairwise loss</li></ol></li><li>eval_metric [ default according to objective ]<br>校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标（rmse for regression, and error for classification, mean average precision for ranking, ‘auc’, ‘logloss’, ‘merror’, and so on…）<br>用户可以添加多种评价指标，对于Python用户要以list传递参数对给程序，而不是map参数</li></ul><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="显示重要特征"><a href="#显示重要特征" class="headerlink" title="显示重要特征"></a>显示重要特征</h3><p>GBDT的一大应用就是特征选择，xgboost提供’plot_importance(model)’方法， 可以基于F-score绘制每个特征的值，值越大，表示该属性越重要。</p><h2 id="xgboost实战应用"><a href="#xgboost实战应用" class="headerlink" title="xgboost实战应用"></a>xgboost实战应用</h2><p>XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现 分类 和 回归 两种任务。</p><h3 id="基于原生接口的分类"><a href="#基于原生接口的分类" class="headerlink" title="基于原生接口的分类"></a>基于原生接口的分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># read in the iris data</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;booster&apos;: &apos;gbtree&apos;,</span><br><span class="line">    &apos;objective&apos;: &apos;multi:softmax&apos;,</span><br><span class="line">    &apos;num_class&apos;: 3,</span><br><span class="line">    &apos;gamma&apos;: 0.1,</span><br><span class="line">    &apos;max_depth&apos;: 6,</span><br><span class="line">    &apos;lambda&apos;: 2,</span><br><span class="line">    &apos;subsample&apos;: 0.7,</span><br><span class="line">    &apos;colsample_bytree&apos;: 0.7,</span><br><span class="line">    &apos;min_child_weight&apos;: 3,</span><br><span class="line">    &apos;silent&apos;: 1,</span><br><span class="line">    &apos;eta&apos;: 0.1,</span><br><span class="line">    &apos;seed&apos;: 1000,</span><br><span class="line">    &apos;nthread&apos;: 4,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plst = params.items()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train)</span><br><span class="line">num_rounds = 500</span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds)</span><br><span class="line"></span><br><span class="line"># 对测试集进行预测</span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">ans = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">cnt1 = 0</span><br><span class="line">cnt2 = 0</span><br><span class="line">for i in range(len(y_test)):</span><br><span class="line">    if ans[i] == y_test[i]:</span><br><span class="line">        cnt1 += 1</span><br><span class="line">    else:</span><br><span class="line">        cnt2 += 1</span><br><span class="line"></span><br><span class="line">print(&quot;Accuracy: %.2f %% &quot; % (100 * cnt1 / (cnt1 + cnt2)))</span><br><span class="line"></span><br><span class="line"># 显示重要特征</span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="基于sklearn的分类"><a href="#基于sklearn的分类" class="headerlink" title="基于sklearn的分类"></a>基于sklearn的分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># read in the iris data</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=160, silent=True, objective=&apos;multi:softmax&apos;)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 对测试集进行预测</span><br><span class="line">ans = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">cnt1 = 0</span><br><span class="line">cnt2 = 0</span><br><span class="line">for i in range(len(y_test)):</span><br><span class="line">    if ans[i] == y_test[i]:</span><br><span class="line">        cnt1 += 1</span><br><span class="line">    else:</span><br><span class="line">        cnt2 += 1</span><br><span class="line"></span><br><span class="line">print(&quot;Accuracy: %.2f %% &quot; % (100 * cnt1 / (cnt1 + cnt2)))</span><br><span class="line"></span><br><span class="line"># 显示重要特征</span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SICP第一章:构造过程抽象</title>
      <link href="/2018/03/22/Reading/SICP%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/03/22/Reading/SICP%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>SICP（《Structure and Interpretation of Computer Science》）被无数外国程序员誉为最值得读的神书之一，同时身在PL研究室的我，EOPL和SICP都是不得不读的书。在这将这本书的读书笔记整理下来。</p><a id="more"></a><h2 id="1-1-程序设计的基本元素"><a href="#1-1-程序设计的基本元素" class="headerlink" title="1.1 程序设计的基本元素"></a>1.1 程序设计的基本元素</h2><p>  一个优秀的程序设计语言，应该是一种框架，可以让程序员通过它自由的组织自己的计算思想。每一种强有力的语言都应该提供三个机制：</p><ul><li>基本表达形式：用于表示语言所关心的最简单的个体</li><li>组合的抽象方法：通过他们可以从较为简单的东西出发构造复杂的元素</li><li>抽象的方法：通过他们可以为复合对象命名，并将他们当作单元去操作<br>在程序设计中，我们需要处理两类要素：过程和数据，数据是一种我们希望去操作的东西，而过程就是有关操作浙西数据的规则描述。<h3 id="1-1-1-正则序求值和应用序求值"><a href="#1-1-1-正则序求值和应用序求值" class="headerlink" title="1.1.1 正则序求值和应用序求值"></a>1.1.1 正则序求值和应用序求值</h3><ul><li>对于一个复合计算过程，“完全展开而后归约”的求值模型称为正则序求值</li><li>“先求值参数而后应用”的方式，被称为应用序求值</li></ul></li></ul><h3 id="1-1-2-牛顿法求平方根"><a href="#1-1-2-牛顿法求平方根" class="headerlink" title="1.1.2 牛顿法求平方根"></a>1.1.2 牛顿法求平方根</h3><p>如果对x的平方根的值有了一个猜测y，那么只需要求出y和x/y的平均值（它一定更接近实际的平方根值）。然后将得到的平均值作为下一个猜测，不断递归。</p><h3 id="1-1-3-过程作为黑箱的抽象"><a href="#1-1-3-过程作为黑箱的抽象" class="headerlink" title="1.1.3 过程作为黑箱的抽象"></a>1.1.3 过程作为黑箱的抽象</h3><p>对于一个复合过程，我们可以将其拆分成多个更小的子过程，而对于父过程而言，子过程即使一个个的黑箱，我们不需要知道子过程的具体实现过程，只需要知道子过程可以完成父过程需要的功能即可。<br><strong>按照过程抽象的思想编程，可以让我们更关注过程本身，至于子过程可以推后实现，相当于将过程解偶</strong></p><h3 id="1-1-4-局部子过程"><a href="#1-1-4-局部子过程" class="headerlink" title="1.1.4 局部子过程"></a>1.1.4 局部子过程</h3><p>对于一般子过程的定义，为了防止该子过程定义后与其他过程重名导致的混淆，应该将只被父过程调用的子过程定义在父过程的内部，即“块结构”。 在其它语言中，可以使用匿名函数，包命名管理等方法。</p><h2 id="1-3-用高阶函数作抽象"><a href="#1-3-用高阶函数作抽象" class="headerlink" title="1.3 用高阶函数作抽象"></a>1.3 用高阶函数作抽象</h2><p>如果一个编程语言将过程限制为只能作为以数作为抽象，那也会严重的限制我们建立抽象的能力。我们需要构造出这样的过程，让它们能以过程作为参数，或者以过程作为返回值。这类能操作过程的过程称为 <strong>高阶过程</strong></p><blockquote><p>函数式语言的设计的核心并不是仅仅想要将函数作为一切操作的核心，而是为了建立更高级，更广泛的过程抽象</p></blockquote><p><strong>let 表达式的一般形式</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(let ((&lt;var1&gt; &lt;exp1&gt;)</span><br><span class="line">      (&lt;var2&gt; &lt;exp2&gt;)</span><br><span class="line">      ...</span><br><span class="line">      )</span><br><span class="line">  &lt;body&gt;)</span><br></pre></td></tr></table></figure></p><h3 id="1-3-4-过程作为返回值"><a href="#1-3-4-过程作为返回值" class="headerlink" title="1.3.4 过程作为返回值"></a>1.3.4 过程作为返回值</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(define (yahaha f)</span><br><span class="line">  (lambda (x) (f x x)))</span><br><span class="line"></span><br><span class="line">&gt;&gt;((yahaha *) 2)</span><br><span class="line">&gt;&gt; 4</span><br></pre></td></tr></table></figure><p>定义过程‘yahaha’，该过程会返回一个由lambda定义的过程，在该返回的过程中，会接收参数x，并返回(f x x)</p>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> SICP </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow之Seq2Seq</title>
      <link href="/2018/03/18/Deep%20Learning/TensorFlow/TensorFlow%E5%87%BD%E6%95%B0%E4%B9%8Bseq2seq/"/>
      <url>/2018/03/18/Deep%20Learning/TensorFlow/TensorFlow%E5%87%BD%E6%95%B0%E4%B9%8Bseq2seq/</url>
      <content type="html"><![CDATA[<p>详解TensorFlow中seq2seq中的常用类和方法。<br><a id="more"></a></p><h2 id="seq2seq-函数详解"><a href="#seq2seq-函数详解" class="headerlink" title="seq2seq 函数详解"></a>seq2seq 函数详解</h2><h3 id="model-with-buckets"><a href="#model-with-buckets" class="headerlink" title="model_with_buckets()"></a>model_with_buckets()</h3><p>将输入长度分成不同的间隔，这样数据的在填充时只需要填充到相应的bucket长度即可，不需要都填充到最大长度。比如buckets取[(5，10), (10，20),(20，30)…]（每个bucket的第一个数字表示source填充的长度，第二个数字表示target填充的长度，eg：‘我爱你’–&gt;‘I love you’，应该会被分配到第一个bucket中，然后‘我爱你’会被pad成长度为5的序列，‘I love you’会被pad成长度为10的序列。其实就是每个bucket表示一个模型的参数配置），这样对每个bucket都构造一个模型，然后训练时取相应长度的序列进行，而这些模型将会共享参数。</p><pre><code>1. encoder_inputs: encoder的输入，一个tensor的列表。列表中每一项都是encoder时的一个词（batch）。2. decoder_inputs: decoder的输入，同上3. targets:        目标值，与decoder_input只相差一个&lt;EOS&gt;符号，int32型4. weights:        目标序列长度值的mask标志，如果是padding则weight=0，否则weight=15. buckets:        就是定义的bucket值，是一个列表：[(5，10), (10，20),(20，30)...]6. seq2seq:        定义好的seq2seq模型，可以使用后面介绍的embedding_attention_seq2seq，embedding_rnn_seq2seq，basic_rnn_seq2seq等7. softmax_loss_function: 计算误差的函数，(labels, logits)，默认为sparse_softmax_cross_entropy_with_logits8. per_example_loss: 如果为真，则调用sequence_loss_by_example，返回一个列表，其每个元素就是一个样本的loss值。如果为假，则调用sequence_loss函数，对一个batch的样本只返回一个求和的loss值，具体见后面的分析</code></pre><h3 id="embedding-attention-seq2seq"><a href="#embedding-attention-seq2seq" class="headerlink" title="embedding_attention_seq2seq()"></a>embedding_attention_seq2seq()</h3><p>从名字我们就可以看出其实现了embedding和attention两个功能</p><ol><li>cell:                RNNCell常见的一些RNNCell定义都可以用.</li><li>num_encoder_symbols: source的vocab_size大小，用于embedding矩阵定义</li><li>num_decoder_symbols: target的vocab_size大小，用于embedding矩阵定义</li><li>embedding_size:      embedding向量的维度</li><li>num_heads:           Attention头的个数，就是使用多少种attention的加权方式，用更多的参数来求出几种attention向量</li><li>output_projection:   输出的映射层，因为decoder输出的维度是output_size，所以想要得到num_decoder_symbols对应的词还需要增加一个映射层，参数是W和B，W:[output_size,num_decoder_symbols],b:[num_decoder_symbols]</li><li>feed_previous:       是否将上一时刻输出作为下一时刻输入，一般测试的时候置为True，此时decoder_inputs除了第一个元素之外其他元素都不会使用。</li><li>initial_state_attention: 默认为False, 初始的attention是零；若为True，将从initial state和attention states开始。</li></ol><h3 id="embedding-attention-decoder"><a href="#embedding-attention-decoder" class="headerlink" title="embedding_attention_decoder()"></a>embedding_attention_decoder()</h3><p>这个函数首先对定义encoder阶段的embedding矩阵，该矩阵用于将decoder的输出转化为下一时刻输入向量或者将decoder_inputs转化为响应的词向量；然后直接调用attention_decoder函数进入attention的解码阶段。</p><ol><li>initial_state:    2D Tensor [batch_size x cell.state_size]，RNN的初始状态</li><li>attention_states: 3D Tensor [batch_size x attn_length x attn_size]，就是上面计算出来的encoder阶段的隐层向量</li><li>num_symbols:      decoder阶段的vocab_size</li><li>update_embedding_for_previous: Boolean; 只有在feed_previous为真的时候才会起作用。就是只更新‘GO’的embedding向量，其他元素保持不变。</li></ol><h3 id="attention-decoder"><a href="#attention-decoder" class="headerlink" title="attention_decoder()"></a>attention_decoder()</h3><p>这个函数可以说是Attention based seq2seq的核心函数了，最重要的attention部分和decoder部分都是在这里实现的</p><h3 id="sequence-mask"><a href="#sequence-mask" class="headerlink" title="sequence_mask()"></a>sequence_mask()</h3><p>根据目标序列长度，选出其中最大值，然后使用该值构建序列长度的mask标志。用一个sequence_mask的例子来说明起作用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([1, 3, 2], 5)</span><br><span class="line">[[True, False, False, False, False],</span><br><span class="line">[True, True, True, False, False],</span><br><span class="line">[True, True, False, False, False]]</span><br></pre></td></tr></table></figure></p><h2 id="tf-contrib-layers"><a href="#tf-contrib-layers" class="headerlink" title="tf.contrib.layers"></a>tf.contrib.layers</h2><h3 id="tf-contrib-layers-embed-sequence"><a href="#tf-contrib-layers-embed-sequence" class="headerlink" title="tf.contrib.layers.embed_sequence()"></a>tf.contrib.layers.embed_sequence()</h3><p>一般用于sequence2sequence网络，可完成对输入序列数据的嵌入工作。一般只需给出前三个参数。<br>tf.contrib.layers.embed_sequence(ids, vocab_size,  embed_dim)</p><ul><li>ids: 形状为[batch_size, doc_length]的int32或int64张量，也就是经过预处理的输入数据。</li><li>vocab_size: 输入数据的总词汇量，指的是总共有多少类词汇，不是总个数</li><li>embed_dim：想要得到的嵌入矩阵的维度</li></ul>]]></content>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于深度学习的电影推荐系统</title>
      <link href="/2018/03/11/Project/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2018/03/11/Project/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
      <content type="html"><![CDATA[<p>本项目是使用python3 + tensorflow，搭建一个基于卷积神经网络模型的离线电影推荐系统，电影数据集用的是MovieLens。核心思想是对于电影和用户的不同属性，构建多个神经网络进而获得每个属性的特征表示。使用这些特征表示构建用户特征矩阵和电影特征矩阵，进而完成：TopK电影推荐，相似用户查找等功能。<br><a id="more"></a></p><p>该项目是我关于深度学习基于推荐系统实践的练习项目，具体代码在我的GitHub上可以找到：<br><a href="https://github.com/YHfeather/Movie_Recommendation_System" target="_blank" rel="noopener">Movie_Recommendation_System</a></p><p>模型的整体架构如下图所示：<br><img src="/images/movie_recommender/overview.jpg" alt="overview"></p><h2 id="技术说明"><a href="#技术说明" class="headerlink" title="技术说明"></a>技术说明</h2><p>该项目主要使用的技术版本如下：</p><ul><li>python：3.5.2</li><li>tensorflow: 1.7.0</li><li>numpy: 1.14.0</li><li>pandas: 0.22</li></ul><h1 id="关于数据集"><a href="#关于数据集" class="headerlink" title="关于数据集"></a>关于数据集</h1><p>本项目使用的是MovieLens的ml-1m数据集，该数据集合包含6,040名用户对3,900个电影的1,000,209个匿名评论。<br><img src="/images/movie_recommender/dataset.jpeg" alt="DataSet"><br>数据集包括movies.dat, ratings.dat, users.dat三个文件</p><h4 id="movies-dat"><a href="#movies-dat" class="headerlink" title="movies.dat"></a>movies.dat</h4><p>该数据集存储了电影信息，包含字段：MovieID，Title，Genres</p><ul><li>MovieID: 电影ID(1-3952)</li><li>Title：电影标题，包括出版年份</li><li>Genres：电影类别（包括喜剧，动作剧，纪录片等..)</li></ul><p>详细内容可以参照ml-1m/README</p><h4 id="users-dat"><a href="#users-dat" class="headerlink" title="users.dat"></a>users.dat</h4><p>该数据集包含了对电影进行评分的用户信息，包括字段：UserID，Gender，Age，Occupation，Zip-code</p><ul><li>UserID：用户ID(1-6040)</li><li>Gender：性别（“M” or “F”）</li><li>Age：年龄，该年龄不是连续变量，而是被分为7个年龄集合（under 18；18-24；25-34；35-44…）</li><li>Occupation：职业，这里用数字0-20表示各个职业</li></ul><p>详细内容可以参照ml-1m/README</p><h4 id="ratings-dat"><a href="#ratings-dat" class="headerlink" title="ratings.dat"></a>ratings.dat</h4><p>该数据集是用户对电影的评分，包括字段：UserID::MovieID::Rating::Timestamp。<br>其中rating取值为：0，1，2，3，4，5<br>Timestamp表示时间戳<br>每个用户有最少20个评分</p><p>详细内容可以参照ml-1m/README</p><h1 id="文件组成"><a href="#文件组成" class="headerlink" title="文件组成"></a>文件组成</h1><p>项目共有三部分组成：1：数据下载和处理，2：模型构建和训练，3：推荐测试</p><h2 id="数据下载和处理"><a href="#数据下载和处理" class="headerlink" title="数据下载和处理"></a>数据下载和处理</h2><p>分别为<code>data_download.py</code>和<code>data_processing.py</code>文件。</p><h3 id="data-download"><a href="#data-download" class="headerlink" title="data_download"></a>data_download</h3><p>运行<code>data_download.py</code>会自动下载ml-1m数据集并解压到当前目录。<br>该文件包含<code>downl_data</code> <code>extract_data</code>两个函数和用来显示下载进度的类DLProgress</p><h3 id="data-processing"><a href="#data-processing" class="headerlink" title="data_processing"></a>data_processing</h3><p>该文件主要包含多个对原始数据进行处理的函数，将原始数据载入并进行处理，然后将处理后的数据和对应的映射和参数保存到本地。使用pickle保存为<code>.p</code>文件</p><ol><li><code>user_data_processing</code>函数对user数据进行处理，其中：<ul><li>UserID 不做处理</li><li>JobID 不做处理</li><li>Gender 将‘F’和‘M’转换0和1</li><li>Age 转换为0~6七个离散数字分别代表不同年龄段</li><li>zip-code 舍弃</li></ul></li><li><code>movie_data_processing</code>函数对movie数据进行处理，其中：<ul><li>movie_ID 不做处理</li><li>Genres 表示电影的类别，每个电影可能有多个genres标签，需要构建genre_to_index映射将每个电影的genres映射成一个定长的integer list表示</li><li>Title 首先去除掉title中的year，然后以word为粒度构建word_to_index映射将每个电影的title映射成一个定长的integer list表示</li></ul></li><li><code>rating_data_processing</code>函数对rating数据进行处理，其中：<ul><li>保留MovieID， UserID，ratings。将timestamps删除</li></ul></li><li><code>get_feature</code>函数将上述函数串联在一起，并将处理过后的数据进行拼接和切割，生成features和target两个数据，并保存在本地磁盘中。</li></ol><h2 id="模型构建和训练"><a href="#模型构建和训练" class="headerlink" title="模型构建和训练"></a>模型构建和训练</h2><p>分为<code>movie_nn.py</code> <code>user_nn.py</code> <code>traing.py</code>文件</p><h3 id="movie-nn"><a href="#movie-nn" class="headerlink" title="movie_nn"></a>movie_nn</h3><p>在该文件中，主要包含构建和movie相关的神经网络模型，主要包括对movie特征的embedding和使用卷积神经网络和常规神经网络进行特征提取。</p><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p><img src="/images/movie_recommender/movie_embedding.jpeg" alt="movie_embedding"><br>对电影的三个特征进行embedding操作。</p><h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>针对movieId和 movie genres分别构建两个神经网络进行训练，对于movie title构建 CNN with sliding window。然后将三个神经网络的输出拼接到一起输入到另一个神经网络，该神经网络的输出就是movie feature representation. 具体结构如下图所示：<br><img src="/images/movie_recommender/movie_nn.jpeg" alt="movie_nn"></p><h4 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h4><ol><li><code>get_inputs</code>定义movie属性：id，genres，title，dropout的placeholder</li><li><code>get_movie_id_embed_layer</code>构建movie id的embedding layer，并返回输入id的embedding</li><li><code>get_movie_categories_embed_layer</code>构建对movie genres的embedding layer，由于movie的genres是一个integer的list，经过embedding后会产生一个二维特征矩阵。目前仅实现对movie所有genre feature representation vector的直接加法。进而对于电影得到一个vector 的genre表示。</li><li><p><code>get_movie_cnn_layer</code>该函数对movie的title进行卷积操作。</p><ul><li>首先对title构建embedding layer。然后将生成的2维representation 扩展到三维。</li><li>然后定义一个sliding_window为一个list，包含多个integer表示窗口大小。该窗口大小即卷积过程中对一个word的卷积大小（卷积核size为：window_size * embedding_size)。</li><li>进行最大池化操作。</li><li>将多个feature map(由多个卷积核产生)连接到一起，表示movie title的representation。</li></ul></li><li><p><code>get_movie_feature_layer</code>构建一个DNN，将上述函数生成的movie属性对应的feature作为输入，训练出movie整体的feature representation。</p></li></ol><h3 id="user-nn"><a href="#user-nn" class="headerlink" title="user_nn"></a>user_nn</h3><p>对user的属性构建神经网络，主要包括对user创建embedding layers，以及创建神经网络进行进行user 特征提取。</p><h4 id="embedding-layer"><a href="#embedding-layer" class="headerlink" title="embedding layer"></a>embedding layer</h4><p>创建四个embedding layers。<br><img src="/images/movie_recommender/user_embedding.jpeg" alt="user_embedding"></p><h4 id="神经网络结构-1"><a href="#神经网络结构-1" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>分别将user的四个embedding representation vector输入到四个神经网络中，然后将四个神经网络的输出进行连接，输入到另一个神经网络中，该神经网络的输出即为user feature representation.具体结构如下图所示：<br><img src="/images/movie_recommender/user_nn.jpeg" alt="user_nn"></p><h4 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a>函数说明</h4><ol><li><code>get_inputs</code> 获取user特征的input placeholder</li><li><code>get_user_embedding</code> 对user属性：user_id, gender, age, job进行embedding</li><li><code>get_user_feature_layer</code>对于user的各个属性的embedding representation，分别构建一个小型的神经网络。然后将每个神经网络的输出，也就是各个属性的feature representation进行顺序连接，然后用一个全连接神经网络对连接后的user total feature进行训练，得到user整体的feature representation。</li><li><code>user_feature</code> 顺序连接上述方法，返回user的feature representation。</li></ol><h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p>对定义的movie_nn 和 user_nn进行训练:<br><img src="/images/movie_recommender/training.jpeg" alt="training"></p><h4 id="流程说明"><a href="#流程说明" class="headerlink" title="流程说明"></a>流程说明</h4><ol><li>首先从user_nn和movie_nn两个文件处获得各自对应的feature representation。</li><li>然后将user feature vector和movie feature vector进行矩阵乘法。得到的数据即为模型的预测输出，表示该user对该movie的预测评分</li><li>使用平方损失函数，AdamgradOptimizer进行训练。</li><li>使用tensorboard保存训练数据</li></ol><h4 id="损失曲线"><a href="#损失曲线" class="headerlink" title="损失曲线"></a>损失曲线</h4><p>训练过程中的loss curve 如下图所示：<br><img src="/images/movie_recommender/loss.png" alt="loss"></p><h2 id="推荐测试"><a href="#推荐测试" class="headerlink" title="推荐测试"></a>推荐测试</h2><p>对训练好的模型从多个方面进行测试，包含在<code>recommendation.py</code>文件中。</p><p><img src="/images/movie_recommender/recommender.jpeg" alt="recommender"></p><h4 id="函数说明："><a href="#函数说明：" class="headerlink" title="函数说明："></a>函数说明：</h4><ol><li><code>get_tensors</code> 获取测试用的placeholder</li><li><code>rating_movie</code> 给定user和movie，对模型进行正向传播，得到的分数即为预测评分</li><li><code>save_movie_feature_matrix</code> 生成电影特征矩阵，对movie部分的神经网络进行正向传播，得到每个movie的feature representation vector，并以矩阵形式保存到本地。</li><li><code>save_user_feature_matrix</code> 生成用户特征矩阵，对user部分的神经网络进行正向传播，得到每个user的feature representation vector，并以矩阵形式保存到本地。</li><li><code>recommend_save_type_movie</code> 给定movie，返回top k个和该movie最相似的movies。使用给定movie的feature vector，与整个movie feature matrix中所有其他的movie计算余弦相似度，返回相似度最大的top k个电影。</li><li><code>recommend_user_favorite_movie</code> 给定user，推荐其可能喜欢的top k个电影。用该给定user的feature vector和movie feature matrix中所有电影计算预测评分，返回top k个预测评分最大的电影。</li><li><code>recommend_other_favorite_movie</code> 给定movie，返回看过这个电影的人还可能喜欢哪些电影。首先选出top k个最喜欢给定movie的人（用给定movie的feature vector和整个user feature matrix相乘），得到这top k个人的user feature。然后分别求出每个user最喜欢的n个电影（分别用每个user的feature vector和movie feature matrix相乘，找到预测得分最高的n个电影）</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>该系统总体上来说实现了整个推荐系统的流程，包括数据处理，模型构建，模型训练以及推荐测试。但仍然有很多可以提高的地方。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Recommender System </tag>
            
            <tag> Project </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSTM和GRU运行原理</title>
      <link href="/2018/03/11/Deep%20Learning/LSTM%E5%92%8CGRU%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
      <url>/2018/03/11/Deep%20Learning/LSTM%E5%92%8CGRU%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/</url>
      <content type="html"><![CDATA[<p>在学习RNN过程中，一定会遇到LSTM和GRU，网上有很多LSTM的入门教程，内容非常详细但在我看来有些过于繁杂，没有突出重点，导致每次看懂LSTM的内在原理后过一段时间都会忘掉。这里我仅仅对LSTM和GRU的特性做出简要总结，力求简短明了，便于记忆。</p><a id="more"></a><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>我们知道RNN是在时间概念上将神经网络串联起来，当前时刻t的输入不仅仅是时刻t对应的数据，还包括RNN隐藏层在t-1时刻的输出。通过这样的方式RNN可以很好的结合时间概念上的上下文信息，这给了RNN对时序数据处理的能力。<br><img src="/images/deep_learning/rnn_basic.png" alt="rnn"><br>RNN的结构如上图所示。</p><h3 id="RNN常规变种"><a href="#RNN常规变种" class="headerlink" title="RNN常规变种"></a>RNN常规变种</h3><p>对于标准RNN，有很多变种：</p><h5 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h5><p>Deep RNN概念非常好理解，就是拥有多个隐藏层的RNN，同样对应的隐藏层的输入不仅仅包含了当前时刻上一个隐藏层的输出，还包括上一个时刻同一隐藏层的输出。</p><h5 id="Bi-Directional-RNN"><a href="#Bi-Directional-RNN" class="headerlink" title="Bi_Directional RNN"></a>Bi_Directional RNN</h5><p>有的时候我们对当前时刻的预测不仅仅根据之前时刻的信息，往往还包括之后时刻的信息。这就是Bi_Directional RNN的主要思想：不仅仅利用前一时刻RNN隐藏层的输出，还利用下一时刻RNN隐藏层的输出。<br>也就是说，当前隐藏层的输入不仅仅包含了当前时刻上一个隐藏层的输出，还包括上一个时刻同一隐藏层的输出，以及下一个时刻同一隐藏层的输出。<br>这样RNN可以更好的结合前后时刻的信息，在充满“上下文语境”的数据上往往会有更好的效果。</p><h3 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h3><p>但RNN在应用的时候往往会遇到两个问题：1.梯度消失； 2.梯度爆炸</p><ol><li>梯度消失<br>梯度消失的具体数学原理不在这里重复，主要原因是在进行反向传播更新参数时，偏导数的值小于1，当多个小于1的值相乘时传递到之前的梯度会变得非常小几乎接近于0，也就是说模型“训练不动了”。<br>对应到RNN中，梯度消失往往会导致长期记忆问题，也就是说RNN只会“着眼”于最近的几个相邻时刻的信息，而对于更远的信息RNN几乎没有办法获取。</li><li>梯度爆炸<br>与梯度消失对应的是梯度爆炸，数学原理同样略去，在网上有很多非常详细的教程。梯度爆炸主要指的是偏导数的值远大于1，多个大于1的值相乘会导致参数的更新变化幅度非常大甚至达到NaN。</li></ol><h2 id="LSTM的原理"><a href="#LSTM的原理" class="headerlink" title="LSTM的原理"></a>LSTM的原理</h2><p>本质上是增加了一个叫cell的结构，用来保存神经网络的“记忆”。每次训练时，一个LSTM中共发生三种变换：遗忘门，记忆门，输出门。<br><img src="/images/deep_learning/lstm.jpg" alt="lstm"></p><ul><li><strong>遗忘门</strong> 首先连接当前时刻输入xt和上一时刻的输出h(t-1)，经过sigmoid，计算遗忘门，与cell相乘，相当于对cell内容进行选择性遗忘。<br><img src="/images/deep_learning/forget_gate.jpg" alt="forget_gate"></li><li><strong>记忆门</strong> 对xt和h(t-1)进行tanh变换得到想要记忆到cell中的信息h’，同时对xt&amp;h(t-1)进行sigmoid变换得到记忆门阀。将h’和记忆门相乘后与cell相加，作为cell当前时刻的state<br><img src="/images/deep_learning/update_gate.png" alt="update_gate"></li><li><strong>输出门</strong> 对x&amp;h(t-1)进行sigmoid得到输出门，同时对当前cell的state进行tanh变换，然后与输出门相乘，作为当前时刻LSTM的输出。<br><img src="/images/deep_learning/output_gate.jpg" alt="output_gate"><h3 id="LSTM运行过程"><a href="#LSTM运行过程" class="headerlink" title="LSTM运行过程"></a>LSTM运行过程</h3>LSTM内部主要有三个阶段，分别对应着三个门：</li></ul><ol><li>遗忘阶段：这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。</li><li>选择记忆阶段：这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。</li><li>输出阶段：这个阶段将决定哪些将会被当成当前状态的输出。</li></ol><h3 id="LSTM总结"><a href="#LSTM总结" class="headerlink" title="LSTM总结"></a>LSTM总结</h3><p>通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。<br>但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p><h2 id="GRU-门循环单元"><a href="#GRU-门循环单元" class="headerlink" title="GRU 门循环单元"></a>GRU 门循环单元</h2><p>GRU是LSTM的一种变体，保持了LSTM的效果同时又使结构更加简单，相比LSTM，GRU能够达到几乎相同的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><h3 id="GRU结构"><a href="#GRU结构" class="headerlink" title="GRU结构"></a>GRU结构</h3><p>GRU的结构和普通的RNN一样，不同于LSTM使用了cell和hidden state两种状态。GRU只有hidden state。</p><h4 id="GRU门结构"><a href="#GRU门结构" class="headerlink" title="GRU门结构"></a>GRU门结构</h4><p>在GRU中，将LSTM的输入门，遗忘门，输出门合并成了两个门：<strong>更新门z和重置门r，两个门都是对当前时刻输入和上一个hidden state结合后的向量进行sigmoid。</strong></p><h4 id="GRU流程"><a href="#GRU流程" class="headerlink" title="GRU流程"></a>GRU流程</h4><h5 id="1-重置hidden-state"><a href="#1-重置hidden-state" class="headerlink" title="1 重置hidden state"></a>1 重置hidden state</h5><p>首先使用重置门r和上一个hidden state h(t-1)相乘，即对state进行重置后，得到重置后的状态h(t-1)’。</p><h5 id="2-计算h’"><a href="#2-计算h’" class="headerlink" title="2 计算h’"></a>2 计算h’</h5><p>然后将其与当前时刻输入xt进行拼接，在通过一个tanh函数将数据压缩到-1~1之间，得到h’。</p><h5 id="3-更新记忆阶段"><a href="#3-更新记忆阶段" class="headerlink" title="3 更新记忆阶段"></a>3 更新记忆阶段</h5><p>GRU的最后一个阶段叫“更新记忆”阶段，在这个阶段，同时进行“遗忘”和“记忆”两个步骤<br>此时，h’主要包含了当前时刻的输入xt数据。接下来，需要有针对的将h’添加到当前时刻的hidden state中。相当于“记忆了当前时刻的状态”。<br>我们用更新门z和上一个时刻的hidden state：h(t-1)以及上一步计算得到的h’计算当前时刻的hidden state：ht。</p><blockquote><p>ht = z*h(t-1) + (1-z)h’</p></blockquote><p>更新门（这里的 z ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。<br><img src="/images/deep_learning/gru_2.jpg" alt="gru_2"></p><h3 id="GRU特点"><a href="#GRU特点" class="headerlink" title="GRU特点"></a>GRU特点</h3><p>GRU最大的特点就是使用了同一个门控 z 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。</p><ul><li>z*h(t-1) 表示对上一个hidden state的选择性遗忘，这时更新门可以看做遗忘门</li><li>(1-z)h’ 表示对包含当前时刻输入信息的h’进行选择性记忆，(1-z)也会选择性的忘记h’中一些不重要的信息。</li><li>综上，ht即包含了对上一个时刻t-1信息的选择遗忘，又包含了对当前时刻的选择性记忆</li><li>这里z和(1-z)是联动的，也就是说，对t-1遗忘的越多，对t的记忆就越多，反之亦然。</li></ul><p><img src="/images/deep_learning/gru.png" alt="gru"><br>图中zt为更新门，rt为重置门</p><h3 id="GRU和LSTM的关系"><a href="#GRU和LSTM的关系" class="headerlink" title="GRU和LSTM的关系"></a>GRU和LSTM的关系</h3><p>GRU提出于2014年，LSTM是1997年。GRU的本质结构是和LSTM相似的。<br>我们知道重置门r可以将h(t-1)和xt转换成h’。<br>这里的h’实际上就是LSTM中的hidden state；而GRU中上一个节点传下来的h(t-1)则对应LSTM中的cell state。 z对应着LSTM中的遗忘门，(1-z)对应LSTM中的更新门。<br><img src="/images/deep_learning/compare.png" alt="compare"></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>推荐系统实战笔记</title>
      <link href="/2018/02/18/Reading/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/02/18/Reading/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="Chapter-One好的推荐系统"><a href="#Chapter-One好的推荐系统" class="headerlink" title="Chapter One好的推荐系统"></a>Chapter One好的推荐系统</h1><h2 id="1-1-什么是推荐系统"><a href="#1-1-什么是推荐系统" class="headerlink" title="1.1 什么是推荐系统"></a>1.1 什么是推荐系统</h2><p>在用户并没有一个明确的需求时，就需要一个自动化工具，它可以分析你的历史兴趣，从庞大的物品中找到可能符合你口味的商品，这个工具就是个性化推荐系统。<br>推荐系统和搜索引擎对于用户来说是两个互补的工具，搜索引擎满足了用户有明确目的时的主动查找需求，而推荐系统能够在用户没有明确目的的时候帮助他们发现感兴趣的新内容。</p><h2 id="1-2-个性化推荐系统的应用"><a href="#1-2-个性化推荐系统的应用" class="headerlink" title="1.2 个性化推荐系统的应用"></a>1.2 个性化推荐系统的应用</h2><p>几乎所有推荐系统应用都是由以下三部分组成：</p><ol><li>前台的展示页面</li><li>后台的日志系统</li><li>推荐算法系统</li></ol><h2 id="1-3-推荐系统评测"><a href="#1-3-推荐系统评测" class="headerlink" title="1.3 推荐系统评测"></a>1.3 推荐系统评测</h2><p>一个完整的推荐系统一般存在3个参与方：</p><ol><li>用户</li><li>物品提供者</li><li>提供推荐系统的网站<br>以图书推荐为例，首先需要满足用户的需求，给用户推荐令他们感兴趣的书。其次，需要让各种小众书籍都被推荐给相应的用户，而不是只推荐大出版社的热门书籍。同时，好的推荐系统设计能够让推荐系统不断收集高质量的用户反馈，不断完善推荐质量。因此，一个好的推荐系统是一个三方共赢的产物。</li></ol><h3 id="1-3-1-推荐系统实验方法"><a href="#1-3-1-推荐系统实验方法" class="headerlink" title="1.3.1 推荐系统实验方法"></a>1.3.1 推荐系统实验方法</h3><p>在推荐系统中，主要有3中评测推荐效果的试验方法：</p><ol><li>离线实验</li><li>用户调查</li><li>在线实验</li></ol><h5 id="1-离线实验"><a href="#1-离线实验" class="headerlink" title="1. 离线实验"></a>1. 离线实验</h5><p>离线实验的方法由一下几个步骤构成：</p><ol><li>通过日志系统获得用户行为数据，并按照一定格式生成标准数据集</li><li>将数据集按照一定的规则分成训练集和测试集</li><li>在训练集上训练用户兴趣模型，并在测试集上进行测试</li><li>通过事先定义的离线指标评测算法在测试集上的效果<br>离线实验可以说是一个静态的试验方法，不需要用户参与，只需要静态数据即可，有 <strong>不需要用户参与；方便快捷；可以测试大量算法的优点</strong> ，缺点是 <strong>无法获得很多商业上关注的指标，如点击率转化率等</strong></li></ol><h5 id="2-用户调查"><a href="#2-用户调查" class="headerlink" title="2. 用户调查"></a>2. 用户调查</h5><p>在离线实验中，高预测准确率并不等于高用户满意度，因此用户调查就成了推荐系统测评的一个重要工具。<br>用户调查的优缺点也很明显，优点是：<strong>可以获得用户主观指标</strong>， 缺点是： <strong>募集测试用户代价较大，成本高，而测试用户如果过少也会失去意义</strong></p><h5 id="3-在线实验"><a href="#3-在线实验" class="headerlink" title="3. 在线实验"></a>3. 在线实验</h5><p>AB测试是一种常见在线实验方法，通过将用户随机分为几组，并对每组采用不同算法，然后通过计算每组不同的指标来评价算法性能。 <strong>优点是准确，缺点是实验复杂，周期长</strong>，所以只会对在1，2步骤表现优秀的算法进行AB实验。</p><h3 id="1-3-2-评测指标"><a href="#1-3-2-评测指标" class="headerlink" title="1.3.2 评测指标"></a>1.3.2 评测指标</h3><h5 id="1-用户满意度"><a href="#1-用户满意度" class="headerlink" title="1 用户满意度"></a>1 用户满意度</h5><p>用户满意度只可以通过用户调查和在线实验获得。需要注意的是，设计调查问卷时需要一些技巧，而不仅仅是简单的询问用户对结果是否满意。在线实验中，可以对一些用户行为进行计算的到满意度。如点击率，停留时间，转化率等指标。</p><h5 id="2-预测准确度"><a href="#2-预测准确度" class="headerlink" title="2 预测准确度"></a>2 预测准确度</h5><p>该指标度量的是算法预测用户行为的能力。是非常重要的离线评测指标。预测准确度的计算方法同传统机器学习方法一样（训练模型，测试模型）。在预测准确度中，也有不同的度量指标：</p><ol><li>评分预测：<ol><li>预测用户对物品的评分，</li><li>预测评分的误差计算可以使用平方根误差RMSE和绝对值误差MAE。</li><li>RMSE被认为加大了对预测不准的用户物品评分的惩罚，因而对系统的评测更加苛刻。</li></ol></li><li>TopN推荐<ol><li>对于一个用户，系统会给该用户生成一个推荐列表，即TopN推荐</li><li>TopN推荐一般通过准确率和召回率来度量</li></ol></li><li>覆盖率<ol><li>简单的定义为系统能够推荐出来的物品占总物品集合的比例。</li><li>覆盖率是一个内容提供商会关心的指标</li><li>覆盖率为100%的系统可以将每个物品都推荐给至少一个用户</li><li>好的系统不仅仅需要较高的用户满意度，还要较高的覆盖率</li><li>可以用每个商品被推荐的数量用来计算覆盖率</li><li>可以用 <strong>覆盖率</strong> 和 <strong>基尼系数</strong> 衡量</li></ol></li><li>多样性<ol><li>推荐列表需要比较多样，覆盖用户绝大多数的兴趣点</li><li>可以用相似度算法计算推荐列表items之间的相似度</li><li>相似度最好的情况是满足用户的喜爱程度。</li></ol></li><li>新颖性</li><li>惊喜度<ol><li>如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果惊喜度很高。而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果</li></ol></li><li>信任度<ol><li>让用户对推荐结果产生信任是非常重要的</li><li>同样结果以信任方式推荐给用户会让用户满意，而如果以广告的形式用户会些许不满。</li><li>首先可以增加推荐系统透明度，主要方法是提供推荐解释。也可以利用好友信息给用户做推荐，并用好友进行推荐解释。</li></ol></li><li>实时性<ol><li>在有些网站中，推荐的实时性会非常重要。</li><li>首先是对用户的实时性，需要对推荐列表的item更新更加准确</li><li>然后是对商品的实时性，对于新加入的商品，如何正确推荐给用户，这涉及到考验系统 <strong>冷启动</strong> 的能力</li></ol></li><li>健壮性<ol><li>大型网站的推荐系统会面临攻击，健壮性指标衡量了一个推荐系统抗击作弊的能力。</li><li>可以通过模拟攻击来计算算法的健壮性</li><li>设计推荐系统时尽量使用代价比较高的用户行为</li><li>在使用数据前，进行攻击检测，从而对攻击数据进行清理</li></ol></li><li>商业指标<ol><li>不同的网站具有不同的商业指标，和盈利能力息息相关。</li></ol></li></ol><h3 id="1-3-3-评价维度"><a href="#1-3-3-评价维度" class="headerlink" title="1.3.3 评价维度"></a>1.3.3 评价维度</h3><p>一个推荐算法，可能整体性能不是很好，但在某种情况下性能比较好，一般来说，评测维度主要是以下三种：</p><pre><code>1. 用户维度2. 物品维度3. 时间维度</code></pre><h1 id="Chapter-Two-利用用户行为数据"><a href="#Chapter-Two-利用用户行为数据" class="headerlink" title="Chapter Two 利用用户行为数据"></a>Chapter Two 利用用户行为数据</h1><p>基于用户行为分析的推荐算法被称为协同过滤算法。</p><h2 id="用户行为数据"><a href="#用户行为数据" class="headerlink" title="用户行为数据"></a>用户行为数据</h2><ol><li>用户行为被保存在服务器上的种种日志文件中</li><li>用户行为在个性推荐系统中一般分为两种：显性反馈行为和隐性反馈行为<ul><li>显性反馈行为包括用户明确表示对物品喜好的行为</li><li>隐性反馈虽然不能明确反应用户喜好，但数量庞大，获取容易<h2 id="基于邻域的算法"><a href="#基于邻域的算法" class="headerlink" title="基于邻域的算法"></a>基于邻域的算法</h2>基于邻域的算法是推荐系统中最基本的算法。基于邻域的算法分为两大类，一类是基于用户的协同过滤算法，另一类是 基于物品的协同过滤算法。<h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3>基于用户的协同过滤算法本质上是 <strong>找到与需要推荐的用户A有相似兴趣的其他用户，然后把那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法称为基于用户的协同过滤算法。</strong><br>从上面的描述中可以看到，基于用户的协同过滤算法主要包括两个步骤。</li></ul></li><li>找到和目标用户兴趣相似的用户集合。</li><li>找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。<br>步骤1的关键在于计算两个用户的相似度。这里协同过滤算法主要利用 <strong>行为的相似度计算兴趣的相似度</strong>。<br>我们可以通过Jaccard公式简单计算u，v两个用户的相似度。<strong>Jaccard公式计算方法为：找到两个用户分别有过正反馈的物品集合，计算两个集合的交集和集合并集的比值即为相似度。</strong> Jaccard相似度的取值为[0, 1]，越大两个用户越相似。<br>还有一种方法是计算余弦相似度。<br>常规UserCF对两两用户都利用余弦相似度计算相似度。这种方法的时间复杂度是O(|U|*|U|)，这在用户数很大时非常耗时。事实上，很多用户相互之间并没有对同样的物品产生过行为。所以对于最基本的UserCF的一个优化就是：<strong>首先找到所有兴趣集合并集不为空的用户对，然后只计算这些用户之间的余弦相似度。</strong><br>为此，可以首先建立 <strong>物品到用户的倒排表</strong>，对于每个物品都保存对该物品产生过行为的用户列表。然后建立稀疏矩阵C[u][v]表示用户u和用户v兴趣集合的并集。最后扫描倒排列表中每个物品对应的用户列表，将用户列表中的两两用户对应的C[u][v]加一。最后可以得到所有有共同兴趣的用户以及共同行为的个数。<br><img src="/images/recommender_practice/daopaibiao.png" alt="用户行为倒排表"><br><img src="/images/recommender_practice/UserCF.png" alt="UserCF算法"><h5 id="用户相似度计算的改进"><a href="#用户相似度计算的改进" class="headerlink" title="用户相似度计算的改进"></a>用户相似度计算的改进</h5>两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度。在普通的UserCF基础上提出了User-IIF算法：<br><img src="/images/recommender_practice/UserIIF.png" alt="User-IIF算法"><h5 id="UserCF实际应用"><a href="#UserCF实际应用" class="headerlink" title="UserCF实际应用"></a>UserCF实际应用</h5>相比基于物品的协同过滤算法(ItemCF)， UserCF在目前的实际应用中使用并不多。主要原因是：首先，随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。其次，基于用户的协同过滤很难对推荐结果作出解释。</li></ol><h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p>基于物品的协同过滤算法是目前业界应用最多的算法。 无论是亚马逊网，还是Netflix、Hulu、YouTube，其推荐算法的基础都是该算法。 <strong>基于物品的协同过滤算法(简称ItemCF)给用户推荐那些和他们之前喜欢的物品相似的物品。</strong> 不过，ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。<strong>该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品 B。</strong></p><h4 id="ItemCF算法步骤"><a href="#ItemCF算法步骤" class="headerlink" title="ItemCF算法步骤"></a>ItemCF算法步骤</h4><p>基于物品的协同过滤算法主要分为两步。</p><ol><li>计算物品之间的相似度。</li><li>根据物品的相似度和用户的历史行为给用户生成推荐列表。<br>两个物品的相似度Wij可以用如下公式计算：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wij = (N(i)交N(j)) / sqrt(N(i)*N(j))</span><br></pre></td></tr></table></figure></li></ol><p>Wij可以解释为：<strong>同时喜欢商品i和商品j的人的交集的人数 / 喜欢商品i的人数*喜欢商品j的人数</strong></p><p>ItemCF通过如下公式计算用户u对一个物品j的兴趣<br><img src="images/recommender_practice/ItemCF.png" alt="ItemCF"><br>这里N(u)是用户喜欢的物品的集合，S(j,K)是和物品j最相似的K个物品的集合，wji是物品j和i 的相似度，rui是用户u对物品i的兴趣。<br>该公式的核心思想是：<strong>找出和商品j最相近的k个商品中被用户喜欢的商品is，计算所有is和j的相似度的和</strong><br>该公式的含义是，<strong>和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</strong></p><h6 id="参数K"><a href="#参数K" class="headerlink" title="参数K"></a>参数K</h6><p>参数K是ItemCF算法的重要参数，它对推荐算法的各种指标都会产生一些列的影响：</p><ul><li>精度（准确率和召回率）：准确率和召回率与参数k并不呈正相关或者负相关，但是选择合适的K对于获得推荐系统高的精度比较重要。</li><li>流行度：随着K的增大，推荐结果的流行度会逐渐提高，但是当K增加到一定的程度，流行度就不会再有明显变化。</li><li>覆盖率：K越大，覆盖率会相应地降低</li></ul><h4 id="用户活跃度对物品相似度的影响"><a href="#用户活跃度对物品相似度的影响" class="headerlink" title="用户活跃度对物品相似度的影响"></a>用户活跃度对物品相似度的影响</h4><p>假设有这么一个用户，他是开书店的，假设当当网有100万本书，他买了80万本准备开店卖。从前面ItemCF算法可知，这意味着因为存在这么一个用户，有80万本书两两之间就产生了相似度，也就是说，内存里即将诞生一个80万乘80万的稠密矩阵。但这个人的存在对于ItemCF是没有意义的。</p><h5 id="ItemCF-IUF-Inverse-User-Frequence"><a href="#ItemCF-IUF-Inverse-User-Frequence" class="headerlink" title="ItemCF-IUF(Inverse User Frequence)"></a>ItemCF-IUF(Inverse User Frequence)</h5><p>IUF即用户活跃度对数的倒数的参数，IUF认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF 参数来修正物品相似度的计算公式:<br><img src="images/recommender_practice/IUF.png" alt="IUF"></p><p>在实际应用中，对于上面这种过于活跃的用户，一般直接将其删除。</p><h4 id="物品相似度的归一化"><a href="#物品相似度的归一化" class="headerlink" title="物品相似度的归一化"></a>物品相似度的归一化</h4><p>如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率。1 其研究表明，如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度 矩阵w’:<br><img src="images/recommender_practice/itemcf_guiyihua.png" alt="guiyihua"><br>归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。 一般来说，物品总是属于很多不同的类，每一类中的物品联系比较紧密。</p><h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p>举一个例子，假设物品分为两类——A和B，A类物品之间的相似 度为0.5，B类物品之间的相似度为0.6，而A类物品和B类物品之间的相似度是0.2。在这种情况下， 如果一个用户喜欢了5个A类物品和5个B类物品，用ItemCF给他进行推荐，推荐的就都是B类物品， 因为B类物品之间的相似度大。但如果归一化之后，A类物品之间的相似度变成了1，B类物品之 间的相似度也是1，那么这种情况下，用户如果喜欢5个A类物品和5个B类物品，那么他的推荐列 表中A类物品和B类物品的数目也应该是大致相等的。从这个例子可以看出，相似度的归一化可 以提高推荐的多样性。</p><h4 id="物品过于热门问题"><a href="#物品过于热门问题" class="headerlink" title="物品过于热门问题"></a>物品过于热门问题</h4><p>如果一个物品过于热门，仅仅在分母上乘该喜欢该物品的人数仍然过于热门，可以在分母的上加上α的幂加大惩罚。通过这种方法可以在适当牺牲准确率和召回率的情况下显著提升结果的覆盖率和新颖性(降低流行度即提高了新颖性)。</p><h3 id="ItemCF和UserCF的对比"><a href="#ItemCF和UserCF的对比" class="headerlink" title="ItemCF和UserCF的对比"></a>ItemCF和UserCF的对比</h3><p>UserCF给用户推荐那些和他有共同兴 趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算 法的原理可以看到，UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF 的推荐结果着重于维系用户的历史兴趣。换句话说，UserCF的推荐更社会化，反映了用户所在的 6 小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。</p><p><img src="images/recommender_practice/UserCompareItem.png" alt="UserCompareItem"></p><h3 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h3><p>LFM（latent factor model）隐语义模型在推荐系统中表现了强大的威力，LFM主要用于找到文本的隐含语义，可以应用在TopK推荐中。</p><h4 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h4><p>LFM的核心思想是：<strong>通过隐含特征联系用户兴趣和物品</strong>，LFM采用基于用户行为统计的自动聚类，LFM有很多衍生的手段，包括pLSA，LDA，隐含类别模型，隐含主题模型，矩阵分解等。<br>LFM通过如下公式计算用户u对物品i的兴趣：<br><img src="images/recommender_practice/lfm_format.png" alt="lfm_format"></p><p>所以，想使用LFM模型，需要一个训练集，对于每个用户u，训练集里都包含了用户u喜欢的物品和不感兴趣的物品，通过学习这个数据集，就可以获得上面的模型参数。而对于隐性反馈数据集来说，只有正样本没有负样本，所以LFM的第一个问题就是如何给每个用户生成负样本。负样本的生成可以遵循以下原则：</p><ol><li>对每个用户，正负样本保持平衡</li><li>对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品。<br>一般认为，很热门而用户却没有行为更加代表用户对这个物品不感兴趣。因为对于冷门的物 品，用户可能是压根没在网站中发现这个物品，所以谈不上是否感兴趣。</li></ol><p>经过采样，可 以得到一个用户—物品集 K = {(u, i)} ，其中如果(u, i)是正样本，则有 rui = 1 ，否则有 rui = 0 。然后，需要优化如下的损失函数来找到最合适的参数p和q:</p><p><img src="images/recommender_practice/lfm_format2.png" alt="lfm_format2"><br>这里面的K即是隐类，LFM通过隐类将User和Item关联起来。</p><h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><ol><li>隐特征的个数K</li><li>梯度下降的学习速率alpha</li><li>正则化参数lambda</li><li>每个用户的正负样本比例ratio<br>通过实验发现，ratio参数对LFM的性能影响最大。随着负样本数目的增加，覆盖率不 断降低，而推荐结果的流行度不断增加，说明ratio参数控制了推荐算法发掘长尾的能力。<h4 id="LFM缺点"><a href="#LFM缺点" class="headerlink" title="LFM缺点"></a>LFM缺点</h4></li></ol><ul><li>当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</li><li>LFM很难实现实时的推荐。经典的LFM模型每次训练时都需要扫描所有的用户行为记录，这样才能计算出用户隐类向量(pu)和物品隐类向量(qi)。而且LFM的训练需要在用户行为记录上反复迭代才能获得比较好的性能。因此，LFM的每次训练都很耗时，一般在实际应用中只能每天训练一次，并且计算出所有用户的推荐结果。</li><li>在新闻推荐中，冷启动问题非常明显。每天都会有大量新的新闻。这些新闻会在很短的时间内获得很多人的关注，但也会在很短的时间内失去用户的关注。因此，它们的生命周期很短，而推荐算法需要在它们短暂的生命周期内就将其推荐给对它们感兴趣的用户。</li></ul><h3 id="LFM和基于邻域方法的比较"><a href="#LFM和基于邻域方法的比较" class="headerlink" title="LFM和基于邻域方法的比较"></a>LFM和基于邻域方法的比较</h3><p>LFM是一种基于机器学习的方法，具有比较好的理论基础。这个方法和基于邻域的方法(比 如UserCF、ItemCF)相比，各有优缺点。下面将从不同的方面对比LFM和基于邻域的方法。</p><ol><li>理论基础： LFM具有较好的理论基础，他是一种学习方法，而基于邻域的方法是一种基于统计的方法。</li><li>离线计算的空间复杂度：基于邻域的方法需要维护一张离线的相关表。假设有M个用户和N个物品， 在计算相关表的过程中，我们可能会获得一张比较稠密的临时相关表， 那么假设是用户相关表，则需要O(M<em>M)的空间，而对于物品相关表，则需要O(N</em>N)的空间。而LFM在建模过程中，如果是F个隐类，那么它需要的存储空间是O(F*(M+N))，这在 M和N很大时可以很好地节省离线计算的内存。</li><li>离线计算的时间复杂度：假设有M个用户、N个物品、K条用户对物品的行为记录。那么， UserCF计算用户相关表的时间复杂度是O(N <em> (K/N)^2)，而ItemCF计算物品相关表的时间 复杂度是O(M</em>(K/M)^2)。而对于LFM，如果用F个隐类，迭代S次，那么它的计算复杂度 是O(K <em> F </em> S)。在一般情况下，LFM的时间复杂度要 稍微高于UserCF和ItemCF，这主要是因为该算法需要多次迭代。但总体上，这两种算法 在时间复杂度上没有质的差别。</li><li>在线实时推荐：UserCF和ItemCF在线服务算法需要将相关表缓存在内存中，然后可以在线进行实时的预测。以ItemCF算法为例，一旦用户喜欢了新的物品，就可以通过查询内存中的相关表将和该物品相似的其他物品推荐给用户。因此，一旦用户有了新的行为， 而且该行为被实时地记录到后台的数据库系统中，他的推荐列表就会发生变化。而从LFM在给用户生成推荐列表时，需要计算用户对所有物品的兴趣 权重，然后排名，返回权重最大的N个物品。因此，LFM不太适合用于物品数非常庞大的系统。另一方面，LFM在生成一个用户推荐列表时速度太慢，因此不能在线实 时计算，而需要离线将所有用户的推荐结果事先计算好存储在数据库中。因此，LFM不 能进行在线实时推荐，也就是说，当用户有了新的行为后，他的推荐列表不会发生变化。</li><li>推荐解释 ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果。 但LFM无法提供这样的解释，它计算出的隐类虽然在语义上确实代表了一类兴趣和物品， 却很难用自然语言描述并生成解释展现给用户。<h3 id="基于图模型"><a href="#基于图模型" class="headerlink" title="基于图模型"></a>基于图模型</h3>用户行为很容易用二分图表示，因此很多图的算法都可以用到推荐系统中。<h4 id="用户行为数据的二分图表示"><a href="#用户行为数据的二分图表示" class="headerlink" title="用户行为数据的二分图表示"></a>用户行为数据的二分图表示</h4>设用户行为 数据是由一系列二元组组成的，其中每个二元组(u, i)表示用户u对物品i产生过行为。这种数据集 很容易用一个二分图表示。<br><img src="images/recommender_practice/erfentu.png" alt="erfentu"></li></ol><p>将用户行为表示为二分图模型后，下面的任务就是在二分图上给用户进行个性化推荐。如果将个性化推荐算法放到二分图模型上，那么给用户u推荐物品的任务就可以转化为 <strong>度量用户顶点 vu和与vu没有边直接相连的物品节点在图上的相关性</strong>，相关性越高的物品在推荐列表中的权重就越高。</p><h4 id="基于随机游走的PersonalRank算法"><a href="#基于随机游走的PersonalRank算法" class="headerlink" title="基于随机游走的PersonalRank算法"></a>基于随机游走的PersonalRank算法</h4><p>假设要给用户u进行个性化推荐，可以从用户u对应的节点vu开始在用户物品二分图上进行随 机游走。游走到任何一个节点时，首先按照概率α决定是继续游走，还是停止这次游走并从vu节点开始重新游走。如果决定继续游走，那么就从当前节点指向的节点中按照均匀分布随机选择一个节点作为游走下次经过的节点。这样，经过很多次随机游走后，每个物品节点被访问到的概率会收敛到一个数。最终的推荐列表中物品的权重就是物品节点的访问概率。<br><img src="images/recommender_practice/PersonalRank.png" alt="PersonalRank"></p><h5 id="PersonalRank缺点"><a href="#PersonalRank缺点" class="headerlink" title="PersonalRank缺点"></a>PersonalRank缺点</h5><p>该算法在时间复杂度上 有明显的缺点。因为在为每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直到 整个图上的每个顶点的PR值收敛。这一过程的时间复杂度非常高，不仅无法在线提供实时推荐， 甚至离线生成推荐结果也很耗时。<br>为了解决PersonalRank每次都需要在全图迭代并因此造成时间复杂度很高的问题，这里给出 两种解决方案。第一种很容易想到，就是减少迭代次数，在收敛之前就停止。这样会影响最终的 精度，但一般来说影响不会特别大。另一种方法就是从矩阵论出发，重新设计算法。我们可以将PersonalRank转化为矩阵的形式。令M为用户物品二 分图的转移概率矩阵</p><h1 id="推荐系统冷启动问题"><a href="#推荐系统冷启动问题" class="headerlink" title="推荐系统冷启动问题"></a>推荐系统冷启动问题</h1><p>冷启动问题主要分3类。</p><ul><li>用户冷启动：对于新用户，我们没有该用户的行为，无法做个性化推荐的问题。</li><li>物品冷启动：对于新物品，我们没有用户对该物品的行为，无法做个性化推荐的问题。</li><li>系统冷启动：系统冷启动主要解决如何在一个新开发的网站上(无用户，只有一些物品的信息)设计个性化推荐系统。</li></ul><p>针对上述冷启动问题，普遍有如下解决方法</p><ul><li>提供非个性化的推荐：直接推荐热门项目，收集User信息</li><li>利用用户注册时的信息做粗粒度个性化</li><li>利用用户社交网络账号登录导入社交网站的好友信息。</li><li>用户注册时要求用户对一些分类或者物品做出反馈</li><li>对于新加入的物品，可以利用内容信息，将它们推荐给喜欢过和它们相似的物品的用户。</li><li>在系统冷启动时，可以引入专家的知识，通过一定的高效方式迅速建立起物品的相关度表。</li></ul><h1 id="利用用户标签数据"><a href="#利用用户标签数据" class="headerlink" title="利用用户标签数据"></a>利用用户标签数据</h1><p>让普通用户给物品打标签（UGC），这种方法是一种表示用户兴趣和物品语义的重要方式。当一个用户对一个物品打上标签，这个标签一方面描述了用户的兴趣，另一方面则表示了物品的语义，从而将用户和物品联系起来。</p><h4 id="To-Be-Continue"><a href="#To-Be-Continue" class="headerlink" title="To Be Continue"></a>To Be Continue</h4>]]></content>
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Recommender System </tag>
            
            <tag> Reading </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Atom运行Python3</title>
      <link href="/2018/02/11/Knowledge/atom%E8%BF%90%E8%A1%8Cpython%E5%B9%B6%E4%BF%AE%E6%94%B9%E4%B8%BApython3/"/>
      <url>/2018/02/11/Knowledge/atom%E8%BF%90%E8%A1%8Cpython%E5%B9%B6%E4%BF%AE%E6%94%B9%E4%B8%BApython3/</url>
      <content type="html"><![CDATA[<p>atom是一个由GitHub开发的开源编辑器，以往我是sublime的忠实粉丝，但sublime在Linux下不支持中文输入，没办法愉快的写注释了，只好投奔atom。该文介绍的是如何在atom下运行python。同时应为Mac的默认python版本为2.7，该文也会介绍如何修改atom的运行版本为python3</p><a id="more"></a><h2 id="下载script插件"><a href="#下载script插件" class="headerlink" title="下载script插件"></a>下载script插件</h2><p>点击左上角“Atom” -&gt; “preference” -&gt; “install”，在搜索框输入“script”下载安装script运行插件。下载完成后，我们可以新建一个py文件进行测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">print(sys.version)</span><br></pre></td></tr></table></figure></p><p>运行快捷键：Mac：”command + i”，我们可以看到，控制台输出的python版本为2.7，如果python2.7刚好满足你的日常编程需要，到此atom运行python就搞定了。</p><h2 id="修改python3"><a href="#修改python3" class="headerlink" title="修改python3"></a>修改python3</h2><p>如果你使用的是python3，那么需要修改一下atom的默认python运行版本。点击左上角atom：<br>preference –&gt; Open Config Folder –&gt; packages –&gt; lib –&gt; grammars –&gt; python.coffee<br>可以看到：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">exports.Python =</span><br><span class="line">  &apos;Selection Based&apos;:</span><br><span class="line">    command: &apos;python3&apos;</span><br><span class="line">    args: (context) -&gt; [&apos;-u&apos;, &apos;-c&apos;, context.getCode()]</span><br><span class="line"></span><br><span class="line">  &apos;File Based&apos;:</span><br><span class="line">    command: &apos;python3&apos;</span><br><span class="line">    args: (&#123;filepath&#125;) -&gt; [&apos;-u&apos;, filepath]</span><br></pre></td></tr></table></figure></p><p>修改command: ‘python’ 为 “python3” 即可（本文已经进行修改）</p><h2 id="python2和python3并存"><a href="#python2和python3并存" class="headerlink" title="python2和python3并存"></a>python2和python3并存</h2><p>如果你是大部分时间需要python2，偶尔运行一下python3，那么往复修改配置文件就会显得麻烦，script插件可以配置多个运行环境满足该需求：</p><ul><li>点击上方“packages” -&gt; “Script” -&gt; “configure script”</li><li>到此会出现一个对话框，在“commend”输入python3即可，并‘run’一下，就会发现当前文件已经使用python3进行运行。</li><li>完成上述配置后，以后如果想要运行python3，可以使用快捷键：’command + shift + k’，就会出现你已经配置好的运行环境，再按一下回车就可以运行了。</li><li>默认的python2运行快捷键仍然是 “command + i”</li></ul>]]></content>
      
      <categories>
          
          <category> Skills </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Skills </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++常用头文件</title>
      <link href="/2018/02/11/Programming%20Language/CPP/C++%E5%B8%B8%E7%94%A8%E5%A4%B4%E6%96%87%E4%BB%B6/"/>
      <url>/2018/02/11/Programming%20Language/CPP/C++%E5%B8%B8%E7%94%A8%E5%A4%B4%E6%96%87%E4%BB%B6/</url>
      <content type="html"><![CDATA[<p>总结C++常用的头文件以及常用的函数。（除STL）<br><a id="more"></a></p><h2 id="ifndef"><a href="#ifndef" class="headerlink" title="#ifndef"></a>#ifndef</h2><p>头件的中的#ifndef，这是一个很关键的东西。比如你有两个C文件，这两个C文件都include了同一个头文件。而编译时，这两个C文件要一同编译成一个可运行文件，于是问题来了，大量的声明冲突。</p><p>还是把头文件的内容都放在#ifndef和#endif中吧。不管你的头文件会不会被多个文件引用，你都要加上这个。一般格式是这样的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#ifndef&lt;标识&gt;</span><br><span class="line">#define&lt;标识&gt;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure></p><p>&lt;标识&gt;在理论上来说可以是自由命名的，但每个头文件的这个“标识”都应该是唯一的。标识的命名规则一般是头文件名全大写，前后加下划线，并把文件名中的“.”也变成下划线，如：stdio.h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#ifndef _STDIO_H_</span><br><span class="line">#define _STDIO_H_</span><br></pre></td></tr></table></figure></p><h2 id="cmath头文件"><a href="#cmath头文件" class="headerlink" title="cmath头文件"></a>cmath头文件</h2><p>include<cmath>，包含了常用的数学处理方法</cmath></p><ul><li>double log(double)</li><li>double pow(double, double)</li><li>double sqrt(double)</li><li>int abs(int)</li><li>double floor(double) 返回一个小于等于传入参数的最大整数</li></ul>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kaggle之Titanic</title>
      <link href="/2018/01/23/Project/Kaggle%E9%A1%B9%E7%9B%AE%E4%B9%8BTitanic/"/>
      <url>/2018/01/23/Project/Kaggle%E9%A1%B9%E7%9B%AE%E4%B9%8BTitanic/</url>
      <content type="html"><![CDATA[<p>Titanic是Taggle上的一篇教程性质的项目，具体内容为根据给定的乘客信息，分析乘客能否获救。</p><a id="more"></a><ul><li>通过该项目，对数据处理，特征工程，以及机器学习应用等方面产生一个较为清晰的认识。  </li><li>同时，由于是教学性质的文章，我会对一些代码写尽可能详细的注释，便于理解和学习。</li></ul><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>数据分析</li><li>特征工程</li><li>baseline</li></ol><h2 id="1-数据分析"><a href="#1-数据分析" class="headerlink" title="1. 数据分析"></a>1. 数据分析</h2><p>首先，将给定数据导入到程序中，然后对给定的每个特征，进行简要的分析和可视化，从而探索出数据，特征之间的关系，为之后的特征工程做准备。</p><h3 id="1-1-包导入"><a href="#1-1-包导入" class="headerlink" title="1.1 包导入"></a>1.1 包导入</h3><p>我们会使用总共三大类的python lib，1.数据处理； 2.可视化； 3.机器学习</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据处理</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ML</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> (GradientBoostingClassifier, GradientBoostingRegressor,</span><br><span class="line">                              RandomForestClassifier, RandomForestRegressor)</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score, StratifiedKFold, learning_curve</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure><h3 id="1-2-数据探索"><a href="#1-2-数据探索" class="headerlink" title="1.2 数据探索"></a>1.2 数据探索</h3><p>首先看一下数据的大概状况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>    </tr><br>  </tbody><br></table><br></div><p>对于每个乘客，共有11个特征，以及对应的每个乘客是否获救的label.<br>乘客信息如下</p><ul><li>PassengerId = 乘客ID</li><li>Pclass = 乘客等级（1/2/3等仓）</li><li>Name,Sex, Age</li><li>SibSp = 兄弟姐妹个数</li><li>Parch = 父母和小孩个数</li><li>Ticket = 船票信息</li><li>Fare = 船票价格</li><li>Cabin = 客仓</li><li>Embarked = 登船港口  </li></ul><p>接下来，对每个特征看一下类型信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId    891 non-null int64Survived       891 non-null int64Pclass         891 non-null int64Name           891 non-null objectSex            891 non-null objectAge            714 non-null float64SibSp          891 non-null int64Parch          891 non-null int64Ticket         891 non-null objectFare           891 non-null float64Cabin          204 non-null objectEmbarked       889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB</code></pre><p>通过以上信息，我们发现在给定数据中，Age，Cabin， Embarked存在<strong>数据缺失</strong>，尤其Cabin缺失很严重<br>我们继续对所有的数值数据进行探索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.describe()</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>count</th><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>714.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>      <td>891.000000</td><br>    </tr><br>    <tr><br>      <th>mean</th><br>      <td>446.000000</td><br>      <td>0.383838</td><br>      <td>2.308642</td><br>      <td>29.699118</td><br>      <td>0.523008</td><br>      <td>0.381594</td><br>      <td>32.204208</td><br>    </tr><br>    <tr><br>      <th>std</th><br>      <td>257.353842</td><br>      <td>0.486592</td><br>      <td>0.836071</td><br>      <td>14.526497</td><br>      <td>1.102743</td><br>      <td>0.806057</td><br>      <td>49.693429</td><br>    </tr><br>    <tr><br>      <th>min</th><br>      <td>1.000000</td><br>      <td>0.000000</td><br>      <td>1.000000</td><br>      <td>0.420000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>    </tr><br>    <tr><br>      <th>25%</th><br>      <td>223.500000</td><br>      <td>0.000000</td><br>      <td>2.000000</td><br>      <td>20.125000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>7.910400</td><br>    </tr><br>    <tr><br>      <th>50%</th><br>      <td>446.000000</td><br>      <td>0.000000</td><br>      <td>3.000000</td><br>      <td>28.000000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>14.454200</td><br>    </tr><br>    <tr><br>      <th>75%</th><br>      <td>668.500000</td><br>      <td>1.000000</td><br>      <td>3.000000</td><br>      <td>38.000000</td><br>      <td>1.000000</td><br>      <td>0.000000</td><br>      <td>31.000000</td><br>    </tr><br>    <tr><br>      <th>max</th><br>      <td>891.000000</td><br>      <td>1.000000</td><br>      <td>3.000000</td><br>      <td>80.000000</td><br>      <td>8.000000</td><br>      <td>6.000000</td><br>      <td>512.329200</td><br>    </tr><br>  </tbody><br></table><br></div><p>观察下各个特征之间的协方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sns.set(context=<span class="string">"paper"</span>, font=<span class="string">"monospace"</span>)</span><br><span class="line">sns.set(style=<span class="string">"white"</span>)</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">train_corr = train.drop(<span class="string">'PassengerId'</span>,axis=<span class="number">1</span>).corr()</span><br><span class="line">sns.heatmap(train_corr, ax=ax, vmax=<span class="number">.9</span>, square=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># ax.set_xticklabels(train_corr.index, size=15)</span></span><br><span class="line"><span class="comment"># ax.set_yticklabels(train_corr.columns[::-1], size=15)</span></span><br><span class="line">ax.set_title(<span class="string">'train feature corr'</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;train feature corr&apos;)</code></pre><p><img src="/img/titanic/output_12_1.png" alt="png"></p><p>通过协方差，我们可以得出以下结论：</p><ul><li>Pclass乘客等级和获救<strong>负相关</strong></li><li>Fare价格和获救<strong>正相关</strong>，给的钱多的优先</li><li>Pclass和Fare负相关，正常，一等仓最贵</li></ul><h4 id="1-2-1-年龄特征分析："><a href="#1-2-1-年龄特征分析：" class="headerlink" title="1.2.1 年龄特征分析："></a>1.2.1 年龄特征分析：</h4><p>对乘客的年龄特征进行分析，首先画出年龄分布曲线，对于缺失值，使用age=-20进行填补</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>,<span class="number">1</span>,figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">sns.distplot(train.Age.fillna(<span class="number">-20</span>), rug=<span class="keyword">True</span>, color=<span class="string">'b'</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">ax0 = axes[<span class="number">0</span>]</span><br><span class="line">ax0.set_title(<span class="string">'age distribution'</span>)</span><br><span class="line">ax0.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax1 = axes[<span class="number">1</span>]</span><br><span class="line">ax1.set_title(<span class="string">'age survived distribution'</span>)</span><br><span class="line">k1 = sns.distplot(train[train.Survived==<span class="number">0</span>].Age.fillna(<span class="number">-20</span>), hist=<span class="keyword">False</span>, color=<span class="string">'r'</span>, ax=ax1, label=<span class="string">'dead'</span>)</span><br><span class="line">k2 = sns.distplot(train[train.Survived==<span class="number">1</span>].Age.fillna(<span class="number">-20</span>), hist=<span class="keyword">False</span>, color=<span class="string">'g'</span>, ax=ax1, label=<span class="string">'alive'</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">''</span>)</span><br><span class="line">ax1.legend(fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282bf1c0f0&gt;</code></pre><p><img src="/img/titanic/output_15_1.png" alt="png"></p><p>通过对年龄分布进行分析，可以得到以下信息：</p><ol><li>可以看出，小孩和20多岁的青年以及中年获救可能性较大</li><li>获救情况和年龄<strong>不成现线性关系，所以如果使用线性模型需要对年龄进行离散处理，作为类型变量带入模型</strong></li><li>获救的人中，年龄缺少的人比例要小于有年龄信息的人（获救之后对乘客信息重新登记导致？）</li></ol><h5 id="1-2-1-年龄和性别分布"><a href="#1-2-1-年龄和性别分布" class="headerlink" title="1.2.1 年龄和性别分布"></a>1.2.1 年龄和性别分布</h5><p>对年龄和性别两个特征进行探索</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">ax.set_title(<span class="string">'Sex-Age_distribution'</span>)</span><br><span class="line">sns.distplot(train[train[<span class="string">'Sex'</span>] == <span class="string">'female'</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'pink'</span>, label=<span class="string">'female'</span>)</span><br><span class="line">sns.distplot(train[train[<span class="string">'Sex'</span>] == <span class="string">'male'</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'blue'</span>, label=<span class="string">'male'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282bf1c198&gt;</code></pre><p><img src="/img/titanic/output_18_1.png" alt="png"></p><ul><li>男性年龄普遍集中在40岁左右，女性年龄普遍集中在21，22岁左右。</li><li>老年人中，男性比例较多，年轻人中，女性比例较多。</li></ul><h5 id="1-2-1-年龄和仓位分布"><a href="#1-2-1-年龄和仓位分布" class="headerlink" title="1.2.1 年龄和仓位分布"></a>1.2.1 年龄和仓位分布</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">ax.set_title(<span class="string">'Pclass Age dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">1</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'pink'</span>, label=<span class="string">'P1'</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">2</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'blue'</span>, label=<span class="string">'p2'</span>)</span><br><span class="line">sns.distplot(train[train.Pclass==<span class="number">3</span>].dropna().Age, hist=<span class="keyword">False</span>, color=<span class="string">'g'</span>, label=<span class="string">'p3'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b78c278&gt;</code></pre><p><img src="/img/titanic/output_21_1.png" alt="png"></p><ul><li>越老，仓位越好（好像是废话）</li></ul><h4 id="1-2-1-Pclass-仓位分析"><a href="#1-2-1-Pclass-仓位分析" class="headerlink" title="1.2.1 Pclass 仓位分析"></a>1.2.1 Pclass 仓位分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_dead = train[train.Survived==<span class="number">0</span>].groupby(<span class="string">'Pclass'</span>)[<span class="string">'Survived'</span>].count()</span><br><span class="line">y_alive = train[train.Survived==<span class="number">1</span>].groupby(<span class="string">'Pclass'</span>)[<span class="string">'Survived'</span>].count()</span><br><span class="line">pos = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.bar(pos, y_dead, color=<span class="string">'r'</span>, alpha=<span class="number">0.6</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y_alive, color=<span class="string">'g'</span>, bottom=y_dead, alpha=<span class="number">0.6</span>, label=<span class="string">'alive'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">16</span>, loc=<span class="string">'best'</span>)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xticklabels([<span class="string">'Pclass%d'</span>%(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>)], size=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Pclass Surveved count'</span>, size=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Pclass Surveved count&apos;)</code></pre><p><img src="/img/titanic/output_24_1.png" alt="png"></p><ul><li>头等舱（Pclass=1），商务舱（Pclass=2），经济舱（Pclass=3）</li><li>经济舱人数遥遥领先</li><li>比较来说，头等舱获救比例最大，经济舱死亡概率最大</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pos = range(0,6)</span></span><br><span class="line"><span class="comment"># age_list = []</span></span><br><span class="line"><span class="comment"># for Pclass_ in range(1,4):</span></span><br><span class="line"><span class="comment">#     for Survived_ in range(0,2):</span></span><br><span class="line"><span class="comment">#         age_list.append(train[(train.Pclass == Pclass_)&amp;(train.Survived == Survived_)].Age.values)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig, axes = plt.subplots(3,1,figsize=(10,6))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># i_Pclass = 1</span></span><br><span class="line"><span class="comment"># for ax in axes:</span></span><br><span class="line"><span class="comment">#     sns.distplot(age_list[i_Pclass*2-2], hist=False, ax=ax, label='Pclass:%d ,survived:0'%(i_Pclass), color='r')</span></span><br><span class="line"><span class="comment">#     sns.distplot(age_list[i_Pclass*2-1], hist=False, ax=ax, label='Pclass:%d ,survived:1'%(i_Pclass), color='g')</span></span><br><span class="line"><span class="comment">#     i_Pclass += 1</span></span><br><span class="line"><span class="comment">#     ax.set_xlabel('age', size=15)</span></span><br><span class="line"><span class="comment">#     ax.legend(fontsize=15)</span></span><br></pre></td></tr></table></figure><h4 id="1-2-3-性别特征"><a href="#1-2-3-性别特征" class="headerlink" title="1.2.3 性别特征"></a>1.2.3 性别特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train.Sex.value_counts())</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(train.groupby(<span class="string">'Sex'</span>)[<span class="string">'Survived'</span>].mean())</span><br></pre></td></tr></table></figure><pre><code>male      577female    314Name: Sex, dtype: int64===================================Sexfemale    0.742038male      0.188908Name: Survived, dtype: float64</code></pre><p>可以看出：</p><ul><li>全部乘客中，男性人数大于女性，接近1.5倍</li><li>获救乘客中，大部分为女性，是男性的4倍，女性生存率远大于男性</li></ul><h5 id="性别和年龄和生存情况"><a href="#性别和年龄和生存情况" class="headerlink" title="性别和年龄和生存情况"></a>性别和年龄和生存情况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ax = plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.violinplot(x=<span class="string">'Sex'</span>, y=<span class="string">'Age'</span>, hue=<span class="string">'Survived'</span>, data=train.dropna(),split=<span class="keyword">True</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Sex'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.set_xticklabels([<span class="string">'Female'</span>, <span class="string">'male'</span>], size=<span class="number">18</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Age'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">25</span>, loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b7242b0&gt;</code></pre><p><img src="/img/titanic/output_31_1.png" alt="png"></p><p>从分布可以看出：</p><ul><li>女性获救年龄主要分布在20岁左右的青年</li><li>青年男性获救概率要大于中年男性，年轻人和小孩更容易获救</li></ul><h5 id="性别仓位和获救情况分布"><a href="#性别仓位和获救情况分布" class="headerlink" title="性别仓位和获救情况分布"></a>性别仓位和获救情况分布</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> sex_i <span class="keyword">in</span> [<span class="string">'female'</span>,<span class="string">'male'</span>]:</span><br><span class="line">    <span class="keyword">for</span> pclass_i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">        label.append(<span class="string">'sex:%s,Pclass:%d'</span>%(sex_i, pclass_i))</span><br><span class="line"></span><br><span class="line">pos = range(<span class="number">6</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>,<span class="number">4</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.bar(pos,</span><br><span class="line">        train[train[<span class="string">'Survived'</span>]==<span class="number">0</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        color=<span class="string">'r'</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        align=<span class="string">'center'</span>,</span><br><span class="line">        tick_label=label,</span><br><span class="line">        label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos,</span><br><span class="line">        train[train[<span class="string">'Survived'</span>]==<span class="number">1</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        bottom=train[train[<span class="string">'Survived'</span>]==<span class="number">0</span>].groupby([<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().values,</span><br><span class="line">        color=<span class="string">'g'</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        align=<span class="string">'center'</span>,</span><br><span class="line">        tick_label=label,</span><br><span class="line">        label=<span class="string">'alive'</span>)</span><br><span class="line">ax.tick_params(labelsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'sex_pclass_survived'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">10</span>,loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b6ebf60&gt;</code></pre><p><img src="/img/titanic/output_34_1.png" alt="png"></p><p>可以发现，头等舱的女性几乎都获救了</p><h4 id="1-2-4-票价-Fare"><a href="#1-2-4-票价-Fare" class="headerlink" title="1.2.4 票价(Fare)"></a>1.2.4 票价(Fare)</h4><p>分别看一下票价的分布以及票价随仓位的分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax = plt.subplot2grid((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">0</span>,<span class="number">0</span>), colspan=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">ax.tick_params(labelsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Fare dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'dist'</span>, size=<span class="number">20</span>)</span><br><span class="line">sns.kdeplot(train.Fare, ax=ax)</span><br><span class="line">sns.distplot(train.Fare, ax=ax)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>)</span><br><span class="line">pos = range(<span class="number">0</span>,<span class="number">400</span>,<span class="number">50</span>)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xlim([<span class="number">0</span>, <span class="number">200</span>])</span><br><span class="line">ax.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot2grid((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">0</span>), colspan=<span class="number">2</span>)</span><br><span class="line">ax.set_title(<span class="string">'Fare Pclass dist'</span>, size=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">    sns.kdeplot(train[train.Pclass==i].Fare, ax=ax1, label=<span class="string">'Pclass %d'</span>%(i))</span><br><span class="line">ax1.set_xlim([<span class="number">0</span>,<span class="number">200</span>])</span><br><span class="line">ax1.legend(fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b632f60&gt;</code></pre><p><img src="/img/titanic/output_37_1.png" alt="png"></p><h5 id="票价和获救情况的关系"><a href="#票价和获救情况的关系" class="headerlink" title="票价和获救情况的关系"></a>票价和获救情况的关系</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.kdeplot(train[train.Survived==<span class="number">0</span>].Fare, ax=ax1, label=<span class="string">'dead'</span>, color=<span class="string">'r'</span>)</span><br><span class="line">sns.kdeplot(train[train.Survived==<span class="number">1</span>].Fare, ax=ax1, label=<span class="string">'alive'</span>, color=<span class="string">'g'</span>)</span><br><span class="line"><span class="comment">#sns.distplot(train[train.Survived==0].Fare, ax=ax1, color='r')</span></span><br><span class="line"><span class="comment">#sns.distplot(train[train.Survived==1].Fare, ax=ax1, color='g')</span></span><br><span class="line">ax1.set_xlim([<span class="number">0</span>,<span class="number">300</span>])</span><br><span class="line">ax1.legend(fontsize=<span class="number">15</span>)</span><br><span class="line">ax1.set_title(<span class="string">'Fare survived'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'Fare'</span>, size=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,0,&apos;Fare&apos;)</code></pre><p><img src="/img/titanic/output_39_1.png" alt="png"></p><h4 id="1-2-5-sibsp-amp-parch-表亲和直亲"><a href="#1-2-5-sibsp-amp-parch-表亲和直亲" class="headerlink" title="1.2.5 sibsp&amp;parch 表亲和直亲"></a>1.2.5 sibsp&amp;parch 表亲和直亲</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">sns.countplot(train.SibSp)</span><br><span class="line">ax1.set_title(<span class="string">'SibSp'</span>, size=<span class="number">20</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>, sharex=ax1)</span><br><span class="line">sns.countplot(train.Parch)</span><br><span class="line">ax2.set_title(<span class="string">'Parch'</span>, size=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Parch&apos;)</code></pre><p><img src="/img/titanic/output_41_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">311</span>)</span><br><span class="line">train.groupby(<span class="string">'SibSp'</span>)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'Sibsp Survived Rate'</span>, size=<span class="number">16</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">312</span>)</span><br><span class="line">train.groupby(<span class="string">'Parch'</span>)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">'Parch Survived Rate'</span>, size=<span class="number">16</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">ax3 = fig.add_subplot(<span class="number">313</span>)</span><br><span class="line">train.groupby(train.SibSp+train.Parch)[<span class="string">'Survived'</span>].mean().plot(kind=<span class="string">'bar'</span>, ax=ax3)</span><br><span class="line">ax3.set_title(<span class="string">'Parch+Sibsp Survived Rate'</span>, size=<span class="number">16</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Parch+Sibsp Survived Rate&apos;)</code></pre><p><img src="/img/titanic/output_42_1.png" alt="png"></p><p>分组统计不同人数亲戚的获救率来看，都近似呈现先高后低, 亲人数目多少和是否获救不是简单的线性关系</p><h4 id="1-2-6-Embarked-上船地点"><a href="#1-2-6-Embarked-上船地点" class="headerlink" title="1.2.6 Embarked 上船地点"></a>1.2.6 Embarked 上船地点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">pos = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y1 = train[train.Survived==<span class="number">0</span>].groupby(<span class="string">'Embarked'</span>)[<span class="string">'Survived'</span>].count().sort_index().values</span><br><span class="line">y2 = train[train.Survived==<span class="number">1</span>].groupby(<span class="string">'Embarked'</span>)[<span class="string">'Survived'</span>].count().sort_index().values</span><br><span class="line">ax.bar(pos, y1, color=<span class="string">'r'</span>, alpha=<span class="number">0.4</span>, align=<span class="string">'center'</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y2, color=<span class="string">'g'</span>, alpha=<span class="number">0.4</span>, align=<span class="string">'center'</span>, label=<span class="string">'alive'</span>, bottom=y1)</span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">ax.set_xticklabels([<span class="string">'C'</span>,<span class="string">'Q'</span>,<span class="string">'S'</span>])</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>, loc=<span class="string">'best'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Embarked survived count'</span>, size=<span class="number">18</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Embarked survived count&apos;)</code></pre><p><img src="/img/titanic/output_44_1.png" alt="png"></p><p>可以看出，从c港上船的乘客更容易获救</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.set_xlim([<span class="number">-20</span>, <span class="number">80</span>])</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'C'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'C'</span>)</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'Q'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'Q'</span>)</span><br><span class="line">sns.kdeplot(train[train.Embarked==<span class="string">'S'</span>].Age.fillna(<span class="number">-10</span>), ax=ax, label=<span class="string">'S'</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">18</span>)</span><br><span class="line">ax.set_title(<span class="string">'Embarked Age Dist '</span>, size=<span class="number">18</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;Embarked Age Dist &apos;)</code></pre><p><img src="/img/titanic/output_46_1.png" alt="png"></p><ul><li>Q上岸的很多没有年龄</li><li>C上岸和S上岸的年龄分布较为相似，区别在于C上岸的年龄分布更加扁平，小孩和老人比例更高</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">y1 = train[train.Survived==<span class="number">0</span>].groupby([<span class="string">'Embarked'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().reset_index()[<span class="string">'Survived'</span>].values</span><br><span class="line">y2 = train[train.Survived==<span class="number">1</span>].groupby([<span class="string">'Embarked'</span>,<span class="string">'Pclass'</span>])[<span class="string">'Survived'</span>].count().reset_index()[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">ax = plt.figure(figsize=(<span class="number">8</span>,<span class="number">3</span>)).add_subplot(<span class="number">111</span>)</span><br><span class="line">pos = range(<span class="number">9</span>)</span><br><span class="line">ax.bar(pos, y1, align=<span class="string">'center'</span>, alpha=<span class="number">0.5</span>, color=<span class="string">'r'</span>, label=<span class="string">'dead'</span>)</span><br><span class="line">ax.bar(pos, y2, align=<span class="string">'center'</span>, bottom=y1, alpha=<span class="number">0.5</span>, color=<span class="string">'g'</span>, label=<span class="string">'alive'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xticks(pos)</span><br><span class="line">xticklabels = []</span><br><span class="line"><span class="keyword">for</span> embarked_val <span class="keyword">in</span> [<span class="string">'C'</span>,<span class="string">'Q'</span>,<span class="string">'S'</span>]:</span><br><span class="line">    <span class="keyword">for</span> pclass_val <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">        xticklabels.append(<span class="string">'%s/%d'</span>%(embarked_val,pclass_val))</span><br><span class="line"></span><br><span class="line">ax.set_xticklabels(xticklabels,size=<span class="number">15</span>)</span><br><span class="line">ax.legend(fontsize=<span class="number">15</span>, loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x7f282b3e07b8&gt;</code></pre><p><img src="/img/titanic/output_48_1.png" alt="png"></p><p>从不同仓位的比例来看，似乎C上岸更容易获救是因为头等舱的人较多？</p><p>但进一步对比C/S 发现，同样的仓位，C获救概率依然更高</p><p>脑洞下：</p><p>C地的人更加抱团，互帮互助- -<br>人数上来看S地的人更多，不同等级分布也更合常理，而C地的人头等舱很多，商务舱几乎没有，屌丝仓的也不少；</p><p>猜想：C地的人更多是权贵，S地的人来自商贸发达的商人？，所有C地的人地位更高- -</p><h4 id="1-2-7-Cabin-船舱号"><a href="#1-2-7-Cabin-船舱号" class="headerlink" title="1.2.7 Cabin 船舱号"></a>1.2.7 Cabin 船舱号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.Cabin.isnull().value_counts()</span><br></pre></td></tr></table></figure><pre><code>True     687False    204Name: Cabin, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(by=train.Cabin.isnull())[<span class="string">'Survived'</span>].mean()</span><br></pre></td></tr></table></figure><pre><code>CabinFalse    0.666667True     0.299854Name: Survived, dtype: float64</code></pre><ul><li>Cabin大部分为空</li><li>为空的获救概率较低，不为空的获救概率较高。<strong>说明该数据可以作为特征</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train.Cabin.apply(<span class="keyword">lambda</span> x:len(x) <span class="keyword">if</span>(x <span class="keyword">is</span> <span class="keyword">not</span> np.nan) <span class="keyword">else</span> <span class="number">0</span>)&gt;<span class="number">4</span>].head()</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>27</th><br>      <td>28</td><br>      <td>0</td><br>      <td>1</td><br>      <td>Fortune, Mr. Charles Alexander</td><br>      <td>male</td><br>      <td>19.0</td><br>      <td>3</td><br>      <td>2</td><br>      <td>19950</td><br>      <td>263.0000</td><br>      <td>C23 C25 C27</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>75</th><br>      <td>76</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Moen, Mr. Sigurd Hansen</td><br>      <td>male</td><br>      <td>25.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>348123</td><br>      <td>7.6500</td><br>      <td>F G73</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>88</th><br>      <td>89</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Fortune, Miss. Mabel Helen</td><br>      <td>female</td><br>      <td>23.0</td><br>      <td>3</td><br>      <td>2</td><br>      <td>19950</td><br>      <td>263.0000</td><br>      <td>C23 C25 C27</td><br>      <td>S</td><br>    </tr><br>    <tr><br>      <th>97</th><br>      <td>98</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Greenfield, Mr. William Bertram</td><br>      <td>male</td><br>      <td>23.0</td><br>      <td>0</td><br>      <td>1</td><br>      <td>PC 17759</td><br>      <td>63.3583</td><br>      <td>D10 D12</td><br>      <td>C</td><br>    </tr><br>    <tr><br>      <th>118</th><br>      <td>119</td><br>      <td>0</td><br>      <td>1</td><br>      <td>Baxter, Mr. Quigg Edmond</td><br>      <td>male</td><br>      <td>24.0</td><br>      <td>0</td><br>      <td>1</td><br>      <td>PC 17558</td><br>      <td>247.5208</td><br>      <td>B58 B60</td><br>      <td>C</td><br>    </tr><br>  </tbody><br></table><br></div><p>不少船舱编号有多个，其个数基本上和直系亲属个数保持一致，说明也许可以通过寻找亲属信息来填充cabin的一部分信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Cabin_Zone'</span>] = train.Cabin.fillna(<span class="string">'0'</span>).str.split(<span class="string">' '</span>).apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">train.groupby(by=<span class="string">'Cabin_Zone'</span>)[<span class="string">'Survived'</span>].agg([<span class="string">'mean'</span>, <span class="string">'count'</span>])</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>mean</th><br>      <th>count</th><br>    </tr><br>    <tr><br>      <th>Cabin_Zone</th><br>      <th></th><br>      <th></th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.299854</td><br>      <td>687</td><br>    </tr><br>    <tr><br>      <th>A</th><br>      <td>0.466667</td><br>      <td>15</td><br>    </tr><br>    <tr><br>      <th>B</th><br>      <td>0.744681</td><br>      <td>47</td><br>    </tr><br>    <tr><br>      <th>C</th><br>      <td>0.593220</td><br>      <td>59</td><br>    </tr><br>    <tr><br>      <th>D</th><br>      <td>0.757576</td><br>      <td>33</td><br>    </tr><br>    <tr><br>      <th>E</th><br>      <td>0.750000</td><br>      <td>32</td><br>    </tr><br>    <tr><br>      <th>F</th><br>      <td>0.615385</td><br>      <td>13</td><br>    </tr><br>    <tr><br>      <th>G</th><br>      <td>0.500000</td><br>      <td>4</td><br>    </tr><br>    <tr><br>      <th>T</th><br>      <td>0.000000</td><br>      <td>1</td><br>    </tr><br>  </tbody><br></table><br></div><p>把每个Cabin中的区域提取出来之后，统计发现不同区域获救概率差别很大，<strong>或许可以作为一个特征</strong></p><h4 id="1-2-8-姓名特征"><a href="#1-2-8-姓名特征" class="headerlink" title="1.2.8 姓名特征"></a>1.2.8 姓名特征</h4><p>最开始认为Name这个特征没有任何意义，但我们发现，所给数据中的姓名信息不仅仅是姓名，而且包括称谓，甚至包括性别，地位，财富，婚姻状况等都可能包含在姓名中。所以，对于Name不能直接将其当作垃圾特征处理。<br>首先，对姓名的长度和获救状况进行探索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(train.Name.apply(<span class="keyword">lambda</span> x:len(x)))[<span class="string">'Survived'</span>].mean().plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f282b56a198&gt;</code></pre><p><img src="/img/titanic/output_59_1.png" alt="png"></p><ul><li>可以发现，似乎名字越长，获救可能性越高，所以我们把名字的长度加入特征：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Name_Len'</span>] = train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line">test[<span class="string">'Name_Len'</span>] = test[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>    </tr><br>  </tbody><br></table><br></div><p>同时我们猜测，不同的称谓应该也和获救情况有关联：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>]))[<span class="string">'Survived'</span>].mean().plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f282b32f6d8&gt;</code></pre><p><img src="/img/titanic/output_63_1.png" alt="png"></p><p>于是我们把称谓信息提取出来，由于有些称谓的人数量过少，我们还需要做一个映射:</p><p>Mme：称呼非英语民族的”上层社会”已婚妇女,及有职业的妇女，相当于Mrs</p><p>Jonkheer:乡绅</p><p>Capt：船长</p><p>Lady：贵族夫人的称呼</p><p>Don唐：是西班牙语中贵族和有地位者的尊称</p><p>sir：先生</p><p>the Countess：女伯爵</p><p>Ms：Ms.或Mz 美国近来用来称呼婚姻状态不明的妇女</p><p>Col：中校:Lieutenant Colonel(Lt. Col.)上校:Colonel(Col.)</p><p>Major：少校</p><p>Mlle:小姐</p><p>Rev：牧师</p><p>测试集合中特殊的Dona：女士尊称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'Don'</span>,<span class="string">'Dona'</span>, <span class="string">'Major'</span>, <span class="string">'Capt'</span>, <span class="string">'Jonkheer'</span>, <span class="string">'Rev'</span>, <span class="string">'Col'</span>,<span class="string">'Sir'</span>,<span class="string">'Dr'</span>],<span class="string">'Mr'</span>)</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'Mlle'</span>,<span class="string">'Ms'</span>], <span class="string">'Miss'</span>)</span><br><span class="line">train[<span class="string">'Title'</span>] = train[<span class="string">'Title'</span>].replace([<span class="string">'the Countess'</span>,<span class="string">'Mme'</span>,<span class="string">'Lady'</span>,<span class="string">'Dr'</span>], <span class="string">'Mrs'</span>)</span><br><span class="line"></span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Name'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">', '</span>)[<span class="number">1</span>]).apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'Don'</span>,<span class="string">'Dona'</span>, <span class="string">'Major'</span>, <span class="string">'Capt'</span>, <span class="string">'Jonkheer'</span>, <span class="string">'Rev'</span>, <span class="string">'Col'</span>,<span class="string">'Sir'</span>,<span class="string">'Dr'</span>],<span class="string">'Mr'</span>)</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'Mlle'</span>,<span class="string">'Ms'</span>], <span class="string">'Miss'</span>)</span><br><span class="line">test[<span class="string">'Title'</span>] = test[<span class="string">'Title'</span>].replace([<span class="string">'the Countess'</span>,<span class="string">'Mme'</span>,<span class="string">'Lady'</span>,<span class="string">'Dr'</span>], <span class="string">'Mrs'</span>)</span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>Title</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>Braund, Mr. Owen Harris</td><br>      <td>male</td><br>      <td>22.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>A/5 21171</td><br>      <td>7.2500</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>      <td>Mr</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th…</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>PC 17599</td><br>      <td>71.2833</td><br>      <td>C85</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>      <td>Mrs</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>Heikkinen, Miss. Laina</td><br>      <td>female</td><br>      <td>26.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>STON/O2. 3101282</td><br>      <td>7.9250</td><br>      <td>NaN</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>      <td>Miss</td><br>    </tr><br>  </tbody><br></table><br></div><h1 id="2-特征工程"><a href="#2-特征工程" class="headerlink" title="2. 特征工程"></a>2. 特征工程</h1><p>首先查看所有数据中为null的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'============train==========='</span>)</span><br><span class="line">print(train.isnull().sum())</span><br><span class="line">print(<span class="string">'============test============'</span>)</span><br><span class="line">print(test.isnull().sum())</span><br></pre></td></tr></table></figure><pre><code>============train===========PassengerId      0Survived         0Pclass           0Name             0Sex              0Age            177SibSp            0Parch            0Ticket           0Fare             0Cabin          687Embarked         2Cabin_Zone       0Name_Len         0Title            0dtype: int64============test============PassengerId      0Pclass           0Name             0Sex              0Age             86SibSp            0Parch            0Ticket           0Fare             1Cabin          327Embarked         0Name_Len         0Title            0dtype: int64</code></pre><ol><li>age 和 cabin在训练集和测试集都有缺失，cabin缺失数量巨大</li><li>embarked 在训练集存在2个缺失</li><li>fare在测试集有一个缺失</li></ol><h3 id="2-1-embarked-处理"><a href="#2-1-embarked-处理" class="headerlink" title="2.1 embarked 处理"></a>2.1 embarked 处理</h3><p>首先，对embarked进行处理，查看embarked的缺失情况。<br>因为只有2个数据存在缺失，所以不适合将“缺失”重新定义一个分类，我们想办法对其进行填补：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train.Embarked.isnull()]</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Name</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Ticket</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>Title</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>61</th><br>      <td>62</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Icard, Miss. Amelie</td><br>      <td>female</td><br>      <td>38.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>113572</td><br>      <td>80.0</td><br>      <td>B28</td><br>      <td>NaN</td><br>      <td>B</td><br>      <td>19</td><br>      <td>Miss</td><br>    </tr><br>    <tr><br>      <th>829</th><br>      <td>830</td><br>      <td>1</td><br>      <td>1</td><br>      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td><br>      <td>female</td><br>      <td>62.0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>113572</td><br>      <td>80.0</td><br>      <td>B28</td><br>      <td>NaN</td><br>      <td>B</td><br>      <td>41</td><br>      <td>Mrs</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train.Embarked.value_counts())</span><br><span class="line">print(<span class="string">'========================='</span>)</span><br><span class="line">print(train[train.Pclass==<span class="number">1</span>].Embarked.value_counts())</span><br></pre></td></tr></table></figure><pre><code>S    644C    168Q     77Name: Embarked, dtype: int64=========================S    127C     85Q      2Name: Embarked, dtype: int64</code></pre><ul><li>不分Pclass仓位来看的话，S embarked人数远大于C，Q</li><li>而且两个缺失的数据Pclass都为1，而Pclass==1的乘客中，S embarked的人数也是最多。  </li></ul><p><strong>所以，因为缺失数据很少，可以考虑随最大可能数据填补缺失值，因此设定缺失值为S</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.Embarked.fillna(<span class="string">'S'</span>, inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-cabin处理"><a href="#2-2-cabin处理" class="headerlink" title="2.2 cabin处理"></a>2.2 cabin处理</h3><p>在之前的数据探索环节，我们知道cabin为空的获救概率较低，不为空的获救概率较高。说明该数据可以作为特征。所以我们对cabin作填充处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#若为NaN,则用字符串‘Null’填充，否则用'Not Null'填充</span></span><br><span class="line">train[<span class="string">'Cabin'</span>] = train[<span class="string">'Cabin'</span>].isnull().apply(<span class="keyword">lambda</span> x: <span class="string">'Null'</span> <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">True</span> <span class="keyword">else</span> <span class="string">'Not Null'</span>)</span><br><span class="line">test[<span class="string">'Cabin'</span>] = test[<span class="string">'Cabin'</span>].isnull().apply(<span class="keyword">lambda</span> x: <span class="string">'Null'</span> <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">True</span> <span class="keyword">else</span> <span class="string">'Not Null'</span>)</span><br></pre></td></tr></table></figure><h3 id="2-3-Name和Ticket处理"><a href="#2-3-Name和Ticket处理" class="headerlink" title="2.3 Name和Ticket处理"></a>2.3 Name和Ticket处理</h3><p>已经从Name属性中提取了有价值的特征，所以删除原Name和Ticket属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> train[<span class="string">'Name'</span>], test[<span class="string">'Name'</span>]</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">'Ticket'</span>], test[<span class="string">'Ticket'</span>]</span><br></pre></td></tr></table></figure><h3 id="2-4-年龄离散化处理"><a href="#2-4-年龄离散化处理" class="headerlink" title="2.4 年龄离散化处理"></a>2.4 年龄离散化处理</h3><p>在我们对数据进行观察时，发现获救情况和年龄不是存在简单的线性关系，所以需要对年龄进行离散化处理<br>同时，对于缺失值，需要进行填补。考虑到年龄缺失的数量较多，所以将缺失设为一类<br>年龄以5岁为一个周期进行离散，同时10岁以下，60岁以上分别归为一类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">group = train.groupby([<span class="string">'Title'</span>, <span class="string">'Pclass'</span>])[<span class="string">'Age'</span>]</span><br><span class="line">train[<span class="string">'Age'</span>] = group.transform(<span class="keyword">lambda</span> x: x.fillna(x.median()))</span><br><span class="line">train = train.drop(<span class="string">'Title'</span>,axis=<span class="number">1</span>)</span><br><span class="line">train[<span class="string">'IsChild'</span>] = np.where(train[<span class="string">'Age'</span>]&lt;=<span class="number">12</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">train[<span class="string">'Age'</span>] = pd.cut(train[<span class="string">'Age'</span>],<span class="number">5</span>)</span><br><span class="line"><span class="comment">#train = train.drop('Age',axis=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group = test.groupby([<span class="string">'Title'</span>, <span class="string">'Pclass'</span>])[<span class="string">'Age'</span>]</span><br><span class="line">test[<span class="string">'Age'</span>] = group.transform(<span class="keyword">lambda</span> x: x.fillna(x.median()))</span><br><span class="line">test = test.drop(<span class="string">'Title'</span>,axis=<span class="number">1</span>)</span><br><span class="line">test[<span class="string">'IsChild'</span>] = np.where(test[<span class="string">'Age'</span>]&lt;=<span class="number">12</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">test[<span class="string">'Age'</span>] = pd.cut(test[<span class="string">'Age'</span>],<span class="number">5</span>)</span><br><span class="line"><span class="comment">#test = test.drop('Age',axis=1)</span></span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>3</td><br>      <td>male</td><br>      <td>(16.336, 32.252]</td><br>      <td>1</td><br>      <td>0</td><br>      <td>7.2500</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>0</td><br>      <td>23</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>female</td><br>      <td>(32.252, 48.168]</td><br>      <td>1</td><br>      <td>0</td><br>      <td>71.2833</td><br>      <td>Not Null</td><br>      <td>C</td><br>      <td>C</td><br>      <td>51</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>3</td><br>      <td>female</td><br>      <td>(16.336, 32.252]</td><br>      <td>0</td><br>      <td>0</td><br>      <td>7.9250</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>0</td><br>      <td>22</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="2-5-fare属性"><a href="#2-5-fare属性" class="headerlink" title="2.5 fare属性"></a>2.5 fare属性</h3><p>在test集中存在一个数据的fare为缺失值，对于这个变量，可以用平均值进行填补</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test[test.Fare.isnull()]</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>152</th><br>      <td>1044</td><br>      <td>3</td><br>      <td>male</td><br>      <td>(45.668, 60.834]</td><br>      <td>0</td><br>      <td>0</td><br>      <td>NaN</td><br>      <td>Null</td><br>      <td>S</td><br>      <td>18</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test.loc[test.Fare.isnull(), <span class="string">'Fare'</span>] = \</span><br><span class="line">test[(test.Pclass==<span class="number">1</span>) &amp; (test.Embarked==<span class="string">'S'</span>) &amp; (test.Sex==<span class="string">'male'</span>)].dropna().Fare.mean()</span><br></pre></td></tr></table></figure><h3 id="2-6-归一化处理"><a href="#2-6-归一化处理" class="headerlink" title="2.6 归一化处理"></a>2.6 归一化处理</h3><p>数据中fare分布波动较大，对其进行归一化处理，加速模型收敛</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">fare_scale_param = scaler.fit(train[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">train.Fare = fare_scale_param.transform(train[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">test.Fare = fare_scale_param.transform(test[<span class="string">'Fare'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="2-7-所有特征转化成数值型编码"><a href="#2-7-所有特征转化成数值型编码" class="headerlink" title="2.7 所有特征转化成数值型编码"></a>2.7 所有特征转化成数值型编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">features = train.drop([<span class="string">"PassengerId"</span>,<span class="string">"Survived"</span>], axis=<span class="number">1</span>).columns</span><br><span class="line">le = LabelEncoder()</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">    le = le.fit(train[feature])</span><br><span class="line">    train[feature] = le.transform(train[feature])</span><br><span class="line"></span><br><span class="line">train.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>PassengerId</th><br>      <th>Survived</th><br>      <th>Pclass</th><br>      <th>Sex</th><br>      <th>Age</th><br>      <th>SibSp</th><br>      <th>Parch</th><br>      <th>Fare</th><br>      <th>Cabin</th><br>      <th>Embarked</th><br>      <th>Cabin_Zone</th><br>      <th>Name_Len</th><br>      <th>IsChild</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>0</td><br>      <td>2</td><br>      <td>1</td><br>      <td>1</td><br>      <td>1</td><br>      <td>0</td><br>      <td>18</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>11</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>2</td><br>      <td>1</td><br>      <td>0</td><br>      <td>207</td><br>      <td>0</td><br>      <td>0</td><br>      <td>3</td><br>      <td>39</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>41</td><br>      <td>1</td><br>      <td>2</td><br>      <td>0</td><br>      <td>10</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="2-8-得到训练-测试数据集"><a href="#2-8-得到训练-测试数据集" class="headerlink" title="2.8 得到训练/测试数据集"></a>2.8 得到训练/测试数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_all = train.iloc[:<span class="number">891</span>,:].drop([<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>], axis=<span class="number">1</span>)</span><br><span class="line">Y_all = train.iloc[:<span class="number">891</span>,:][<span class="string">'Survived'</span>]</span><br><span class="line">X_test = train.iloc[<span class="number">891</span>:,:].drop([<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="3-模型训练和调优"><a href="#3-模型训练和调优" class="headerlink" title="3 模型训练和调优"></a>3 模型训练和调优</h1><p>分别考察逻辑回归、支持向量机、最近邻、决策树、随机森林、gbdt、xgbGBDT几类算法的性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression()</span><br><span class="line">svc = SVC()</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors = <span class="number">3</span>)</span><br><span class="line">decision_tree = DecisionTreeClassifier()</span><br><span class="line">random_forest = RandomForestClassifier(n_estimators=<span class="number">300</span>,min_samples_leaf=<span class="number">4</span>,class_weight=&#123;<span class="number">0</span>:<span class="number">0.745</span>,<span class="number">1</span>:<span class="number">0.255</span>&#125;)</span><br><span class="line">gbdt = GradientBoostingClassifier(n_estimators=<span class="number">500</span>,learning_rate=<span class="number">0.03</span>,max_depth=<span class="number">3</span>)</span><br><span class="line">xgbGBDT = XGBClassifier(max_depth=<span class="number">3</span>, n_estimators=<span class="number">300</span>, learning_rate=<span class="number">0.05</span>)</span><br><span class="line">clfs = [lr, svc, knn, decision_tree, random_forest, gbdt, xgbGBDT]</span><br><span class="line"></span><br><span class="line">kfold = <span class="number">10</span></span><br><span class="line">cv_results = []</span><br><span class="line"><span class="keyword">for</span> classifier <span class="keyword">in</span> clfs :</span><br><span class="line">    cv_results.append(cross_val_score(classifier, X_all, y = Y_all, scoring = <span class="string">"accuracy"</span>, cv = kfold, n_jobs=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">cv_means = []</span><br><span class="line">cv_std = []</span><br><span class="line"><span class="keyword">for</span> cv_result <span class="keyword">in</span> cv_results:</span><br><span class="line">    cv_means.append(cv_result.mean())</span><br><span class="line">    cv_std.append(cv_result.std())</span><br><span class="line"></span><br><span class="line">cv_res = pd.DataFrame(&#123;<span class="string">"CrossValMeans"</span>:cv_means,<span class="string">"CrossValerrors"</span>: cv_std,</span><br><span class="line">                       <span class="string">"Algorithm"</span>:[<span class="string">"LR"</span>,<span class="string">"SVC"</span>,<span class="string">'KNN'</span>,<span class="string">'decision_tree'</span>,<span class="string">"random_forest"</span>,<span class="string">"GBDT"</span>,<span class="string">"xgbGBDT"</span>]&#125;)</span><br><span class="line"></span><br><span class="line">g = sns.barplot(<span class="string">"CrossValMeans"</span>,<span class="string">"Algorithm"</span>,data = cv_res, palette=<span class="string">"Set3"</span>,orient = <span class="string">"h"</span>,**&#123;<span class="string">'xerr'</span>:cv_std&#125;)</span><br><span class="line">g.set_xlabel(<span class="string">"Mean Accuracy"</span>)</span><br><span class="line">g = g.set_title(<span class="string">"Cross validation scores"</span>)</span><br></pre></td></tr></table></figure><p><img src="/img/titanic/output_90_0.png" alt="png"></p><p>通过观察，发现不同的模型的feature importance有比较大的差别，所以考虑到把他们组合在一起可能有更好的效果<br>首先定义集成框架：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ensemble</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,estimators)</span>:</span></span><br><span class="line">        self.estimator_names = []</span><br><span class="line">        self.estimators = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> estimators:</span><br><span class="line">            self.estimator_names.append(i[<span class="number">0</span>])</span><br><span class="line">            self.estimators.append(i[<span class="number">1</span>])</span><br><span class="line">        self.clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_x, train_y)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators:</span><br><span class="line">            i.fit(train_x,train_y)</span><br><span class="line">        x = np.array([i.predict(train_x) <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators]).T</span><br><span class="line">        y = train_y</span><br><span class="line">        self.clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = np.array([i.predict(x) <span class="keyword">for</span> i <span class="keyword">in</span> self.estimators]).T</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> self.clf.predict(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        s = precision_score(y,self.predict(x))</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p>将基分类器放入集成框架中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bag = Ensemble(</span><br><span class="line">    [(<span class="string">'xgb'</span>,xgbGBDT),(<span class="string">'lr'</span>,lr),(<span class="string">'rf'</span>,random_forest),(<span class="string">'svc'</span>,svc),(<span class="string">'gbdt'</span>,gbdt),(<span class="string">'dt'</span>,decision_tree),(<span class="string">'knn'</span>,knn)])</span><br><span class="line">score = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">    num_test = <span class="number">0.20</span></span><br><span class="line">    X_train, X_cv, Y_train, Y_cv = train_test_split(X_all, Y_all, test_size=num_test)</span><br><span class="line">    bag.fit(X_train, Y_train)</span><br><span class="line">    <span class="comment">#Y_test = bag.predict(X_test)</span></span><br><span class="line">    acc_xgb = round(bag.score(X_cv, Y_cv) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    score+=acc_xgb</span><br><span class="line">print(score/<span class="number">10</span>)  <span class="comment">#0.8786</span></span><br></pre></td></tr></table></figure><pre><code>68.73299999999999</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def plot_learning_curve(clf, title, X, y, ylim=None, cv=None, n_jobs=3, train_sizes=np.linspace(.05, 1., 5)):</span></span><br><span class="line"><span class="comment">#     train_sizes, train_scores, test_scores = learning_curve(</span></span><br><span class="line"><span class="comment">#         clf, x, y, train_sizes=train_sizes)</span></span><br><span class="line"><span class="comment">#     train_scores_mean = np.mean(train_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     train_scores_std = np.std(train_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     test_scores_mean = np.mean(test_scores, axis=1)</span></span><br><span class="line"><span class="comment">#     test_scores_std = np.std(test_scores, axis=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax = plt.figure().add_subplot(111)</span></span><br><span class="line"><span class="comment">#     ax.set_title(title)</span></span><br><span class="line"><span class="comment">#     if ylim is not None:</span></span><br><span class="line"><span class="comment">#         ax.ylim(*ylim)</span></span><br><span class="line"><span class="comment">#     ax.set_xlabel(u"train_num_of_samples")</span></span><br><span class="line"><span class="comment">#     ax.set_ylabel(u"score")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,</span></span><br><span class="line"><span class="comment">#                      alpha=0.1, color="b")</span></span><br><span class="line"><span class="comment">#     ax.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,</span></span><br><span class="line"><span class="comment">#                      alpha=0.1, color="r")</span></span><br><span class="line"><span class="comment">#     ax.plot(train_sizes, train_scores_mean, 'o-', color="b", label=u"train score")</span></span><br><span class="line"><span class="comment">#     ax.plot(train_sizes, test_scores_mean, 'o-', color="r", label=u"testCV score")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     ax.legend(loc="best")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2</span></span><br><span class="line"><span class="comment">#     diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])</span></span><br><span class="line"><span class="comment">#     return midpoint, diff</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot_learning_curve(grd, u"learning_rate", train_x, train_y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gender_submission = pd.DataFrame(&#123;'PassengerId':test.iloc[:,0],'Survived':grd.predict(test_x)&#125;)</span></span><br><span class="line"><span class="comment"># gender_submission.to_csv('C:/Users/evilpsycho/Desktop/gender_submission.csv', index=None)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Project </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:命名空间和模板</title>
      <link href="/2018/01/22/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B09%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E5%92%8C%E6%A8%A1%E6%9D%BF/"/>
      <url>/2018/01/22/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B09%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E5%92%8C%E6%A8%A1%E6%9D%BF/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第九部分。<br><a id="more"></a></p><h2 id="命名空间"><a href="#命名空间" class="headerlink" title="命名空间"></a>命名空间</h2><p>在 C++ 应用程序中可能出现命名冲突的问题。例如，您可能会写一个名为 xyz() 的函数，在另一个可用的库中也存在一个相同的函数 xyz()。这样，编译器就无法判断您所使用的是哪一个 xyz() 函数。</p><p>因此，引入了命名空间这个概念，专门用于解决上面的问题，它可作为附加信息来区分不同库中相同名称的函数、类、变量等。使用了命名空间即定义了上下文。本质上，命名空间就是定义了一个范围。</p><h3 id="定义命名空间"><a href="#定义命名空间" class="headerlink" title="定义命名空间"></a>定义命名空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">namespace space_name&#123;</span><br><span class="line">    //代码声明</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调用带有命名空间的函数或变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">space_name::code;</span><br></pre></td></tr></table></figure></p><p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">namespace first_space&#123;</span><br><span class="line">   void func()&#123;</span><br><span class="line">      cout &lt;&lt; &quot;Inside first_space&quot; &lt;&lt; endl;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">namespace second_space&#123;</span><br><span class="line">   void func()&#123;</span><br><span class="line">      cout &lt;&lt; &quot;Inside second_space&quot; &lt;&lt; endl;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main ()&#123;</span><br><span class="line">   first_space::func();</span><br><span class="line">   second_space::func();</span><br><span class="line">   return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="using指令"><a href="#using指令" class="headerlink" title="using指令"></a>using指令</h3><p>您可以使用 using namespace 指令，这样在使用命名空间时就可以不用在前面加上命名空间的名称。这个指令会告诉编译器，后续的代码将使用指定的命名空间中的名称。<br>using 指令也可以用来指定命名空间中的特定项目。例如，如果只打算使用 std 命名空间中的 cout 部分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">using std::cout;</span><br></pre></td></tr></table></figure></p><h2 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h2><p>模板是泛型编程的基础，泛型编程即以一种独立于任何特定类型的方式编写代码。</p><p>模板是创建泛型类或函数的蓝图或公式。库容器，比如迭代器和算法，都是泛型编程的例子，它们都使用了模板的概念。</p><p>每个容器都有一个单一的定义，比如 向量，我们可以定义许多不同类型的向量，比如 vector <int> 或 vector <string>。</string></int></p><h3 id="函数模板"><a href="#函数模板" class="headerlink" title="函数模板"></a>函数模板</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename T&gt;</span><br><span class="line">inline T const&amp; Max(T const&amp; a, T const&amp; b)&#123;</span><br><span class="line">    return a &lt; b ? a: b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="类模板"><a href="#类模板" class="headerlink" title="类模板"></a>类模板</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">template &lt;class T&gt;</span><br><span class="line">class Stack &#123;</span><br><span class="line">  private:</span><br><span class="line">    vector&lt;T&gt; elems;     // 元素</span><br><span class="line"></span><br><span class="line">  public:</span><br><span class="line">    void push(T const&amp;);  // 入栈</span><br><span class="line">    void pop();               // 出栈</span><br><span class="line">    T top() const;            // 返回栈顶元素</span><br><span class="line">    bool empty() const&#123;       // 如果为空则返回真。</span><br><span class="line">        return elems.empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">template &lt;class T&gt;</span><br><span class="line">void Stack&lt;T&gt;::push (T const&amp; elem)</span><br><span class="line">&#123;</span><br><span class="line">    elems.push_back(elem);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template &lt;class T&gt;</span><br><span class="line">void Stack&lt;T&gt;::pop ()</span><br><span class="line">&#123;</span><br><span class="line">    if (elems.empty()) &#123;</span><br><span class="line">        throw out_of_range(&quot;Stack&lt;&gt;::pop(): empty stack&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    elems.pop_back();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template &lt;class T&gt;</span><br><span class="line">T Stack&lt;T&gt;::top () const</span><br><span class="line">&#123;</span><br><span class="line">    if (elems.empty()) &#123;</span><br><span class="line">        throw out_of_range(&quot;Stack&lt;&gt;::top(): empty stack&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    return elems.back();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:动态内存</title>
      <link href="/2018/01/19/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B08%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98/"/>
      <url>/2018/01/19/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B08%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98/</url>
      <content type="html"><![CDATA[<p>C++ 复习笔记第8部分。</p><a id="more"></a><h2 id="动态内存"><a href="#动态内存" class="headerlink" title="动态内存"></a>动态内存</h2><p>C++ 程序中的内存分为两个部分：</p><ul><li>栈：在函数内部声明的所有变量都将占用栈内存。</li><li>堆：这是程序中未使用的内存，在程序运行时可用于动态分配内存。<br>很多时候，您无法提前预知需要多少内存来存储某个定义变量中的特定信息，所需内存的大小需要在运行时才能确定。</li></ul><p>在 C++ 中，您可以使用特殊的运算符为给定类型的变量在运行时分配堆内的内存，这会返回所分配的空间地址。这种运算符即 new 运算符。</p><p>如果您不再需要动态分配的内存空间，可以使用 delete 运算符，删除之前由 new 运算符分配的内存。</p><p>如果自由存储区已被用完，可能无法成功分配内存。所以建议检查 new 运算符是否返回 NULL 指针，并采取以下适当的操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">double *p = NULL;</span><br><span class="line">if(!(p = new double))&#123;</span><br><span class="line">    cout&lt;&lt;&quot;Error&quot;&lt;&lt;endl;</span><br><span class="line">    exit(1);</span><br><span class="line">&#125;</span><br><span class="line">delete p;</span><br></pre></td></tr></table></figure></p><h3 id="数组的动态内存分配"><a href="#数组的动态内存分配" class="headerlink" title="数组的动态内存分配"></a>数组的动态内存分配</h3><p>创建一个长度为20的char数组。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">char* pvalue = new char[20];</span><br><span class="line">delete [] pvalue</span><br></pre></td></tr></table></figure></p><p>创建一个二维int数组：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">int m=3, n=4;</span><br><span class="line">int **matrix = new *int[m];</span><br><span class="line">for(int i = 0; i&lt;m; i++)&#123;</span><br><span class="line">    matrix[i] = new int[n];</span><br><span class="line">&#125;</span><br><span class="line">for(int i = 0; i&lt;m; i++)&#123;</span><br><span class="line">    delete [] matrix[i];</span><br><span class="line">&#125;</span><br><span class="line">delete [] matrix;</span><br></pre></td></tr></table></figure></p><h3 id="为对象分配动态内存"><a href="#为对象分配动态内存" class="headerlink" title="为对象分配动态内存"></a>为对象分配动态内存</h3><p>和简单的内存分配没有什么不同<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class Box&#123;...&#125;</span><br><span class="line">Box *boxes = new Box[4];</span><br><span class="line">delete [] boxes;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:STL</title>
      <link href="/2018/01/17/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B07STL/"/>
      <url>/2018/01/17/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B07STL/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第七部分。<br><a id="more"></a></p><h2 id="STL"><a href="#STL" class="headerlink" title="STL"></a>STL</h2><p>C++ STL（标准模板库）是一套功能强大的 C++ 模板类，提供了通用的模板类和函数，这些模板类和函数可以实现多种流行和常用的算法和数据结构，如向量、链表、队列、栈。</p><p>C++ 标准模板库的核心包括以下三个组件：</p><ul><li>容器（Containers）    容器是用来管理某一类对象的集合。C++ 提供了各种不同类型的容器，比如 deque、list、vector、map 等。</li><li>算法（Algorithms）    算法作用于容器。它们提供了执行各种操作的方式，包括对容器内容执行初始化、排序、搜索和转换等操作。</li><li>迭代器（iterators）    迭代器用于遍历对象集合的元素。这些集合可能是容器，也可能是容器的子集。</li></ul><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>容器类自动申请和释放内存，无需new和delete操作。</p><h4 id="序列式容器"><a href="#序列式容器" class="headerlink" title="序列式容器"></a>序列式容器</h4><p>每个元素都有固定位置－－取决于插入时机和地点，和元素值无关，vector、deque、list；</p><ul><li>Vectors：将元素置于一个动态数组中加以管理，可以用索引存取，数组尾部添加或移除元素非常快速。但是在中部或头部安插元素比较费时；</li><li>Lists：双向链表，不提供随机存取（按顺序走到需存取的元素，O(n)），在任何位置上执行插入或删除动作都非常迅速；<h4 id="关联式容器"><a href="#关联式容器" class="headerlink" title="关联式容器"></a>关联式容器</h4>元素位置取决于特定的排序准则，和插入顺序无关，set、multiset、map、multimap；</li><li>Sets/Multisets：内部的元素依据其值自动排序，Set内的相同数值的元素只能出现一次，Multisets内可包含多个数值相同的元素，内部由二叉树实现，便于查找；</li><li>Maps/Multimaps：Map的元素是成对的键值/实值，内部的元素依据其值自动排序，Map内的相同数值的元素只能出现一次，Multimaps内可包含多个数值相同的元素，内部由二叉树实现，便于查找；<h4 id="共性"><a href="#共性" class="headerlink" title="共性"></a>共性</h4><h5 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h5>每类容器都包含四个迭代器：iterator(正向迭代器)、const_iterator(常正向迭代器)、reverse_iterator(反向迭代器)、const_reverse_iterator(常反向迭代器)。因此你可以按照下面的方式获取每个容器的相应的迭代器：<br>获取正向迭代器<br>C<t>::iterator it = c.begin();<br>C <t>::iterator it = c.end();<br>获取反向迭代器<br>C <t>::reverse_iterator it = c.rbegin();<br>C <t>:: reverse_iterator it = c.rend()<br>获取常正向迭代器<br>C<t>::const_iterator it = c.begin();<br>C <t>:: const_iterator it = c.end();<br>获取常反向迭代器<br>C <t>:: const_ reverse_iterator it = c.rbegin();<br>C <t>:: const_ reverse_iterator it = c.rend()<h5 id="容器大小"><a href="#容器大小" class="headerlink" title="容器大小"></a>容器大小</h5>获取容器的大小<br>c.size();<br>判断容器是否为空<br>c.empty();<h4 id="顺序容器共性"><a href="#顺序容器共性" class="headerlink" title="顺序容器共性"></a>顺序容器共性</h4>序列容器之间的共性除了容器之间应有的共性之外，还有对数据操作的接口(非实现)上：<br>c.push_back<br>c.pop_back<br>c.push_front<br>c.pop_front<br>c.back<br>c.front<br>c.erase<br>c.remove<h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4>vector和数组具有同样的内存处理方式。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//1.定义和初始化</span><br><span class="line">    vector&lt;int&gt; vec1;    //默认初始化，vec1为空</span><br><span class="line">    vector&lt;int&gt; vec2(vec1);  //使用vec1初始化vec2</span><br><span class="line">    vector&lt;int&gt; vec3(vec1.begin(),vec1.end());//使用vec1初始化vec2</span><br><span class="line">    vector&lt;int&gt; vec4(10);    //10个值为的元素</span><br><span class="line">    vector&lt;int&gt; vec5(10,4);  //10个值为的元素</span><br><span class="line">//2.常用操作方法</span><br><span class="line">    vec1.insert(vec1.end(),5,3);    //从vec1.back位置插入个值为的元素</span><br><span class="line">    vec1.erase(vec1.begin(),vec1.end());//删除之间的元素，其他元素前移</span><br><span class="line"></span><br><span class="line">    vector&lt;int&gt;::iterator iter = vec1.begin();    //获取迭代器首地址</span><br><span class="line">    vec1.clear();                 //清空元素</span><br><span class="line">//3.遍历</span><br><span class="line">    //迭代器法</span><br><span class="line">    vector&lt;int&gt;::const_iterator iterator = vec1.begin();</span><br><span class="line">    for(;iterator != vec1.end();iterator++)&#123;</span><br><span class="line">       cout&lt;&lt;*iterator;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></t></t></t></t></t></t></t></t></li></ul><p>v.begin(),v.end()分别返回头尾两个位置。</p><h4 id="List"><a href="#List" class="headerlink" title="List"></a>List</h4><p>list是链表的抽象数据结构(ADT)。list中的所有数据在空间分配上不一定是连续存放的。相对vector，list没有capaciy属性。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">构造list</span><br><span class="line">int ia[] = &#123;123,343,12,100,343,5,5&#125;;</span><br><span class="line">list&lt;int&gt; ls;</span><br><span class="line">list&lt;int&gt; ls(ia,ia+7); //&#123; 123,343,12,100,343,5,5&#125;</span><br><span class="line">list&lt;int&gt; ls(2,4); //&#123;4,4&#125;</span><br><span class="line">list&lt;int&gt; ls(4);&#123; 0,0,0,0&#125;</span><br><span class="line">输出list中的所有数据</span><br><span class="line">list&lt;int&gt;::iterator itEnd = ls.end();</span><br><span class="line">for(list&lt;int&gt;::iterator it = ls.begin(); it != itEnd; ++it)</span><br><span class="line">std::cout &lt;&lt; *it &lt;&lt; endl;</span><br></pre></td></tr></table></figure></p><h4 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h4><p>deque，即双向链表。相对vector，deque也是连续空间，但不是vector的连续线性空间。<br>deque中的迭代器的种类为随机存取迭代器(random-access-iterator)。</p><h3 id="关联容器"><a href="#关联容器" class="headerlink" title="关联容器"></a>关联容器</h3><p>关联容器(Associative Container)提供了根据key快速检索数据的能力。在关联容器(Associative Container)中，key和元素都是成对(pair)存在的,你可以调用std::make_pair使用key和元素值来构建一个pair。<br>STL提供的关联容器包括set、multiset、map、multimap。<br>set和map只支持唯一键(unique key)，即对个key最多只保存一个元素。multiset和multimap则支持多个key，一个key可以对应多个元素。<br>set和map的区别在于，在set里面key和元素是同一个值，而在map里面key和元素分开存储。</p><h4 id="set"><a href="#set" class="headerlink" title="set"></a>set</h4><p>set是集合的抽象数据结构(ADT)。不同于数学意义上的集合，STL中的set的所有的元素都是有序的而且set中所有的元素都是唯一的。<br>set中的迭代器的种类为双向存取迭代器(bidirectional-access-iterator)。<br>请看下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">构建一个set</span><br><span class="line">set&lt;string&gt; fruits;</span><br><span class="line">往set中添加数据</span><br><span class="line">fruits.insert(&quot;apple&quot;);</span><br><span class="line">fruits.insert(&quot;orange&quot;);</span><br><span class="line">fruits.insert(&quot;banana&quot;);</span><br><span class="line">输出set</span><br><span class="line">set&lt;string&gt;::iterator itEnd = fruits.end();</span><br><span class="line">for (set&lt;string&gt;::iterator it=fruits.begin(); it != itEnd; it++)</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; &quot; &quot;;</span><br><span class="line">输出结果</span><br><span class="line">         apple banana orange</span><br></pre></td></tr></table></figure></p><h4 id="multiset"><a href="#multiset" class="headerlink" title="multiset"></a>multiset</h4><p>multiset和set基本相同，所不同的是，一个multiset中的元素是可以重复的。<br>multiset中的迭代器的种类为双向存取迭代器(bidirectional-access-iterator)。<br>multiset的操作同set。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">请看下面的例子：</span><br><span class="line">构建一个set</span><br><span class="line">set&lt;string&gt; fruits;</span><br><span class="line">往set中添加数据</span><br><span class="line">fruits.insert(&quot;apple&quot;);</span><br><span class="line">fruits.insert(&quot;orange&quot;);</span><br><span class="line">fruits.insert(&quot;apple&quot;);</span><br><span class="line">fruits.insert(&quot;banana&quot;);</span><br><span class="line">fruits.insert(&quot;banana&quot;);</span><br><span class="line">输出set</span><br><span class="line">set&lt;string&gt;::iterator itEnd = fruits.end();</span><br><span class="line">for (set&lt;string&gt;::iterator it=fruits.begin(); it != itEnd; it++)</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; &quot; &quot;;</span><br><span class="line">输出结果</span><br><span class="line">         apple apple banana banana orange</span><br></pre></td></tr></table></figure></p><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>map是字典的抽象数据结构(ADT)。map中的所有的元素都会根据key自动进行排序，而且所有的元素都是唯一的。map中的所有的元素都是pair，即键(key)和值(value)组成的序列(pair中的第一个元素为key，第二个元素为value)。<br>map中的迭代器的种类为双向存取迭代器(bidirectional-access-iterator)。<br>请看下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">构建一个map</span><br><span class="line">typedef std::map&lt;int,string&gt; EMPLOYEE_MAP;</span><br><span class="line">EMPLOYEE_MAP employees;</span><br><span class="line">往map中添加数据</span><br><span class="line">employees.insert(EMPLOYEE_MAP::value_type(25301, &quot;A&quot;));</span><br><span class="line">employees.insert(EMPLOYEE_MAP::value_type(25302, &quot;B&quot;));</span><br><span class="line">employees.insert(EMPLOYEE_MAP::value_type(25303, &quot;C&quot;));</span><br><span class="line">employees.insert(EMPLOYEE_MAP::value_type(25304, &quot;D&quot;));</span><br><span class="line">employees.insert(EMPLOYEE_MAP::value_type(25305, &quot;E&quot;));</span><br><span class="line">输出map</span><br><span class="line">EMPLOYEE_MAP::iterator itEnd = employees.end();</span><br><span class="line">for (EMPLOYEE_MAP::iterator it = employees.begin(); it != itEnd; ++it)</span><br><span class="line">&#123;</span><br><span class="line">std::cout &lt;&lt; it-&gt;first &lt;&lt; &quot;-&quot;;</span><br><span class="line">        std::cout &lt;&lt; it-&gt;second &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">输出结果</span><br><span class="line">         25301-A</span><br><span class="line">25302-B</span><br><span class="line">25303-C</span><br><span class="line">25304-D</span><br><span class="line">25305-E</span><br></pre></td></tr></table></figure></p><h4 id="multimap"><a href="#multimap" class="headerlink" title="multimap"></a>multimap</h4><p>multimap和map基本相同，所不同的是，一个multimap中的key可以重复。<br>multimap中的迭代器的种类为双向存取迭代器(bidirectional-access-iterator)。<br>multimap的操作同set。</p><h4 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h4><p>这里要提到的就是hashtable，即哈希表的抽象数据结构(ADT)。<br>因为hashtable不是STL标准的一部分，部分编译器(如Microsoft Visual C++)并没有提供hashtable的实现。<br>hashtable中的迭代器的种类为前向迭代器(forward-iterator)。<br>在SGI的STL中，有hashtable的实现，其中的hash_set,hash_map,hash_multiset,<br>hash_multimap都是基于hashtable而构建的，是hashtable的适配器(Adaptor)。而在<br>Microsoft Visual C++的STL中，并没有提供STL的实现。其中的hash_set,hash_map,hash_multiset,hash_multimap都是基于STL中的hash算法而构建的。<br>hash_set的操作基本同set；<br>hash_map的操作基本同map；<br>hash_multiset的操作基本同multiset；<br>hash_multimap的操作基本同multimap。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:文件和异常</title>
      <link href="/2018/01/17/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B06%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8/"/>
      <url>/2018/01/17/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B06%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第六部分。<br><a id="more"></a></p><h2 id="文件和流"><a href="#文件和流" class="headerlink" title="文件和流"></a>文件和流</h2><p>使用C++的’fstream’标准库可以完成对文件的处理。该库提供了如下三个数据类型：</p><ul><li>ofstream    该数据类型表示输出文件流，用于创建文件并向文件写入信息。</li><li>ifstream    该数据类型表示输入文件流，用于从文件读取信息。</li><li>fstream    该数据类型通常表示文件流，且同时具有 ofstream 和 ifstream 两种功能，这意味着它可以创建文件，向文件写入信息，从文件读取信息。<h3 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h3>在从文件读取信息或者向文件写入信息之前，必须先打开文件。ofstream 和 fstream 对象都可以用来打开文件进行写操作，如果只需要打开文件进行读操作，则使用 ifstream 对象。<br>下面是 open() 函数的标准语法，open() 函数是 fstream、ifstream 和 ofstream 对象的一个成员。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void open(const char *filename, ios::openmode mode);</span><br></pre></td></tr></table></figure></li></ul><p>其中第一个参数指定要打开的文件路径，第二个参数指定打开方式，有如下打开方式：</p><ul><li>ios::app    追加模式。所有写入都追加到文件末尾。</li><li>ios::ate    文件打开后定位到文件末尾。</li><li>ios::in    打开文件用于读取。</li><li>ios::out    打开文件用于写入。</li><li>ios::trunc    如果该文件已经存在，其内容将在打开文件之前被截断，即把文件长度设为 0。<br>可以把以上两种或两种以上的模式结合使用。例如，如果您想要以写入模式打开文件，并希望截断文件，以防文件已存在，那么您可以使用下面的语法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;fstream&gt;</span><br><span class="line">ofstream outfile;</span><br><span class="line">outfile.open(&quot;file.dat&quot;, ios::out | ios::trunc)</span><br></pre></td></tr></table></figure></li></ul><p>如果想打开一个文件用于读写：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fstream afile;</span><br><span class="line">afile.open(&quot;file.dat&quot;, ios::in | ios::out)</span><br></pre></td></tr></table></figure></p><h3 id="关闭文件"><a href="#关闭文件" class="headerlink" title="关闭文件"></a>关闭文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fstream file;</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><h3 id="读取和写入"><a href="#读取和写入" class="headerlink" title="读取和写入"></a>读取和写入</h3><p>在 C++ 编程中，我们使用流插入运算符（ &lt;&lt; ）向文件写入信息，就像使用该运算符输出信息到屏幕上一样。唯一不同的是，在这里您使用的是 ofstream 或 fstream 对象，而不是 cout 对象。<br>在 C++ 编程中，我们使用流提取运算符（ &gt;&gt; ）从文件读取信息，就像使用该运算符从键盘输入信息一样。唯一不同的是，在这里您使用的是 ifstream 或 fstream 对象，而不是 cin 对象。</p><h2 id="C-异常"><a href="#C-异常" class="headerlink" title="C++异常"></a>C++异常</h2><p>常见的异常由try-catch组成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">try&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;catch(ExceptionName e1)&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;catch(ExceptionName e2)&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="抛出异常"><a href="#抛出异常" class="headerlink" title="抛出异常"></a>抛出异常</h3><p>使用throw抛出异常。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">double division(int a, int b)&#123;</span><br><span class="line">   if( b == 0 )&#123;</span><br><span class="line">      throw &quot;Division by zero condition!&quot;;</span><br><span class="line">   &#125;</span><br><span class="line">   return (a/b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:继承和多态</title>
      <link href="/2018/01/16/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B05%E7%BB%A7%E6%89%BF%E5%92%8C%E5%A4%9A%E6%80%81/"/>
      <url>/2018/01/16/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B05%E7%BB%A7%E6%89%BF%E5%92%8C%E5%A4%9A%E6%80%81/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第五部分。<br><a id="more"></a></p><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>当创建一个类时，您不需要重新编写新的数据成员和成员函数，只需指定新建的类继承了一个已有的类的成员即可。这个已有的类称为基类，新建的类称为派生类。</p><p>在C++中，一个类可以派生自多个类，意味着子类可以从多个父类继承数据和函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class derived_class: access-specifier base_class1, access-specifier base_class2</span><br><span class="line">&#123;class_body&#125;</span><br></pre></td></tr></table></figure></p><p>access-specifier如果没有声明则默认为private</p><h3 id="访问控制修饰继承"><a href="#访问控制修饰继承" class="headerlink" title="访问控制修饰继承"></a>访问控制修饰继承</h3><p>C++有三种继承方式，对应着三种修饰符，它们相应的改变了父类成员的访问属性。</p><ul><li>public 继承：基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：public, protected, private</li><li>protected 继承：基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：protected, protected, private</li><li>private 继承：基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：private, private, private</li></ul><p>但无论哪种继承方式，上面两点都没有改变：</p><ol><li>private 成员只能被本类成员（类内）和友元访问，不能被派生类访问；</li><li>protected 成员可以被派生类访问。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class A&#123;&#125;;</span><br><span class="line">class B : public A&#123;&#125; //B公有继承自A</span><br><span class="line">class C : private A&#123;&#125; // C私有继承自A</span><br><span class="line">class D : protected A&#123;&#125; // D保护继承自A</span><br></pre></td></tr></table></figure></li></ol><p>一个派生类继承了所有的基类方法，但下列情况除外：</p><ul><li>基类的构造函数、析构函数和拷贝构造函数。</li><li>基类的重载运算符。</li><li>基类的友元函数。</li></ul><p>正常情况下，<strong>几乎不使用 protected 或 private 继承，通常使用 public 继承。</strong></p><h2 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h2><h3 id="函数重载"><a href="#函数重载" class="headerlink" title="函数重载"></a>函数重载</h3><p>在同一个作用域内，可以声明几个功能类似的同名函数，但是这些同名函数的形式参数（指参数的个数、类型或者顺序）必须不同。您不能仅通过返回类型的不同来重载函数。</p><h3 id="运算符重载"><a href="#运算符重载" class="headerlink" title="运算符重载"></a>运算符重载</h3><p>重载的运算符是带有特殊名称的函数，函数名是由关键字 operator 和其后要重载的运算符符号构成的。与其他函数一样，重载运算符有一个返回类型和一个参数列表。<br>大多数的重载运算符可被定义为普通的非成员函数或者被定义为类成员函数。如果我们定义上面的函数为类的非成员函数，那么我们需要为每次操作传递两个参数，如果定义为类的成员函数，则只需要一个参数。<br>例如，对Box对象的加法进行重载：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Box operator+(const Box&amp;, const Box&amp;);</span><br><span class="line">Box:: Box operator+(const Box&amp;)</span><br></pre></td></tr></table></figure></p><h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>多态按字面的意思就是多种形态。当类之间存在层次结构，并且类之间是通过继承关联时，就会用到多态。<br>C++ 多态意味着调用成员函数时，会根据调用函数的对象的类型来执行不同的函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">class Shape &#123;</span><br><span class="line">   protected:</span><br><span class="line">      int width, height;</span><br><span class="line">   public:</span><br><span class="line">      Shape( int a=0, int b=0)&#123;</span><br><span class="line">         width = a;</span><br><span class="line">         height = b;</span><br><span class="line">      &#125;</span><br><span class="line">      void area()&#123;</span><br><span class="line">         cout &lt;&lt; &quot;Parent class area :&quot; &lt;&lt;endl;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;;</span><br><span class="line">class Rectangle: public Shape&#123;</span><br><span class="line">   public:</span><br><span class="line">      Rectangle( int a=0, int b=0):Shape(a, b) &#123; &#125;</span><br><span class="line">      void area ()&#123;</span><br><span class="line">         cout &lt;&lt; &quot;Rectangle class area :&quot; &lt;&lt;endl;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;;</span><br><span class="line">class Triangle: public Shape&#123;</span><br><span class="line">   public:</span><br><span class="line">      Triangle( int a=0, int b=0):Shape(a, b) &#123; &#125;</span><br><span class="line">      void area ()&#123;</span><br><span class="line">         cout &lt;&lt; &quot;Triangle class area :&quot; &lt;&lt;endl;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int main( )&#123;</span><br><span class="line">   Shape *shape;</span><br><span class="line">   Rectangle rec(10,7);</span><br><span class="line">   Triangle  tri(10,5);</span><br><span class="line"></span><br><span class="line">   shape = &amp;rec;</span><br><span class="line">   shape-&gt;area();</span><br><span class="line">   shape = &amp;tri;</span><br><span class="line">   shape-&gt;area();</span><br><span class="line"></span><br><span class="line">   return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出结果为：</span><br><span class="line">Parent class area</span><br><span class="line">Parent class area</span><br></pre></td></tr></table></figure></p><p>导致错误输出的原因是，调用函数 area() 被编译器设置为基类中的版本，这就是所谓的静态多态，或静态链接 - 函数调用在程序执行前就准备好了。有时候这也被称为早绑定，因为 area() 函数在程序编译期间就已经设置好了。<br>如果想要达到我们预想的输出，就需要修改父类中被继承的area函数，使用关键字 virtual。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public:</span><br><span class="line">    virtual int area()&#123;...&#125;</span><br></pre></td></tr></table></figure></p><p>此时，编译器看的是指针的内容，而不是它的类型。因此，由于 tri 和 rec 类的对象的地址存储在 *shape 中，所以会调用各自的 area() 函数。</p><h3 id="虚函数"><a href="#虚函数" class="headerlink" title="虚函数"></a>虚函数</h3><p>在上述例子中，父类的area函数被关键字virtual修饰，即为虚函数。在派生类中重新定义基类中定义的虚函数时，会告诉编译器不要静态链接到该函数。我们想要的是在程序中任意点可以根据所调用的对象类型来选择调用的函数，这种操作被称为动态链接，或后期绑定。</p><h3 id="纯虚函数"><a href="#纯虚函数" class="headerlink" title="纯虚函数"></a>纯虚函数</h3><p>有时可能想要在基类中定义虚函数，以便在派生类中重新定义该函数更好地适用于对象，但是在基类中又不能对虚函数给出有意义的实现，这个时候就会用到纯虚函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public:</span><br><span class="line">    virtual int area() = 0;</span><br></pre></td></tr></table></figure></p><p>= 0 告诉编译器，函数没有主体，上面的虚函数是纯虚函数。<br>使用纯虚函数，我们可以定义抽象类，如果类中至少有一个函数被声明为纯虚函数，则这个类就是抽象类。该抽象类即可作为接口。抽象类不能被用于实例化对象，它只能作为接口使用。如果试图实例化一个抽象类的对象，会导致编译错误。因此，如果一个 ABC 的子类需要被实例化，则必须实现每个虚函数，这也意味着 C++ 支持使用 ABC 声明接口。如果没有在派生类中重载纯虚函数，就尝试实例化该类的对象，会导致编译错误。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:结构体和类</title>
      <link href="/2018/01/15/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B04%E7%BB%93%E6%9E%84%E4%BD%93%E5%92%8C%E7%B1%BB/"/>
      <url>/2018/01/15/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B04%E7%BB%93%E6%9E%84%E4%BD%93%E5%92%8C%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第四部分。<br><a id="more"></a></p><h2 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h2><p>C++可以自定义结构体，便于自定义数据类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct type_name&#123;</span><br><span class="line">    member_type1 member_name1;</span><br><span class="line">    member_type2 member_name2;</span><br><span class="line">    ...</span><br><span class="line">&#125; object_name;</span><br></pre></td></tr></table></figure></p><p>比如定义一个book数据类型：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">struct Book&#123;</span><br><span class="line">    char title[50];</span><br><span class="line">    char author[20];</span><br><span class="line">    double price;</span><br><span class="line">    int bood_id;</span><br><span class="line">&#125; book1;</span><br></pre></td></tr></table></figure></p><h3 id="访问结构体成员"><a href="#访问结构体成员" class="headerlink" title="访问结构体成员"></a>访问结构体成员</h3><p>使用成员访问运算符’.’，来访问结构体中的成员,继续刚才的例子。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Book book1;</span><br><span class="line">strcpy(book1.title, &quot;C++ primer&quot;);</span><br><span class="line">book1.price = 20.8;</span><br></pre></td></tr></table></figure></p><h3 id="结构体作为函数参数"><a href="#结构体作为函数参数" class="headerlink" title="结构体作为函数参数"></a>结构体作为函数参数</h3><p>结构体也可以作为参数传递到函数中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void printBook(struct Book book)&#123;</span><br><span class="line">    cout&lt;&lt;book.title&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;book.author&lt;&lt;endl;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="结构体指针"><a href="#结构体指针" class="headerlink" title="结构体指针"></a>结构体指针</h3><p>可以定义指向结构体的指针：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Book book1;</span><br><span class="line">struct Book *pbook;</span><br><span class="line">pbook = &amp;book1;</span><br></pre></td></tr></table></figure></p><p> 通过 ‘-&gt;’ 运算符,使用指向该结构的指针访问结构的成员：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">char *title = pbook-&gt;title;</span><br></pre></td></tr></table></figure></p><h3 id="typeof-关键字"><a href="#typeof-关键字" class="headerlink" title="typeof 关键字"></a>typeof 关键字</h3><p>在定义结构体时，使用typeof关键字可以在接下来定义结构体变量时省略struct关键字。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">typedef struct Books</span><br><span class="line">&#123;</span><br><span class="line">   char  title[50];</span><br><span class="line">   char  author[50];</span><br><span class="line">   char  subject[100];</span><br><span class="line">   int   book_id;</span><br><span class="line">&#125;Books;</span><br><span class="line"></span><br><span class="line">Books book1, book2;</span><br></pre></td></tr></table></figure></p><h3 id="Note：两种访问符的区别"><a href="#Note：两种访问符的区别" class="headerlink" title="Note：两种访问符的区别"></a>Note：两种访问符的区别</h3><p>对于变量，可以使用访问符’.’来访问结构体/类得成员，如果是指向变量的指针，则需要使用访问符’-&gt;’来访问成员：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">struct AAA&#123;</span><br><span class="line">    int number;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">AAA s;</span><br><span class="line">AAA *ps;</span><br><span class="line"></span><br><span class="line">// 一下三者是等价的</span><br><span class="line">s.number = 1;</span><br><span class="line">(*ps).number = 1;</span><br><span class="line">ps-&gt;number = 1;</span><br></pre></td></tr></table></figure></p><p><strong>箭头（-&gt;）：左边必须为指针；</strong><br><strong>点号（.）：左边必须为实体。</strong></p><h2 id="C-类"><a href="#C-类" class="headerlink" title="C++类"></a>C++类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Box&#123;</span><br><span class="line">public:</span><br><span class="line">    double length;</span><br><span class="line">    double breadth;</span><br><span class="line">    double height;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Box box1;</span><br><span class="line">box1.height = 1.0; //使用成员访问符访问类成员</span><br></pre></td></tr></table></figure><h3 id="类外部定义函数"><a href="#类外部定义函数" class="headerlink" title="类外部定义函数"></a>类外部定义函数</h3><p>我们不仅可以将函数定义写在类的内部作为类的成员函数，也可以将函数写在类的外部，然后使用范围解析运算符’::’ 来定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Box&#123;</span><br><span class="line">public:</span><br><span class="line">    double getVolume(void);</span><br><span class="line">&#125;;</span><br><span class="line">double Box::getVolume(void)&#123;...&#125;</span><br></pre></td></tr></table></figure></p><h3 id="类访问修饰符"><a href="#类访问修饰符" class="headerlink" title="类访问修饰符"></a>类访问修饰符</h3><p>访问修饰符可以限制函数直接访问类的内部成员。C++有：public，private，protected三种修饰符。</p><ul><li>public为公有成员，在程序中类的外部可以直接访问。</li><li>private私有成员，私有成员变量或函数在类的外部是不可访问的。只有类内部函数和友元函数可以访问。</li><li>protected保护成员，于私有成员类似，不同的是：<strong>保护成员在派生类（子类）中是可以访问的</strong></li></ul><h3 id="构造函数和初始化列表"><a href="#构造函数和初始化列表" class="headerlink" title="构造函数和初始化列表"></a>构造函数和初始化列表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Line&#123;</span><br><span class="line">public:</span><br><span class="line">    Line(double len, int tempx);</span><br><span class="line">private:</span><br><span class="line">    double length;</span><br><span class="line">    int x;</span><br><span class="line">&#125;;</span><br><span class="line">Line::Line(double len, int tempx): length(len), x(tempx)&#123;</span><br><span class="line">    cout&lt;&lt;&quot;object is created&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line">等价于：</span><br><span class="line">Line::Line(double len, int tempx)&#123;</span><br><span class="line">    length = len;</span><br><span class="line">    x = tempx;</span><br><span class="line">    cout&lt;&lt;&quot;object is created&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="拷贝构造函数"><a href="#拷贝构造函数" class="headerlink" title="拷贝构造函数"></a>拷贝构造函数</h3><p>拷贝构造函数是一种特殊的构造函数，它在创建对象时，是使用同一类中之前创建的对象来初始化新创建的对象。拷贝构造函数通常用于：</p><ul><li>通过使用另一个同类型的对象来初始化新创建的对象。</li><li>复制对象把它作为参数传递给函数。</li><li>复制对象，并从函数返回这个对象。<br>如果在类中没有定义拷贝构造函数，编译器会自行定义一个。如果类带有指针变量，并有动态内存分配，则它必须有一个拷贝构造函数。拷贝构造函数的最常见形式如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">className (const className &amp;obj)&#123;&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="类的析构函数"><a href="#类的析构函数" class="headerlink" title="类的析构函数"></a>类的析构函数</h3><p>类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。</p><p>析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Line::~Line(void)&#123;</span><br><span class="line">    cout&lt;&lt;&quot;obj is deleted&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Note：构建和析构顺序"><a href="#Note：构建和析构顺序" class="headerlink" title="Note：构建和析构顺序"></a>Note：构建和析构顺序</h3><ol><li>对于包含内部成员类的构造，首先初始化内部成员类，然后初始化该类。析构过程中，首先析构内部成员类，然后析构本类。</li><li>对于继承类的构造来说，首先初始化父类，然后初始化继承类。析构过程中，首先析构继承类，然后析构父类。</li></ol><h3 id="友元函数"><a href="#友元函数" class="headerlink" title="友元函数"></a>友元函数</h3><p>类的友元函数是定义在类外部，但有权访问类的所有私有（private）成员和保护（protected）成员。尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。</p><p>友元可以是一个函数，该函数被称为友元函数；友元也可以是一个类，该类被称为友元类，在这种情况下，整个类及其所有成员都是友元。</p><p>如果要声明函数为一个类的友元，需要在类定义中该函数原型前使用关键字 friend。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Box&#123;</span><br><span class="line">public:</span><br><span class="line">    double width;</span><br><span class="line">    friend void printWidth(Box box);</span><br><span class="line">&#125;</span><br><span class="line">void printWidth(Box box)&#123;cout&lt;&lt;box.width&lt;&lt;endl;&#125;</span><br></pre></td></tr></table></figure></p><h3 id="内联函数"><a href="#内联函数" class="headerlink" title="内联函数"></a>内联函数</h3><p>引入内联函数的目的是为了解决程序中函数调用的效率问题。C++ 使用关键字inline定义内联函数。如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方， 而不是按通常的函数调用机制进行调用。如果已定义的函数多于10行，编译器会忽略 inline 限定符。</p><p>程序在编译器编译的时候，编译器将程序中出现的内联函数的调用表达式用内联函数的函数体进行替换，而对于其他的函数，都是在运行时候才被替代。这其实就是个空间代价换时间的i节省。所以内联函数一般都是1-5行的小函数。在使用内联函数时要留神：</p><p>1.在内联函数内不允许使用循环语句和开关语句；<br>2.内联函数的定义必须出现在内联函数第一次调用之前；<br>3.类结构中所在的类说明内部定义的函数是内联函数。</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点: 当函数体比较小的时候, 内联该函数可以令目标代码更加高效. 对于存取函数以及其它函数体比较短, 性能关键的函数, 鼓励使用内联.</p><p>缺点: 滥用内联将导致程序变慢. 内联可能使目标代码量或增或减, 这取决于内联函数的大小. 内联非常短小的存取函数通常会减少代码大小, 但内联一个相当大的函数将戏剧性的增加代码大小</p><h3 id="类的静态成员"><a href="#类的静态成员" class="headerlink" title="类的静态成员"></a>类的静态成员</h3><h4 id="静态变量"><a href="#静态变量" class="headerlink" title="静态变量"></a>静态变量</h4><p>可以使用 static 关键字来把类成员定义为静态的。当我们声明类的成员为静态时，这意味着无论创建多少个类的对象，静态成员都只有一个副本。</p><p>静态成员在类的所有对象中是共享的。如果不存在其他的初始化语句，在创建第一个对象时，所有的静态数据都会被初始化为零。我们不能把静态成员的初始化放置在类的定义中，但是可以在类的外部通过使用范围解析运算符 :: 来重新声明静态变量从而对它进行初始化</p><h4 id="静态成员函数"><a href="#静态成员函数" class="headerlink" title="静态成员函数"></a>静态成员函数</h4><p>如果把函数成员声明为静态的，就可以把函数与类的任何特定对象独立开来。静态成员函数即使在类对象不存在的情况下也能被调用，静态函数只要使用类名加范围解析运算符 :: 就可以访问。</p><p>静态成员函数只能访问静态成员数据、其他静态成员函数和类外部的其他函数。</p><p>静态成员函数有一个类范围，他们不能访问类的 this 指针。可以使用静态成员函数来判断类的某些对象是否已被创建。</p><h5 id="静态成员函数与普通成员函数的区别："><a href="#静态成员函数与普通成员函数的区别：" class="headerlink" title="静态成员函数与普通成员函数的区别："></a>静态成员函数与普通成员函数的区别：</h5><ul><li>静态成员函数没有 this 指针，只能访问静态成员（包括静态成员变量和静态成员函数）。</li><li>普通成员函数有 this 指针，可以访问类中的任意成员；而静态成员函数没有 this 指针。</li></ul>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:指针和引用</title>
      <link href="/2018/01/14/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B03%E6%8C%87%E9%92%88%E5%92%8C%E5%BC%95%E7%94%A8/"/>
      <url>/2018/01/14/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B03%E6%8C%87%E9%92%88%E5%92%8C%E5%BC%95%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第三部分。<br><a id="more"></a></p><h2 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h2><p>指针是C++中最重要的特性之一，指针是一个变量，其值为一个地址（该地址存储着另一个变量）。</p><h3 id="常见指针操作"><a href="#常见指针操作" class="headerlink" title="常见指针操作"></a>常见指针操作</h3><p>使用指针时会频繁进行以下几个操作：定义一个指针变量、把变量地址赋值给指针、访问指针变量中可用地址的值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int var = 10;</span><br><span class="line">int *p; //定义一个指向int变量地址的指针</span><br><span class="line">p = &amp;var; //将变量var的地址赋值给int指针p</span><br><span class="line">cout&lt;&lt;*p&lt;&lt;endl; //取指针p指向的地址对应的变量的值</span><br></pre></td></tr></table></figure></p><h3 id="空指针"><a href="#空指针" class="headerlink" title="空指针"></a>空指针</h3><p>在指针声明时，如果没有确切的地址赋值，则将改指针指向NULL是一个好的习惯。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int *p = NULL;</span><br><span class="line">if(p) &#123;&#125; //如果指针p不为空</span><br><span class="line">if(!p) &#123;&#125; //如果指针p为空</span><br></pre></td></tr></table></figure></p><h3 id="指针的算数运算"><a href="#指针的算数运算" class="headerlink" title="指针的算数运算"></a>指针的算数运算</h3><p>指针是一个用数值表示的地址。因此可以对指针执行算术运算。可以对指针进行四种算术运算：++、–、+、-。<br>假设int类型指针ptr指向的地址为1000，而int在C++中占4字节，所以如果执行ptr++后，ptr指向的地址变为1004.用这个方法可以遍历数组。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int var[3] = &#123;1,2,3&#125;;</span><br><span class="line">int *p;</span><br><span class="line">p = var;</span><br><span class="line">for(int i = 0; i&lt;3; i++)&#123;</span><br><span class="line">    cout&lt;&lt;*p&lt;&lt;endl;</span><br><span class="line">    p++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="指向指针的指针"><a href="#指向指针的指针" class="headerlink" title="指向指针的指针"></a>指向指针的指针</h3><p>指向指针的指针是一种多级间接寻址的形式，或者说是一个指针链。通常，一个指针包含一个变量的地址。当我们定义一个指向指针的指针时，第一个指针包含了第二个指针的地址，第二个指针指向包含实际值的位置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int value = 1;</span><br><span class="line">int *p = NULL;</span><br><span class="line">int **pp = NULL;</span><br><span class="line">p = &amp;value;</span><br><span class="line">pp = &amp;p;</span><br><span class="line">value == **pp;</span><br><span class="line">value == *p;</span><br></pre></td></tr></table></figure></p><h3 id="指针数组"><a href="#指针数组" class="headerlink" title="指针数组"></a>指针数组</h3><p>数组中的元素为指针，即为指针数组，声明如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#define LEN 3</span><br><span class="line">int value[LEN] = &#123;1,2,3&#125;;</span><br><span class="line">int *p[LEN]; //声明一个长度为LEN的数组，数组中的元素为int类型的指针。</span><br><span class="line">for(int i=0;i&lt;LEN; i++)&#123;</span><br><span class="line">    p[i] = &amp;value[i]; // 将value数组复制给p数组</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="空指针-1"><a href="#空指针-1" class="headerlink" title="空指针"></a>空指针</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int *p = 0;         // 比较常用的写法</span><br><span class="line">int *p = NULL;      // NULL包含在cstdlib中</span><br><span class="line">int *p = nullptr;   // c++11新特性</span><br></pre></td></tr></table></figure><h3 id="void-指针"><a href="#void-指针" class="headerlink" title="void* 指针"></a>void* 指针</h3><p>void* 指针是一个种特殊的指针类型，可以用于存放任意对象的地址，我们对该地址中到底是什么类型的对象并不了解。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>引用可以理解为是某个已存在变量的另一个名字。一旦把引用初始化为某个变量，就可以使用该引用名称或变量名称来指向变量。</p><h3 id="引用和指针的区别"><a href="#引用和指针的区别" class="headerlink" title="引用和指针的区别"></a>引用和指针的区别</h3><p>它们之间主要有三个不同：</p><ol><li>不存在空引用，但存在空指针</li><li>引用在某种程度上可以认为是常量，一旦引用被初始化为一个对象，就不能被指向到另一个对象。指针可以在任何时候指向到另一个对象。</li><li>引用必须在创建时被初始化。指针可以在任何时间被初始化。<h3 id="引用的使用"><a href="#引用的使用" class="headerlink" title="引用的使用"></a>引用的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int i = 0;</span><br><span class="line">int&amp; j = i; //初始化为 i 的整型引用</span><br></pre></td></tr></table></figure></li></ol><p>引用可以作为实参传递到函数中，这样会导致在函数中对形参的更新影响到原有的实参的值。也就是说，函数会对原有的值进行操作和更改（类似于传入指针）。</p><h3 id="引用作为返回值"><a href="#引用作为返回值" class="headerlink" title="引用作为返回值"></a>引用作为返回值</h3><p>通过使用引用来替代指针，会使 C++ 程序更容易阅读和维护。C++ 函数可以返回一个引用，方式与返回一个指针类似。<br>当函数返回一个引用时，则返回一个指向返回值的隐式指针。这样，函数就可以放在赋值语句的左边。例子如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">int value[3] = &#123;1,2,3&#125;;</span><br><span class="line">int&amp; setValues(int i)&#123;</span><br><span class="line">    return values[i];</span><br><span class="line">&#125;</span><br><span class="line">void main()&#123;</span><br><span class="line">    setValues(0) = -1; //将value的第0个元素修改为-1</span><br><span class="line">    setValues(1) = -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当返回一个引用时，要注意被引用的对象不能超出作用域。所以返回一个对局部变量的引用是不合法的，但是，可以返回一个对静态变量的引用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int&amp; func()&#123;</span><br><span class="line">    static int a = 1;</span><br><span class="line">    int b = 1;</span><br><span class="line">    return b; //非法，因为b为局部变量</span><br><span class="line">    return a; //合法，因为a为static</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="指向指针的引用"><a href="#指向指针的引用" class="headerlink" title="指向指针的引用"></a>指向指针的引用</h3><p>引用本身不是一个对象，因此不能定义指向引用的指针，但指针是对象，所以存在对指针的引用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int i = 1;</span><br><span class="line">int *p;</span><br><span class="line">int *&amp;r = p;  // r是一个对指针p的引用</span><br><span class="line">r = &amp;i;   // 令p指向i</span><br><span class="line">*r = 0;   // 将i置为0</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:函数 数组和字符串</title>
      <link href="/2018/01/13/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B02%E5%87%BD%E6%95%B0%20%E6%95%B0%E7%BB%84%E5%92%8C%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2018/01/13/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B02%E5%87%BD%E6%95%B0%20%E6%95%B0%E7%BB%84%E5%92%8C%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      <content type="html"><![CDATA[<p>C++复习笔记的第二部分。<br><a id="more"></a></p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>和Java不同的是，C++需要函数声明：当在一个源文件中定义函数且在另一个文件中调用函数时，函数声明是必需的。在这种情况下，应该在调用函数的文件顶部声明函数。</p><h3 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h3><p>在函数调用时，存在如下调用传参方法：</p><ol><li>传值调用：该方法把实参的实际值复制给函数的形参，修改函数内的形参对实参没有影响。默认情况下，C++ 使用 <strong>传值调用来传递参数。</strong></li><li>指针调用：该方法把实参的地址复制给形参，用于访问调用中要用到的实参。修改形参会影响实际参数。</li><li>引用调用：该方法把实参的引用复制给形参，用于访问调用中要用到的实参，修改形参会影响实际参数。<h3 id="lambda表达式"><a href="#lambda表达式" class="headerlink" title="lambda表达式"></a>lambda表达式</h3>C++中lambda表达式的声明方式如下：<br><a href="parameters">capture</a>-&gt;return-type{body}<br>例如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[](int x, int y)&#123;return x&lt;y;&#125;</span><br><span class="line">[](int x, int y) -&gt; int &#123;int z = x + y; return z+x;&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>C++的数组声明：’type arrayName[size]’<br>包括如下初始化方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int array[2] = &#123;1, 2&#125;;</span><br><span class="line">int array[] = &#123;1, 2&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="多维数组"><a href="#多维数组" class="headerlink" title="多维数组"></a>多维数组</h3><p>多维数组声明如下：’type arrayName[size1][size2][size3]…’<br>以二维数组为例，本质上仍然是一位数组，一维数组中每个元素仍然为一个一维数组。<br>多维数组可以用嵌套大括号来初始化，也可以省略掉嵌套大括号。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int a[3][4] = &#123;  </span><br><span class="line"> &#123;0, 1, 2, 3&#125; ,   /*  初始化索引号为 0 的行 */</span><br><span class="line"> &#123;4, 5, 6, 7&#125; ,   /*  初始化索引号为 1 的行 */</span><br><span class="line"> &#123;8, 9, 10, 11&#125;   /*  初始化索引号为 2 的行 */</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int a[3][4] = &#123;0,1,2,3,4,5,6,7,8,9,10,11&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="数组作为参数"><a href="#数组作为参数" class="headerlink" title="数组作为参数"></a>数组作为参数</h3><p>C++ 传数组给一个函数，数组类型自动转换为指针类型，因而传的实际是地址。<br>函数声明可以用一下三种方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">void func(int *params)&#123;&#125;</span><br><span class="line">void func(int params[10])&#123;&#125;</span><br><span class="line">void func(int params[])&#123;&#125;</span><br></pre></td></tr></table></figure></p><h3 id="从函数返回数组"><a href="#从函数返回数组" class="headerlink" title="从函数返回数组"></a>从函数返回数组</h3><p>C++ 不允许返回一个完整的数组作为函数的参数。但是，可以通过指定不带索引的数组名来返回一个指向数组的指针。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int * function()&#123;&#125; //表示会返回一个int数组</span><br></pre></td></tr></table></figure></p><p>需要注意的是：<strong>C++不支持在返回函数内的局部变量地址，除非定义这个局部变量为static</strong></p><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><p>C++提供两种类型的字符串表示形式：</p><ol><li>C风格字符串</li><li>C++的string类类型<h3 id="C风格字符串"><a href="#C风格字符串" class="headerlink" title="C风格字符串"></a>C风格字符串</h3>字符串实际上是使用 null 字符 ‘\0’ 终止的一维字符数组。因此，一个以 null 结尾的字符串，包含了组成字符串的字符。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">char abc[6] = &#123;&apos;H&apos;, &apos;e&apos;, &apos;l&apos;, &apos;l&apos;, &apos;o&apos;, &apos;\0&apos;&#125;;</span><br><span class="line">char abc[] = &quot;Hello&quot;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h4><p>C++中提供处理以null结尾的字符串函数，使用时需要: ‘#include <cstring>‘</cstring></p><ol><li>strcpy(s1, s2); 复制字符串s2到字符串s1</li><li>strcat(s1, s2); 连接字符串s2到s1的尾部</li><li>strlen(s1); 返回s1的长度</li><li>strcmp(s1, s2); 如果 s1 和 s2 是相同的，则返回 0；如果 s1<s2 则返回值小于="" 0；如果="" s1="">s2 则返回值大于 0。</s2></li><li>strchr(s1, ch); 返回一个指针，指向s1中字符ch第一次出现的位置</li><li>strstr(s1, s2); 返回一个指针，指向字符串 s1 中字符串 s2 的第一次出现的位置。<h3 id="C-的String类"><a href="#C-的String类" class="headerlink" title="C++的String类"></a>C++的String类</h3>C++字符串类不仅支持以上所有函数，同时还有更多其他功能。使用时需要：’#include <string>‘。这时可以创建字符串类。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;string&gt;</span><br><span class="line">string str1 = &quot;hello&quot;;</span><br><span class="line">string str2 = &quot;world&quot;;</span><br><span class="line">string str3;</span><br><span class="line"></span><br><span class="line">str3 = str1 + str2;</span><br><span class="line">int len  = str3.size();</span><br></pre></td></tr></table></figure></string></li></ol><h3 id="原生字符串"><a href="#原生字符串" class="headerlink" title="原生字符串"></a>原生字符串</h3><p>在C++11中引入原生字符串机制，即对字符串内部的文本不进行转义，达到所见即所得的效果。使用方法是在字符串前面加R。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">string s = R&quot;(\zeng\\mei)&quot;;</span><br></pre></td></tr></table></figure></p><h3 id="中文字符"><a href="#中文字符" class="headerlink" title="中文字符"></a>中文字符</h3><p>用wchar_t表示中文字符，它占用2个字节的内存空间。因为我们的代码中有扩展的中文字符，所以要求我们的代码文件使用UTF-8的编码格式进行保存。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wchar_t cChs = L&apos;哈&apos;;</span><br><span class="line">wcout.imbue(locale(&quot;chs&quot;));</span><br><span class="line">wcout&lt;&lt;cChs&lt;&lt;endl;</span><br></pre></td></tr></table></figure></p><p>字符常量或字符串常量前面的L前缀，表示对这里的字符或字符串采用wide-character字符集（宽字符集，通常是 UNICODE字符集）对其进行编码。而如果不使用L前缀，则表示使用multibyte-character字符集（多字节字符集）对其进行编码。所 以，如果我们要用某个字符或字符串常量对一个宽字符（wchar_t）或宽字符串（wstring）变量进行初始化或赋值时，我们应该在这个字符或字符串 常量前加上L前缀。反之，如果是对char和string类型的变量进行初始化或赋值时，则不需要添加L前缀。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++复习笔记:变量和数据类型</title>
      <link href="/2018/01/12/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B01%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2018/01/12/Programming%20Language/CPP/C++%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B01%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>C++的复习笔记，在n年使用Java而没碰过C++的情况下，复习一下早年的C++知识。<br><a id="more"></a></p><h2 id="C-简介"><a href="#C-简介" class="headerlink" title="C++ 简介"></a>C++ 简介</h2><h3 id="标准库"><a href="#标准库" class="headerlink" title="标准库"></a>标准库</h3><p>标准的C++由三个部分组成：</p><ol><li>核心语言，提供了所有构件块，包括变量、数据类型和常量，等等。</li><li>C++ 标准库，提供了大量的函数，用于操作文件、字符串等。</li><li>标准模板库（STL），提供了大量的方法，用于操作数据结构等。</li></ol><h2 id="C-变量和数据类型"><a href="#C-变量和数据类型" class="headerlink" title="C++ 变量和数据类型"></a>C++ 变量和数据类型</h2><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><h4 id="基本类型的大小"><a href="#基本类型的大小" class="headerlink" title="基本类型的大小"></a>基本类型的大小</h4><ul><li>char类型：一字节</li><li>int类型：四字节</li><li>long int类型：八字节</li><li>float类型：四字节</li><li>double类型：八字节<h4 id="枚举类型"><a href="#枚举类型" class="headerlink" title="枚举类型"></a>枚举类型</h4>使用关键字enum创建枚举类型：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">enum color&#123;</span><br><span class="line">    red,</span><br><span class="line">    green=5,</span><br><span class="line">    blue</span><br><span class="line">&#125; obj1, obj2, obj3; // 这三个变量的取值只可能是enum中声明的几个</span><br><span class="line">obj1 = red; // == 0</span><br><span class="line">obj2 = green; // == 5</span><br><span class="line">obj3 = blue; // == 6</span><br><span class="line">enum color obj4 = red;</span><br></pre></td></tr></table></figure></li></ul><p>枚举常量代表该枚举类型的变量可能取的值，编译系统为每个枚举常量指定一个整数值，缺省状态下，这个整数就是所列举元素的序号，序号从0开始。 可以在定义枚举类型时为部分或全部枚举常量指定整数值，在指定值之前的枚举常量仍按缺省方式取值，而指定值之后的枚举常量按依次加1的原则取值。</p><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><h4 id="变量声明定义初始化"><a href="#变量声明定义初始化" class="headerlink" title="变量声明定义初始化"></a>变量声明定义初始化</h4><p>使用extern关键字可以在任何地方声明一个变量。<strong>该变量定义在其它模块中，这里仅仅是声明，如同函数声明一样</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 变量声明</span><br><span class="line">extern int a, b;</span><br><span class="line">extern float f;</span><br><span class="line"></span><br><span class="line">int main ()&#123;</span><br><span class="line">  // 变量定义</span><br><span class="line">  int a, b;</span><br><span class="line">  float f;</span><br><span class="line"></span><br><span class="line">  // 实际初始化</span><br><span class="line">  a = 10;</span><br><span class="line">  b = 20;</span><br><span class="line"></span><br><span class="line">  f = 70.0/3.0;</span><br><span class="line"></span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当局部变量被定义时，系统不会对其初始化，您必须自行对其初始化。定义全局变量时，系统会自动初始化.</p><h4 id="变量修饰符"><a href="#变量修饰符" class="headerlink" title="变量修饰符"></a>变量修饰符</h4><p>C++允许对char，int，double进行修饰：</p><ul><li>signed 有符号</li><li>unsigned 无符号</li><li>long 长整数、双精度</li><li>short 短整数<br>C++允许对变量进行修饰：</li><li>const 常量</li><li>volatile 声明程序直接从内存读取变量，而不是寄存器</li></ul><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>两种方法定义常量：</p><ol><li>#define 预处理器</li><li>const 关键字<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#define LENGHT 10</span><br><span class="line">const int WIDTH = 5;</span><br></pre></td></tr></table></figure></li></ol><h2 id="类型别名"><a href="#类型别名" class="headerlink" title="类型别名"></a>类型别名</h2><p>类型别名是给已经存在的类型起一个别名，这样可以让复杂的类型名字变得简单明了。使用typedef或者using定义类型别名：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">typedef double wages; // wages是double的同义词</span><br><span class="line">using SI = SaleItem; // SI是SaleItem的同义词</span><br><span class="line"></span><br><span class="line">wages hourly;</span><br><span class="line">SI item;</span><br></pre></td></tr></table></figure></p><p>指针，常量和类型别名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">typedef char* pstring; //pstring是char*的别名</span><br><span class="line">pstring cstr = 0; //cstr是指向char的指针</span><br></pre></td></tr></table></figure></p><h2 id="auto类型说明符"><a href="#auto类型说明符" class="headerlink" title="auto类型说明符"></a>auto类型说明符</h2><p>编程时经常需要用表达式的结果给变量进行赋值，而往往在声明变量时不知道表达式结果的类型，C++11引入auto类型说明符，可以让编译器自动分析表达式所属的类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auto item = val1 + val2; //由val1和val2相加的结果推断出item的类型。</span><br></pre></td></tr></table></figure></p><h2 id="decltype类型说明符"><a href="#decltype类型说明符" class="headerlink" title="decltype类型说明符"></a>decltype类型说明符</h2><p>有时候我们需要用表达式结果的类型推断出定义的类型，但不想用该表达式的值初始化变量，decltype说明符可以解决这个问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decltype(f()) sum = x; //sum的类型是函数f的返回类型</span><br></pre></td></tr></table></figure></p><p>编译器并不实际调用函数f，而是使用f的返回值类型作为sum的类型。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kaggle解题步骤</title>
      <link href="/2017/12/04/Project/Kaggle%E8%A7%A3%E9%A2%98%E6%AD%A5%E9%AA%A4/"/>
      <url>/2017/12/04/Project/Kaggle%E8%A7%A3%E9%A2%98%E6%AD%A5%E9%AA%A4/</url>
      <content type="html"><![CDATA[<p>最近为了提高ML项目能力，准备刷一下Kaggle的竞赛题，而面对纷繁复杂的题目，最开始的我总是不知道如何下手。在阅读了别人的kernel并完成了几个基础tutorial后，大概对整个流程有了浅显的理解，特此总结了整个Kaggle竞赛的解题步骤。</p><a id="more"></a><h1 id="解题流程"><a href="#解题流程" class="headerlink" title="解题流程"></a>解题流程</h1><h2 id="竞赛之前"><a href="#竞赛之前" class="headerlink" title="竞赛之前"></a>竞赛之前</h2><h4 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h4><p>在选择参加的赛题时候，需要先对赛题进行简单的分析，将其转化为机器学习问题，Kaggle中常见问题类型有：</p><ol><li>回归问题</li><li>分类问题（多分类，多标签）</li></ol><h4 id="数据理解"><a href="#数据理解" class="headerlink" title="数据理解"></a>数据理解</h4><p>同时也需要大概看一眼数据，确定对于自己来说，该问题是否可解（对于显卡渣来说，图像类竞赛还是早早放弃吧= =），而且有些金融类问题，数据集存在着大量的缺失，预测标签也存在较大偏差值，这类问题需要慎重，可能需要专业的金融知识进行特征工程。</p><h4 id="开始竞赛"><a href="#开始竞赛" class="headerlink" title="开始竞赛"></a>开始竞赛</h4><p>在确定对题目有较大信心，数据集合适后，那就开始Kaggle竞赛吧。</p><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h4 id="数据概览"><a href="#数据概览" class="headerlink" title="数据概览"></a>数据概览</h4><p>好了，现在开心的打开你的jupyter notebook，开始Kaggle之旅吧。在载入数据后，我们先要做的是对数据整体有个大概的认识：训练集和测试集都多大？有多少个属性？每个属性是什么类型的？属性是否有缺失值？数值型属性的分布情况（min，max，mean，meduim，std等统计值）？等等，有了这些信息，会让你在脑海里对数据有个大体的认识，这是几个在这个环节常用的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data.shape</span><br><span class="line">data.head()</span><br><span class="line">data.info()</span><br><span class="line">data.describe()</span><br><span class="line">data.isnull().sum()[data.isnull().sum()&gt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><h4 id="具体分析和可视化"><a href="#具体分析和可视化" class="headerlink" title="具体分析和可视化"></a>具体分析和可视化</h4><p>在我们对数据整体有个大体认识的情况下，针对每个特征，进行进一步的探索分析。在这个过程中，使用常见可视化工具，可以较为直观的展现数据。常见的可视化工具有：matplotlib和seaborn，其中，seaborn是对matplotlib的进一步封装，让工程师可以更加便捷的进行可视化，常见的可视化准则有：</p><ul><li>sns.jointplot(): 绘制属性随标签的变化趋势</li><li>sns.heatmap(data.corr()): 绘制热力图，可以对属性的相关性进行简单探索</li><li>sns.distplot(): 绘制分布图，可以展示连续自变量和标签的关系</li><li>sns.boxplot(): 绘制箱型图，适合用于探索取值较少的属性和标签的关系</li></ul><p>在对各个属性进行细致可视化观察过程中，需要记录下这个属性和标签的相关性，进而分析该属性是否是一个有价值的属性，并<br>在观察数据的过程中，需要思考一下几个问题：</p><ol><li>数据应该怎样清洗和处理才是合理的？离散化？连续化？</li><li>根据数据类型可以挖掘出什么特征？</li><li>数据中的哪些特征对预测有帮助？</li><li>（进阶）是否可以构造出新的特征？</li></ol><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>特征是决定效果最关键的一环，利用人为先验知识，从数据中总结出特征。特征工程是整个项目中最重要的一环之一，好的特征工程决定了预测结果的上限，机器学习模型只是帮助你不断逼近这个上限而已。</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>首先，对于刚拿到的数据，会出现噪声，离群点，缺失等问题，我们需要对数据进行清理和加工，对于不同类型的变量，有不同的处理方法：</p><ol><li>对于数值型变量，需要处理离散群点，缺失值，异常值等情况。</li><li>对于类别型变量，可以转化为one-hot编码。</li><li>文本数据较为复杂，文本中会有垃圾字符，错别字，数学公式，不统一度量衡日期格式等，包括标点符号，分词。对于英文文本可能还需要词性还原。</li></ol><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><ul><li>我们应该尽可能多的抽取特征，只要是认为某个特征对解决问题有帮助，它就可以成为一个特征。</li><li>特征抽取需要不断迭代，最为耗费时间。</li><li>常见特征抽取方法：<ol><li>对于数值型特征，可以通过线性组合，多项式组合来发现新的特征。</li><li>对于文本数据，有一些常规的特征：文本长度，embeddings，TF-IDF，LDA等</li><li>如果对数据有更深刻的理解，可以试着构造magic feature。</li><li>通过错误分析也可以发现新的特征。</li></ol></li></ul><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><ul><li>过多的特征会造成冗余，噪声，过拟合等问题。因此需要特征筛选，可以加快模型训练速度，提升效果。</li><li>特征选择方法多种多样，最简单的时相关度系数，用以衡量两个变量之间的线性关系。</li><li>可以通过分析构建相关系数矩阵。特征和标签之间的相关度可以看做该特征的重要度，特征与特征之间的相关度高，则说明这两个特征存在冗余。</li><li>也可以通过训练模型来筛选特征。</li></ul><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><h4 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h4><p>机器学习模型很多，可以都做尝试，不仅可以测试效果，还可以学习各种模型技巧。常见模型：</p><pre><code>1. KNN2. SVM3. Linear Model4. Extra Tree5. RandomForest6. Gradient Boost Tree7. Neural Network</code></pre><h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><p>在训练模型前，我们已经预设了一些模型参数（比如树的深度）和优化过程（比如学习率）。这种参数被称为超参。</p><p>调参虽然被称为一门玄学，但还是有些章法可寻：</p><pre><code>1. 根据经验，选出对模型效果影响最大的超参2. 按照经验设置超参的搜索空间，比如学习率：[0.0001, 0.1]3. 选择搜索算法4. 验证模型的泛化能力</code></pre><p>同时，也可以用可视化工具将不同参数模型在测试集的效果可视化，进而选择最优的参数。</p><h4 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h4><ul><li>简单分割</li><li>交叉验证<ol><li>将整个训练数据随机分成k份，训练k个moxing，取k-1份train，1份valid。</li><li>也叫k-fold</li><li>k一般选值在3到10之间</li></ol></li></ul><h4 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h4><ul><li>每个模型都会犯一些错误，为了了解模型在犯什么错误，我们可以观察被误判的样本，总结他们的共同特征，就可以再训练一个效果更好的模型。</li><li>错误分析-&gt;发现新特征-&gt;训练新模型-&gt;错误分析。 可以不断迭代出更好的效果。</li></ul><h4 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h4><p>我们可以使用模型集成方法，将多个ML模型集成到一起，最后我们可以综合考虑所有ML模型的预测结果，进而可以给出一个更加准确的预测。</p><ul><li>常见方法有：bagging, boosting, stacking, blending.</li><li>bagging<ol><li>bagging是将多个模型的预测结果简单地加权平均或者投票，bagging的好处在于可以并行的训练基学习器。</li><li>bagging通常没有一个明确的优化目标。</li></ol></li><li>Boosting<ol><li>Boosting思想接近于知错能改，每训练一个基学习器，是为了弥补上一个基学习器所犯的错误。</li><li>著名算法有：AdaBoost，GradientBoost</li></ol></li><li>Stacking<ol><li>是用新的模型（次学习器）去学习怎么组合基学习器。</li><li>Stacking的思想是多个基学习器的加权非线性组合</li></ol></li><li>Blending</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Kaggle项目是一个不断往复的过程，正如吴恩达在《Machine Learning》课上所说，对于机器学习项目，我们首先做出一个非常简单粗暴的模型，这个模型的准确率往往是欠佳的，而我们接下来要做的就是不断优化我们的模型，不断修改特征工程，提炼出更好的模型，更好的特征，进而不断提高预测准确率。</p>]]></content>
      
      <categories>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Project </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow常用函数整理</title>
      <link href="/2017/11/28/Deep%20Learning/TensorFlow/TensorFlow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86/"/>
      <url>/2017/11/28/Deep%20Learning/TensorFlow/TensorFlow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>TensorFlow常用方法整理</p><a id="more"></a><h2 id="变量相关"><a href="#变量相关" class="headerlink" title="变量相关"></a>变量相关</h2><p>涉及到新建变量有两个函数，一个是tf.Variable(),一个是tf.get_variable()， 两个函数的签名如下：</p><ul><li>tf.Variable(initial_value=None, …)</li><li>tf.get_variable(name, shape=None, initializer=None, …)<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3></li></ul><ol><li>使用tf.Variable()时，如果有命名冲突，系统会自动处理，get_variable()则会报错<br>Variable()会自动修改冲突的名称：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(3, name=&apos;w1&apos;)</span><br><span class="line">w2 = tf.Variable(1, name=&apos;w1&apos;)</span><br><span class="line">print(w1.name)</span><br><span class="line">print(w2.name)</span><br><span class="line"></span><br><span class="line">w1:0</span><br><span class="line">w1_1:0</span><br></pre></td></tr></table></figure></li></ol><p>get_variable()报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.get_variable(name=&apos;w1&apos;, initializer=1)</span><br><span class="line">w2 = tf.get_variable(name=&apos;w1&apos;, initializer=2)</span><br><span class="line"></span><br><span class="line">#ValueError: Variable w_1 already exists, disallowed. Did</span><br><span class="line">#you mean to set reuse=True in VarScope?</span><br></pre></td></tr></table></figure></p><p>基于这两个函数的特性，<strong>当我们需要共享变量的时候，需要使用tf.get_variable()</strong>。在其他情况下，这两个的用法是一样的.</p><h3 id="变量共享"><a href="#变量共享" class="headerlink" title="变量共享"></a>变量共享</h3><p>通过设置reuse=True可以实现变量共享：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&apos;scope1&apos;):</span><br><span class="line">    w1 = tf.get_variable(&apos;w1&apos;, shape=[])</span><br><span class="line">    w2 = tf.Variable(0.0, name=&apos;w2&apos;)</span><br><span class="line">with tf.variable_scope(&apos;scope1&apos;, reuse=True):</span><br><span class="line">    w1_p = tf.get_variable(&apos;w1&apos;, shape=[])</span><br><span class="line">    w2_p = tf.Variable(0.0, name=&apos;w2&apos;)</span><br><span class="line"></span><br><span class="line">print(w1 is w1_p)  # True</span><br><span class="line">print(w2 is w2_p)  # False</span><br></pre></td></tr></table></figure></p><h2 id="tensor形状相关操作"><a href="#tensor形状相关操作" class="headerlink" title="tensor形状相关操作"></a>tensor形状相关操作</h2><h3 id="tf-shape-input"><a href="#tf-shape-input" class="headerlink" title="tf.shape(input)"></a>tf.shape(input)</h3><p>返回输入tensor的形状</p><h3 id="tf-size-input"><a href="#tf-size-input" class="headerlink" title="tf.size(input)"></a>tf.size(input)</h3><p>返回输入tensor中所有元素的个数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]</span><br><span class="line">tf.size(t) ==&gt; 12</span><br></pre></td></tr></table></figure></p><h3 id="tf-rank-input"><a href="#tf-rank-input" class="headerlink" title="tf.rank(input)"></a>tf.rank(input)</h3><p>返回输入tensor的维度数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]</span><br><span class="line"># shape of tensor &apos;t&apos; is [2, 2, 3]</span><br><span class="line">tf.rank(t) ==&gt; 3</span><br></pre></td></tr></table></figure></p><h3 id="tf-reshape-input-shape"><a href="#tf-reshape-input-shape" class="headerlink" title="tf.reshape(input, shape)"></a>tf.reshape(input, shape)</h3><p>改变输入tensor的形状为shape，shape是一个list，表示每个维度的大小。可以让某个维度的值为-1，tf会自动计算该维度大小。</p><h3 id="tf-expand-dims-input-axis-None"><a href="#tf-expand-dims-input-axis-None" class="headerlink" title="tf.expand_dims(input, axis=None)"></a>tf.expand_dims(input, axis=None)</h3><p>在第axis位置上增加一个维度。<br>如果要将批量维度添加到单个元素，则此操作非常有用。 例如，如果有一个单一形状的图片：image=[height，width，channels]，可以使用expand_dims(image, 0)使其成为一批图像（目前仅有一个），这将使形状 images=[1，height，width，channels]。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line"># &apos;t&apos; is a tensor of shape [2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 0)) ===&gt; [1,2,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 1)) ===&gt; [2,1,3,5]</span><br><span class="line">tf.shape(expand_dims(t, 2)) ===&gt; [2,3,1,5]</span><br></pre></td></tr></table></figure></p><h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze()"></a>tf.squeeze()</h3><p>从tensor中删除所有大小是1的维度<br>给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸为1的维度，可以通过指定squeeze_dims来删除特定尺寸为1的维度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &apos;t&apos; is a tensor of shape [1, 3, 1, 4]</span><br><span class="line">tf.shape(tf.squeeze(t)) ===&gt; [3, 4]</span><br></pre></td></tr></table></figure></p><h3 id="tf-split-value-name-or-size-split-axis-0"><a href="#tf-split-value-name-or-size-split-axis-0" class="headerlink" title="tf.split(value, name_or_size_split, axis=0)"></a>tf.split(value, name_or_size_split, axis=0)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度不变</p><ul><li>value：输入的tensor</li><li>num_or_size_split：如果是整数你， 就将tensor分割成n个子tensor。如果是个tensor T，就将输入的tensor分割为len(T)个子tensor</li><li>axis：按照指定维度分割，指向的维度的大小一定能被num_or_size_split整除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.split(A, num_or_size_split=3, axis=1)</span><br><span class="line">a1 = [ [[1],[4]],   [[2],[5]],   [[3],[6]] ]</span><br><span class="line">a2 = tf.split(A, num_or_size_split=2, axis=0)</span><br><span class="line">a2 = [ [[1,2,3]],  [[4,5,6]] ]</span><br></pre></td></tr></table></figure></li></ul><h3 id="tf-unstack-value-num-axis"><a href="#tf-unstack-value-num-axis" class="headerlink" title="tf.unstack(value, num, axis)"></a>tf.unstack(value, num, axis)</h3><p>将输入的tensor按照axis分割成若干个小的tensor，切割后的子tensor维度-1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A=[[1,2,3],</span><br><span class="line">   [4,5,6]]</span><br><span class="line">a1 = tf.unstack(A, num=3, axis=1)</span><br><span class="line">a1 = [ [1, 4],   [2, 5],   [3, 6] ]</span><br><span class="line">a2 = tf.unstack(A, num=2, axis=0)</span><br><span class="line">a2 = [ [1,2,3],  [4,5,6] ]</span><br></pre></td></tr></table></figure></p><h2 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h2><p>TensorFlow提供三种卷积操作，分别对应着1维，2维，3维数据。</p><h3 id="tf-nn-conv1d"><a href="#tf-nn-conv1d" class="headerlink" title="tf.nn.conv1d()"></a>tf.nn.conv1d()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv1d(</span><br><span class="line">    value,</span><br><span class="line">    filters,</span><br><span class="line">    stride,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=None,</span><br><span class="line">    data_format=None,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>常见的卷积输入都为2维，但特殊情况下也会输入1维tensor。和2维处理原理相同，只是形状需要改变。<br>其中：</p><ol><li>input为输入tensor，形状为[batch, in_width, in_channels]</li><li>filter为卷积核，形状为[filter_width, in_channels, output_channels]</li><li>strides为步长，形状为一个整数</li><li>padding为对input的padding方法， 有两个取值：’VALID’和’SAME’，前者表示不做0填充，后者表示对input进行0填充使得卷积的输出大小和输入相同。</li></ol><p>实际上，一维卷积方法在运行时，会把数据增加一维，然后使用conv2d计算。</p><h3 id="tf-nn-conv2d"><a href="#tf-nn-conv2d" class="headerlink" title="tf.nn.conv2d()"></a>tf.nn.conv2d()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=True,</span><br><span class="line">    data_format=&apos;NHWC&apos;,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>其中：</p><ol><li>input为输入tensor，形状为[batch, n_height, in_width, in_channels]</li><li>filter为卷积核，形状为[filter_height, filter_width, in_channels, output_channels]</li><li>strides为步长，形状为[1, step_dim1, step_dim2, 1]</li><li>padding为对input的padding方法， 有两个取值：’VALID’和’SAME’，前者表示不做0填充，后者表示对input进行0填充使得卷积的输出大小和输入相同。<h3 id="tf-nn-conv3d"><a href="#tf-nn-conv3d" class="headerlink" title="tf.nn.conv3d()"></a>tf.nn.conv3d()</h3>和商量个相同，只是把维度提高到了三维</li><li>input形状为：[batch, in_depth, in_height, in_width, in_channels]</li><li>filter形状为：[filter_depth, filter_height, filter_width, in_channels, output_channels]<h3 id="tf-pad"><a href="#tf-pad" class="headerlink" title="tf.pad()"></a>tf.pad()</h3>tf.pad(tensor, paddings, mode)<br>对一个tensor根据指定的padding进行填充，padding是一个shape=[n,2]的int型张量。n表示的是输入tensor的rank。For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension.<br>model有三个模式：</li><li>‘CONSTANT’ 表示用指定值填充，通常为0</li><li>‘REFLECT’ 表示映射填充。</li><li>‘SYMMETRIC’ 表示是对称填充<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tf = tf.constant([[1,2,3],[4,5,6]])</span><br><span class="line">paddings = tf.constant([[1,1],[2,2]])</span><br><span class="line"># &apos;constant_values&apos; is 0.</span><br><span class="line"># rank of &apos;t&apos; is 2.</span><br><span class="line">tf.pad(t, paddings, &quot;CONSTANT&quot;)  # [[0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">                                 #  [0, 0, 1, 2, 3, 0, 0],</span><br><span class="line">                                 #  [0, 0, 4, 5, 6, 0, 0],</span><br><span class="line">                                 #  [0, 0, 0, 0, 0, 0, 0]]</span><br><span class="line"></span><br><span class="line">tf.pad(t, paddings, &quot;REFLECT&quot;)  # [[6, 5, 4, 5, 6, 5, 4],</span><br><span class="line">                                #  [3, 2, 1, 2, 3, 2, 1],</span><br><span class="line">                                #  [6, 5, 4, 5, 6, 5, 4],</span><br><span class="line">                                #  [3, 2, 1, 2, 3, 2, 1]]</span><br><span class="line"></span><br><span class="line">tf.pad(t, paddings, &quot;SYMMETRIC&quot;)  # [[2, 1, 1, 2, 3, 3, 2],</span><br><span class="line">                                  #  [2, 1, 1, 2, 3, 3, 2],</span><br><span class="line">                                  #  [5, 4, 4, 5, 6, 6, 5],</span><br><span class="line">                                  #  [5, 4, 4, 5, 6, 6, 5]]</span><br></pre></td></tr></table></figure></li></ol><p>从第一个例子可以看出，在维度D=1，上下两端都padding了一个行，正好等于paddings[0,0]和paddings[0,1]。同理，在维度D=2，左右两端padding大小等于paddings[1,0]和paddings[1,1]</p><h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><h3 id="tf-control-dependencies-和tf-identity"><a href="#tf-control-dependencies-和tf-identity" class="headerlink" title="tf.control_dependencies()和tf.identity()"></a>tf.control_dependencies()和tf.identity()</h3><p>该函数是用来设计控制计算流图的，给图中的某些计算指定顺序，总结起来就是：在执行某些op，tensor之前，某些op，tensor需要首先被执行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([a, b, c]):</span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br><span class="line">    # d and e will only run after a, b, c have executed.</span><br></pre></td></tr></table></figure></p><p>对于tf.identity()的功能，首先看如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(1.0)</span><br><span class="line">y = tf.Variable(0.0)</span><br><span class="line"></span><br><span class="line">#返回一个op，表示给变量x加1的操作</span><br><span class="line">x_plus_1 = tf.assign_add(x, 1)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i in xrange(5):</span><br><span class="line">        print(y.eval())#按照我们的预期，由于control_dependencies的作用，所以应该执行print前都会先执行x_plus_1，但是这种情况会出问题</span><br></pre></td></tr></table></figure></p><p>这个打印的是1，1，1，1，1 。可以看到，没有达到我们预期的效果，y只被赋值了一次。<br>而如果想完成这个目的，需要将代码做如下改动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(1.0)</span><br><span class="line">y = tf.Variable(0.0)</span><br><span class="line">x_plus_1 = tf.assign_add(x, 1)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)#修改部分</span><br><span class="line"></span><br><span class="line">with tf.Session() as session:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i in xrange(5):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure></p><p>这时候打印的是2，3，4，5，6</p><p>解释：对于control_dependencies这个管理器，只有当里面的操作是一个op时，才会生效，也就是先执行传入的参数op，再执行里面的op。而y=x仅仅是tensor的一个简单赋值，不是定义的op，所以在图中不会形成一个节点，这样该管理器就失效了。tf.identity是返回一个一模一样新的tensor的op，这会增加一个新节点到gragh中，这时control_dependencies就会生效，所以第二种情况的输出符合预期。</p><h3 id="tf-assign-x-new-number"><a href="#tf-assign-x-new-number" class="headerlink" title="tf.assign(x, new_number)"></a>tf.assign(x, new_number)</h3><p>这个函数的功能主要是把A的值变为new_number</p><h2 id="TensorFlow优化器方法和梯度裁剪"><a href="#TensorFlow优化器方法和梯度裁剪" class="headerlink" title="TensorFlow优化器方法和梯度裁剪"></a>TensorFlow优化器方法和梯度裁剪</h2><h3 id="tf-train-AdamOptimizer"><a href="#tf-train-AdamOptimizer" class="headerlink" title="tf.train.AdamOptimizer()"></a>tf.train.AdamOptimizer()</h3><h4 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h4><ul><li>compute_gradients(loss)<br>  根据传入的loss计算梯度，返回值是一个list，list中的元素是(gradient, variable)对，表示对每个变量的更新梯度</li><li>apply_gradients(grads_and_vars,global_step=None)<br>  参数grads_and_vars是一个(gradient,variable)的list。（compute_gradients）的返回值<br>  对每个variable根据其gradient进行更新。<br>  返回一个operation，表示运行该apply_gradients</li><li>minimize(loss) 同时运行compute_gradients() 和 apply_gradients()</li></ul><p>通常情况下，直接使用minimize()函数即可，但当需要对梯度进行操作时（梯度裁剪），就需要分别运行两个步骤。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">gradients = optimizer.compute_gradients(loss)</span><br><span class="line">clipped_gradients = []</span><br><span class="line">for grad, var in gradients:</span><br><span class="line">    if grad is not None:</span><br><span class="line">        grad = tf.clip_by_value(grad, -1., 1.)</span><br><span class="line">        clipped_gradients.append((grad, var))</span><br><span class="line">train_op = optimizer.apply_gradients(clipped_gradients)</span><br></pre></td></tr></table></figure></p><h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><p>在TensorFlow中，提供如下几种梯度裁剪方法：tf.clip_by_norm, tf.clip_by_global_norm, tf.clip_by_average_norm, tf.clip_by_value.</p><ul><li>tf.clip_by_norm(tensor, clip_norm)<br>如果输入tensor的l2范式值大于clip_norm，则tensor = tensor * clip_norm / l2(tensor)</li><li>tf.clip_by_global_norm(t_list, clip_norm)<br>对t_list中的各个梯度tensor进行裁剪，t_list[i] <em> clip_norm / max(global_norm, clip_norm)<br>并且：global_norm = sqrt(sum([l2norm(t)\</em>*2 for t in t_list]))</li><li>tf.clip_by_value(t, clip_value_min, clip_value_max)<br>最直接的裁剪方法，直接裁剪输入tensor的最大值和最小值。</li></ul><h2 id="tf中的Batch-Normalization"><a href="#tf中的Batch-Normalization" class="headerlink" title="tf中的Batch Normalization"></a>tf中的Batch Normalization</h2><ul><li><p>tf.nn.moments(tensor, axis) 返回两个参数，分别是均值和方差。<br>输入tensor的shape为[batch_size, height, width, kernels]，第二个参数axis是一个int数组，表示要进行计算的维度。</p></li><li><p>tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def batch_norm(tensor):</span><br><span class="line">    epsilon = 1e-3</span><br><span class="line">    axis = list(range(len(tensor.get_shape()) - 1))</span><br><span class="line">    batch_mean, batch_var = tf.nn.moments(tensor, axis)</span><br><span class="line">    BN_tensor = tf.nn.batch_normalization(tensor, batch_mean, batch_var, offset=None, scale=None, variance_epsilon=epsilon)</span><br><span class="line">    return BN_tensor</span><br></pre></td></tr></table></figure></li></ul><h3 id="TensorFlow加载部分模型"><a href="#TensorFlow加载部分模型" class="headerlink" title="TensorFlow加载部分模型"></a>TensorFlow加载部分模型</h3><p>常规保存和加载模型的方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, &apos;model.ckpt&apos;)  # 保存</span><br><span class="line">saver.restore(sess, &apos;model.ckpt&apos;)  # 加载</span><br></pre></td></tr></table></figure></p><p>前面的描述相当于是保存了所有的参数，然后加载所有的参数。但是目前的情况有所变化了，不能加载所有的参数，最后一层的参数不一样了，需要随机初始化。<br>首先对每一层添加name scope，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">with name_scope(&apos;conv1&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;conv2&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;fc1&apos;):</span><br><span class="line">    xxx</span><br><span class="line">with name_scope(&apos;output&apos;):</span><br><span class="line">    xxx</span><br></pre></td></tr></table></figure></p><p>然后根据变量的名字，选择加载哪些变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#得到该网络中，所有可以加载的参数</span><br><span class="line">variables = tf.contrib.framework.get_variables_to_restore()</span><br><span class="line">#删除output层中的参数</span><br><span class="line">variables_to_resotre = [v for v in varialbes if v.name.split(&apos;/&apos;)[0]!=&apos;output&apos;]</span><br><span class="line">#构建这部分参数的saver</span><br><span class="line">saver = tf.train.Saver(variables_to_restore)</span><br><span class="line">saver.restore(sess,&apos;model.ckpt&apos;)</span><br></pre></td></tr></table></figure></p><h3 id="TensorFlow梯度更新"><a href="#TensorFlow梯度更新" class="headerlink" title="TensorFlow梯度更新"></a>TensorFlow梯度更新</h3><ol><li><p>根据name_scope选择部分参数进行梯度更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&apos;to_train&apos;):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">with tf.name_scope(&apos;not_train&apos;):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">def optimizer():</span><br><span class="line">    train_vars = tf.trainable_variables()</span><br><span class="line">    trainable_vars = [var for var in train_vars if var.name.startswith(&apos;to_train&apos;)]</span><br><span class="line">    untrainable_vars = [var for var in train_vars if var.name.startswith(&apos;not_train&apos;)]</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></table></figure></li><li><p>使用tf.gradients(ys, xs)计算梯度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.get_variable(&apos;w1&apos;, shape=[3])</span><br><span class="line">w2 = tf.get_variable(&apos;w2&apos;, shape=[3])</span><br><span class="line"></span><br><span class="line">w3 = tf.get_variable(&apos;w3&apos;, shape=[3])</span><br><span class="line">w4 = tf.get_variable(&apos;w4&apos;, shape=[3])</span><br><span class="line"></span><br><span class="line">z1 = 2*w1 + 3*w2+ 4*w3</span><br><span class="line">z2 = 6*w3 + 7*w4</span><br><span class="line"></span><br><span class="line">grads1 = tf.gradients([z1], [w1, w2, w3])</span><br><span class="line">grads2 = tf.gradients([z1, z2], [w1, w2, w3, w4])</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    print(sess.run(grads1))</span><br><span class="line">    print(sess.run(grads2))</span><br></pre></td></tr></table></figure></li><li><p>使用tf.stop_gradient()阻挡节点的BP梯度更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(2.0)</span><br><span class="line">w2 = tf.Variable(2.0)</span><br><span class="line"></span><br><span class="line">a = tf.multiply(w1, 3.0)</span><br><span class="line">a_stoped = tf.stop_gradient(a)</span><br><span class="line"></span><br><span class="line"># b=w1*3.0*w2</span><br><span class="line">b = tf.multiply(a_stoped, w2)</span><br><span class="line">gradients = tf.gradients(b, xs=[w1, w2])</span><br><span class="line">print(gradients)</span><br><span class="line">#输出</span><br><span class="line">#[None, &lt;tf.Tensor &apos;gradients/Mul_1_grad/Reshape_1:0&apos; shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure></li></ol><p>可见，一个节点被 stop之后，这个节点上的梯度，就无法再向前BP了。由于w1变量的梯度只能来自a节点，所以，计算梯度返回的是None。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(1.0)</span><br><span class="line">b = tf.Variable(1.0)</span><br><span class="line"></span><br><span class="line">c = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">c_stoped = tf.stop_gradient(c)</span><br><span class="line"></span><br><span class="line">d = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">e = tf.add(c_stoped, d)</span><br><span class="line"></span><br><span class="line">gradients = tf.gradients(e, xs=[a, b])</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    print(sess.run(gradients))</span><br><span class="line">#输出 [1.0, 1.0]</span><br></pre></td></tr></table></figure></p><p>虽然 c节点被stop了，但是a，b还有从d传回的梯度，所以还是可以输出梯度值的。</p><h3 id="TensorFlow-top-k相关函数用法"><a href="#TensorFlow-top-k相关函数用法" class="headerlink" title="TensorFlow top_k相关函数用法"></a>TensorFlow top_k相关函数用法</h3><h4 id="tf-nn-in-top-k"><a href="#tf-nn-in-top-k" class="headerlink" title="tf.nn.in_top_k()"></a>tf.nn.in_top_k()</h4><p>在训练过程中输出top k的预测准确率，返回的是布尔值，即label是否在预测的top k中存在。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(16).reshape((4,4)).astype(np.float32)</span><br><span class="line">a = tf.convert_to_tensor(a)</span><br><span class="line">label = [0,1,2,3]</span><br><span class="line">pre = tf.nn.in_top_k(a, label, k=2)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(pre))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[False False  True  True]</span><br></pre></td></tr></table></figure></p><h4 id="tf-nn-top-k"><a href="#tf-nn-top-k" class="headerlink" title="tf.nn.top_k()"></a>tf.nn.top_k()</h4><p>根据输入的tensor返回top k个概率最大的预测对应的index和possibility。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([0.1, 0.2, 0.3, 0.4])</span><br><span class="line">a = tf.convert_to_tensor(a)</span><br><span class="line">b = tf.nn.top_k(a, k=3)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">poss, index = sess.run(b)</span><br><span class="line">print(poss, index)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; [0.4 0.3 0.2] [3 2 1]</span><br></pre></td></tr></table></figure></p><h4 id="tensorflow-collection"><a href="#tensorflow-collection" class="headerlink" title="tensorflow collection"></a>tensorflow collection</h4><p>tensorflow的collection提供一个全局的存储机制，不会受到变量名生存空间的影响。一处保存，到处可取。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant(1.0)</span><br><span class="line">l1 = tf.nn.l2_loss(x)</span><br><span class="line">y = tf.constant([1.0, 2.0])</span><br><span class="line">l2 = tf.nn.l2_loss(y)</span><br><span class="line"></span><br><span class="line"># 手动指定一个collection并将loss添加进去</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, l1)</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, l2)</span><br><span class="line"></span><br><span class="line"># 可以将同一个变量放到不同的collection中</span><br><span class="line">tf.add_to_collection(&apos;other&apos;, l1)</span><br><span class="line"></span><br><span class="line"># 获取指定的collection中的元素，是一个tensor类型的list</span><br><span class="line">losses = tf.get_collection(&apos;losses&apos;)</span><br><span class="line">final_loss = tf.add_n(losses)</span><br><span class="line">other = tf.get_collection(&apos;other&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(final_loss))</span><br><span class="line">print(sess.run(other))</span><br></pre></td></tr></table></figure></p><h4 id="TensorFlow矩阵拼接和分解"><a href="#TensorFlow矩阵拼接和分解" class="headerlink" title="TensorFlow矩阵拼接和分解"></a>TensorFlow矩阵拼接和分解</h4><p>tf.stack() 用于矩阵拼接，tf.unstack()用于矩阵分解<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([1, 2, 3])</span><br><span class="line">b = tf.constant([4 ,5, 6])</span><br><span class="line">c = tf.stack([a ,b], axis=0)</span><br><span class="line">d = tf.unstack(c, axis=0)</span><br><span class="line">e = tf.unstack(c, axis=1)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">    print(sess.run(d))</span><br><span class="line">    print(sess.run(e))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[[1 2 3]</span><br><span class="line">     [4 5 6]]</span><br><span class="line">&gt;&gt;&gt;[array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]</span><br><span class="line">&gt;&gt;&gt;[array([1, 4], dtype=int32), array([2, 5], dtype=int32), array([3, 6], dtype=int32)]</span><br></pre></td></tr></table></figure></p><h3 id="tensorflow-布尔掩码"><a href="#tensorflow-布尔掩码" class="headerlink" title="tensorflow 布尔掩码"></a>tensorflow 布尔掩码</h3><h4 id="tf-boolean-mask-tensor-mark"><a href="#tf-boolean-mask-tensor-mark" class="headerlink" title="tf.boolean_mask(tensor, mark)"></a>tf.boolean_mask(tensor, mark)</h4><p>根据mark的真值对tensor进行掩码操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">＃1-D 示例</span><br><span class="line">tensor =  [ 0 ， 1 ， 2 ， 3 ]</span><br><span class="line">mask = np.array（[True，False，True，False] ）</span><br><span class="line">boolean_mask （tensor，mask） == &gt;  [ 0 ， 2 ]</span><br><span class="line"></span><br><span class="line">＃2-D示例</span><br><span class="line">tensor =  [ [ 1 ， 2 ] ， [ 3 ， 4 ] ， [ 5 ， 6 ] ]</span><br><span class="line">mask = np.array（[True，False，True] ）</span><br><span class="line">boolean_mask （tensor，mask） == &gt;  [ [ 1 ， 2 ] ， [ 5 ， 6 ] ]</span><br></pre></td></tr></table></figure></p><h4 id="tf-sequence-mask-lengths-maxlen"><a href="#tf-sequence-mask-lengths-maxlen" class="headerlink" title="tf.sequence_mask(lengths, maxlen)"></a>tf.sequence_mask(lengths, maxlen)</h4><p>返回一个表示每个单元的前N个位置的mask张量。</p><p>如果lengths的形状为[d_1, d_2, …, d_n]，由此产生的张量mask有dtype类型和形状[d_1, d_2, …, d_n, maxlen]</p><p>第二个参数maxlen可以为None，这时将自动用length list中最大长度作为maxlen<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],</span><br><span class="line">                                #  [True, True, True, False, False],</span><br><span class="line">                                #  [True, True, False, False, False]]</span><br><span class="line"></span><br><span class="line">tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],</span><br><span class="line">                                  #   [True, True, True]],</span><br><span class="line">                                  #  [[True, True, False],</span><br><span class="line">                                  #   [False, False, False]]]</span><br></pre></td></tr></table></figure></p><h3 id="TensorFlow-graph"><a href="#TensorFlow-graph" class="headerlink" title="TensorFlow graph"></a>TensorFlow graph</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">class Graph():</span><br><span class="line">    def __init__(self, x, y):</span><br><span class="line">        # tf.reset_default_graph()</span><br><span class="line">        self.a = tf.constant(x)</span><br><span class="line">        self.b = tf.constant(y)</span><br><span class="line">        self.c = tf.multiply(self.a, self.b)</span><br><span class="line"></span><br><span class="line">class DoubleGraph():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        gg = tf.Graph()</span><br><span class="line">        with gg.as_default():</span><br><span class="line">            g1 = Graph(1, 2)</span><br><span class="line">        ggg = tf.Graph()</span><br><span class="line">        with ggg.as_default():</span><br><span class="line">            g2 = Graph(3, 4)</span><br><span class="line">        print(g1.a.graph)</span><br><span class="line">        print(g2.a.graph)</span><br><span class="line">        session1 = tf.Session(graph=gg)</span><br><span class="line">        r = session1.run(g1.c)</span><br><span class="line">        print(r)</span><br><span class="line">        session2 = tf.Session(graph=ggg)</span><br><span class="line">        r = session2.run(g2.c)</span><br><span class="line">        print(r)</span><br><span class="line"></span><br><span class="line">DoubleGraph()</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（四）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍CUDA流。<br><a id="more"></a></p><h1 id="流操作"><a href="#流操作" class="headerlink" title="流操作"></a>流操作</h1><h2 id="页锁定主机内存"><a href="#页锁定主机内存" class="headerlink" title="页锁定主机内存"></a>页锁定主机内存</h2><p>CUDA提供自己独特的方式来分配主机内存’cudaHostAlloc()’。和标准的c内存分配’malloc()’不同的是，后者将分配标准的，可分页的主机内存，而前者将分配页锁定的主机内存。页锁定内存也称为固定内存或者不可分页内存。它有一个重要属性：操作系统将不会对这块内存分页并交换到磁盘上，从而确保该内存始终驻留在物理内存中。由于GPU知道内存的物理地址，因此可以通过“直接内存访问DMA”技术来在GPU和主机之间复制数据，而使用锁页内存时，DMA不需要重新查找内存地址，所以速度会比正常的可分页内存快2倍左右。</p><p>当然使用页锁定内存的缺点也是显而易见的：主机的虚拟内存机制完全作废，每个页锁定内存都需要分配物理内存，这意味着系统将更快的耗尽内存。因此应用程序在物理内存较小的机器上会运行失败，也意味着应用程序将影响系统上其他运行的应用程序性能。</p><h2 id="CUDA流"><a href="#CUDA流" class="headerlink" title="CUDA流"></a>CUDA流</h2><p>CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。</p><p>一个简单的cuda流使用代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#define N (1024*1024)</span><br><span class="line">#define FULL_DATA_SIZE (N*20)</span><br><span class="line"></span><br><span class="line">__global__ void kernel(int *a, int *b, int *c)&#123;</span><br><span class="line">    int tid = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    if (tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    cudaStreamCreate(&amp;stream); //定义并初始化CUDA流</span><br><span class="line"></span><br><span class="line">    int *host_a, *host_b, *host_c;</span><br><span class="line">    int *dev_a, *dev_b, *dev_c;</span><br><span class="line">    //分配设备内存</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line">    //分配由流使用的页锁定内存</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_a, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_b, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line">    cudaHostAlloc((void**)&amp;host_c, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">    for(int i = 0; i&lt;FULL_DATA_SIZE; i++)&#123;</span><br><span class="line">        host_a[i] = i;</span><br><span class="line">        host_b[i] = 2 * i + 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //在整个数据上循环，每个数据块的大小为N</span><br><span class="line">    for(int i = 0;i&lt;FULL_DATA_SIZE; i+=N)&#123;</span><br><span class="line">        //将锁定内存以异步方式复制到设备上</span><br><span class="line">        cudaMemcpyAsync(dev_a, host_a+i, N*sizeof(int), cudaMemcpyHostToDevice, stream);</span><br><span class="line">        cudaMemcpyAsync(dev_b, host_b+i, N*sizeof(int), cudaMemcpyHostToDevice, stream);</span><br><span class="line">        kernel&lt;&lt;&lt;N/256, 256, 0, stream&gt;&gt;&gt;(dev_a, dev_b, dev_c);//声明核函数异步执行stram中的操作</span><br><span class="line">        //将数据从设备复制回锁定内存</span><br><span class="line">        cudaMemcpyAsync(host_c+i, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost, stream);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在使用流操作时，不需要将输入缓冲区整体都复制到GPU，而是将输入缓冲区划分为更小的块，并在每个块上执行一个包含三个步骤的过程：<strong>1.我们将部分输入缓冲区复制到GPU；2.在这部分缓冲区上运行核函数；3.然后将输出缓冲区中的这部分结果复制回主机</strong>。这么操作的应用场景通常是：GPU的内存远小于主机内存，无法一次将整个缓冲区都填充到GPU中。</p><h4 id="cudaHostAlloc"><a href="#cudaHostAlloc" class="headerlink" title="cudaHostAlloc()"></a>cudaHostAlloc()</h4><p>CUDA运行在主机上分配内存，这个内存是不可分页内存（malloc分配的是可分页内存）</p><h4 id="cudaMemcpyAsync"><a href="#cudaMemcpyAsync" class="headerlink" title="cudaMemcpyAsync()"></a>cudaMemcpyAsync()</h4><p>没有使用常见的’cudaMemcpy()’，而是使用一个新函数’cudaMemcpyAsync()’。前者是以同步方式执行，意味着 <strong>当函数返回时，复制操作就已经完成，并且在输出缓冲区中包含了复制进去的内容</strong>。异步函数相反，<strong>调用时只是放置一个请求，表示流中执行一次内存复制操作，这个流是通过参数stream来指定的，当函数返回时，我们无法确保复制操作是否已经启动，只能保证的是复制操作肯定会当下一个被放入流中的操作之前执行。流就像一个有序的GPU工作队列，GPU每次从该队列中取出工作并执行</strong></p><h2 id="使用多个CUDA流"><a href="#使用多个CUDA流" class="headerlink" title="使用多个CUDA流"></a>使用多个CUDA流</h2><p>在上个例子中，由于仅仅使用了一个流，并不会带来多大的性能提升，CUDA流只有在存在多个流操作时才会显示它的强大威力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;cuda_runtime.h&quot;    </span><br><span class="line">#include &lt;iostream&gt;  </span><br><span class="line">#include &lt;stdio.h&gt;    </span><br><span class="line">#include &lt;math.h&gt;    </span><br><span class="line"></span><br><span class="line">#define N (1024*1024)    </span><br><span class="line">#define FULL_DATA_SIZE N*20    </span><br><span class="line"></span><br><span class="line">__global__ void kernel(int* a, int *b, int*c)</span><br><span class="line">&#123;</span><br><span class="line">int threadID = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">if (threadID &lt; N)</span><br><span class="line">&#123;</span><br><span class="line">c[threadID] = (a[threadID] + b[threadID]) / 2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    //创建两个CUDA流  </span><br><span class="line">cudaStream_t stream[2];</span><br><span class="line">cudaStreamCreate(&amp;stream[0]);</span><br><span class="line">cudaStreamCreate(&amp;stream[1]);</span><br><span class="line"></span><br><span class="line">int *host_a, *host_b, *host_c;</span><br><span class="line">int *dev_a, *dev_b, *dev_c;</span><br><span class="line">int *dev_a1, *dev_b1, *dev_c1;</span><br><span class="line"></span><br><span class="line">//在GPU上分配内存  </span><br><span class="line">cudaMalloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">cudaMalloc((void**)&amp;dev_a1, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_b1, N * sizeof(int));</span><br><span class="line">cudaMalloc((void**)&amp;dev_c1, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">//在CPU上分配页锁定内存  </span><br><span class="line">cudaHostAlloc((void**)&amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line">cudaHostAlloc((void**)&amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line">cudaHostAlloc((void**)&amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">//主机上的内存赋值  </span><br><span class="line">for (int i = 0; i &lt; FULL_DATA_SIZE; i++)</span><br><span class="line">&#123;</span><br><span class="line">host_a[i] = i;</span><br><span class="line">host_b[i] = 2 * i + 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; FULL_DATA_SIZE; i += 2 * N)</span><br><span class="line">&#123;</span><br><span class="line">cudaMemcpyAsync(dev_a, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream[0]);</span><br><span class="line">cudaMemcpyAsync(dev_b, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream[0]);</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream[1]);</span><br><span class="line">cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream[1]);</span><br><span class="line"></span><br><span class="line">kernel &lt;&lt;&lt;N / 1024, 1024, 0, stream[0] &gt;&gt;&gt; (dev_a, dev_b, dev_c);</span><br><span class="line">kernel &lt;&lt;&lt;N / 1024, 1024, 0, stream[1] &gt;&gt;&gt; (dev_a, dev_b, dev_c1);</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(host_c + i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost, stream[0]);</span><br><span class="line">cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream[1]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 等待Stream流执行完成</span><br><span class="line">cudaStreamSynchronize(stream[0]);</span><br><span class="line">cudaStreamSynchronize(stream[1]);</span><br><span class="line"></span><br><span class="line">// free stream and mem    </span><br><span class="line">cudaFreeHost(host_a);</span><br><span class="line">cudaFreeHost(host_b);</span><br><span class="line">cudaFreeHost(host_c);</span><br><span class="line"></span><br><span class="line">cudaFree(dev_a);</span><br><span class="line">cudaFree(dev_b);</span><br><span class="line">cudaFree(dev_c);</span><br><span class="line"></span><br><span class="line">cudaFree(dev_a1);</span><br><span class="line">cudaFree(dev_b1);</span><br><span class="line">cudaFree(dev_c1);</span><br><span class="line"></span><br><span class="line">cudaStreamDestroy(stream);</span><br><span class="line">cudaStreamDestroy(stream1);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用两个流的执行时间一本上是一个流消耗时间的二分之一。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（五）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍一些补充内容。<br><a id="more"></a></p><h1 id="图形互操作性"><a href="#图形互操作性" class="headerlink" title="图形互操作性"></a>图形互操作性</h1><p>如今的GPU已经不仅仅局限于图形处理，更是广泛的应用于通用处理。本章介绍GPU的图形计算和通用计算之间的互操作。</p><h1 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h1><p>本章内容：</p><ol><li>了解NIVIDIA GPU的计算功能集</li><li>了解原子操作以及为什么需要使用它们</li><li>了解如何在CUDA C核函数中执行带有原子操作的运算</li></ol><p>NVIDIA将对于支持CUDA的不同GPU的各种功能统称为计算功能集，高版本计算功能集是低版本计算功能集的超集。</p><p>在NVIDIA的计算功能集中，只有版本高于1.1的功能集才支持全局内存原子操作，高于1.2的功能集才支持共享内存原子操作。所以在编译时我们需要告诉编译器该代码只有在指定版本的更高版本才可以编译优化，指令如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -arch=sm_11</span><br></pre></td></tr></table></figure></p><h2 id="GPU的工作调度机制"><a href="#GPU的工作调度机制" class="headerlink" title="GPU的工作调度机制"></a>GPU的工作调度机制</h2><h1 id="多个GPU系统上的CUDA"><a href="#多个GPU系统上的CUDA" class="headerlink" title="多个GPU系统上的CUDA"></a>多个GPU系统上的CUDA</h1><p>见书164页</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>## CUBLAS<br>著名的线性代数库</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（二）</title>
      <link href="/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，这是第二篇，主要讲的是线程块内共享内存你的概念，然后通过向量点乘的例子理解共享内存的使用方法。</p><a id="more"></a><h1 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h1><h2 id="共享内存和同步"><a href="#共享内存和同步" class="headerlink" title="共享内存和同步"></a>共享内存和同步</h2><p>在CUDA C用 <strong>shared</strong> 关键字添加变量声明，这将使这个变量驻留在共享内存中。主要作用是存放一个 <strong>线程块</strong> 中的线程会频繁访问的数据。对比全局内存来说，共享内存大小要小几个数量级，但访问速度会比全局内存快。<br>共享内存：对于GPU上启动的每个线程块，CUDA编译器都会创建该变量的一个副本，线程块中每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本，这使得线程块中的多个线程能够在计算上进行通信和协作，同时，共享内存缓冲区驻留在GPU上，所以访问共享内存的延迟要远远低于访问普通缓冲区的延迟。但与此同时，需要一种机制来实现线程之间的同步。</p><h4 id="向量内积"><a href="#向量内积" class="headerlink" title="向量内积"></a>向量内积</h4><p>通过共享内存来实现向量内积：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">const int N = 12 * 256;</span><br><span class="line">const int threadsPerBlock = 256;</span><br><span class="line">const int blocksPerGrid = (N+threadsPerBlock-1) / threadsPerBlock;</span><br><span class="line"></span><br><span class="line">__global__ void dot(float *a, float *b, float *c)&#123;</span><br><span class="line">    __shared__ float cache[threadsPerBlock]; //GPU共享内存</span><br><span class="line">    int tid = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    int cacheIndex = threadIdx.x;</span><br><span class="line">    float temp = 0;</span><br><span class="line">    while (tid &lt; N) &#123;</span><br><span class="line">        temp += a[tid] * b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">    cache[cacheIndex] = temp;</span><br><span class="line"></span><br><span class="line">    // 对所有线程块中的线程进行同步，</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    //归约运算</span><br><span class="line">    int i = blockDim.x/2;</span><br><span class="line">    while (i != 0)&#123;</span><br><span class="line">        if (cacheIndex &lt; i)</span><br><span class="line">            cache[cacheIndex] += cache[cacheIndex + i];</span><br><span class="line">        __syncthreads();</span><br><span class="line">        i /= 2;</span><br><span class="line">    &#125;</span><br><span class="line">    if (cacheIndex == 0)&#123;//为了防止带来不必要的冗余计算，只让cacheIndex==0线程进行最后的保存操作</span><br><span class="line">        c[blockIdx.x] = cache[0];</span><br><span class="line">        printf(&quot;%f\n&quot;, cache[0]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(void) &#123;</span><br><span class="line">    float a[N], b[N], c, partial_c[N];</span><br><span class="line">    float *dev_a, *dev_b, *dev_partial_c;</span><br><span class="line"></span><br><span class="line">    //在GPU上分配内存</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, N*sizeof(float));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_b, N*sizeof(float));</span><br><span class="line">    cudaMalloc((void**)&amp;dev_partial_c, blocksPerGrid*sizeof(float));</span><br><span class="line"></span><br><span class="line">    //填充主机内存</span><br><span class="line">    for (int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        a[i] = (float)i;</span><br><span class="line">        b[i] = (float)i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //将主机内存复制到GPU上</span><br><span class="line">    cudaMemcpy(dev_a, a, N*sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(dev_b, b, N*sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line">    dot&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(dev_a, dev_b, dev_partial_c);</span><br><span class="line">    cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid*sizeof(float), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    //在CPU上完成最终求和运算</span><br><span class="line">    c = 0;</span><br><span class="line">    for (int i = 0; i&lt;blocksPerGrid; i++)&#123;</span><br><span class="line">        c += partial_c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\n%f\n&quot;, c);</span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_partial_c);</span><br><span class="line"></span><br><span class="line">    delete [] a;</span><br><span class="line">    delete [] b;</span><br><span class="line">    delete [] partial_c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>整体思路：首先所有线程计算自己对应的乘法，然后将结果保存到block的共享内存中。然后每个block中指定一个thread将<br>该block内所有thread保存在共享内存的乘积结果归约加和到一起，得到该block中所有thread乘积的和。最后再将每个block的共享内存的求和结果最后求和，得到结果。</p><h4 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h4><p>‘ __syncthreads(); ‘<br>对所有线程块中的线程进行同步，<br>该函数确保线程块中的每个线程都执行完syncthreads前面的语句后，才会执行下一条语句。</p><h4 id="归约运算"><a href="#归约运算" class="headerlink" title="归约运算"></a>归约运算</h4><p>基本思想：每个线程将cache[]中的两个值加起来，然后将结果保存回cache[]，因为是将两个值归约成一个值，所以每次执行下来，得到的结果数量为开始时的一半。在log2(threadPerBlock)个步骤后，结果就是cache[]的总和。</p><h4 id="syncthreads-注意事项"><a href="#syncthreads-注意事项" class="headerlink" title="syncthreads()注意事项"></a>syncthreads()注意事项</h4><p>在核函数中，经常出现基于threadIdx的判断语句，说明有些thread是不能执行一部分if条件内的代码的， <strong>当某些线程需要执行一条指令，而其他线程不需要执行时，这种情况就被称为线程发散（Thread Divergence），正常情况下，发散的分支只会使得某些线程处于空闲状态，而其他线程将执行分支中的代码</strong>。 但在syncthreads()的情况中，线程发散造成的后果很糟糕，CUDA将确保： <strong>除非线程块中的每个线程都执行了syncthreads()，否则没有任何线程能执行syncthreads()后面的代码。也就是说，如果syncthreads()位于发散分支中，那么一些线程将永远无法执行syncthreads()，会导致程序出错。</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过向量点乘的例子，理解了共享内存的概念和简单应用方法</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Sklearn之TF-IDF</title>
      <link href="/2017/11/12/Machine%20Learning/TF-IDF%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/"/>
      <url>/2017/11/12/Machine%20Learning/TF-IDF%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>TF-IDF是一种统计方法， 广泛应用于信息检索领域。用于评估一个词对一个文件集或者一个文件的重要程度。<br><a id="more"></a></p><h2 id="TF-IDF概念"><a href="#TF-IDF概念" class="headerlink" title="TF-IDF概念"></a>TF-IDF概念</h2><p><strong>一个词在文件中的重要性随着它在文件中出现的次数成正比，但同时随着它在整个文件集中的出现频率成反比</strong>。 也就是说，一个词在文件中出现的越多，并且在整个其它文件中出现的次数越少，越能代表该文件。</p><h3 id="词频（TF）"><a href="#词频（TF）" class="headerlink" title="词频（TF）"></a>词频（TF）</h3><p>指的是一个词语在该文件中的出现次数，这个数字通常会被归一化（该词词频除以文章总词数），以防其偏向于长的文件。</p><h3 id="逆向文件频率（IDF）"><a href="#逆向文件频率（IDF）" class="headerlink" title="逆向文件频率（IDF）"></a>逆向文件频率（IDF）</h3><p>IDF指的是：对于一个词语，总文件数 除以 包含该词语的所有文件数，再将得到的商取对数，即为IDF。IDF的思想是：如果包含词语t的文档越少，IDF越大，说明该词条有很强的文件区分能力。<br>IDF = log(文件总数 / 包含词条的文件个数+1)<br>分母+1是为了防止除零</p><h3 id="TF-IDF计算公式"><a href="#TF-IDF计算公式" class="headerlink" title="TF-IDF计算公式"></a>TF-IDF计算公式</h3><p>TF-IDF = TF<em>IDF<br>表示一个词语在该文件中的出现次数 </em> IDF</p><h2 id="TF-IDF在Sklearn中"><a href="#TF-IDF在Sklearn中" class="headerlink" title="TF-IDF在Sklearn中"></a>TF-IDF在Sklearn中</h2><p>包含在’sklearn.feature_extraction.txt’中，共包含两个类：’CountVectorizer’和’TfidfTransformer’.</p><h3 id="CountVectorizer"><a href="#CountVectorizer" class="headerlink" title="CountVectorizer"></a>CountVectorizer</h3><p>可以将一个文件集合转换成 <strong>词频矩阵</strong>。例如矩阵中包含一个元素a[i][j]，它表示j词在i类文本下的词频。它通过’fit_transform()’函数计算各个词语出现的次数，通过’get_feature_names()’可获取词袋中所有文本的关键字，通过’toarray()’可看到词频矩阵的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">#语料</span><br><span class="line">corpus = [</span><br><span class="line">    &apos;This is the first document.&apos;,</span><br><span class="line">    &apos;This is the second second document.&apos;,</span><br><span class="line">    &apos;And the third one.&apos;,</span><br><span class="line">    &apos;Is this the first document?&apos;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line">#获取词袋中所有文本关键词</span><br><span class="line">word = vectorizer.get_feature_names()</span><br><span class="line">print(word)</span><br><span class="line">#查看词频结果</span><br><span class="line">print(X.toarray())</span><br></pre></td></tr></table></figure></p><p>输出结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[u&apos;and&apos;, u&apos;document&apos;, u&apos;first&apos;, u&apos;is&apos;, u&apos;one&apos;, u&apos;second&apos;, u&apos;the&apos;, u&apos;third&apos;, u&apos;this&apos;]</span><br><span class="line">[[0 1 1 1 0 0 1 0 1]</span><br><span class="line"> [0 1 0 1 0 2 1 0 1]</span><br><span class="line"> [1 0 0 0 1 0 1 1 0]</span><br><span class="line"> [0 1 1 1 0 0 1 0 1]]</span><br></pre></td></tr></table></figure></p><h3 id="TfidfTransformaer"><a href="#TfidfTransformaer" class="headerlink" title="TfidfTransformaer"></a>TfidfTransformaer</h3><p>用于统计每个词语的TF-IDF值，tfidf[i][j]表示i类文本中第j个词的tf-idf权重。仍然使用上面这个例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">#将词频矩阵X统计成TF-IDF值</span><br><span class="line">tfidf = transformer.fit_transform(X)</span><br><span class="line">#查看数据结构</span><br><span class="line">print(tfidf.toarray())</span><br></pre></td></tr></table></figure></p><h3 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h3><p>通常情况下，对于给定的corpus，可以同时使用CountVectorizer和TfidfTransformer()。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector = CountVectorizer()</span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">tf_idf = transformer.fit_transform(vector.fit_transform(corpus));</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA学习笔记(一)</title>
      <link href="/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
      <url>/2017/11/12/CUDA/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。</p><a id="more"></a><h2 id="第一个CUDA程序"><a href="#第一个CUDA程序" class="headerlink" title="第一个CUDA程序"></a>第一个CUDA程序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">__global__ void kernel(void) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</span><br><span class="line">    printf(&quot;Hello, Word\n&quot;);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与一般c的hello word不同的是，该代码有两个不同的地方：</p><ol><li>一个空函数kernel(),并且带有修饰符<strong>global</strong></li><li>对这个空函数的调用，并带有修饰字符&lt;&lt;&lt;1, 1&gt;&gt;&gt;</li></ol><p>__global__修饰符告诉编译器，该函数应该编译为在设备而不是主机上运行。</p><p>&lt;&lt;&lt;1, 1&gt;&gt;&gt;表示调用设备代码，尖括号表示将一些参数传递给运行时系统，这些参数并不是传递给设备代码的参数，而是告诉运行时如何启动设备代码。</p><h2 id="CUDA加法"><a href="#CUDA加法" class="headerlink" title="CUDA加法"></a>CUDA加法</h2><p>下面我们来看一个更复杂的例子，用CUDA实现GPU加法计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__global__ void add(int a, int b, int *c)&#123;</span><br><span class="line">    *c = a + b;</span><br><span class="line">&#125;</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    int c;</span><br><span class="line">    int *dev_c;</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((void**)&amp;dev_c, sizeof(int)));</span><br><span class="line">    add&lt;&lt;&lt;1, 1&gt;&gt;&gt;(2, 7, dev_c);</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost));</span><br><span class="line">    printf(&quot;2 + 7 = %d\n&quot;, c);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这段代码有两个核心概念：</p><ol><li>可以像调用c函数一样将参数传递给核函数</li><li>当设备执行任何有用的操作时，都需要分配内存，例如将计算值返回给主机。</li></ol><h3 id="CUDA常用函数"><a href="#CUDA常用函数" class="headerlink" title="CUDA常用函数"></a>CUDA常用函数</h3><h4 id="cudaMalloc-函数"><a href="#cudaMalloc-函数" class="headerlink" title="cudaMalloc()函数"></a>cudaMalloc()函数</h4><p>设备函数调用的传参过程和普通c函数类似，需要注意的是通过’cudaMalloc()’来分配内存，<br>‘cudaMalloc()’函数告诉CUDA运行时在设备上分配内存，</p><pre><code>1. 第一个参数是一个指针，指向用于保存新分配内存地址的变量，2. 第二个参数是分配内存的大小。3. 该函数的返回类型和malloc()相同，都是&apos;void*&apos;</code></pre><p>CUDA大大淡化了主机代码和设备代码之间的差异，但程序员一定不能在主机代码中对’cudaMalloc()’返回的指针进行读取或者写入内存。总的来说，主机指针只能访问主机代码中的内存，而设备指针只能访问设备代码中的内存。</p><h4 id="cudaMemcpy-函数"><a href="#cudaMemcpy-函数" class="headerlink" title="cudaMemcpy()函数"></a>cudaMemcpy()函数</h4><p>‘cudaMemcpy()’表示对设备指针的内容和主机指针的内容复制</p><pre><code>1. 第一个参数是主机指针2. 第二个参数是设备指针3. 第三个参数是内存大小4. 第四个参数&apos;cudaMemcpyDeviceToHost&apos;表示从设备复制到主机，同样的&apos;cudaMemcpyHostToDevice&apos;表示主机复制到设备。&apos;cudaMemcpyDeviceToDevice&apos;表示两个指针都位于设备上。</code></pre><h2 id="查询设备"><a href="#查询设备" class="headerlink" title="查询设备"></a>查询设备</h2><p>在编写设备代码并在GPU上进行计算时，如果程序能知道设备中拥有多少内存，都有哪些设备等信息无疑是有用的。</p><ol><li>获得支持CUDA的GPU数量：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int count;</span><br><span class="line">cudaGetDeviceCount(&amp;count);</span><br></pre></td></tr></table></figure></li></ol><p>在获取CUDA设备数量后，可以使用’cudaGetDeviceProperties()’查询每个设备的相关信息，CUDA运行时将返回一个’cudaDeviceProp’类型的结构，包含了设备相关属性。结构内的详细信息请Google。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">intmain(void) &#123;</span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    int count;</span><br><span class="line">    cudaGetDeviceCount(count);</span><br><span class="line">    for(int i = 0;i&lt;count; i++)&#123;</span><br><span class="line">        cudaGetDeviceProperties(&amp;prop, i);</span><br><span class="line">        printf(&quot;Name: %s\n&quot;, prop.name);</span><br><span class="line">        printf(&quot;Total global memory: %ld\n&quot;, prop.totalGlobalMem);</span><br><span class="line">        printf(&quot;Total constant memory: %ld\n&quot;, prop.totalConstMem);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在编写代码中，偶尔会需要指定CUDA设备，比如说主机和设备之间通信频繁的话，最好选用集成GPU。我们可以通过’cudaChooseDevice()’选择设备。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">int main(void) &#123;</span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    int dev;</span><br><span class="line">    cudaGetDevice(&amp;dev);</span><br><span class="line">    printf(&quot;ID of current CUDA device: %d\n&quot;, dev);</span><br><span class="line">    memset(&amp;prop, 0, sizeof(cudaDeviceProp));</span><br><span class="line">    prop.major = 1;</span><br><span class="line">    prop.minor = 3;</span><br><span class="line">    cudaChooseDevice(&amp;dev, &amp;prop);</span><br><span class="line">    printf(&quot;ID of CUDA device choset to revision 1.3: %d\n&quot;, dev);</span><br><span class="line">    cudaSetDevice(dev);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="向量加法"><a href="#向量加法" class="headerlink" title="向量加法"></a>向量加法</h2><p>下面我们尝试使用并行的思想，用CUDA实现向量加法：输入两个vector，将其对应位相加的结果保存在第三个vector中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#define N 10</span><br><span class="line">int main(void) &#123;</span><br><span class="line">    int a[N], b[N], c[N];</span><br><span class="line">    int *dev_a, *dev_b, *dev_c;</span><br><span class="line"></span><br><span class="line">    // 在GPU上分配内存</span><br><span class="line">    cudaMelloc((void**)&amp;dev_a, N * sizeof(int));</span><br><span class="line">    cudaMelloc((void**)&amp;dev_b, N * sizeof(int));</span><br><span class="line">    cudaMelloc((void**)&amp;dev_c, N * sizeof(int));</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i&lt;N; i++)&#123;//在CPU上初始化三个数组</span><br><span class="line">        a[i] = -i;</span><br><span class="line">        b[i] = i * i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //将a，b两个数组复制到GPU内存上</span><br><span class="line">    cudaMemcpy(dev_a, a, N*sizeof(int), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    add&lt;&lt;&lt;N, 1&gt;&gt;&gt;(dev_a, dev_b, dev_c);</span><br><span class="line"></span><br><span class="line">    //将GPU上的计算结果返回到主机中</span><br><span class="line">    cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost)</span><br><span class="line"></span><br><span class="line">    for(int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        printf(&quot;%d + %d = %d\n&quot;, a[i], b[i], c[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void add(int *a, int *b, int *c)&#123;</span><br><span class="line">    int tid = blockIdx.x;</span><br><span class="line">    if (tid &lt; N)</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里我们要说明的是，在调用设备函数add&lt;&lt;&lt;N, M&gt;&gt;&gt;时，两个参数的意义：</p><pre><code>1. 第一个参数表示运行时创建N个设备函数的副本并以并行的方式运行他们，我们将每个并行执行环境都称为一个线程块(block)。如果指定kernel&lt;&lt;&lt;256,1&gt;&gt;&gt;,那么将有256个线程块在GPU上运行。2. 第二个参数表示在每个线程块中创建的线程数量M。3. 在GPU中，一共有N*M个并行线程运行</code></pre><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><h4 id="线程索引"><a href="#线程索引" class="headerlink" title="线程索引"></a>线程索引</h4><p>但在运行核函数的时候，我们往往需要知道当前的核函数是哪个block，所以在核函数中，我们看到’int tid = blockIdx.x’，blockIdx是一个内置变量，在CUDA运行时已经预先定义了这个变量。而后面的’.x’是因为CUDA支持二维线性块数组，所以有了’.x’, ‘.y’。<br>在启动核函数时，我们将并行线程块的数量指定为N，这个并行线程块集合也称为一个线程格(Grid)，这个告诉运行时，我们想要一个一维的线程格，其中包括N个线程块。每个线程块的blockIdx.x值都是不同的，第一个线程块的blockIdx.x为0，最后一个为N-1。</p><h4 id="线程索引优化"><a href="#线程索引优化" class="headerlink" title="线程索引优化"></a>线程索引优化</h4><p>在上一个矢量加法中，我们调用核函数’add&lt;&lt;&lt;N, 1&gt;&gt;&gt;’，表示启动N个核函数，每个核函数中有1个线程。在本章我们稍作修改，改为启动一个线程块，该线程块包含N个线程：’add&lt;&lt;&lt;1, N&gt;&gt;&gt;’，对应的，在核函数中，取线程块索引改为取线程索引：’int tid = threadIdx.x + blockIdx.x * blockDim.x’。新出现的变量blockDim保存的是线程块中每一维的线程数量，由于使用的是一维线程块，所以只用到blockDim.x。</p><p>两种方法都可以在GPU上启动多个线程，对比这两种方法，GPU可以一次启动65535个线程块，而一般而言，每个线程块内的最多启动512个线程，这是第一个不同。</p><p>而如果我们需要实现任意长度vector加法，以上的数量限制显然是不能实现的，这里我们只需要修改一下核函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__global__ add(int *a, int *b, int *c) &#123;</span><br><span class="line">    int tid = threadIdx.x + blockIdx.x blockDim.x;</span><br><span class="line">    while (tid &lt; N) &#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += gridDim.x * blockDim.x; // 每次位移一个线程grid内的所有线程数量</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>通过在循环内’tid += gridDim.x * blockDim.x’，每次可以让tid移动一个线程grid内线程数量的位移，然后再判断tid是否小于N。通过这种方法实现了计算任意长度的向量加法。</p><h2 id="核函数GPU参数"><a href="#核函数GPU参数" class="headerlink" title="核函数GPU参数"></a>核函数GPU参数</h2><p>核函数只能在主机端调用，调用时必须申明执行参数。调用形式如下：</p><pre><code>Kernel&lt;&lt;&lt;    &gt;&gt;&gt;(param list);</code></pre><p>&lt;&lt;&lt;&gt;&gt;&gt;运算符内是核函数的执行参数，告诉编译器运行时如何启动核函数，用于说明内核函数中的线程数量，以及线程是如何组织的。</p><p>&lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt;运算符对kernel函数完整的执行配置参数形式是</p><ul><li>参数Dg用于定义整个grid的维度和尺寸，即一个grid有多少个block。为dim3类型。Dim3 Dg(Dg.x, Dg.y, 1)表示grid中每行有Dg.x个block，每列有Dg.y个block，第三维恒为1(目前一个核函数只有一个grid)。整个grid中共有Dg.x*Dg.y个block，其中Dg.x和Dg.y最大值为65535。</li><li>参数Db用于定义一个block的维度和尺寸，即一个block有多少个thread。为dim3类型。Dim3 Db(Db.x, Db.y, Db.z)表示整个block中每行有Db.x个thread，每列有Db.y个thread，高度为Db.z。Db.x和Db.y最大值为512，Db.z最大值为62。 一个block中共有Db.x<em>Db.y</em>Db.z个thread。计算能力为1.0,1.1的硬件该乘积的最大值为768，计算能力为1.2,1.3的硬件支持的最大值为1024。</li><li>参数Ns是一个可选参数，用于设置每个block除了静态分配的shared Memory以外，最多能动态分配的shared memory大小，单位为byte。不需要动态分配时该值为0或省略不写。</li><li>参数S是一个cudaStream_t类型的可选参数，初始值为零，表示该核函数处在哪个流之中。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>到此，我们已经学习了如何声明GPU函数，开辟GPU内存以及GPU和CPU内存之间的复制。通过两个例子也大概理解了CUDA的运行过程和工作机制，最后，我们通过优化tid的更新方式实现了任意长度向量的加法。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CUDA 学习笔记（三）</title>
      <link href="/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2017/11/12/CUDA/CUDA%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我学习CUDA整理的笔记，从入门的第一个程序开始，主要参考教材是《CUDA by Example》。本节主要介绍了常量内存和纹理内存。<br><a id="more"></a></p><h1 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h1><p>本章介绍通过GPU上特殊的内存区域来加速应用程序的执行：常量内存。以及如果通过事件来测量CUDA应用程序的性能。</p><h2 id="使用常量内存"><a href="#使用常量内存" class="headerlink" title="使用常量内存"></a>使用常量内存</h2><p>从名字可以看出，常量内存用于保存在核函数执行期间不会发生变化的数据，在某些情况中，用常量内存替换全局内存能有效的减少内存带宽。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#define N 40</span><br><span class="line">using namespace std;</span><br><span class="line">const int nMax = 50;</span><br><span class="line"></span><br><span class="line">__constant__ float const_num[N];</span><br><span class="line">__global__ void exchangeKernel(float *nums)&#123;</span><br><span class="line">    int offset = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    nums[offset] = const_num[offset];</span><br><span class="line">&#125;</span><br><span class="line">int main()&#123;</span><br><span class="line">    float *dev_a, temp[N], res[N];</span><br><span class="line">    cudaMalloc((void**)&amp;dev_a, 40*sizeof(float));</span><br><span class="line">    for(int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">        temp[i] = 1.5 * i;</span><br><span class="line">    &#125;</span><br><span class="line">    cudaMemcpyToSymbol(const_num, temp, N*sizeof(float));</span><br><span class="line">    exchangeKernel&lt;&lt;&lt;4, N/4&gt;&gt;&gt;(dev_a);</span><br><span class="line">    cudaMemcpy(res, dev_a, N*sizeof(float), cudaMemcpyDeviceToHost);</span><br><span class="line">    for(int i = 0; i&lt;N; i++)&#123;</span><br><span class="line">        cout &lt;&lt; res[i] &lt;&lt; &quot; &quot;&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在这个例子中，我们先声明了N个float大小的’<strong>constant</strong>‘空间。在主机上将主机内存中的数值通过’cudaMemcpyToSymbol’复制到device上的常量内存上。然后调用核函数每次获取常量内存上的一个值。最后将核函数的运行结果复制回主机内存。</p><h4 id="cudaMemcpyToSymbol"><a href="#cudaMemcpyToSymbol" class="headerlink" title="cudaMemcpyToSymbol()"></a>cudaMemcpyToSymbol()</h4><p>使用’cudaMemcpyToSymbol(dev_const, host_const, size)’从主机内存复制到GPU的常量内存中。</p><h4 id="常量内存提高带宽"><a href="#常量内存提高带宽" class="headerlink" title="常量内存提高带宽"></a>常量内存提高带宽</h4><p>使用常量内存可以节省内存带宽，主要原因有两个：</p><pre><code>1. 对于常量内存的单次读操作可以广播到其他的“邻近”线程，这将节约15次读取操作。2. 常量内存的数据将被缓存起来，因此对相同地址的连续操作将不会产生额外的内存通信量。</code></pre><p>为了回答“临近线程”的概念，我们需要先知道“线程束(Warp)”是什么。<strong>线程束可以看成是一组线程通过交织而形成的一个整体，在CUDA架构中，线程束是指一个包含32个线程的集合，这个线程集合被“编织在一起”并且以“步调一致”的形式执行。</strong> 在程序中的每一行，线程束中的每个线程都将在不同的数据上执行相同的指令。</p><p>当处理常量内存时， NVIDIA将把单词内存读取操作广播到每个半线程束（half-warp）包含了16个线程，如果半个线程束中的每个线程都从常量内存上读取数据，那么GPU只会产生一次读取请求然后将数据广播到每个线程。如果从常量内存中读取大量的数据，那么这种方式产生的内存流量只是使用全局内存的1/16。同时在实际应用中，不仅仅减少15/16，由于这块内存时不会发生变化的，所以硬件会主动把常量数据缓存在GPU缓存中，在第一次从常量内存读取后，当其他半线程束请求同一个地址时，将直接命中缓存，更加减少了内存流量。</p><p>需要注意的是： 半线程束广播实际上是一把双刃剑，</p><pre><code>- 当16个线程都读取相同地址时，性能确实可以大大提高，但当所有16个线程分别读取不同地址时，它的效率实际上会降低- 只有当16个线程是相同读取请求时，才值得将这个读取操作广播到16个线程。然而，如果16个线程需要访问的常量内存中的不同数据，那么这16次不同的读取操作会被 **串行化**， 从而需要16倍的时间来发出请求，但如果从全局内存中读取，这些请求会同时发出，这种情况下，从常量内存读取就会慢于从全局内存读取。</code></pre><h1 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h1><p>和常量内存一样，纹理内存是另一种 <strong>只读内存</strong>，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。<br>纹理内存是专门为那些在内存访问模式中存在大量空间局部性的图形应用程序设计的，在某个计算程序中，这意味着 <strong>一个线程读取的位置可能与邻近线程读取的位置“非常接近”</strong>。</p><h2 id="使用纹理内存"><a href="#使用纹理内存" class="headerlink" title="使用纹理内存"></a>使用纹理内存</h2><p>定义纹理变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;float&gt; textureA;</span><br><span class="line">texture&lt;float&gt; textureB;</span><br><span class="line"></span><br><span class="line">cudaMalloc((void**)&amp;deviceA, sizeof(float));</span><br><span class="line">cudaBindTexture(NULL, textureA, deviceA, sizeof(float));</span><br><span class="line"></span><br><span class="line">cudaUnbindTexture(textureA); //释放纹理绑定</span><br><span class="line">cudaUnbindTexture(textureB);</span><br></pre></td></tr></table></figure></p><h4 id="cudaBindTexture"><a href="#cudaBindTexture" class="headerlink" title="cudaBindTexture()"></a>cudaBindTexture()</h4><p>定义之后，我们需要为纹理变量分配内存，使用’cudaBindTexture()’将这些变量绑定到内存缓冲区，相当于告诉CUDA两件事：</p><ol><li>我们希望将指定的缓冲区作为纹理来使用。</li><li>我们希望将纹理的引用作为纹理的“名字”。</li></ol><h2 id="使用二维纹理内存"><a href="#使用二维纹理内存" class="headerlink" title="使用二维纹理内存"></a>使用二维纹理内存</h2><p>声明二维纹理内存方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;float, 2&gt; textureA;</span><br><span class="line">texture&lt;float, 2&gt; textureB;</span><br></pre></td></tr></table></figure></p><p>纹理内存的使用是常见的GPU优化手段之一，多用于图形计算领域。但由于纹理内存的特殊性，在使用纹理内存时要慎重，使用不当往往会适得其反。</p><h1 id="用事件来测试性能"><a href="#用事件来测试性能" class="headerlink" title="用事件来测试性能"></a>用事件来测试性能</h1><p>CUDA事件本质上是一个GPU时间戳，使用起来很容易，只需要两步：首先创建一个事件，然后记录一个事件。当然，最后也需要另一个事件记录结束时间。<br>下面是使用事件来计算GPU计算时间的常用流程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, end;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventCreate(&amp;end);</span><br><span class="line">cudaEventRecord(start, 0);</span><br><span class="line"></span><br><span class="line">// do something in GPU</span><br><span class="line"></span><br><span class="line">cudaEventRecord(end, 0);</span><br><span class="line">cudaEventSynchronize(stop); //同步事件，告诉运行时阻塞后面的语句，直到GPU执行到达stop事件。</span><br><span class="line"></span><br><span class="line">float elapsedTime;</span><br><span class="line">cudaEventElapsedTime(&amp;elapsedTime, start, end);//计算用时</span><br><span class="line">cudaEventDestroy(start);</span><br><span class="line">cudaEventDestroy(end);</span><br></pre></td></tr></table></figure></p><p>但在CUDA程序中，实际上当GPU开始执行代码，在GPU执行完之前，CPU会继续执行程序的下一行代码。这样做可以提高性能，让GPU和CPU并行，但从逻辑上来说，计时工作就会变得复杂。CUDA有’cudaEventSynchronize()’函数来告诉CPU在某个事件上同步。</p><p>‘cudaEventElapsedTime(float*, cudaEvent_t, cudaEvent_t)’用于计算时间时间，返回单位为毫秒。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节主要介绍了CUDA的两种 <strong>只读内存：常量内存和纹理内存</strong>，使用GPU只读内存可以对CUDA程序的运行速度进一步优化，但使用时需要精心设计避免适得其反。同时也介绍了CUDA事件，事件的本质是事件戳，用来记录GPU运行事件，计算性能。</p>]]></content>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>生成模型和判别模型</title>
      <link href="/2017/10/12/Machine%20Learning/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2017/10/12/Machine%20Learning/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>监督学习的任务就是从数据中学习一个模型，应用这一模型，对给定的输入X预测相应的输出Y。这个模型的一般形式为决策函数Y=f(X)或者条件概率分布P(Y|X)。而生成模型和判别模型的区别就是如何求得P(Y|X).<br><a id="more"></a></p><p>决策函数Y=f(X)：你输入一个X，它就输出一个Y，这个Y与一个阈值比较，根据比较结果判定X属于哪个类别。例如两类（w1和w2）分类问题，如果Y大于阈值，X就属于类w1，如果小于阈值就属于类w2。这样就得到了该X对应的类别了。</p><p>条件概率分布P(Y|X)：你输入一个X，它通过比较它属于所有类的概率，然后输出概率最大的那个作为该X对应的类别。例如：如果P(w1|X)大于P(w2|X)，那么我们就认为X是属于w1类的。</p><h2 id="生成方法和判别方法"><a href="#生成方法和判别方法" class="headerlink" title="生成方法和判别方法"></a>生成方法和判别方法</h2><p>监督学习方法分生成方法（Generative approach）和判别方法（Discriminative approach），所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。</p><h4 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h4><p>由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>基本思想是有限样本条件下建立判别函数，不关心样本是以何种概率分布产生的，直接研究预测模型。典型的判别模型包括： <strong>k近邻，感知机，决策树，支持向量机等。</strong></p><h6 id="判别方法的特点"><a href="#判别方法的特点" class="headerlink" title="判别方法的特点"></a>判别方法的特点</h6><ul><li>判别方法寻找不同类别之间的最优分类面，反映的是异类数据之间的差异;</li><li>判别方法利用了训练数据的类别标识信息，直接学习的是条件概率P(Y|X)或者决策函数f(X)，直接面对预测，往往学习的准确率更高；</li><li>由于直接学习条件概率P(Y|X)或者决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li><li>缺点是不能反映训练数据本身的特性</li></ul><h4 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h4><p>由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。<br>这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。典型的生成模型有：<strong>朴素贝叶斯和隐马尔科夫模型等。</strong> 这种方法一般建立在统计学和Bayes理论的基础之上。</p><h6 id="生成方法的特点"><a href="#生成方法的特点" class="headerlink" title="生成方法的特点"></a>生成方法的特点</h6><ul><li>从统计的角度表示数据的分布情况，能够反映同类数据本身的分布情况;</li><li>生成方法还原出联合概率分布P(X, y)，而判别方法不能；</li><li>生成方法的学习收敛速度更快、即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型（由于已经学习到了概率分布）；</li><li>当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用</li></ul><h2 id="判别模型和生成模型对比"><a href="#判别模型和生成模型对比" class="headerlink" title="判别模型和生成模型对比"></a>判别模型和生成模型对比</h2><p>（1）训练时，二者优化准则不同<br>生成模型优化训练数据的联合分布概率；<br>判别模型优化训练数据的条件分布概率，判别模型与序列标记问题有较好的对应性。<br>（2）对于观察序列的处理不同<br>生成模型中，观察序列作为模型的一部分；<br>判别模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。<br>（3）训练复杂度不同<br>判别模型训练复杂度较高。<br>（4）本质区别<br>判别模型估计的是条件概率分布P(Y|X), 生成模型估计的是联合概率分布P(X, Y)<br>（5）由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>例如我们有一个输入数据x，然后我们想将它分类为标签y。（迎面走过来一个人，你告诉我这个是男的还是女的）</p><p>生成模型学习联合概率分布p(x,y)，而判别模型学习条件概率分布p(y|x)。</p><p>下面是个简单的例子：</p><p>例如我们有以下(x,y)形式的数据：(1,0), (1,0), (2,0), (2, 1)</p><p>那么p(x,y)是：</p><pre><code>      y=0   y=1    -----------x=1 | 1/2   0x=2 | 1/4   1/4</code></pre><p>而p(y|x) 是：</p><pre><code>    y=0   y=1    -----------x=1| 1     0x=2| 1/2   1/2</code></pre>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java字符串</title>
      <link href="/2017/09/12/Programming%20Language/Java/Java%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2017/09/12/Programming%20Language/Java/Java%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      <content type="html"><![CDATA[<p>Java String类和StringBuffer类的常见函数<br><a id="more"></a></p><h3 id="Java-String类"><a href="#Java-String类" class="headerlink" title="Java String类"></a>Java String类</h3><h4 id="创建字符串"><a href="#创建字符串" class="headerlink" title="创建字符串"></a>创建字符串</h4><p>Java String 共有11种构造函数，包括传入一个 char 数组进行构造。</p><p>注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了（详看笔记部分解析）。</p><p>如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。</p><h4 id="split"><a href="#split" class="headerlink" title="split()"></a>split()</h4><p>split()函数是根据参数如”,”, “-“, “ “等, 分割String字符串, 返回一个String的数组。</p><p>如果未找到, 则返回整个String字符串, 作为String数组(String[])的第0个元素.</p><h4 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h4><ul><li>string1.concat(string2);</li><li>该函数将返回两个字符串连接在一起的新串</li><li>直接使用 ‘+’ 进行字符串连接</li><li>char和string可以直接相加</li></ul><h4 id="string-contains"><a href="#string-contains" class="headerlink" title="string.contains()"></a>string.contains()</h4><ul><li>string.contains(substring) 返回布尔，字符串是否包含该子字符串</li></ul><h4 id="字符串比较字典序-string1-compareTo-string2"><a href="#字符串比较字典序-string1-compareTo-string2" class="headerlink" title="字符串比较字典序 string1.compareTo(string2)"></a>字符串比较字典序 string1.compareTo(string2)</h4><p>比较两个字符串的字典序，如果string1&lt;string2则返回-1， 如果相等则返回0， 如果string1&gt;string2则返回1.</p><h4 id="数字字符串转int"><a href="#数字字符串转int" class="headerlink" title="数字字符串转int"></a>数字字符串转int</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String s = &quot;123&quot;;</span><br><span class="line">int a = Integer.valueOf(s);</span><br><span class="line">// 或者</span><br><span class="line">int b = Integer.parseInt(s);</span><br></pre></td></tr></table></figure><h4 id="int转字符串："><a href="#int转字符串：" class="headerlink" title="int转字符串："></a>int转字符串：</h4><ul><li>String s = String.valueOf(intv);</li><li>String s = Integer.toString(intv);</li><li>String s = “” + intv;</li></ul><h4 id="字符串反转"><a href="#字符串反转" class="headerlink" title="字符串反转"></a>字符串反转</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String reverse = new StringBuffer(s).reverse().toString();</span><br></pre></td></tr></table></figure><h4 id="charAt"><a href="#charAt" class="headerlink" title="charAt()"></a>charAt()</h4><p>string.charAt(index) 返回index处的char值</p><h4 id="string-equal"><a href="#string-equal" class="headerlink" title="string.equal()"></a>string.equal()</h4><p>string1.equal(string2) 判断两个字符串是否相等</p><h4 id="string-getChars"><a href="#string-getChars" class="headerlink" title="string.getChars()"></a>string.getChars()</h4><p>string.getChars(int srcBegin, int srcEnd, char[] dst,  int dstBegin)</p><ul><li>复制字符串的一个子串到一个char数组中</li><li>参数分别为：起始index，结束index，输出到的char数组，和目标数组中的起始偏移量(置零）。</li></ul><h4 id="string-index"><a href="#string-index" class="headerlink" title="string.index()"></a>string.index()</h4><ul><li>string.index(char c)</li><li>string.index(String substring)</li><li>返回参数（char或者字符串）出现的index</li></ul><h4 id="string-replace"><a href="#string-replace" class="headerlink" title="string.replace()"></a>string.replace()</h4><ul><li>public String replace(char oldChar,<pre><code>char newChar)</code></pre></li><li>在字符串中使用new char 替换 old char</li><li>删除字符串中指定的字符，用空白替换方法：<br>string = string.replace(“delete”, “”)</li></ul><h4 id="string-trim"><a href="#string-trim" class="headerlink" title="string.trim()"></a>string.trim()</h4><ul><li>public String trim()</li><li>用于删除字符串的头尾空白符。</li></ul><h4 id="大小写转换"><a href="#大小写转换" class="headerlink" title="大小写转换"></a>大小写转换</h4><ul><li>string.toUpperCase()</li><li>string.toLowerCase()</li></ul><h4 id="string-toCharArray"><a href="#string-toCharArray" class="headerlink" title="string.toCharArray()"></a>string.toCharArray()</h4><ul><li>将字符串转换为字符数组</li></ul><h4 id="string-substring"><a href="#string-substring" class="headerlink" title="string.substring()"></a>string.substring()</h4><ul><li>返回字符串的子字符串。</li><li>public String substring(int beginIndex, int endIndex)</li><li>public String substring(int beginIndex)</li></ul><h4 id="string-indexOf-substring-m"><a href="#string-indexOf-substring-m" class="headerlink" title="string.indexOf(substring, m)"></a>string.indexOf(substring, m)</h4><p>返回substring 在string中的起始index，如果string不包含则返回1。第二个参数m表示string从第m个元素开始判断，缺失时默认从0开始判断。</p><h4 id="string-lastIndexOf-substring"><a href="#string-lastIndexOf-substring" class="headerlink" title="string.lastIndexOf(substring)"></a>string.lastIndexOf(substring)</h4><p>从后往前判断substring在string中的位置。</p><h3 id="Java-StringBuffer类"><a href="#Java-StringBuffer类" class="headerlink" title="Java StringBuffer类"></a>Java StringBuffer类</h3><p>StringBuffer是线程安全的，所以在多线程程序中也可以很方便的进行使用，但是程序的执行效率相对来说就要稍微慢一些。</p><h4 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StringBuffer sb = new StringBuffer(&quot;abc&quot;)</span><br></pre></td></tr></table></figure><h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><h4 id="sb-append-“string1”"><a href="#sb-append-“string1”" class="headerlink" title="sb.append(“string1”)"></a>sb.append(“string1”)</h4><p>该方法的作用是追加内容到当前StringBuffer对象的末尾，类似于字符串的连接。</p><h4 id="sb-deleteCharAt-index"><a href="#sb-deleteCharAt-index" class="headerlink" title="sb.deleteCharAt(index)"></a>sb.deleteCharAt(index)</h4><p>该方法的作用是删除指定位置的字符，然后将剩余的内容形成新的字符串。</p><h4 id="sb-delete-start-end"><a href="#sb-delete-start-end" class="headerlink" title="sb.delete(start, end)"></a>sb.delete(start, end)</h4><p>该方法的作用是删除指定区间以内的所有字符，包含start，不包含end索引值的区间。</p><h4 id="sb-insert-index-string1"><a href="#sb-insert-index-string1" class="headerlink" title="sb.insert(index, string1)"></a>sb.insert(index, string1)</h4><p>该方法的作用是在StringBuffer对象中插入内容，然后形成新的字符串。</p><h4 id="sb-reverse"><a href="#sb-reverse" class="headerlink" title="sb.reverse()"></a>sb.reverse()</h4><p>该方法的作用是将StringBuffer对象中的内容反转，然后形成新的字符串。</p><h4 id="sb-setCharAt-index-char1"><a href="#sb-setCharAt-index-char1" class="headerlink" title="sb.setCharAt(index, char1)"></a>sb.setCharAt(index, char1)</h4><p>该方法的作用是修改对象中索引值为index位置的字符为新的字符ch。</p>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming Language </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
